<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[博客迁移公告]]></title>
    <url>%2F2025%2F07%2F20%2F%E8%BF%81%E7%A7%BB%E5%85%AC%E5%91%8A%2F</url>
    <content type="text"><![CDATA[博客新地址：https://moe.sakanoy.com 十年前接触到了 Github，当时主要就是为了搭建自己的免费博客，以记录学习过程中的心得，当时最火的方案毫无疑问是 Hexo + NexT 主题，上学期间对什么对感兴趣，也有时间去研究，写了不少乱七八糟的东西，虽不说有多少精品，数量上几百篇应该是够的。 虽然最近几年写的东西越来越少，时间精力确实大不如之前，不是没东西可写而是写一篇长文太耗费时间精力了，一些几句话能说明白的也不屑于形成一篇博客，都放在我的 BBS/Memo 里记录存档了。 还有一个关键是文章多了之后 Hexo 编译一次太慢了，NexT 的原作者早就退坑，现在看旧博客有种年久失修的苍老感。 搭建新博客这个想法已经很久很久了，一直在拖延，这两天有时间搞了一下，其实一旦开了头进入状态了还是很不错的一件事。 新博客使用的是 Hugo，基于 Go 的一个静态网站工具，彻底解决 Hexo 编译慢的问题，当然它的可玩性和资源可能不如 Hexo，毕竟我也过了折腾的年纪。 昨天把旧博客的一些配置迁移到了新项目，目前也算是比较满意了，文章的话现在还不打算迁移过来，等我哪天有空挑一下把有用的搞过来，剩下的就保持原样吧。]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一台小主机的 AIO 之旅]]></title>
    <url>%2F2024%2F11%2F14%2FAIO%E6%8A%98%E8%85%BE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[All in boom 来啦，PVE + OpenWrt + 黑群晖 DSM + Windwos10 + Ubuntu + Docker + HAOS。亡命之徒直接冲 RAID0 ！！硬件：GMK M6 + D2-320 磁盘阵列 2x4T。Q：”文章可以多配些步骤图吗，更直观”A：”不行，我懒人，太麻烦，省电模式中，再说吧” 安装 PVE去官网下载 ISO 镜像，我使用 Ventoy 来进行引导，也可以通过写盘工具直接把 ISO 写到 U 盘，这里最开始我卡 loading，原因是 Ventoy 的版本太老了，升级一下就正常了（不需要格盘）。安装 PVE 的教程很多，也没什么注意的点，IP 只要写对就没啥问题，安装的时候 Ventoy 引导就保持默认第一个。参考视频: 利用PVE虚拟机，来打造属于自己的All In One系统吧！ PVE 装完后的两个比较好用的工具：https://bbs.x86pi.cn/thread?topicId=20https://github.com/ivanhao/pvetools就是改个源，去除个订阅提示，硬盘直通可以使用 pvetools。 关于标记：点数据中心 - 选项 - 标记样式设定 - 完整；然后去每个节点上最上面一栏加标记即可。 OpenWrt软路由还是建议安装一下，但是不推荐主力使用，做一个旁路网关就好，例如安装魔法后解决后面 Docker 拉不到镜像等问题。软路由推荐不良林的视频，说的很清楚：视频链接 由于官方固件太简洁，这里选择 immortalwrt，首先去下载固件，建议选择 COMBINED-EFI (SQUASHFS-COMBINED-EFI.IMG.GZ)版本，因为是 X86 架构直接选。 上传镜像，然后使用命令:qm importdisk 100 /var/lib/vz/template/iso/openwrt.img local-lvm 导入到虚拟机。 安装魔法三件套，openClash、homeProxy、passwall 网卡优先半虚拟化不行的话就切回 E1000，CPU 能 Host 就 Host。 经过测速，起码 PVE8 是不需要 img2kvm 这个工具了，直接 qm 命令导入即可，一般情况下 2 核 1G 的配置够用了。 不过到现在我也还是没搞明白 OpenWrt 的配置，还是挺复杂的，理解为啥有那么多人用爱快了，后面再研究吧，现在网卡都让我删没了。。。但是能用！ 安装 DDNS-Go 配合路由器的端口转发可以实现外网访问，需要一个域名、光猫是桥接，有公网 IP。 DSM采用的是 GXNAS 的引导文件，或者可以看看著名的原生 RR 引导，复活版的哦 https://github.com/RROrg/rr GXNAS 大佬的镜像直接是支持虚拟网卡的，所以直接选半虚拟化就行，性能更好。 作为 All in boom，我这里当然选的是 RAID0，直接拉满，数据安全什么的全靠我定期冷备份。 启用 MacOS 的时间机器支持：官方文档，总结就是新建好共享文件夹，给新建个用户(配额还是建议写一下 300-500G 够用了)，开启 SMB 服务，高级设置里启用 Bonjour 服务发现和启用通过 SMB 进行 Bonjour Time Machine 播送，设置下文件夹完成。 备份 PVE：和时间机器差不多，新建个文件夹，开启共享的 NFS 服务，然后在 NFS 权限里添加 PVE 的 IP 权限，映射为 admin，所有能勾的都勾上。然后去 PVE 存储里添加 NFS 即可，内容选 VZDump备份（或者你还想存其他东西）。PS：不要备份 DSM 本身，会出问题，快照最快最小，需要本体才能恢复，停止和挂起基本一致，可以 PVE Boom 了进行恢复，建议停止模式，备份时虚拟机会暂停几分钟直到备份后启动。 Windows/Linux我这里装的是 Win10 LTSC，Win11 有点复杂，要开 UEFI + TPM，还要登录，就不想搞的那么复杂。 PS：目前最新的 Win11 优化的也很到位了 2G 内存跑轻度任务都是很流畅的。 VirtIO 驱动镜像可以通过访问页面找到download the latest stable 点击下载，详情移步 https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers Win 一般肯定是通过远程桌面使用，在计算机右键属性里打开即可，就可以使用 mstsc 进行连接了；Win10 的 LTSC 刚装上可能会导致风扇狂转，这是一个 bug，更新下系统重启后就好了。 Linux 没啥可以说的很简单，一路下一步和实体机没区别，我装了个 Ubuntu Server。Docker 安装可以使用一键脚本 bash &lt;(curl -sSL https://linuxmirrors.cn/docker.sh) ，换源也有类似的一键换源脚本 bash &lt;(curl -sSL https://linuxmirrors.cn/main.sh)。 需要允许远程 ssh 登陆的话修改 /etc/ssh/sshd_config 将 PermitRootLogin prohibit-password 改为 PermitRootLogin yes 然后重启服务 service ssh restart Docker这里我选择使用 LXC 来创建一个 Docker，首先要下载一个 CT 模板，我使用的是 Ubuntu 的，因为要使用 DockerHub 这里网关使用前面 OpenWrt 的；一定要去掉五特权容器的勾；完成后去选项里面把功能里的嵌套之类全勾上。 需要注意的是，LXC 虚拟虽然效率高但是不是完全的虚拟，还是会共用宿主机的一些东西或者有限制，所以 Linux 我没用 LXC，Docker 的话感觉还是可以的。 为了保证 Docker 可以启动，需要在 PVE 修改一下对应 LXC 的配置，在 /etc/pve/lxc/{CTID}.conf 中加入： 123lxc.apparmor.profile: unconfinedlxc.cgroup.devices.allow: alxc.cap.drop: 大部分情况下 Docker 是可以正常运行了。 使用快捷指令 bash &lt;(curl -sSL https://linuxmirrors.cn/docker.sh) 安装 Docker，源就不换了。 安装完毕后先装一个 portainer： 1docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data outlovecn/portainer-cn:latest 服务推荐：跑一个 speedtest 测速下局域网的网速，或者使用 iperf3 也可以。 HAOS建议使用冬瓜 HAOS，直接使用 PE 版本的 ISO，一键安装。 还没时间仔细研究，一些插件： 米家 - Xiaomi MIoT 美的美居 - Midea AC LAN 海尔智家 - haier 格力 - Gree Climate 石头 - homeassistant-roborock HomeKit Bridge - 设备接入苹果家庭 还没仔细研究，貌似有点复杂。 Tailscale最开始是想装到 Docker 里的，没发现特别好的文章，也不想自己搞了，后来看很多都是装 OpenWrt，有大部分是爱快里装的，比较简单，但是显然不适合我，但是有两个项目感觉不错： https://github.com/CH3NGYZ/tailscale-openwrthttps://github.com/adyanth/openwrt-tailscale-enabler 不过很可惜的是装完我能登录，但是配置的子网路由无效，大概可能是和我 OpenWrt 前面折腾删掉网口有关，最后还是放弃； 最终采用了这位大佬 的方案，使用 LXC 来安装，和之前配置 Docker 一样，需要在 /etc/pve/lxc/{CTID}.conf 文件中加入开启 TUN： 123# 这个 10 200 可以通过在 pve 上执行 ls -al /dev/net/tun 获得，一般都是 10 200lxc.cgroup2.devices.allow: c 10:200 rwmlxc.mount.entry: /dev/net/tun dev/net/tun none bind,create=file 还需要在 LXC 中的 /etc/sysctl.conf 开启转发： 12345net.ipv4.ip_forward=1net.ipv6.conf.all.forwarding=1# 执行生效# sysctl -p /etc/sysctl.conf 然后使用官方的脚本安装：curl -fsSL https://tailscale.com/install.sh | sh执行 tailscale up --authkey=xxxxx --accept-routes --advertise-routes=192.168.0.0/24 开启，记得换成自己的网段和 authkey。配置自动启动：使用 systemd 来进行开启自启。在 /etc/systemd/system 下新建一个配置文件: tailscale.service, 文件内容如下： 1234567891011Description=AutoStart tailscale After=tailscale.serviceRequires=tailscale.service [Service] Type=oneshot ExecStart=/usr/bin/tailscale up --authkey=你的authKey --accept-routes --advertise-routes=你的转发范围 ExecStop=/usr/bin/tailscale down RemainAfterExit=yes Restart=on-failure [Install] WantedBy=multi-user.target 然后执行如下命令： 12systemctl enable tailscale.servicesystemctl start tailscale.service 登录后首先去 Web 控制台重命名设备名字，关掉登录过期，开启自定义路由转发，之后就可以像访问内网那样在外面访问内网服务了。 LXC修改 LXC 的主机名： 12/var/lib/lxc/容器的id编号/config/etc/pve/lxc/容器的id编号.conf 只修改后面这个文件也可以，建议都改。 串流，未完待续。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>AIO</tag>
        <tag>DSM</tag>
        <tag>PVE</tag>
        <tag>OpenWrt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Kubernetes容器编排]]></title>
    <url>%2F2023%2F05%2F28%2FKubernetes%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[写这篇笔记是在已经大体了解 Kubernetes 是什么东西的基础上的，如果不理解我很久之前写过一篇烂尾文可以参考 K8s入门。。。那么为什么又想起来看 K8s，云原生是趋势，对于小公司和项目也许选择云计算服务商会是个低成本还算好的解决方案，但是一旦做大，这笔账其实就不合算了，最终都会自建云原生平台，作为一个后端，也愈发感觉到 DevOps 的重要性，开发与运维的边界在某些情况下变得模糊，或者说都要懂一点才能更好的适应现在的环境。这一系列文章记录我学习历程，作为后端并不会在这上面耗费太多精力，力求达到平均水平即可。学习新东西，对我来说最大的问题是开始阶段，无从下口的感觉，不知道从哪开始，这时候如果能建立一个大体的框架，然后再慢慢去填充这种方式感觉比较合理；但是大体框架的建立也是让人不知所措，我一般从入门书籍或者网上找课程的方式建立，本次依旧是这样，祝我学习顺利。 更新记录k8s 确实有点过于复杂，搭建过程有很多细节，并且需要非常多的组件来支持，机器性能也是个问题，碍于手头机器配置不够，云主机也不太想购买，目前只能停在纸上谈兵上，等日后再实践踩坑；本阶段先扫盲一下相关的概念。 ChatGPT 出来后，对于一些常用命令和资源清单的编写貌似不需要罗列了，让它给你写比看文章更高效，不过记得校验一下，如果不正确还是要进行二次确认。 安装安装上可以简单分为三类： Minikube （推荐安装 K3s）Minikube 可以实现一种轻量级的 Kubernetes 集群，通过在本地计算机上创建虚拟机并部署只包含单个节点的简单集群。Minikube 适用于 Linux，MacOS 和 Windows 系统。Minikube CLI 提供集群管理的基本操作，包括 start、stop、status、和 delete。简单说，就是为了便于开发，开发我门可能没那么多机器和资源，使用它就可以快速的体验 k8s。 云计算厂商安装最简单，master 节点一般由厂商进行托管，通过 UI 选一下机器和配置下一步付钱即可。 裸机就是准备几台物理机，不管是实际的机器还是 VPS，还是虚拟化的机器。 其中裸机上最常见的安装方式，一般分为两种：二进制和 kubeadm 方式 （使用 kubeadm config print init-defaults 查看默认配置），后者官方比较推荐，kubeadm 方式比较简单，适合规模不大的情况，基本都是利用容器的方式来安装，这样也会导致完全启动的时间会比较长；大型生产环境中建议使用二进制方式。 从 Kubernetes 1.24 开始，dockershim 已经从 kubelet 中移除，但因为历史问题 Docker 却不支持 Kubernetes 主推的 CRI（容器运行时接口）标准，所以 Docker 不能再作为 Kubernetes 的容器运行时了，即从Kubernetes v1.24 开始不再使用 Docker了。 但是如果想继续使用 Docker 的话，可以在 Kubelet 和 Docker 之间加上一个中间层 cri-docker。cri-docker 是一个支持 CRI 标准的 shim（垫片）。一头通过 CRI 跟 Kubelet 交互，另一头跟 Docker Api 交互，从而间接的实现了 Kubernetes 以 Docker 作为容器运行时。但是这种架构缺点也很明显，调用链更长，效率更低。 可以考虑将 Containerd 作为 Kubernetes 的容器运行时. 相关内容可以搜一下相关科普文章，或者看我整理的Docker、Containerd、RunC扫盲 具体的安装步骤就不说了，还是挺复杂的，网上也一大把的文档，总结几个关键点；修改主机名（hostnamectl）、节点信息写入 host、关防火墙和 swap、安装 ntpdate 同步服务器时间、通过一个虚拟 IP（VIP）做负载均衡，入口是在 service，然后访问 pod，master 节点的网段跟 service 不要在一个网段、高可用（主节点安装 HAproxy，keep-alive）、如果使用的是 docker 还需要将 cgroup driver 设置为 systemd（默认是 cgroupfs），配置日志 journald 等等。 集群架构K8s 必然是跑在集群中的，再来复习一下基本的概念： master主节点，控制平台，不需要很高性能，不跑任务，通常一个就行了，也可以开多个主节点来提高集群可用度。 worker工作节点，可以是虚拟机或物理计算机，任务都在这里跑，机器性能需要好点；通常都有很多个，可以不断加机器扩大集群；每个工作节点由主节点管理 所以正常情况下，使用 k8s 必须有两台机器，一台 master 节点，一台做 worker 节点。 对于高可用架构，例如 Master 节点尽量维持在奇数，避免投票机制带来的选举冲突问题。 Node Node 是 Kubernetes 中的工作节点，最开始被称为 minion。一个 Node 可以是 VM 或物理机。每个 Node（节点）具有运行 pod 的一些必要服务，并由 Master 组件进行管理，Node 节点上的服务包括 Docker、kubelet 和 kube-proxy。 Node 也算是 K8s 的一个组件，节点组件运行在 Node，提供 Kubernetes 运行时环境，以及维护 Pod。可以说 Node 节点的作用就是来运行应用的工作节点。处理生产级流量的 Kubernetes 集群至少应具有三个 Node。Node 使用 Master 暴露的 Kubernetes API 与 Master 通信。终端用户也可以使用 Kubernetes API 与集群交互。 PodK8S 调度、管理的最小单位，一个 Pod 可以包含一个或多个容器，每个 Pod 有自己的虚拟 IP。一个工作节点可以有多个 pod，主节点会考量负载自动调度 pod 到哪个节点运行。同一容器集（Pod）中的所有容器共享同一个 IP 地址、IPC、主机名称及其它资源（存储、网络、以及怎样运行这些容器的声明）。容器集会将网络和存储从底层容器中抽象出来。注意：自己手动创建的容器无法被管理。 Pod实现共享网络的机制： Pod 创建时，会先起一个 Pause 容器，或者叫 init 容器、根容器，之后才会创建一个或多个业务容器，这些业务容器会共用 Pause 网络协议栈（以及存储等），所以业务容器之间的端口不能重复。 在一个 Pod 中起多个容器的情况，多数是他们有互相依赖关系，尤其是 I/O、网络的依赖。 Pod控制器下面来说说 Pod 控制器，RC（ReplicationController）、RS（ReplicaSet），RC 用于确保在集群中运行指定数量的 Pod 副本，并在 Pod 发生故障时进行自动替换，我们称之为自愈能力。在新版本中官方更推荐使用 RC 的升级版 RS。 RC 和 RS 是实现 Pod 副本控制的核心控制器，而 Deployment 是在它们的基础上构建的更高级别的控制器。Deployment 可以看作是 RC 和 RS 的上层抽象，它使用 RS 来确保 Pod 副本数量的正确性，并实现了滚动升级和回滚等功能。 Deployment 不直接参与 Pod 创建，是通过 RS 来实现的，Deployment 实现回滚和滚动升级的原理是通过管理多个 RS 版本来实现的；每次更新 Deployment 时，它会创建一个新的 RS；回滚时，Deployment 会找到上一个版本的 RS，并将其 Pod 数量恢复到原来的状态。如何判断是否满足数量其实是根据标签 label 来匹配的。在更新和回滚的时候，默认是按 25% 来进行操作，例如先创建 25% 新的 Pod，然后删除 25% 的旧的，通过对应的 RS 操作完成。有一种特殊情况是：当需要创建 5 个，创建到 3 个也就是没有完全创建完成时，突然更新了版本，那么 RS 会立即杀掉创建好的 3 个，然后开始建新版本，并不会等到旧版本全部创建完毕后再进行。同时默认保存所有历史版本。 HPAHPA 是 Horizontal Pod Autoscaler 的缩写，它是 Kubernetes 提供的一种自动扩展 Pod 数量的机制。HPA 可以根据 CPU 使用率、内存使用率等指标自动调整 Pod 的数量，从而实现自动扩缩容。HPA 的工作原理是通过定期检查 Pod 的 CPU 使用率、内存使用率等指标，然后根据指标的变化情况来调整 Pod 的数量。HPA 支持 ReplicaSet、StatefulSet 和 Deployment 部署。 DaemonSetDaemonSet 通常用于在每个节点上运行集群守护进程、日志收集守护进程、监控守护进程等。DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时，也会为他们新增一个 Pod。当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 关于 apiVersion 的选择，在 Kubernetes 1.9 及更早期版本中，DaemonSet 只能通过 extensions/v1beta1 API 进行管理，而在 Kubernetes 1.10 及之后的版本中，DaemonSet 已经被调整为 apps/v1 API，并逐步废弃了 extensions/v1beta1 API。v1 是最基本的 API 版本，一般情况下足够用。 Job与CronJobJob 用于管理短暂任务，会创建一个或者多个 Pod，如果非正常终止，将继续重试 Pod 的执行，直到指定数量的 Pod 成功终止。 随着 Pod 成功结束，Job 跟踪记录成功完成的 Pod 个数。当数量达到指定的成功个数阈值时，任务（即 Job）结束。 删除 Job 的操作会清除所创建的全部 Pod。 挂起 Job 的操作会删除 Job 的所有活跃 Pod，直到 Job 被再次恢复执行。当一个 Job 执行完毕，会保留在 API 中，以便观察是否成功以及执行过程，当然也可以配置自动清理。 CronJob 用于创建基于时隔重复调度的 Job，CronJob 用于执行排期操作，例如备份、生成报告等。 一个 CronJob 对象就像 Unix 系统上的 crontab（cron table）文件中的一行。 它用 Cron 格式进行编写， 并周期性地在给定的调度时间执行 Job。 Pod生命周期任何技术中的生命周期都非常重要，了解生命周期可以更方便的进行排查问题、扩展等。 init 容器如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。它们用于在启动应用程序之前执行一些必要的操作或者检查，并确保所需的资源可用。例如，可能需要等待某个服务启动完毕、加载配置文件、运行数据库迁移脚本等操作。如果这些操作失败，则 Pod 不会进入 Running 状态，所以可以作为一种延迟主容器启动的方案。如果有多个，Init Container 将按照定义的顺序逐个运行。每个容器都必须成功完成后，才能执行下一个容器，执行完毕后就会退出，不会一直存在，也就是只存在于初始阶段。 在 Kubernetes 中，Secret 是一种用于存储敏感信息的机制，例如密码、API 密钥或其他机密数据，Secret 可以加密存储，并且只能通过授权的 Pod 访问。Init Container 可以访问和使用 Secret 存储的敏感信息。这对于需要在容器启动之前加载密码或其他机密信息的应用程序非常有用。在 Pod 启动时，Kubernetes 将加载该 Secret，并将其作为环境变量传递给容器，所以也可以用环境变量的方式来获取，不过对于机密文件，可能就需要 init c 了。 一个 Pod 里可能有多个容器，每一个容器都有一套完整的以上 init 过程。 Pending 挂起Pod 已经被创建，但是它的容器还没有被调度到一个节点上。例如需要的镜像还没有下载完成。 RunningPod 中的容器已经被调度到了一个节点上，并且正在运行中（包括正处于启动和重启中）。如果配置了 postStart 回调，那么该回调已经执行且已完成。处于该状态时，并不代表你的服务可以正常访问（就绪 Ready 才是表示可以对外服务）。 SucceededPod 中的所有容器都已经成功地执行完任务并退出了。 FailedPod 中的某个容器因为错误而终止了执行。 UnknownKubernetes 无法确定 Pod 的状态。这通常是由于与 Pod 相关的信息丢失或不可访问导致的。 在 init 后主容器运行时，可以在开始与结尾进行一个回调，做一下准备和收尾工作，这个就不展开了很简单。与生命周期密切相关的，还有两种探测（由 kubelet 执行）： 生存探测（Liveness Probe）用于检查容器是否在运行中，如果容器无法响应，则 Kubernetes 会将其标记为失败，并尝试重启该容器。例如，可以通过执行某个特定的命令或者发送一个 HTTP 请求来进行生存探测。一般会一直伴随主容器的运行，支持的方式可以是 TCP、HTTP、命令等。 就绪探测（Readiness Probe）用于检查容器是否已经准备好接受流量，如果容器还没有完全启动或者正在加载数据，则 Kubernetes 不会将流量路由到该容器。这有助于确保应用程序不会接收到无法处理的请求。例如，可以通过监视某个特定的端口或者等待某个文件创建完成来进行就绪探测。一般发生在 init 后主容器的开始阶段。 有些时候容器虽然 Running 但是可能进程还没准备好对外服务，或者进程出现了假死；这种情况探测技术就变的非常有用。 StatefulSetStatefulSet 主要用于部署有状态服务，需要搭配 HeadLess 和 PVC 使用，可能需要先了解一下这两部分知识： 稳定的持久化存储，PVC 实现举例来说，当 StatefulSet 中的一个 Pod 被删除后，根据副本数量会重新创建，新的 Pod 的 IP 与被删的肯定是不一致的，但是由于 Pod 仍然具有相同的网络标识符，因此会尝试重用之前分配给该 Pod 的 PVC，所以新 Pod 与 PVC 的绑定关系不会丢失（每一个副本都有自己的 PVC）。 稳定的网络标识，HeadLess（SVC 的一种）实现（clusterIP: None），在下面 SVC 中理解。DNS 规则一般是：&lt;pod-name&gt;.&lt;headless-service-name&gt;.&lt;namespace&gt;.svc.cluster.local，当服务处于相同的命名空间下时，可以省略写为：&lt;pod-name&gt;.&lt;headless-service-name&gt;如果查询 &lt;headless-service-name&gt;.&lt;namespace&gt;.svc.cluster.local 的 DNS 记录，将会返回所有副本 Pod 节点的 IP。 有序部署和回收，基于 Pause（init c），例如先启动 MySQL 再 Java 在 StatefulSet 中，由于每个 Pod 都有自己独立的名称（例如 web-0、web-1 等），因此也需要为每个 Pod 分配一个唯一的 PVC。这样才能保证每个 Pod 都有自己独立的存储卷，并且可以在重启或迁移时保留其之前的数据。通过 Headless Service 则可以将每个 Pod 映射到一个唯一的 DNS 记录中。这样，在调用应用程序时，就可以通过 DNS 解析来直接访问特定的 Pod，从而实现数据的有序访问。在应用程序中，使用 Headless Service 的名称加上 Pod 的名称或索引编号等信息作为域名，来直接访问特定的 Pod。 在创建 StatefulSet 资源清单时，可以直接使用模板（volumeClaimTemplate）来创建 PVC，当副本先被创建时，如果 PVC 还不存在， Kubernetes 控制面会为该副本自动创建一个 PVC。当资源被删除时，可以使用 StatefulSetAutoDeletePVC 特性来自动删除 PVC，它有几种策略可选，默认与之前策略一致，删除时 PVC 不受影响。 但是目前来说，总有一些有状态服务不方便使用 StatefulSet 部署，甚至说不方便使用 K8s 部署，例如 MySQL，这里说的不方便不是不能，而是没有想象中的那么稳定；也不是说数据库服务就不行，例如 MongoDB 就可以很稳定的部署；这个具体分情况。 K8s组件与工具控制相关： kube-apiserver API 服务器公开了 Kubernetes API etcd 键值数据库可以作为保存 Kubernetes 所有集群数据的后台数据库，分布式、高可用。相比 Redis，etcd 更适合分布式、强一致性、高可靠性的场景。例如服务发现、集群协调等场景。 kube-scheduler调度 Pod 到哪个节点运行 kube-controller集群控制器 cloud-controller与云服务商交互 Node 相关（节点组件会在每个节点上运行，负责维护运行的 Pod 并提供 Kubernetes 运行环境）： kubelet在集群中每个节点（node）上运行。 它保证容器（containers）都运行在 Pod 中，并确保处于运行的健康状态；kubelet 不会管理不是由 Kubernetes 创建的容器（自主式与控制器管理的）。 kube-proxy是 Node 上的运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。它维护节点上的一些网络规则， 这些网络规则会允许从集群内部或外部的网络会话与 Pod 进行网络通信。如果操作系统提供了可用的数据包过滤层，则 kube-proxy 会通过它来实现网络规则。 否则，kube-proxy 仅做流量转发。 Container RuntimeKubernetes 支持许多容器运行环境，例如 containerd、 CRI-O 以及 Kubernetes CRI (容器运行环境接口) 的其他任何实现。 其他工具： kubectlKubernetes 命令行工具 kubectl， 让你可以对 Kubernetes 集群运行命令。 你可以使用 kubectl 来部署应用、监测和管理集群资源以及查看日志。 CoreDNS可以让集群内（SVC）使用自定义域名来访问 Ingress官方默认 4 层负载均衡，它可以实现 7 层。 Prometheus集群监控能力 ELK日志接入、分析平台 Federation跨集群统一管理支持 在命令行界面使用 Kubectl 创建和管理 Deployment。Kubectl 是使用 Kubernetes API 与集群进行交互的。 Service/SVCService 是将运行在一个或一组 Pod 上的网络应用程序公开为网络服务的方法（或者说是用于暴露应用程序网络服务的对象类型）。Kubernetes 服务代理会自动将服务请求分发到正确的容器集，无论这个容器集会移到集群中的哪个位置，甚至可以被替换掉（Pod 销毁重建后 IP 会改变）。具有的特性： Service 通过 label 关联对应的 Pod Servcie 生命周期不跟 Pod 绑定，不会因为 Pod 重创改变 IP 提供了负载均衡功能，自动转发流量到不同 Pod 可对集群外部提供访问端口 集群内部可通过服务名字访问 Service 可以将应用程序的一个或多个副本绑定在一个虚拟 IP（ClusterIP）上，并为集群中的其他对象提供一个固定的 DNS 名称。Service 可以用来实现应用程序的负载均衡、服务发现和内部通信等功能。Kubernetes 支持以下几种类型的 Service： ClusterIP：将 Service 暴露在集群内部，只能通过集群内部的 IP 地址访问，仅在集群内部可用（默认）。适用于服务间通信，通常用于微服务架构中，例如 Web 后端和数据库之间的通信。可以避免服务重启后的 IP 变动的影响； NodePort：将 Service 暴露在每个节点上的固定端口上，可以通过节点的 IP 地址和端口号访问，内部也可以使用该 Service 的 DNS 名称来访问集群内的服务，有一定的 LB 效果。适用于需要将 Service 暴露给集群外部的情况。 LoadBalancer：将 Service 暴露在外部负载均衡器上，可以通过负载均衡器的 IP 地址和端口号访问，一般需要花钱购买云服务商提供的 LB 服务。 ExternalName：将 Service 作为一个 DNS 名称暴露出去，可以通过该名称访问 Service，相当于是一条内部的 CNAME 记录。如果正在使用外部系统作为后端服务，并希望在 Kubernetes 中将其命名为 Service，可以使用 ExternalName 来实现。 Service 可以与 Deployment、StatefulSet、DaemonSet 等控制器对象类型配合使用，以实现应用程序的水平扩展和负载均衡等功能。 Service 的 ClusterIP 地址是由 kube-proxy 组件负责维护和更新的。kube-proxy 主要有三种模式：userspace、iptables 和 ipvs。不同的模式实现方式不同，但其基本原理都相同，都是通过监听 Kubernetes API Server 发送的事件来实现 Service IP 地址的自动更新。主要用到的就是两种，目前最新版本都推荐使用 IPVS； iptableskube-proxy 组件使用 iptables 规则来实现 Service 的负载均衡和 IP 地址转发。在该模式下，kube-proxy 会监控 Kubernetes API Server 发送的事件，并根据事件信息生成或更新相应的 iptables 规则。这些规则可以将请求转发到正确的后端 Pod，并保证源 IP 地址不变。客户端访问 SVC，通过 iptables 路由到具体 pod。 ipvskube-proxy 组件使用 Linux 内核中的 IPVS（IP Virtual Server）模块来实现 Service 的负载均衡和 IP 地址转发。与 iptables 不同，ipvs 可以快速处理大量的网络连接，并支持多种负载均衡算法和自动故障检测与恢复机制。在该模式下，kube-proxy 会监控 Kubernetes API Server 发送的事件，并根据事件信息生成或更新相应的 ipvs 规则。 总之就是它们原理上差不多，但是 ipvs 具有更好的性能。这里有个点可能不太好理解，就是 SVC 也要使用 Node 节点的 IP 访问，我直接暴露 pod 的端口不就行了吗？ 首先，集群内部的情况下，更普遍地会使用 Service 名称来访问 Service，而不是直接使用其 IP 地址。当创建一个 Service 后，Kubernetes 会自动为该 Service 分配一个 DNS 名称，并将该名称与 Service 的 ClusterIP 绑定。例如使用 NodePort 方式时，会在每个节点上分配一个端口，并将该端口映射到 Service 的 ClusterIP 上。ClusterIP 是一个虚拟 IP 地址，这个 ClusterIP 不属于任何一个节点，而是由 Kubernetes 控制平面管理。外部客户端也可以使用任何的节点 IP 和暴露的端口进行访问，请求会根据其端口对应的 Service 配置找到与该服务关联的 ClusterIP 地址，并将客户端的请求路由到该地址。一旦请求到达 Service 后，Kubernetes 将使用负载均衡算法（例如 Round Robin），将请求路由到后面运行的一组 Pod 上。这些 Pod 可以运行任何位置，包括不同节点上的多个 Pod。 Headless Service 是一种特殊的 Service 类型，它不会为 Service 分配 Cluster IP（clusterIP: None），而是直接返回后端 Pod 的 IP 地址列表。换句话说，Headless Service 不会负责转发请求到具体的 Pod。一个非常有用的作用是 StatefulSet。它主要用于 DNS 解析和集群内服务的发现。例如，当用户使用 DNS 名称来访问 Service 时，Kubernetes 可以通过 Headless Service 将 DNS 请求解析成与 Service 相关联的所有 Pod 的 IP 地址列表，并返回给用户。这样，用户就可以直接与 Pod 进行通信，而无需经过 Service 和其背后的负载均衡器。由于 Headless Service 不涉及负载均衡和 IP 地址转发，因此它的性能和稳定性通常比普通 Service 更高。此外，Headless Service 还支持 Kubernetes 中的服务发现机制，能够自动监测和更新后端 Pod 的状态变化，确保始终返回最新的 IP 地址列表。因为如果直接暴露了 Pod 的 IP 地址，因此必须提前考虑好网络安全和访问控制等问题。 IngressIngress 是一种 API 对象，用于将外部流量路由到集群内的 Service 上。它提供了一种灵活、统一和扩展性强的方式来管理 HTTP 和 HTTPS 流量，并支持多种负载均衡算法、路径匹配规则和 TLS 加密等功能。 使用 Ingress 能够方便地管理集群中的多个 Service，并将其暴露给外部客户端。Ingress 可以实现以下功能： 路径匹配：可以根据请求的 URL 路径将流量路由到不同的后端 Service 上。 主机名匹配：可以根据请求的主机名（即域名）将流量路由到不同的后端 Service 上。 TLS 加密：可以对 Ingress 进行 TLS 加密，保护客户端与服务端之间的通信安全。 多种负载均衡算法：可以根据需求选择不同的负载均衡算法，如轮询、最小连接数等。 需要注意的是，要使用 Ingress 需要满足以下条件： 集群中必须存在一个符合规范的 Ingress Controller（如 Nginx、Traefik、Haproxy 等），用于监听 Ingress 对象的变化并更新对应的负载均衡器或反向代理。 需要为每个 Ingress 配置一个规则，指定其路径、主机名、后端 Service （其类型设置为 NodePort 或 LoadBalancer）等相关信息。 如果需要对 Ingress 进行 TLS 加密，则需要先创建一个 Kubernetes Secret，用于存储证书和私钥等敏感信息，并将其与 Ingress 绑定。 使用最多的还是 Ingress-Nginx，本质来说其实就是启动了一个 Nginx，只不过不需要我们去写 NG 的配置文件而使用 k8s yml 自动完成了。Ingress 可以帮我们实现七层代理，原本只支持到四层代理。 七层代理和四层代理都是计算机网络中常用的代理类型，它们在不同的 OSI 模型层次上工作并提供了不同的功能。 四层代理（传输层代理）：工作在 OSI 模型的传输层，主要负责对网络连接进行管理和控制。四层代理能够根据源 IP 地址、目标 IP 地址、源端口号和目标端口号等信息来对传输的数据进行转发，以达到负载均衡、流量控制和安全过滤等目的。 七层代理（应用层代理）：工作在 OSI 模型的应用层，具有更高的可扩展性和灵活性。七层代理能够深入了解应用程序的协议、数据格式和语义，并对传输的数据进行精细化处理。七层代理可以根据应用程序的特殊需求，对数据进行修改、重写和过滤，以实现更高级别的功能，如内容缓存、反向代理、Web 加速和应用层防火墙等。 四层代理更依赖于基本的网络传输协议，而七层代理则更加注重应用程序的细节和语义。简单说就是四层代理更加底层，速度快延迟低，占用资源也少，而七层具有更高的灵活性。 Ingress 与 SVC 之间可能会有点难以理解，尤其是跟 SVC 的 NodePort 对比，简单说 Ingress 是已经暴露了一个 IP 和端口，终端用户可以直接通过 Ingress 暴露的 IP 进行访问。此时，后面运行的 Service 的 type 类型是否为 ClusterIP 或 NodePort 在某种程度上已经不再重要了。 不过在同等条件下，NodePort 类型的 Service 可能会比 ClusterIP 类型的 Service 略微快一些；NodePort 类型的 Service 可能会更加不可靠一些，因为它需要暴露节点上的端口，可能会遇到端口号冲突、网络分区等问题。 存储ConfigMapConfigMap 是一种用于存储配置数据的 API 对象，它可以将配置数据从 Pod 代码中分离出来，实现了配置和代码的分离，提高了应用程序的可移植性和可维护性。ConfigMap 可以是一个文件或者文件夹，或者是硬编码到 YAML 中；使用上可以通过环境变量或者挂载 Volume 的方式；需要注意的是，ConfigMap 存储的数据为纯文本格式，因此不适合存储大型二进制文件。 SecretSecret 是一种用于存储敏感信息的 API 对象，例如密码、证书等。Secret 可以对数据进行加密处理，并提供各种不同类型的加密方式和安全性保障。Kubernetes 中的 Secret 存储常用的三种类型： Opaque：Opaque 类型的 Secret 是最通用的类型，它可以存储任意格式的数据。Opaque Secret 中的数据将会被编码为 base64 格式，但并未进行加密。 Service Account：Service Account 类型的 Secret 主要用于身份验证和授权，包含了一个或多个令牌，用于验证 Pod 中的容器是否具有访问其他资源的权限。 docker-registry：用来存储 Docker 仓库的认证信息和配置信息，并在容器中使用该 Secret 来进行身份验证和授权操作。在资源清单中使用 imagePullSecrets 来在 pull 镜像时进行认证。 Kubernetes 中的 Service Account 只能用于访问 Kubernetes API 或其他与 Kubernetes 集群相关的资源和对象，并且不适用于外部服务或资源的访问。Service Account 通常由应用程序自动使用，并由 Kubernetes 自动处理。如果声明了 Service Account 会自动挂载到 /run/secrets/kubernetes.io/serviceaccount，部分发行版或者老的 Linux 会在 /var/run 下。 Opaque 在使用时，不管是环境变量还是挂载，都会自动解码，当然存储的时候也要求值必须是 base64. VolumeVolume 是一种用于持久化和共享数据的抽象概念。Volume 可以将不同类型的存储介质（例如本地磁盘、网络存储或云存储等）封装为统一的接口，并在 Pod 中使用。Volume 一些常见的类型： emptyDiremptyDir 是一种临时性的 Volume，用于在单个 Pod 的多个容器之间共享数据。emptyDir 将数据存储在 Pod 所在的节点上，并在 Pod 被删除或重新调度时自动清理。需要注意的是，当 Pod 中的容器崩溃尝试重启/新建时 Volume 不会清除数据。 hostPathhostPath 是一种将主机节点上的文件或目录作为 Volume 挂载到 Pod 中的方法。hostPath 可以让 Pod 访问主机节点上的文件系统（或者主机访问 Pod 中的文件），但也可能导致安全性问题和资源争用。 其他的还有很多类型，因为用的不多就不贴了。 PV/PVCPersistentVolume（PV）是一种用于持久化存储数据的资源对象。PV 可以将不同类型和容量的存储介质（例如本地磁盘、网络存储或云存储等）封装为统一的接口，并在整个 Kubernetes 集群中使用。区别与 Volume 的几个特点： 范围Volume 是一个容器级别的概念，用于将数据持久化到 Pod 中。而 PV 是一个集群级别的概念，用于将数据持久化到整个 Kubernetes 集群中。 生命周期Volume 的生命周期与 Pod 相关联，当 Pod 被删除或重启时，相关的 Volume 也会被删除或重新创建。而 PV 则独立于 Pod 存在，并且可以在多个 Pod 之间共享和复用。 管理方式Volume 可以在 Pod 中直接定义和管理，而 PV 则需要通过 PersistentVolume 和 PersistentVolumeClaim 对象进行定义和管理。 耦合性PV 将存储介质与 Pod 解耦，使得 Pod 可以从实际存储细节中隔离出来。这使得存储介质可以轻松地更换和升级，而无需对 Pod 进行修改。同理，当 Pod 删除时，PVC 并不会被删除，依然可以为其他 Pod 提供服务。 而 PersistentVolumeClaim（PVC）则可以理解为存储资源的使用者，它定义了对 PV 的访问要求，例如存储类型、访问模式和容量等信息。PVC 可以请求 Kubernetes 动态分配或静态绑定一个 PV，并将其挂载到 Pod 中，以实现数据的持久化和共享。 PVC 概念上与 Pod 类似。 Pod 会耗用节点资源，而 PVC 申领会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU 和内存）；同样 PVC 申领也可以请求特定的大小和访问模式 （例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载）。 当 PVC 请求一个 PV 时，Kubernetes 会根据 PVC 的要求，在可用的 PV 池中寻找匹配的 PV。如果找到多个匹配的 PV，则 Kubernetes 会按照一定的策略进行选择和绑定。一旦绑定成功，PVC 和 PV 就形成了一种类似于 “占位符” 的关系。当 PVC 被删除时，它会释放与之关联的 PV。一个 PV 可以被多个 PVC 绑定和使用，但每个 PVC 只能绑定一个 PV。如果多个 PVC 请求同一个 PV，则 Kubernetes 会按照先来先服务的原则进行分配，直到 PV 的容量用尽为止。 根据 PV 的创建方式可分为两种： 静态PV手动分配的方式，它要求管理员提前创建好一定数量的 PV，并将其存储属性和访问模式等信息进行配置。静态 PV 的优点是稳定可靠，但缺点是不够灵活，难以应对动态变化的需求。 动态PV自动分配的方式，它利用 Kubernetes 动态资源管理特性，根据 PVC 对 PV 的请求，动态地创建、绑定和管理 PV。动态 PV 的优点是灵活方便，可以动态适应不同的需求，但缺点是可能会产生较大的管理负担。 动态 PV 一般是配合云计算服务来使用的，主要是价格昂贵。 每个 PersistentVolume（PV）都有一个访问模式（Access Mode），用于定义 Pod 如何访问 PV 中的数据。Kubernetes 支持以下三种访问模式： ReadWriteOnce（RWO）：只允许单个节点以读写方式挂载 PV。这意味着同一时间内只能有一个 Pod 访问该 PV，并且只能将其挂载到同一个节点上。 ReadOnlyMany（ROX）：允许多个节点以只读方式挂载 PV。这意味着多个 Pod 可以同时访问该 PV，并且可以将其挂载到多个节点上，但不能进行写操作。 ReadWriteMany（RWX）：允许多个节点以读写方式挂载 PV。这意味着多个 Pod 可以同时访问该 PV，并且可以将其挂载到多个节点上，也可以进行读写操作。 当 PersistentVolume（PV）上的数据不再需要时（当其被从申领中释放时如何处理该数据卷），可以使用回收策略对 PV 进行清理和释放。Kubernetes 提供了以下三种回收策略： Retain（保留）：指定在被释放后保留其数据。这意味着 PV 中的数据不会被删除，并且管理员需要手动清理它们。通常用于需要手动备份或迁移数据的场景。例如当 PVC 被删除时，由于卷上仍然存在这前一申领人的数据，该卷还不能用于其他申领。 Delete（删除）：删除 PVC 时也会将 PV 对象从 Kubernetes 中移除，同时也会从外部基础设施（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）中移除所关联的存储资产。 Recycle（回收）：在被释放后重新格式化存储介质并清除所有数据。在新版本中，回收策略 Recycle 已被废弃。取而代之的建议方案是使用动态制备。 PV 的回收策略是在 PersistentVolumeClaim（PVC）被删除后触发的。具体而言，当一个 PVC 被删除时，Kubernetes 会检查其绑定的 PV 是否仍然被其他 PVC 使用，如果没有，则会按照该 PV 的回收策略进行清理和释放。 使用 PV 时，每个卷会处于以下阶段（Phase）之一： Available（可用）– 卷是一个空闲资源，尚未绑定到任何申领； Bound（已绑定）– 该卷已经绑定到某申领（PVC）； Released（已释放）– 所绑定的申领已被删除，但是资源尚未被集群回收；也就是对应的 PVC 被删除。 Failed（失败）– 卷的自动回收操作失败。 区别于 PVC 的状态，当查看 PVC 状态时，也会列出绑定的 PV 的状态，而 PVC 的状态一般是：Pending/Bound/Lost。 对于 StatefulSet 当 Pod 重建后依然有相同的网络标识符，可以绑定之前的 PVC 保证数据不会丢失（每一个副本会有单独的 PVC）；对于在 Deployment 资源清单中配置的副本和 PVC，所有 Pod 会共享 PVC，也不需要考虑新建还是更新的了（猜测，未证实）。 网络策略在 Kubernetes 中，有两种主要的网络模型：Overlay 网络和基于主机的网络。 Overlay 网络：Overlay 网络是一种虚拟网络，它在物理网络之上创建一个逻辑网络，用于容器之间的通信。Overlay 网络通常使用隧道技术（如 VXLAN、GRE、IPsec 等）将容器数据包封装在物理网络的数据包中传输。Kubernetes 中的 Flannel、Calico（更推荐吧）、Weave Net 等网络插件都是基于 Overlay 网络实现的。 基于主机的网络：基于主机的网络是一种直接将容器连接到主机网络上的网络模型，容器与主机之间共享同一个网络命名空间。这种网络模型通常使用 Linux Bridge 或者直接使用主机网络接口来实现。Kubernetes 中的 kube-proxy 就是基于主机的网络模型实现的。 Flannel 是一个用于容器网络的开源项目，旨在为 Kubernetes 集群提供简单、快速、可靠的网络连接。Flannel 的主要作用是将容器连接到同一网络中，并允许它们之间进行通信。Flannel 通过在每个节点上创建一个虚拟网络（VXLAN 或 UDP），将容器的 IP 地址映射到集群中的其他节点上。这种方式可以实现跨节点的容器通信，同时保证容器的 IP 地址在整个集群中唯一。对于实现的原理，简单说就是维护一个路由表（使用 ETCD），因为每个 Node 都有 Flanneld，利用路由表可以共享 IP 对应信息，在进行集群初始化的时候需要安装一下，可以在 kubeadm 的默认模板中事先配置好。 1234567wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改配置文件，例如 - --iface=eth0# 安装kubectl apply -f kube-flannel.yml# 查看 podkubectl get pods --all-namespaces 以上，可以在 kubeadm 初始化完成后使用 kubectl 快速安装。 集群调度在 Kubernetes 中，调度 是指将 Pod 放置到合适的节点上，以便对应节点上的 Kubelet 能够运行这些 Pod。默认的调度器就是 kube-scheduler。调度过程一般可划分为两个阶段，过滤和打分；过滤阶段会将所有满足 Pod 调度需求的节点选出来。 例如，PodFitsResources 过滤函数会检查候选节点的可用资源能否满足 Pod 的资源请求。在打分阶段，调度器会为 Pod 从所有可调度节点中选取一个最合适的节点。 根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。最后会将 Pod 调度到得分最高的节点上。如果存在多个得分最高的节点，kube-scheduler 会从中随机选取一个。这里会有一些可选的调度策略，例如 CPU 或者 内存占用最低的优先。 podspec.nodename 是 PodSpec 的另一个属性，用于指定 Pod 所部署的节点名称。当 Pod 的 podspec.nodename 属性被设置时，Kubernetes 调度器将不会再考虑其他节点来调度该 Pod，并且会直接将该 Pod 分配给指定的节点上运行；这种方式称为静态 Pod 调度，当然也可以使用标签选择。 节点亲和性（Node Affinity）是指将 Pod 调度到与其相关的节点上的能力。通俗来说，它可以让我们通过一些限制条件（如硬件配置、软件环境等）来控制 Pod 应该部署在哪些节点上；其中又分为软策略和硬策略，也就是尽量满足和必须满足。 污点（Taint）是一种标记，用于指示节点上存在特定的限制条件，这些条件会影响 Pod 在该节点上的调度。与节点亲和性（Node Affinity）相反，污点可以使节点更具有选择性，以便根据实际需求来控制 Pod 不能被调度到某些节点上。容忍度（Toleration） 是应用于 Pod 上的。容忍度允许调度器调度带有对应污点的 Pod。 容忍度允许调度但并不保证调度：作为其功能的一部分， 调度器也会评估其他参数。污点和容忍度（Toleration）相互配合，可以用来避免 Pod 被分配到不合适的节点上。 每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的 Pod， 是不会被该节点接受的。我们建立的 Pod 不会运行在 Master 节点就是使用了这个特性打了一个 NoSchedule。 K3sRancher Labs 是业界领先的容器软件提供商，其旗舰产品 Rancher 是一款开源的企业级 Kubernetes 管理平台，极为出色地管理和安装 Kubernetes 集群。 k3s 将安装 Kubernetes 所需的一切打包进仅有 60MB 大小的二进制文件中，并且完全实现了 Kubernetes API。为了减少运行 Kubernetes 所需的内存，Rancher 删除了很多不必要的驱动程序，并用附加组件对其进行替换。K3s 仅需要 kernel 和 cgroup 挂载。k3s 是一款完全通过 CNCF 认证的 Kubernetes 发行版，这意味着你可以编写 YAML 来对完整版的 Kubernetes 进行操作，并且它们也将适用于 k3s 集群。由于它只需要极低的资源就可以运行，因此它能够在任何 512MB RAM 以上的设备上运行集群，换言之，我们可以让 pod 在 master 和节点上运行。专为物联网及边缘计算设计。 所以，对于日常开发用，K3s 是个比 Minikube 更好的选择，介于 Minikube 和 K8s 之间的最优解。如果你处于边缘计算等小型部署的场景或仅仅需要部署一些非核心集群进行开发/测试，那么选择 k3s 则是性价比更高的选择。 与 K8s 对比： 移除过时的功能、Alpha 功能、非默认功能，这些功能在大多数 Kubernetes 集群中已不可用。 删除内置插件(比如云供应商插件和存储插件)，可用外部插件程序替换。K3s 在默认状态下只会启动除自身进程之外的两个应用：coredns 和 traefik。 添加 SQLite3 作为默认的数据存储。etcd3 仍然可用，但并非默认项。 默认执行容器从 docker 换成了 containerd。 封装在简单的启动程序中，该启动程序处理很多复杂的 TLS 和选项。 内置了 local storage provider、service load balancer、helm controller、Traefik ingress controller，开箱即用。 所有 Kubernetes 控制平面组件如 api-server、scheduler 等封装成为一个精简二进制程序，控制平面只需要一个进程即可运行。 综上，一般来说，K3s 基本也满足大部分需求了。k3s 官方准备了一键安装脚步，部署非常方便： 12345# 主/Server 节点curl -sfL https://get.k3s.io | sh -# node/Agent 节点curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh - 接着就可以部署 Rancher 进行多集群管理了，最好找一台独立的机器，支持 Helm 或者 Docker 的方式进行部署，操作也非常简单，一行命令搞定，然后根据引导界面选择新建集群还是导入已有集群，也就是执行几行命令的事，因为 K8s 已经不支持 dockershim，之前都是用 docker 作为容器运行时，目前新版的 k3s 已经切换到 containerd，当然也可以通过 cri-dockerd 继续使用 Docker。 而另一个 Rancher 出品的 k8s 集群部署工具 RKE 也挺不错的，功能类似 kubeadm 都主要用来快速初始化集群。 RKE（Rancher Kubernetes Engine）是经过认证的 Kubernetes 发行版，也是用于创建和管理 Kubernetes 集群的 CLI 工具和库。RKE 是一款经过 CNCF 认证的开源 Kubernetes 发行版，可以在 Docker 容器内运行。它通过删除大部分主机依赖项，并为部署、升级和回滚提供一个稳定的路径，从而解决了 Kubernetes 最常见的安装复杂性问题。 RKE2，也称为 RKE Government，是 Rancher 的下一代 Kubernetes 发行版。RKE2 完美结合了 1.x 版本的 RKE（以下简称 RKE1）和 K3s。重要的是，RKE2 不像 RKE1 一样依赖 Docker。RKE1 使用 Docker 来部署和管理 control plane 组件以及 Kubernetes 的容器运行时。RKE2 将 control plane 组件作为由 kubelet 管理的静态 pod 启动。嵌入式容器运行时是 containerd。 无论安装哪一个，安装的时候一定要注意版本对应，可以从这里查阅版本的支持范围。 k3s 文档 RKE 文档 RKE2 文档 Rancher 文档 不得不说，Rancher 真是把 k8s 弄到了开箱即用，生产环境下也完全不是问题，并且文档有中文，赞。爱折腾的可以尝试使用 Rancher Desktop 来代替开发机的 Docker，它内置 k3s 应该表现还是可以的。 常用命令整理minikube12345678910# 启动集群minikube start# 查看节点。kubectl 是一个用来跟 K8S 集群进行交互的命令行工具kubectl get node# 停止集群minikube stop# 清空集群minikube delete --all# 安装集群可视化 Web UI 控制台minikube dashboard kubectl1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 部署应用，使用 create 创建如果存在会返回失败kubectl apply -f app.yaml# 快速创建一个 pod，1.18 后是使用 Deployment 来创建的kubectl run mypod --image=myimage# 查看 deployment，rs 也一样# get 可以跟 pod/deployment/svc/rskubectl get deployment# 查看 pod 详细信息kubectl get pod -o wide# 查看系统命名空间的 pod，默认是 defaultkubectl get pod -n kube-system# 查看 pod 详情kubectl describe pod pod-name# 查看 logkubectl logs pod-name# 进入 Pod 容器终端， -c container-name 可以指定进入哪个容器。kubectl exec -it pod-name -- bash# 伸缩扩展副本kubectl scale deployment test-k8s --replicas=5# 把集群内端口映射到节点kubectl port-forward pod-name 8090:8080# 查看历史kubectl rollout history deployment test-k8s# 回到上个版本kubectl rollout undo deployment test-k8s# 回到指定版本kubectl rollout undo deployment test-k8s --to-revision=2# 删除部署kubectl delete deployment test-k8s# 查看全部kubectl get all# 重新部署kubectl rollout restart deployment test-k8s# 命令修改镜像，--record 表示把这个命令记录到操作历史中kubectl set image deployment test-k8s test-k8s=ccr.ccs.tencentyun.com/k8s-tutorial/test-k8s:v2-with-error --record# 暂停运行，暂停后，对 deployment 的修改不会立刻生效，恢复后才应用设置kubectl rollout pause deployment test-k8s# 恢复kubectl rollout resume deployment test-k8s# 输出到文件kubectl get deployment test-k8s -o yaml &gt;&gt; app2.yaml# 删除全部资源kubectl delete all --all# 将一个已有的 Kubernetes 资源对象暴露为一个新的 Service# 将名为 myapp 的 Deployment 暴露为一个新的 LoadBalancer 类型的 Servicekubectl expose deployment myapp --type=LoadBalancer --port=80 --target-port=8080kubectl get svc# 修改 SVC 网络类型kubectl edit svc xxxx 更多可以直接问 ChatGPT。 资源清单在 Kubernetes 中，资源清单（Resource Manifest）是一个包含 Kubernetes 对象定义的 YAML 文件。它描述了在 Kubernetes 集群中创建、配置或管理的任何一种对象，例如 Pod、Service、Deployment、ConfigMap 等。通常可以与 Kubectl 命令一起使用，例如：kubectl apply -f deployment.yaml 资源分类： 命名空间级别即：只在当前的命名空间有效， 集群级别全集群可用，可跨命名空间 元数据型类似 HPA 这种通过指标来进行操作。也可归属于上面两种 资源清单中有一些属性属于必选： apiVersionKubernetes API 的版本号。可通过 kubectl api-versions 查询 kind资源对象的类型，例如 Pod、Service 或 Deployment。 metadata元数据，包括资源对象的名称和标签等信息。 spec资源对象的规格，包括容器设置、网络设置、存储设置以及其他相关信息。具体内容取决于资源对象的类型。 对于 metadata 和 spec，下面细分了很多项，下面展示一个最简单的清单文件： 123456789101112apiVersion: v1kind: Podmetadata: name: mypod # 标签易于管理 labels: app: myapp version: v1spec: containers: - name: mycontainer image: nginx 后续生成资源清单可以使用 Helm，然后配合 ChatGPT 进行微调。 补充：CgroupCgroup，全称 Control Group（控制组），是 Linux 系统内核提供的一个特性（Linux 2.6.24内核开始将 Cgroup 加入主线），主要用于限制和隔离一组进程对系统资源的使用，也就是做资源 QoS。可控制的资源主要包括 CPU、内存、block I/O、网络带宽等等。 Cgroup 提供了一个原生接口并通过 cgroupfs 提供（从这句话我们可以知道 cgroupfs 就是 Cgroup 的一个接口的封装）。类似于 procfs 和 sysfs，是一种虚拟文件系统。并且 cgroupfs 是可以挂载的，默认情况下挂载在 /sys/fs/cgroup 目录。 Systemd 也是对于 Cgroup 接口的一个封装。systemd 以 PID 的形式在系统启动的时候运行，并提供了一套系统管理守护程序、库和实用程序，用来控制、管理 Linux 计算机操作系统资源。 相对来说 Systemd 更加简单，而且目前已经被主流 Linux 发行版所支持（Red Hat 系列、Debian 系列等），而且经过几个版本的迭代已经很成熟了，所以不管是 Docker 本身还是在 K8S 中建议使用 Systemd 来进行资源控制与管理。 cgroupfs 是文件驱动修改，内核功能没有提供任何的系统调用接口，而是对 linux vfs 的一个实现，因此可以用类似文件系统的方式进行操作。 systemd 封装了 cgroups 的软件也能让你通过它们定义的接口控制 cgroups 的内容，因此是通过接口调用驱动修改。 相关工具对于一些场景，我们需要使用代码来控制 K8s，这方面可以使用 Kubernetes API 来实现，官方提供了 REST API 的接口，也可以使用相关语言的 SDK 来实现。 文档参考：https://kubernetes.io/zh-cn/docs/concepts/overview/kubernetes-api/ 集群之间都是通过证书来进行认证的，证书由 Master 进行签发，而 Pod 因为会频繁新建和销毁，签发证书的方式就不太合适，使用的是 Service Accounts 它被用于为 Pod 提供访问 Kubernetes API 和其他 Kubernetes 资源的标准方式。每个 Service Account 都会分配一个唯一的名称和一个对应的 Token，该 Token 可以被应用程序用来进行身份验证和授权。每个 Pod 都有一个默认的 Service Account，上面 Secret 中介绍过。 关于鉴权，则推荐使用 RBAC 方式，它声明了四种 Kubernetes 对象：Role、ClusterRole、RoleBinding 和 ClusterRoleBinding。带有 Cluster 的则表示是集群级别的权限，否则需要指定是那个命名空间下的权限。Role 通常与 Role Binding 结合使用，用于将指定的角色分配给特定的用户或服务账户。而 ClusterRole 则通常与 ClusterRoleBinding 结合使用，用于将角色分配给整个集群的用户或服务账户。K8s 不提供用户管理，在证书签发时已经包含了这部分信息。 可视化可以使用 kubernetes-dashboard监控可以用 Prometheus、Grafana 参考K8s入门Docker、Containerd、RunC扫盲kubectl命令表Kubernetes组件k3s vs k8s vs k9s鉴权]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>Kubernetes</tag>
        <tag>k3s</tag>
        <tag>Rancher</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ClashX使用入门]]></title>
    <url>%2F2021%2F06%2F23%2FClashX%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[起因还要从去年还是前年的那次大规模封杀说起，因为种种原因 SS 原作者早已弃坑，SSR 作者也是如此，以现在的防火墙技术识别 SS 的流量特征应该不难，每次都是大规模的『关机保平安』；我买的机场也开始提供 SS + V2ray 的方式，并且建议优先使用 V2ray，我也就顺势切换了，接下来遇到的一个问题就是 SS 客户端不支持，需要找一个替代的，最好多种协议都支持，一轮调研下来，Clash 脱颖而出（当时起码是 Star 最高的），于是就开始使用 ClashX。 不像其他的客户端，可以有完整的 GUI 配置，ClashX 比较传统的使用配置文件方式，说起配置文件，V2ray 本身就以复杂的配置著称，不过换来的是高稳定性与强大的功能。起初我只是找了份模版，然后简单加上节点信息，这样跟之前的 SS 没太大区别，用了好长时间。后来，闲得无聊，搜了下配置文件相关的信息，发现可配置的东西是真的多，功能也是真牛逼，我也没完全掌握，以后用到相关的功能再更新吧。 版本选择Clash 有两个版本，普通版和 Pro 版，对应到 Mac 平台就是 ClashX 和 ClashX Pro，倒不是说 Pro 收费，是 Pro 支持更多的功能，如 TUN、代理集、规则集、脚本等，但是不开源，但是还是建议使用 Pro 版本，获得更好的体验（例如通过 TUN 可接管设备的所有 TCP 和 UDP 流量，可以做软路由等等）。 基本配置首先说明，因为我的机场不提供订阅服务，虽然可以进行转换，但我还是更愿意使用传统的手动增加节点方式，订阅相关的规则暂时略过。 ClashX 主文件 Config.yaml 内容： 12345678910111213141516171819202122232425#---------------------------------------------------### 配置文件需要放置在 $HOME/.config/clash/*.yaml## 这份文件是clashX的基础配置文件，请尽量新建配置文件进行修改。## ！！！只有这份文件的端口设置会随ClashX启动生效#---------------------------------------------------#port: 1090socks-port: 1080allow-lan: falsemode: Rulelog-level: infoexternal-controller: 127.0.0.1:9090Proxy:Proxy Group:Rule:- DOMAIN-SUFFIX,google.com,DIRECT- DOMAIN-KEYWORD,google,DIRECT- DOMAIN,google.com,DIRECT- DOMAIN-SUFFIX,ad.com,REJECT- GEOIP,CN,DIRECT- MATCH,DIRECT 这些都是最基本的端口与规则模式信息，一般情况都是会另新建一个配置文件来配置详细的规则和节点。 DNS这部分的功能非常实用，虽然最开始我完全看不懂，也不知道什么意思，在这里的配置也让我踩坑了好几次，首先来看一个示例配置（Pro 版）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140# DNS 服务器配置(可选；若不配置，程序内置的 DNS 服务会被关闭)dns: enable: true listen: 0.0.0.0:53 ipv6: true # 当此选项为 false 时, AAAA 请求将返回空 # 以下填写的 DNS 服务器将会被用来解析 DNS 服务的域名 # 仅填写 DNS 服务器的 IP 地址 default-nameserver: - 223.5.5.5 - 114.114.114.114 enhanced-mode: fake-ip # 或 redir-host fake-ip-range: 198.18.0.1/16 # Fake IP 地址池 (CIDR 形式) # use-hosts: true # 查询 hosts 并返回 IP 记录 # 在以下列表的域名将不会被解析为 fake ip，这些域名相关的解析请求将会返回它们真实的 IP 地址 fake-ip-filter: # 以下域名列表参考自 vernesong/OpenClash 项目，并由 Hackl0us 整理补充 # === LAN === - '*.lan' # === Linksys Wireless Router === - '*.linksys.com' - '*.linksyssmartwifi.com' # === Apple Software Update Service === - 'swscan.apple.com' - 'mesu.apple.com' # === Windows 10 Connnect Detection === - '*.msftconnecttest.com' - '*.msftncsi.com' # === NTP Service === - 'time.*.com' - 'time.*.gov' - 'time.*.edu.cn' - 'time.*.apple.com' - 'time1.*.com' - 'time2.*.com' - 'time3.*.com' - 'time4.*.com' - 'time5.*.com' - 'time6.*.com' - 'time7.*.com' - 'ntp.*.com' - 'ntp.*.com' - 'ntp1.*.com' - 'ntp2.*.com' - 'ntp3.*.com' - 'ntp4.*.com' - 'ntp5.*.com' - 'ntp6.*.com' - 'ntp7.*.com' - '*.time.edu.cn' - '*.ntp.org.cn' - '+.pool.ntp.org' - 'time1.cloud.tencent.com' # === Music Service === ## NetEase - '+.music.163.com' - '*.126.net' ## Baidu - 'musicapi.taihe.com' - 'music.taihe.com' ## Kugou - 'songsearch.kugou.com' - 'trackercdn.kugou.com' ## Kuwo - '*.kuwo.cn' ## JOOX - 'api-jooxtt.sanook.com' - 'api.joox.com' - 'joox.com' ## QQ - '+.y.qq.com' - '+.music.tc.qq.com' - 'aqqmusic.tc.qq.com' - '+.stream.qqmusic.qq.com' ## Xiami - '*.xiami.com' ## Migu - '+.music.migu.cn' # === Game Service === ## Nintendo Switch - '+.srv.nintendo.net' ## Sony PlayStation - '+.stun.playstation.net' ## Microsoft Xbox - 'xbox.*.microsoft.com' - '+.xboxlive.com' # === Other === ## QQ Quick Login - 'localhost.ptlogin2.qq.com' ## Golang - 'proxy.golang.org' ## STUN Server - 'stun.*.*' - 'stun.*.*.*' # 支持 UDP / TCP / DoT / DoH 协议的 DNS 服务，可以指明具体的连接端口号。 # 所有 DNS 请求将会直接发送到服务器，不经过任何代理。 # Clash 会使用最先获得的解析记录回复 DNS 请求 nameserver: - https://doh.pub/dns-query - https://dns.alidns.com/dns-query # 当 fallback 参数被配置时, DNS 请求将同时发送至上方 nameserver 列表和下方 fallback 列表中配置的所有 DNS 服务器. # 当解析得到的 IP 地址的地理位置不是 CN 时，clash 将会选用 fallback 中 DNS 服务器的解析结果。 # fallback: # - https://dns.google/dns-query # 如果使用 nameserver 列表中的服务器解析的 IP 地址在下方列表中的子网中，则它们被认为是无效的， # Clash 会选用 fallback 列表中配置 DNS 服务器解析得到的结果。 # # 当 fallback-filter.geoip 为 true 且 IP 地址的地理位置为 CN 时， # Clash 会选用 nameserver 列表中配置 DNS 服务器解析得到的结果。 # # 当 fallback-filter.geoip 为 false, 如果解析结果不在 fallback-filter.ipcidr 范围内， # Clash 总会选用 nameserver 列表中配置 DNS 服务器解析得到的结果。 # # 采取以上逻辑进行域名解析是为了对抗 DNS 投毒攻击。 fallback-filter: geoip: false ipcidr: - 240.0.0.0/4 - 0.0.0.0/32 # domain: # - '+.google.com' # - '+.facebook.com' # - '+.youtube.com'tun: enable: true #如果需要启用 TUN 模式，请设置为 true stack: system # 或 gvisor macOS-auto-route: true macOS-auto-detect-interface: true dns-hijack: - tcp://8.8.8.8:53 - tcp://8.8.4.4:53 clash DNS 请求逻辑： 当访问一个域名时，nameserver 与 fallback 列表内的所有服务器并发请求，得到域名对应的 IP 地址。 clash 将选取 nameserver 列表内，解析最快的结果。 若解析结果中，IP 地址属于国外，那么 clash 将选择 fallback 列表内，解析最快的结果。 因此，在 nameserver 和 fallback 内都放置无污染、解析速度较快的国内 DNS 服务器，以达到最快的解析速度。但是 fallback 列表内服务器会用在解析境外网站，为了结果绝对无污染，尽量使用支持 DoT/DoH 的服务器。 DNS 配置注意事项： 如果您为了确保 DNS 解析结果无污染，请仅保留列表内以 tls:// 或 https:// 开头的 DNS 服务器，但是通常对于国内域名没有必要。 如果您不在乎可能解析到污染的结果，更加追求速度。请将 nameserver 列表的服务器插入至 fallback 列表内，并移除重复项。 关于 DNS over HTTPS (DoH) 和 DNS over TLS (DoT) 的选择：对于两项技术双方各执一词，而且会无休止的争论，各有利弊。各位请根据具体需求自行选择，但是配置文件内默认启用 DoT，因为目前国内没有封锁或管制。DoH: 以 https:// 开头的 DNS 服务器。拥有更好的伪装性，且几乎不可能被运营商或网络管理封锁，但查询效率和安全性可能略低。DoT: 以 tls:// 开头的 DNS 服务器。拥有更高的安全性和查询效率，但端口有可能被管制或封锁。 这里有个坑，或许是我网络的问题，fallback 中的地址我全部连不上，这就导致一个很严重的后果，国外的网站全部无法命中，甚至 AppStore 都挂了。。。后面通过调整日志等级到 DEBUG 才知道是 DNS 解析的问题。除非你所在的地区 DNS 污染特别严重，否则非常不建议使用 fallback，拖慢速度还不稳定，一般情况 fake-Ip 足够了。 fake-ip下面补充说明下这个 fake-ip 是什么东西； 虽然 Fake IP 这个概念早在 2001 年就被提出来了，但是到 Clash 提供 fake-ip 增强模式以后，依然有很多人对 Fake IP 这个概念以及其作用知之甚少。 参考：https://blog.skk.moe/post/what-happend-to-dns-in-proxy/ 当 TCP 连接建立时，Clash DNS 会直接返回一个保留地址的 IP（即 Fake IP；Clash 默认使用 198.18.0.0/16），同时 Clash 继续解析域名规则和 IP 规则。PS：开启增强模式后可以尝试 nslookup 看一下解析情况。 由于 TCP/IP 的协议特性，在应用发起 TCP 连接时，会先发出一个 DNS question（发一个 IP Packet），获取要连接的服务器的 IP 地址，然后直接向这个 IP 地址发起连接。在不使用代理的情况下，DNS 查询流程想必大家很熟悉了，如果使用了代理，直连模式下以使用 SOCKS5 代理的浏览器为例： 浏览器不再需要从自己的 DNS 缓存中寻找域名对应的 ip，因为已经有了 SOCKS5 代理，浏览器可以直接将域名封装在 SOCKS5 流量之中发往代理客户端（clash） 代理客户端从 SOCKS5 流量中抽出域名并设法获得解析结果 代理客户端将你的 SOCKS5 流量还原成标准的 TCP 请求 代理客户端将这个 TCP 连接建立起来，TCP 连接可以承载的是 HTTPS 传统上，大部分浏览器等应用都会调用系统的内置方法去解析域名，这时候如果你想做一些魔法操作，那么就是在系统这一层上，你可以在本地或者其他地方搭建一个黑魔法 DNS 服务器，然后设置系统的 DNS 为它，就可以实现一些黑魔法效果。 例如，在上面的 2 和 3 之间，可以插入一步：代理客户端使用 某种协议 将浏览器发出的 SOCKS5 的流量重组并发给远端服务器；远端服务器使用相同的协议还原，然后拿到域名，进行解析；这样就实现了域名在远端进行解析。这种就是非直连的方式代理，各有优劣，各自体会。 然后再说说分流；分流是一个麻烦事。一般情况下，你可能会需要使用域名进行分流（不论是白名单还是黑名单）。不过更多情况下你会使用到基于 IP 的规则来进行分流。这里可以通过 GUI 的界面来观察连接，如果 TUN 显示一个域名使用了大量端口占用了大量的 ip 池资源，可以考虑将它放到 fake-ip-filter 中不使用 fake-ip。 在域名规则下，如果判断是直连，那么代理客户端没必要进行 DNS 解析，交给原本的流程使用系统接口即可。在 ip 规则下，当然需要先解析域名，然后匹配规则，但是由于某种协议可以封装域名，因此最终还是会讲域名发给远端，也就是你本地匹配规则的 ip 与最终代理请求的 ip 可能并不是一个。 全局流量代理TUN全局流量代理可能会出现在路由器上或者 TUN/TAP 型的支持全局代理客户端上。用户不再主动为每个应用程序设置代理。此时应用程序是不会感知到代理客户端的存在，它们会正常的发起 TCP 连接，ClashX 的增强模式或者说 TUN 模式，会接管设备的 TCP 协议栈，并且由于 TCP/IP 协议在拿到 DNS 解析结果之前，连接是不能建立的。 这时候配合上面说的 DNS 解析过程就比较有趣了，前半部分不变，最终会调用系统接口进行解析域名，这时候会向系统配置的 DNS 服务器发起请求；如果我们在系统的网络设置之中有设置上游 DNS 地址，例如代理客户端可能会修改系统设置中的 DNS 到 127.0.0.1 或者别的 IP、也可能保留用户之前的设置，这无所谓，因为… 操作系统发出的 DNS 解析请求会经过代理客户端并最终被截获；代理客户端可以将这个解析请求原样发出去、或者用自己的黑魔法，总之都会拿到一个解析结果；代理客户端将这个解析结果返回回去，操作系统拿到了这个解析结果并返回给浏览器 浏览器对这个解析结果的 IP 建立一个 TCP 连接并发送出去，这个 TCP 连接被代理客户端截获。由于之前代理客户端进行的 DNS 解析请求这一动作，代理客户端可以找到这个只包含目标 IP 的 TCP 连接原来的目标域名；如果是支持 redir 的代理客户端，那么代理客户端就会直接将域名和 TCP 连接中的其它数据封装成 某种协议 发给远端服务器；或者封装成 SOCKS5 后交给支持 SOCKS5 的代理客户端。 和应用程序直接将流量封装成 SOCKS5 大有不同，在类似于透明代理的环境下浏览器和其它应用程序是正常地发起 TCP 连接。因此除非得到一个 DNS 解析结果，否则 TCP 连接不会建立；代理客户端也会需要通过这个 DNS 查询动作，才能找到之后的 TCP 连接的域名。你大概能够发现，浏览器、应用程序直接设置 SOCKS5 代理的话，可以不在代理客户端发起 DNS 解析请求就能将流量发送给远端服务器；而在透明代理模式下，不论是否需要 IP 规则分流都需要先进行一次 DNS 解析才能建立连接。 有没有办法能像直接设置 SOCKS5 代理一样省掉一次 DNS 解析呢？有，就是代理客户端自己不先执行查询动作，丢一个 Fake IP 回去让浏览器、应用程序立刻建立 TCP 连接。 有了 Fake IP，代理客户端无需进行 DNS 解析。最后不论是浏览器、代理客户端还是远端服务器都不会去和 Fake IP 进行连接，因为在代理客户端这里就已经完成了截获、重新封装。即使按照域名规则分流，代理客户端都没有进行 DNS 解析的需要。只有在遇到了按照 IP 进行分流的规则时，代理客户端才需要进行一次解析拿到一个 IP 用于判断。即便如此，这个 IP 只用于分流规则的匹配，不会被用于实际的连接。 PS：Clash 的增强模式既有 redir-host 也有 Fake IP，目前流行的是 Fake IP 模式。 这里有个很有意思的问题，如果操作系统或者浏览器缓存了 Fake IP，但是代理客户端中 Fake IP 和域名的映射表丢失以后，会出现什么状况？可能会出现什么错误信息？你应该大概意识到 Clash 在 Fake IP 模式下偶发的无法上网的原因了。 在使用 Fake-ip 模式后，Application 拿到的是 Clash DNS 返回的 Fake IP，所以也不会出现某些应用程序拒绝连接一些 IP 的情况；和 redir-host 模式一样，在大部分情况下 fake-ip 模式下也可以完全无视 DNS 污染。 节点这部分可参考 SS-Rule，写的很好，或者可以看看官方推荐的文档或者 wiki，我因为没有订阅连接，只能自己手动配置，下面是我用到的几个：需要注意的是，在 v1.9 版本后，作者调整了配置的格式（改动很小），下面使用的是最新的个数，详情可以看官方的说明。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132proxies: # shadowsocks # 支持加密方式： # aes-128-gcm aes-192-gcm aes-256-gcm # aes-128-cfb aes-192-cfb aes-256-cfb # aes-128-ctr aes-192-ctr aes-256-ctr # rc4-md5 chacha20 chacha20-ietf xchacha20 # chacha20-ietf-poly1305 xchacha20-ietf-poly1305 - name: "ss1" type: ss server: server port: 443 cipher: chacha20-ietf-poly1305 password: "password" # udp: true # vmess # 支持加密方式：auto / aes-128-gcm / chacha20-poly1305 / none - name: "vmess" type: vmess server: server port: 443 uuid: uuid alterId: 32 cipher: auto # udp: true # tls: true # skip-cert-verify: true # servername: example.com # 优先级高于 wss host # network: ws # ws-opts: # path: /path # headers: # Host: v2ray.com # max-early-data: 2048 # early-data-header-name: Sec-WebSocket-Protocol - name: "vmess-http" type: vmess server: server port: 443 uuid: uuid alterId: 32 cipher: auto # udp: true # network: http # http-opts: # # method: "GET" # # path: # # - '/' # # - '/video' # # headers: # # Connection: # # - keep-alive - name: vmess-grpc server: server port: 443 type: vmess uuid: uuid alterId: 32 cipher: auto network: grpc tls: true servername: example.com # skip-cert-verify: true grpc-opts: grpc-service-name: "example" # socks5 - name: "socks" type: socks5 server: server port: 443 # username: username # password: password # tls: true # skip-cert-verify: true # udp: true # http - name: "http" type: http server: server port: 443 # username: username # password: password # tls: true # https # skip-cert-verify: true # Trojan - name: "trojan" type: trojan server: server port: 443 - name: "trojan" type: trojan server: server port: 443 password: yourpsk # udp: true # sni: example.com # aka server name # alpn: # - h2 # - http/1.1 # skip-cert-verify: true - name: trojan-grpc server: server port: 443 type: trojan password: "example" network: grpc sni: example.com # skip-cert-verify: true udp: true grpc-opts: grpc-service-name: "example" - name: trojan-ws server: server port: 443 type: trojan password: "example" network: ws sni: example.com # skip-cert-verify: true udp: true # ws-opts: # path: /path # headers: # Host: example.com 我用的是原版 SS，SSR 上面没贴，如果需要支持 UDP，需要手动开启；Trojan 也支持，听说很牛逼，vmess 如果效果还是不理想可以切换试试看，我暂时还没用过。我主力就是 vmess，订阅模式使用 proxy-providers 来定义，体验应该最好，使用的时候通过 use 关键字。 123456789101112131415161718192021222324252627proxy-groups: - name: Proxy type: url-test use: - provider1proxy-providers: provider1: type: http # 使用 url 在线订阅 url: "url" interval: 3600 path: ./conf/provider1.yaml health-check: enable: true interval: 600 url: http://cp.cloudflare.com/generate_204 test: type: file # 从文件中读取 path: /test.yaml # 可以使用正则来过滤节点 filter: '(香港|台湾|美国).*' health-check: enable: true interval: 36000 url: http://www.gstatic.com/generate_204 使用 proxy-providers 省去了我们自己维护 proxy 节点，直接从在线或者本地文件读取 proxy 节点信息，其他规则还是我们自己定义，顺便提一嘴，如果你没机场只是偶尔临时用，可以看看 proxypool 这个项目，从互联网爬取免费的节点，还有好心人提供了 proxy-providers 的在线地址，可以临时顶一顶，不过毕竟免费风险还是有的，这个自己取舍。 订阅模式需要注意的是拉取的不一定只有节点，包括代理组、规则集可能都有，这样就可能面临一个问题，如果你使用远程订阅，你自定义的一些规则等配置在下一次更新订阅后可能会丢失，使用 proxy-providers 是一个很不错的解决方案，我目前就是使用的这种方式来订阅多个机场，并且统一使用我自定义的配置，或者也可以尝试使用 Parser 规则解决。如果你的机场不提供 clash 订阅连接，可以使用在线服务进行转换，这个一搜一大堆不多说。 代理组这部分也很重要，配合上门的节点分组，避免一个节点挂掉还得手动换，选择最佳的节点连接。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051proxy-groups: # 代理的转发链, 在 proxies 中不应该包含 relay. 不支持 UDP. # 流量: clash &lt;-&gt; http &lt;-&gt; vmess &lt;-&gt; ss1 &lt;-&gt; ss2 &lt;-&gt; 互联网 - name: "relay" type: relay proxies: - http - vmess - ss1 - ss2 # url-test 可以自动选择与指定 URL 测速后，延迟最短的服务器 - name: "auto" type: url-test proxies: - ss1 url: 'http://www.gstatic.com/generate_204' interval: 300 - name: "auto" type: url-test # 使用订阅节点 use: - provider1 tolerance: 300 # fallback 可以尽量按照用户书写的服务器顺序，在确保服务器可用的情况下，自动选择服务器 - name: "fallback-auto" type: fallback proxies: - ss1 url: 'http://cp.cloudflare.com/generate_204' interval: 300 # load-balance 可以使相同 eTLD 请求在同一条代理线路上 - name: "load-balance" type: load-balance proxies: - vmess1 url: 'http://www.youtube.com/generate_204' interval: 300 # select 用来允许用户手动选择 代理服务器 或 服务器组 # 您也可以使用 RESTful API 去切换服务器，这种方式推荐在 GUI 中使用 - name: Proxy type: select proxies: - ss1 - ss2 - vmess1 - auto 这里注意 Proxy 这个关键组，GUI 默认使用这个（其实是后面的规则配的是这个），它的类型是 select 可以允许我们在 GUI 中手动选择一个节点或者组，默认我使用 auto，也就是 url-test 模式的。如果你在配置文件中设置了 tolerance，Clash 将会计算所有代理服务器的延迟时间，然后以最快的代理服务器的延迟时间为基准，根据 tolerance 的值来筛选其他的代理服务器。只有当一个代理服务器的延迟时间小于基准延迟时间加上 tolerance 时，它才会被选择作为请求的代理服务器，换句话说就是只要在这个范围就不会自动切换，以避免频繁切换带来的体验差。 规则集这个功能只有 Pro 版本才支持，普通版只有规则；简单说就是一组规则的集合，只不过可以在线获取，定时更新，也就是可以直接用别人写好的分流规则，非常爽啊： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465rule-providers: apple-direct: type: http behavior: classical url: "https://cdn.jsdelivr.net/gh/Hackl0us/SS-Rule-Snippet@master/Rulesets/Clash/Basic/Apple-direct.yaml" path: ./ruleset/Apple-direct.yaml interval: 3600 icloud: type: http behavior: domain url: "https://cdn.jsdelivr.net/gh/Loyalsoldier/clash-rules@release/icloud.txt" path: ./ruleset/icloud.yaml interval: 86400 apple: type: http behavior: domain url: "https://cdn.jsdelivr.net/gh/Loyalsoldier/clash-rules@release/apple.txt" path: ./ruleset/apple.yaml interval: 86400 private: type: http behavior: domain url: "https://cdn.jsdelivr.net/gh/Loyalsoldier/clash-rules@release/private.txt" path: ./ruleset/private.yaml interval: 86400 cn: type: http behavior: classical url: "https://cdn.jsdelivr.net/gh/Hackl0us/SS-Rule-Snippet@master/Rulesets/Clash/Basic/CN.yaml" path: ./ruleset/CN.yaml interval: 3600 ad-keyword: type: http behavior: classical url: "https://cdn.jsdelivr.net/gh/Hackl0us/SS-Rule-Snippet@master/Rulesets/Clash/Basic/common-ad-keyword.yaml" path: ./ruleset/common-ad-keyword.yaml interval: 3600 foreign: type: http behavior: classical url: "https://cdn.jsdelivr.net/gh/Hackl0us/SS-Rule-Snippet@master/Rulesets/Clash/Basic/foreign.yaml" path: ./ruleset/foreign.yaml interval: 3600 telegram: type: http behavior: classical url: "https://cdn.jsdelivr.net/gh/Hackl0us/SS-Rule-Snippet@master/Rulesets/Clash/App/social/Telegram.yaml" path: ./ruleset/Telegram.yaml interval: 3600 lan: type: http behavior: classical url: "https://cdn.jsdelivr.net/gh/Hackl0us/SS-Rule-Snippet@master/Rulesets/Clash/Basic/LAN.yaml" path: ./ruleset/LAN.yaml interval: 3600 microsoft: &#123;type: http, behavior: classical, path: ./Filter/Microsoft, url: https://cdn.jsdelivr.net/gh/Semporia/Clash-X@master/Filter/Microsoft.yaml, interval: 3600&#125; 然后配置下最终规则，整个配置就算完成了： 123456789101112131415161718rules: - RULE-SET,private,DIRECT - RULE-SET,icloud,DIRECT - RULE-SET,apple,DIRECT - RULE-SET,microsoft,DIRECT - DOMAIN,clash.razord.top,DIRECT - DOMAIN,yacd.haishan.me,DIRECT - RULE-SET,apple-proxy,Proxy - RULE-SET,apple-direct,DIRECT - RULE-SET,cn,DIRECT - RULE-SET,ad-keyword,REJECT - RULE-SET,foreign,Proxy - RULE-SET,telegram,Proxy - RULE-SET,lan,DIRECT - GEOIP,CN,DIRECT - MATCH,Proxy# - RULE-SET,apple,DIRECT,no-resolve 这里说下 Clash 支持的几种规则； DOMAIN-SUFFIX：域名后缀匹配 DOMAIN：域名匹配 DOMAIN-KEYWORD：域名关键字匹配 IP-CIDR：IP段匹配 SRC-IP-CIDR：源IP段匹配 GEOIP：GEOIP 数据库（国家代码）匹配 DST-PORT：目标端口匹配 SRC-PORT：源端口匹配 PROCESS-NAME：源进程名匹配 RULE-SET：Rule Provider 规则匹配 MATCH：全匹配 写法上面已经有示例了，特别的情况，如果你不想让 Clash 进行 DNS 解析，可以在后面加上 no-resolve。自带的两种规则是 REJECT 和 DIRECT，很好理解，广告相关的一般直接使用 REJECT，不需要代理的就使用 DIRECT，剩下的就需要指定我们配的代理组，例如本例的 Proxy。MATCH需要位于规则列表末尾，除了那些漏网之鱼。 脚本同样，这也是 Pro 的专有功能，除了全局、直连、规则，还增加了一个更灵活的脚本模式，来应对日益增多的 Rule 规则。但是目前用的人很少，我也没这个需求，暂不关注。如果有更个性化的节点处理需求，可以尝试使用 parsers。 派生如果 Clash 不能满足你的需求，可以尝试 Clash.Meta 版本，对应的 GUI 推荐 clash-verge，可能会满足你的需求。 参考规则这里推荐几个开箱即用的规则： Profilesclash-rulesSS-Rule-SnippetGeoIP2-CN]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java核心编程知识索引]]></title>
    <url>%2F2020%2F07%2F03%2FJava%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E7%9F%A5%E8%AF%86%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[这篇索引并不会介绍过多语言层面的基础概念，侧重点主要在于 JVM 和 JUC 这两块，属于 Java 中比较进阶的知识了，我的知识也主要来自于两本书：《深入理解Java虚拟机》和《Java并发编程的艺术》以及网上搜集的资料，这两本书写的非常棒，入门是够了，通篇基本没废话，我的笔记也基本都是摘录自这两本书。也正是如此，相关总结就直接放到 Github 了，不抄到这里了，这一篇主要来列一个大纲，以及做一些补充。 相关笔记点击可直达，基本是我自己总结的书中的重点（我觉得），当然一些太高深我理解不了的东西也跳过了，比如 JVM 的源码，C++、汇编、编译原理相关的。 而 Java 语言基础这里基本不涉及，大多数都是内建 API 和语法之类的东西，有个两三年学习经验的基本都比较熟的，不同语言之间差距也不大，复盘可参考笔记『Java基础复习计划』和我博客里搜索『Java基础』关键字，配合下面 JVM 等相关知识，会有更深入的理解。 由于目前 Java8 已经进入大众阶段，也不再单独介绍相关特性，复盘还是参考我的笔记和博客亦或者《Java8实战》这本书，之前写过相关笔记。 深入理解JVM(一)内存区域划分、GC 前置基础知识、JDK7 之前的内置 GC 解析。 深入理解JVM(二)类文件结构、类加载过程 深入理解JVM(补全)类文件相关补充、常量池、字节码指令、JIT 优化、并发与锁等 一入Java深似海总结自小马哥的课程，对 Java 基础的深入探讨，另一种不同的角度去看待 Java。 Java11 FeaturesJava 8 - 11 新特性总结 Java并发编程《Java并发编程的艺术》笔记 深入JVM第三版补充（未完成） 重构的艺术教你如何优雅的优化你写的代码，除了《重构》另外一本比较老的书没事也可以看看《代码整洁之道》 主要还是侧重 JVM、JUC、Java 基础这些东西，上面的笔记都挺长的，平均每一篇也得至少 1w+ 字，我觉得可都是精华。 JVM知识点算是提纲吧，看着这个能说出来就不错了，忘记了就去相关笔记里翻吧。 运行时数据区域划分与职责方法区、程序计数器、虚拟机栈、本地方法栈、堆；HotSpot 的实现。永久代与直接内存（元空间） 对象访问定位句柄、直接指针 GC基础判断对象死活：引用计数（循环引用）、可达性分析、GCRoots；引用：强软弱虚；拓展之 ThreadLocal、直接引用；方法区回收；HotSpot 实现：枚举根节点 STW、安全点（抢先式中断、主动式中断）、安全区域；MinorGC、MajorGC/FullGC、动态年龄； GC方案分代收集、标记清除、复制算法（担保机制）、标记整理；收集器：Serial、ParNew、Parallel Scavenge、Serial Old、Parallel Old、CMS（标记清除、低停顿）、G1；新收集器：Epsilon、ZGC、Shenandoah JDK提供的分析工具经典：jps、jinfo、jstat、jstack、jconsole、jmap、VisualVM；新增：JFR、JMC、jshell 类文件8 字节为单位二进制流、魔数、常量池（u2 自定义大小，包含字面量[表类型]和符号引用）、访问标志、字段表（字段重载）、方法表（重载）、属性表（严格顺序，可定制，其他表引用，32 位 solt 复用，u4/u2 方法长度 Code）、字节码指令（一个字节，非填充对齐，精简 int） JVM 加载机制加载、连接（验证、准备、解析）、初始化、使用、卸载；零值、init/cinit、符号引用、直接引用、对象创建；类加载器：双亲委派模型、Bootstrap ClassLoader、Extension ClassLoader、Application ClassLoader、破坏（模块化与动态性、OSGi、JDBC、SPI、JNDI） 字节码执行引擎基于栈（可移植，紧凑、实现简单、速度慢）、基于寄存器、32 bit solt（64 位的原子性问题）、动态连接、虚方法、非虚方法、动态分派（重写覆盖）、静态分派（多态）；重叠操作数栈复用； 编译器（早期）优化词法分析、语法分析（AST）、语法糖、注解处理器（Lombok 生成）、语义分析与字节码生成、自动拆箱、循环遍历（迭代器）；方案：常量折叠、final 校验、解语法糖、字节码提升/生成（字符串拼接 SB）、条件编译、泛型擦除；（动态）提前编译（AOT）：JDKJaotc、GCJ 等；以及 Android 的 ART（蹂躏 Dalvik） 运行时（晚期）优化解释器+编译器混合模式、逃生门（逆优化）、Client Compiler（c1）、Server Compiler（c2）、分层编译（3 层）、JIT 基于方法；热点代码：方法（方法调用计数器）、循环（回边计数器，跑分）、基于采样热点探测（除前两种的其他）、基于计数热点探测、热度衰减、栈上替换（OSR）编译；c1：简单的三段式编译，速度快，优化例如：方法内联、消除访问冗余、复写传播、无用代码消除。c2：JIT + 监控，激进优化+逃生门，公共子表达式消除、数组边界检查消除、方法内联、逃逸分析（方法、线程逃逸，栈上分配、同步消除、标量替换） 锁优化适应性自旋、锁消除、锁粗化、轻量级锁、偏向锁、锁升级（锁膨胀） 在《深入理解JVM》中也提到过一些 JUC 的知识，不过仅仅是入门，这里就不列了放到 JUC 条目中。 选择 GC 的指导原则： 如果应⽤属于⼩规模数据应⽤（内存资源⼤概在 100 MB 左右）的话，那么串⾏收集器是⼀种不错的选择（-XX:+UseSerialGC） 如果应⽤运⾏在单处理器并对停顿时间不敏感的话，那么它可以考虑串⾏收集器（XX:+UseSerialGC） 如果应⽤属于性能敏感但停顿时间要求不⾼（如停顿⼀秒以上）的话，那么它可以选择并⾏收集 器（-XX:+UseParallelGC） 如果应⽤认为响应时间⽐吞吐量和停顿时间更为重要的话，那么它可以考虑 CMS 或 G1 收集器 当应⽤在单处理器中运⾏时，Parallel Collector 不会⽐ Serial Collector 表现更好，因为并⾏ 执⾏反⽽需要额外的开销（如同步和线程切换等） 当应⽤在双处理器中运⾏时，即使 Heap 达到中型或⼤型的空间，Serial Collector 可能是最 合适的选择。 Parallel Collector ⽬标调整： 最⼤垃圾收集停顿时间（Pause Times），默认无限制 吞吐量（Throughput），决定于垃圾收集与其他应⽤执⾏时间的占⽐ 通过 JVM 参数 -XX:GCTimeRatio=&lt;N&gt; 调整 GC 时间与⾮ GC 时间的⽐重，GC 停顿时间等于 1 / N+1 N 默认值为 99，意味着 GC 时间占 1/100 的应⽤执⾏时间 内存⾜迹（Footprint），指定各空间的大小。 JVM 默认的优先级顺序就是如上，Parallel Collector 每次垃圾收集过后，收集器将会保存并更新相关的统计信息，如平均的停顿时间。 Java基础这里暂时先说一下比较容易忽视的知识点或者骚操作，那些面试常客，基本的就略过了。 语言基础Java 访问修饰符是否四种？（JDK9 前后）、异常打印建议限制堆栈的深度；Java 是否是纯面向对象的？（原生类型）String 是否是不可变？（反射）、评价 Clone 方法（设计缺陷）、集合的线程安全（View）；可以在 final 修饰类中定义抽象方法？（枚举、字节码提升）；取余转为位运算（限制条件）、函数式编程、ClassLoader 加载（异常）、GC选择 集合框架ConcurrentHashMap 或者说线程安全的 Map，为什么不允许 null 元素？（消歧义）；Set、List、Map、Deque、Navigable*、Array/Linked/Tree；单例扩展：Collections.singleton*、空集合、转换集合接口、列举集合接口（of）；集合包装（Collections）：只读（unmodifiable）、同步（synchronized）、类型安全（checked，避免泛型擦除影响）；特殊集合：优先级队列 PriorityQueue，枚举 Set： EnumSet；覆盖 equals 是否要覆盖 hashcode（建议），反之呢（必须），集合的比较重复逻辑（hashcode &amp;&amp; (== || equals)）鉴定 Map：IdentityHashMap（覆盖了 hashcode）MethodHandle；是否有必要覆盖 POJO 的 hashcode 和 eq？（用于 map、比较、排序建议覆盖，默认的递归层次深）；自带排序：插入（7-）、快排（7+）、Tim排序（对象类型，更多的数量） 并发基础ReentrantLock、ReentrantReadWriteLock、StampedLock（读多写少，读写锁升级版）；线程池（默认的『无限』等待队列也不是很要紧，Tomcat、Nginx 这种会挡掉大部分）；CountDownLatch、CyclicBarrier、Semaphore 、ThreadPoolExecutor 和 ScheduledExecutorService；并发集合：CopyOnWrite、ConcurrentSkipList、ConcurrentHashMap、BlockingQueue、Fork/Join、CompletableFuture、Flow（JDK9）、大小计算（并发集合中 size 一般是需要计算的）、InterruptedException；其他参考并发编程的列举 Java元编程JavaBeans（参考 Spring 的 ApplicationContextEvent）、Introspection、APT（Lombok、META-INF） I/ONIO2、Fluent API（Builder API、Chain API）、try-with-resources（AutoCloseable）；NIO：Buffers（0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity）、Channels、Selectors、flip、rewind、Channels；其他：压缩（compact）、标记（mark，mark 设置为 position）、复制（duplicate）等 其他ListIterator、快速失败（fail-fast）、失败安全（fail-safe）、user.dir、-XX:+PrintFlagsFinal 并发编程这里提一句，美团技术团队的博客写的不错，个人比较推崇的 Java 国内圈俩神仙：美团技术团队和阿里中间件团队。 处理器实现原子操作锁缓存、锁总线、Java 中 CAS 的缺点 再谈Volatile缓存一致性协议，总线嗅探、缓存行对齐（JIT 无效化）、JVM 通过 lock 指令实现 再谈Synchronized偏向锁获得和撤销、轻量级锁及膨胀、优缺点与使用场景 JMM解决主要矛盾（如何通讯、如何同步）、解决方案（共享内存和消息传递）、抽象概念；三种重排、内存屏障、happens-before 规则、顺序一致性、final 域（static、Synchronized） AQS等待/通知经典范式（唤醒要继续判断）、等待超时模式、Lock 接口；AQS 使用方式、可重写方法、模版方法（三类）、同步队列、独占式、共享式、何时阻塞、读写锁；LockSupport 工具类、Condition 接口、等待队列（多对一） JUCConcurrentHashMap 原理（Java8 前后）、ConcurrentLinkedQueue（HOPS 优化更新频率）、长度计算；线程池、FutureTask JVM性能监控与调优我们并不能保证写的代码都是安全、高效的，尤其是时间紧的情况下，很多时候生产环境下跑才会发现问题，这时候如果有服务器权限并且我们有能力找出问题并解决，那么也是一个非常棒的技能。最常见的就是 GC （内存泄漏）和线程（死锁、死循环）相关引起的问题。 TBD 临时参考博客的这篇文章]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang之并发编程]]></title>
    <url>%2F2020%2F07%2F02%2FGolang%E4%B9%8B%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[接上次的基础语法部分，还剩下的主要是并发相关的内容，现在补全；也略微提及了点测试相关的内容；在 Go 中，并发程序主要使用的是 Go 的 goroutine 和 channel；相对传统的语言，简化了很多，调度上的调整也使程序高效了很多；没讲到的应该就是反射和 Web，这两块以后随用随学吧，目前这些东西也够了。没有贴太多的代码，以及剩下的 Go 标准库使用的相关代码都放到这个仓库了： bfchengnuo/GoCS goroutineGo 语言的并发主要通过 goroutine 实现。goroutine 类似于线程，属于用户态的线程，我们可以根据需要创建成千上万个 goroutine 并发工作。goroutine 是由 Go 语言的运行时（runtime）调度完成，而线程是由操作系统调度完成。Go 语言还提供 channel 用于在多个 goroutine 间进行通信。goroutine 和 channel 是 Go 语言秉承的 CSP（Communicating Sequential Process）现代并发模式的重要实现基础。 传统上我们实现并发编程的时候，我们通常需要自己维护一个线程池，并且需要自己去包装一个又一个的任务，还要处理好调度问题，一不小心就出问题，还难以调试；那么能不能有一种机制，程序员只需要定义很多个任务，让系统去帮助我们把这些任务分配到 CPU 上实现并发执行呢？Go 语言中的 goroutine 就是这样一种机制，goroutine 的概念类似于线程，但 goroutine 是由 Go 的运行时（runtime）调度和管理的。Go 程序会智能地将 goroutine 中的任务合理地分配给每个CPU。Go 语言之所以被称为现代化的编程语言，就是因为它在语言层面已经内置了调度和上下文切换的机制。 在 Go 语言编程中你不需要去自己写进程、线程、协程，你的技能包里只有一个技能：goroutine，当你需要让某个任务并发执行的时候，你只需要把这个任务包装成一个函数，开启一个 goroutine 去执行这个函数就可以了，就是这么简单粗暴。Go 语言中使用 goroutine 非常简单，只需要在调用函数的时候在前面加上 go 关键字，就可以为一个函数创建一个 goroutine。一个 goroutine 必定对应一个函数，可以创建多个 goroutine 去执行相同的函数。 主函数也是运行在一个 goroutine 中，我们称为 main goroutine；当主函数返回时，所有的 goroutine 都会被直接打断，程序退出，这也算是一种终结 goroutine 的方式。另一种比较友好的方式就是使用 goroutine 之间的通信来告知其他 goroutine 自行结束。 1234567891011121314151617var wg sync.WaitGroup// Go 的并发主要依赖 goroutine 和 channelfunc main() &#123; for i := 0; i &lt; 10; i++ &#123; wg.Add(1) // 计数 +1 go hello(i) // 值传递，copy &#125; // 等待所有 wg 完成 wg.Wait()&#125;func hello(i int) &#123; defer wg.Done() // goroutine 结束就登记 -1 fmt.Println("Hello Goroutine!", i)&#125; 为了避免主线程结束其他打断 goroutine，暂时使用了 sync 的 WaitGroup 进行计数等待。 可增长栈OS 线程（操作系统线程）一般都有固定的栈内存（通常为 2MB）,一个 goroutine 的栈在其生命周期开始时只有很小的栈（典型情况下 2KB），goroutine 的栈是不固定的，他可以按需增大和缩小，goroutine 的栈大小限制可以达到 1GB，虽然极少会用到这个大。所以在 Go 语言中一次创建十万左右的 goroutine 也是可以的。 GPM调度GPM 是 Go 语言运行时（runtime）层面的实现，是 go 语言自己实现的一套调度系统。区别于操作系统调度 OS 线程。 G 很好理解，就是 goroutine，里面除了存放本 goroutine 信息外 还有与所在 P 的绑定等信息。 P 管理着一组 goroutine 队列，P 里面会存储当前 goroutine 运行的上下文环境（函数指针，堆栈地址及地址边界），P 会对自己管理的 goroutine 队列做一些调度（比如把占用 CPU 时间较长的 goroutine 暂停、运行后续的 goroutine 等等）当自己的队列消费完了就去全局队列里取，如果全局队列里也消费完了会去其他 P 的队列里抢任务。 M（machine）是 Go 运行时（runtime）对操作系统内核线程的虚拟， M 与内核线程一般是一一映射的关系， 一个 groutine 最终是要放到 M 上执行的； P 与 M 一般也是一一对应的。他们关系是： P 管理着一组 G 挂载在 M 上运行。当一个 G 长久阻塞在一个 M 上时，runtime 会新建一个 M，这时 P 会把其他阻塞的 G 挂载在新建的 M 上。当耗时的 G 阻塞完成或者认为其已经死掉时，会回收旧的 M。 P 的个数是通过 runtime.GOMAXPROCS 设定（最大 256），Go1.5 版本之后默认为物理线程数。 在并发量大的时候会增加一些 P 和 M，但不会太多，切换太频繁的话得不偿失；这个值可以理解为有多少个系统线程同时执行 Go 代码。 Go 运行时的调度器使用 GOMAXPROCS 参数来确定需要使用多少个 OS 线程来同时执行 Go 代码。默认值是机器上的 CPU 核心数。例如在一个 8 核心的机器上，调度器会把 Go 代码同时调度到 8 个 OS 线程上。Go 语言中可以通过 runtime.GOMAXPROCS() 函数设置当前程序并发时占用的 CPU 逻辑核心数。Go1.5 版本之前，默认使用的是单核心执行。Go1.5 版本之后，默认使用全部的 CPU 逻辑核心数（跑满！）。 单从线程调度讲，Go 语言相比起其他语言的优势在于 OS 线程是由 OS 内核来调度的，goroutine 则是由 Go 运行时（runtime）自己的调度器调度的，这个调度器使用一个称为 m:n 调度的技术（复用/调度 m 个 goroutine 到 n 个 OS 线程）。 goroutine 的调度是在用户态下完成的，不涉及内核态与用户态之间的频繁切换，包括内存的分配与释放，都是在用户态维护着一块大的内存池，不直接调用系统的 malloc 函数（除非内存池需要改变），成本比调度 OS 线程低很多。另一方面充分利用了多核的硬件资源，近似的把若干 goroutine 均分在物理线程上，再加上本身 goroutine 的超轻量，保证了 go 调度方面的性能。 Go 中的 goroutine 与操作系统线程的区别： 一个操作系统线程对应用户态多个 goroutine go 程序可以同时使用多个操作系统线程 goroutine 和 OS 线程是多对多的关系，即 m:n channel虽然可以使用共享内存进行数据交换，但是共享内存在不同的 goroutine 中容易发生竞态问题。为了保证数据交换的正确性，必须使用互斥量对内存进行加锁，这种做法势必造成性能问题。Go 语言并发编程模型提倡通过通信共享内存而不是通过共享内存而实现通信。可以将 channel 看作 goroutine 之间的连接。channel 是可以让一个 goroutine 发送特定值到另一个 goroutine 的通信机制。channel 遵循先进先出（FIFO），保证收发数据的顺序，并且 channel 具有具体的类型，一个 channel 只允许同一种类型通过；它可以进行比较，如果引用的是相同对象即为真。通道有发送（send）、接收(receive）和关闭（close）三种操作。发送和接收都使用 &lt;- 符号。 123456789101112131415161718func main() &#123; // 定义 var ch1 chan int // 第二个参数是缓冲区大小，可选 var ch2 = make(chan int, 20) fmt.Println(ch1 == nil) // 零值，必须使用 make 初始化后才能使用 // send ch2 &lt;- 233 // receive fmt.Println(&lt;-ch2) // 丢弃 &lt;-ch2 // 手动关闭，非必须，可由 GC 感知回收 close(ch2)&#125; 只有在通知接收方 goroutine 所有的数据都发送完毕的时候才需要关闭通道。通道是可以被垃圾回收机制回收的，它和关闭文件是不一样的，在结束操作之后关闭文件是必须要做的，但关闭通道不是必须的。对一个关闭的通道来说： 发送值就会导致 panic。 进行接收会一直获取值直到通道为空。 如果没有值的通道执行接收操作会得到对应类型的零值。 关闭一个已经关闭的通道会导致 panic。 通道分为带缓存和无缓存，或者叫缓冲，区别就是 make 的时候传没传第二个参数，这两种有点细微区别； 无缓存通道无缓冲的通道又称为阻塞的通道，因为向无缓存通道发送数据，必须要有人在接收值，否则会一直阻塞。当两个 goroutine 进行无缓存通道通信时，就会导致发送接收的同步化，所有又被称为是『同步通道』；通过 channel 可以将多个 goroutine 串联起来。 有缓存通道可以看作里面维护了一个队列，可以使用内置的 len 函数获取通道内元素的数量，使用 cap 函数获取通道的容量，虽然我们很少会这么做。其他方面与无缓存类似，只不过是通道满了以后才阻塞； 从管道取内容的时候，为了避免关闭后取完一直是零值，我们可以使用 for-range 的方式，取完之后会自动结束；虽然使用 ok 判断也能实现，但是 range 的方式更加优雅。 单方向的通道多个 goroutine 使用通道进行传值的时候，很多情况是单向的，为了避免乱传，可以使用 Go 提供的单向通道： 12345678// chan&lt;- int 单向向通道输出// x &lt;- chan int 单向从通道输出func squarer(out chan&lt;- int, in &lt;-chan int) &#123; for i := range in &#123; out &lt;- i * i &#125; close(out)&#125; 在函数传参及任何赋值操作中可以将双向通道转换为单向通道，但反过来是不可以的。 select多路复用Go 内置了 select 关键字，可以同时响应多个通道的操作。它使用类似于 switch 语句，它有一系列 case 分支和一个默认的分支。每个 case 会对应一个通道的通信（接收或发送）过程。 select 会一直等待，直到某个 case 的通信操作完成时，就会执行 case 分支对应的语句。 1234567891011121314151617func func1() &#123; ch := make(chan int, 1) for i := 0; i &lt; 10; i++ &#123; select &#123; case x := &lt;-ch: // 0 2 4 6 8，因为大小为 1 fmt.Println(x) case ch &lt;- i: fmt.Println("send ", i) default: fmt.Println("默认操作") &#125; &#125; time.Sleep(time.Duration(5) * time.Second)&#125; 使用多路复用使代码更易读，并且有以下特性： 可处理一个或多个 channel 的发送/接收操作。 如果多个 case 同时满足，select 会随机选择一个。 对于没有 case 的 select{} 会一直等待，可用于阻塞 main 函数。 并发退出并发退出的情况是很常见的，让主线程直接结束的方式并不优雅，那就是最好通过通信完成；但是 channel 的消息被消费后其他的『线程』就获取不到了，所以，我们采用 close channel 的方式来广播退出通知： 12345678910var done = make(chan struct&#123;&#125;)// 通过 close chan 并发退出func cancelled() bool &#123; select &#123; case &lt;-done: return true default: return false &#125;&#125; 这样在其他的『线程』中使用循环来 if 这个函数即可，主线程将 done 进行 close 其它的就会接受到这个信号，从而退出。 锁当设计操作共享变量的时候，自然就需要用到锁，虽然建议尽量使用 channel 完成逻辑；但是锁总是不可避免的； 互斥锁能保证只有一个 goroutine 进入临界区，唤醒策略是随机的；互斥锁一般使用 sync.Mutex 的 lock 和 unlock 方法； 读写互斥锁适用于读多写少的场景，读是不需要加锁的；即如果 goroutine 获取的是读锁，其他 goroutine 还可以获得读锁进行读取；如果 goroutine 获取了写锁，其他 goroutine 都需要等待；读写锁使用 sync.RWMutex 的 lock/unlock 是写锁，rlock/runlock 是读锁； Go 语言中可以使用 sync.WaitGroup 来实现并发任务的同步；需要注意sync.WaitGroup是一个结构体，传递的时候要传递指针。请注意，Go 中没有可重入锁的概念，请尽量避免使用，否则会导致死锁；同时也要注意可见性的问题，在多核 CPU 执行期间，互相的缓存是不可见的。 sync.Once如果初始化消耗比较大，那么将初始化延迟进行是个不错的选择，并且是一次性的，例如配置文件的读取，sync.Once 就是来做这个事情的。理论上来讲，一次性初始化需要一个互斥锁和一个布尔变量来记录是否初始化完成，还是牵扯指令重排的问题，避免获取到初始化一半的情况；例如用 once 实现的单例模式： 1234567891011type singleton struct &#123;&#125;var instance *singletonvar once sync.Oncefunc GetInstance() *singleton &#123; once.Do(func() &#123; instance = &amp;singleton&#123;&#125; &#125;) return instance&#125; sync.MapGo 默认提供的 map 是非并发安全的，所以在 sync 下提供了并发安全的 map；它开箱即用表示不用像内置的 map 一样使用 make 函数初始化就能直接使用；同时还内置了 Store、Load、LoadOrStore、Delete、Range 等操作方法（也必须使用这些方法才能保证安全）。 竞争检测但是我们不可能想的那么全面，总有一些漏网之鱼的竞争关系，这时候可以使用 Go 提供的工具来检查，只要在 build、run、test 命令后面加上 -race 的 flag，具体的使用方法参考：https://golang.google.cn/ref/mem 原子操作使用锁意味着上下文的切换都资源的消耗，针对基本类型我们还可以使用原子操作来保证并发安全，原子操作是 Go 语言提供的方法它在用户态就可以完成，因此性能比加锁操作更好。Go 语言中原子操作由内置的标准库 sync/atomic 提供。 方法 解释 func LoadInt32(addr int32) (val int32) func LoadInt64(addr int64) (val int64) func LoadUint32(addr uint32) (val uint32)func LoadUint64(addr uint64) (val uint64)func LoadUintptr(addr uintptr) (val uintptr)func LoadPointer(addr unsafe.Pointer) (val unsafe.Pointer) 读取操作 func StoreInt32(addr int32, val int32) func StoreInt64(addr int64, val int64) func StoreUint32(addr uint32, val uint32) func StoreUint64(addr uint64, val uint64) func StoreUintptr(addr uintptr, val uintptr) func StorePointer(addr unsafe.Pointer, val unsafe.Pointer) 写入操作 func AddInt32(addr int32, delta int32) (new int32) func AddInt64(addr int64, delta int64) (new int64) func AddUint32(addr uint32, delta uint32) (new uint32) func AddUint64(addr uint64, delta uint64) (new uint64) func AddUintptr(addr *uintptr, delta uintptr) (new uintptr) 修改操作 func SwapInt32(addr int32, new int32) (old int32) func SwapInt64(addr int64, new int64) (old int64) func SwapUint32(addr uint32, new uint32) (old uint32) func SwapUint64(addr uint64, new uint64) (old uint64) func SwapUintptr(addr uintptr, new uintptr) (old uintptr) func SwapPointer(addr unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer) 交换操作 func CompareAndSwapInt32(addr int32, old, new int32) (swapped bool) func CompareAndSwapInt64(addr int64, old, new int64) (swapped bool) func CompareAndSwapUint32(addr uint32, old, new uint32) (swapped bool) func CompareAndSwapUint64(addr uint64, old, new uint64) (swapped bool) func CompareAndSwapUintptr(addr uintptr, old, new uintptr) (swapped bool) func CompareAndSwapPointer(addr unsafe.Pointer, old, new unsafe.Pointer) (swapped bool) 比较并交换操作 测试Go 语言中的测试依赖 go test 命令。编写测试代码和编写普通的 Go 代码过程是类似的，并不需要学习新的语法、规则或工具。在包目录内，所有以 _test.go 为后缀名的源代码文件都是 go test 测试的一部分，不会被 go build 编译到最终的可执行文件中。在测试文件中有三种类型的函数： 类型 格式 作用 测试函数 函数名前缀为 Test 测试程序的一些逻辑行为是否正确 基准函数 函数名前缀为 Benchmark 测试函数的性能 示例函数 函数名前缀为 Example 为文档提供示例文档 go test 命令会遍历所有的 *_test.go 文件中符合上述命名规则的函数，然后生成一个临时的 main 包用于调用相应的测试函数，然后构建并运行、报告测试结果，最后清理测试中生成的临时文件。 123456789// 测试函数名必须以Test开头，必须接收一个 *testing.T 类型参数func TestSplit(t *testing.T) &#123; got := Split("a:b:c", ":") want := []string&#123;"a", "b", "c"&#125; // 期望的结果 // 因为 slice 不能比较直接，借助反射包中的方法比较 if !reflect.DeepEqual(want, got) &#123; t.Errorf("excepted:%v, got:%v", want, got) // 测试失败输出错误提示 &#125;&#125; 关于测试的内容暂时不去看太多了，先能玩起来再说，再加上标准库基本就可以写东西了。 参考《Go语言圣经》https://www.liwenzhou.com/posts/Go/14_concurrence/]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebFlux基础]]></title>
    <url>%2F2020%2F06%2F02%2FWebFlux%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[响应式（或者叫反应式）的异步非阻塞编程模式，大概率是未来的主流，函数式编程是基础（所以要求 Java8+），可以看作是观察者模式（或者说生产者消费者模式）的延伸。Spring 5 中最重要改动是把反应式编程的思想应用到了框架的各个方面，Spring 5 的反应式编程以 Reactor 库为基础，之前我其实已经过了把瘾（然后发现忘的差不多了，所以来复盘），毕竟 Spring5 也已经出来很久了，更不要说 RxJava，不过也仅仅是尝鲜，这一篇整理下相关基础知识入个门。开发人员可以使用 WebFlux 创建高性能的 Web 应用和客户端（包括其中的 HTTP、服务器推送事件和 WebSocket 支持）。在 SpringBoot2 中也跟进了 WebFlux 的支持，默认使用 Netty，也可以切换 Servlet3.0+ 的容器。反应式编程主要解决的是吞吐量的问题（或者说内存、线程压力），而不是速度问题，一定要搞清楚这一点，意味着使用相同的资源可以处理更加多的请求。随着网络应用和微服务的不断发展，高并发、高吞吐量的特性越来越吸引人，Node、Go 显示出了强劲的竞争力，Java 需要更新自己来适应潮流，拥有了 WebFlux （或者说 Reactive Programming）就有了一战的底气。 比较尴尬的是例如 JDBC 等配套设施还没有太多的跟进，所以目前来看用的还不算多，不过这些问题迟早会解决。 WebFlux 简介WebFlux 模块的名称是 spring-webflux，名称中的 Flux 来源于 Reactor 中的类 Flux。该模块中包含了对反应式 HTTP、服务器推送事件和 WebSocket 的客户端和服务器端的支持。对于开发人员来说，比较重要的是服务器端的开发。在服务器端，WebFlux 支持两种不同的编程模型： Spring MVC 中使用的基于 Java 注解的方式； 基于 Java 8 的 lambda 表达式的函数式编程模型。 这两种编程模型只是在代码编写方式上存在不同。它们运行在同样的反应式底层架构之上，因此在运行时是相同的。 WebFlux 需要底层提供运行时的支持，WebFlux 可以运行在支持 Servlet 3.1 非阻塞 IO API 的 Servlet 容器上，或是其他异步运行时环境，如 Netty 和 Undertow。 对标 JSR-315 和 JSR-340，分别对应 Servlet 规范的 3.0 和 3.13.0 提供了异步化；而 3.1 提供了非阻塞。 最方便的构建 WebFlux 应用的方式是使用 SpringBoot 的初始化器，选择 Reactive Web 依赖。 Reactor简介Reactor 是一个基础库，可用它构建时效性流式数据应用，或者有低延迟和容错性要求的微/纳/皮级服务。简单说，Reactor 是一个轻量级 JVM 基础库，帮助你的服务或应用高效，异步地传递消息。Reactor 仅仅致力于解决异步和函数调用问题。和 Spring 天然无缝整合（毕竟 Reactor 框架是 Pivotal 公司开发的，实现了 Reactive Programming 思想）。 “高效”是指什么? 消息从 A 传递到 B 时，产生很少的内存垃圾，甚至不产生。 解决消费者处理消息的效率低于生产者时带来的溢出问题。 尽可能提供非阻塞异步流。 PS：Spring 5 其最大的意义就是能将反应式编程技术（它就是常见的观察者模式的一种延伸）的普及向前推进一大步。而作为在背后支持 Spring 5 反应式编程的框架 Reactor，也相应的发布了 3.1.0 版本。 从经验可知（主要是 rage 和 drunk 的推特），异步编程很难，而像 JVM 这类提供众多可选参数的平台则尤其困难。Reactor 旨在帮助大多数用例真正非阻塞地运行。提供的 API 比 JDK 的 JUC 库低级原语更高效。Reactor 提供了下列功能的替代函数 (并建议不使用 JDK 原生语句)： 阻塞等待： 如 Future.get() 不安全的数据访问： 如 ReentrantLock.lock() 异常冒泡： 如 try…catch…finally 同步阻塞： 如 synchronized{} Wrapper 分配（GC 压力）： 如 new Wrapper(event) 当消息传递效率成为系统性能瓶颈的时候(10k msg/s，100k msg/s，1M…)，非阻塞机制就显得尤为重要。例如看下面的一段代码： 123456789101112131415private ExecutorService threadPool = Executors.newFixedThreadPool(8);final List&lt;T&gt; batches = new ArrayList&lt;T&gt;();Callable&lt;T&gt; t = new Callable&lt;T&gt;() &#123; // *1 public T run() &#123; synchronized(batches) &#123; // *2 T result = callDatabase(msg); // *3 batches.add(result); return result; &#125; &#125;&#125;;Future&lt;T&gt; f = threadPool.submit(t); // *4T result = f.get() // *5 注释中标注的几点： Callable 分配：可能导致 GC 压力。 同步过程强制每个线程执行停：检查操作。 消息的消费可能比生产慢。 使用线程池（ThreadPool）将任务传递给目标线程：通过 FutureTask 方式肯定会产生 GC 压力。 阻塞直至 callDatabase() 回调。 在这个简单的例子中，存在的显著问题有： 分配对象可能产生 GC 压力，特别是当任务运行时间过长。 每次 GC 暂停都会影响全局性能。 默认的队列是无界的，任务会因为数据库调用而堆积。 积压虽然不会直接导致内存泄漏，但会带来严重副作用：GC 暂停时要扫描更多的对象；有丢失重要数据位的风险；等等 … 典型链式队列节点分配时会产生大量内存压力。 阻塞回调容易产生恶性循环。 阻塞回调会降低消息生产者的效率。在实践中，任务提交后需要等待结果返回，此时流式过程几乎演变为同步的了。 会话过程抛出的任何带数据存储的异常都会以不受控的方式被传递给生产者，否定了任何通常在线程边界附近可用的容错性。 要实现完全非阻塞是很难办到的，尤其是在有着类似微服务架构这样时髦绰号的分布式系统的世界里。因此 Reactor 做了部分妥协，尝试利用最优的可用模式，使开发者觉得他们是在写异步纳米服务，而不是什么数学论文。到了某个阶段，延迟是每一个系统到都要面对的实实在在的问题。为此： Reactor 提供的框架可以帮助减轻应用中由延迟产生的副作用，只需要增加一点点开销： 使用了一些聪明的结构，通过启动预分配策略解决运行时分配问题； 通过确定信息传递主结构的边界，避免任务的无限堆叠； 采用主流的响应与事件驱动构架模式，提供包含反馈在内的非阻塞端对端流； 引入新的 Reactive Streams 标准,拒绝超过当前容量请求，从而保证限制结构的有效性； 在 IPC 上也使用了类似理念，提供对流控制友好的非阻塞 IO 驱动； 开放了帮助开发者们以零副作用方式组织他们代码的函数接口，借助这些函数来处理容错性和线程安全。 为实现异步目标，响应式技术和 Reactor 模块该如何搭配： Spring XD + Reactor-Net (Core/Stream)： 使用 Reactor 作为 Sink/Source IO 驱动。 Grails | Spring + Reactor-Stream (Core)： 用 Stream 和 Promise 做后台处理。 Spring Data + Reactor-Bus (Core)： 发射数据库事件 (保存/删除/…)。 Spring Integration Java DSL + Reactor Stream (Core)： Spring 集成的微批量信息通道。 RxJavaReactiveStreams + RxJava + Reactor-Core： 融合富结构与高效异步 IO 处理 RxJavaReactiveStreams + RxJava + Reactor-Net (Core/Stream)： 用 RxJava 做数据输入，异步 IO 驱动做传输。 Reactor 核心含有如下特性： 通用 IO &amp; 函数式类型，一些 Java 8 接口的反向移植函数，提供者，消费者，谓词，双向消费者，双向函数 元组 资源池、暂停器、定时器 缓冲器，编解码和少量预定义的编解码器 环境上下文 调度者约定和几个预定义调度者 预定义响应式数据流处理者 Reactor-核心自身可替代其它消息传递机制，完成时序任务调度，或者帮你将代码组织为函数块，实现 Java 8 的反向移植接口。这种拆分便于同其他的响应式库配合使用，而没耐心的开发者也不用再去费劲弄懂环形缓冲区了。 反应式编程反应式编程（Reactive Programming）这种新的编程范式越来越受到开发人员的欢迎。在 Java 社区中比较流行的是 RxJava 和 RxJava 2。Spring5 中使用的是另外一个新的反应式编程库 Reactor。 Reactive Programming，中文称反应式编程，是一种高性能应用的编程方式。其最早是由微软提出并引入到 .NET 平台中，随后 ES6 也引入了类似的技术。在 Java 平台上，较早采用反应式编程技术的是 Netflix 公司开源的 RxJava 框架。现在大家比较熟知的 Hystrix 就是以 RxJava 为基础开发的。 反应式编程来源于数据流和变化的传播，举个例子：比如求值一个简单的表达式 c=a+b，当 a 或者 b 的值发生变化时，传统的编程范式需要对 a+b 进行重新计算来得到 c 的值。如果使用反应式编程，当 a 或者 b 的值发生变化时，c 的值会自动更新。反应式编程最早由 .NET 平台上的 Reactive Extensions (Rx) 库来实现。后来迁移到 Java 平台之后就产生了著名的 RxJava 库，并产生了很多其他编程语言上的对应实现。在这些实现的基础上产生了后来的反应式流（Reactive Streams）规范。该规范定义了反应式流的相关接口，并将集成到 Java 9 中。 在传统的编程范式中，我们一般通过迭代器（Iterator）模式来遍历一个序列。这种遍历方式是由调用者来控制节奏的，采用的是拉的方式：每次由调用者通过 next()方法来获取序列中的下一个值。使用反应式流时采用的则是推的方式，即常见的发布者-订阅者模式：当发布者有新的数据产生时，这些数据会被推送到订阅者来进行处理。在反应式流上可以添加各种不同的操作来对数据进行处理，形成数据处理链。这个以声明式的方式添加的处理链只在订阅者进行订阅操作时才会真正执行。 反应式流中第一个重要概念是负压（backpressure）。在基本的消息推送模式中，当消息发布者产生数据的速度过快时，会使得消息订阅者的处理速度无法跟上产生的速度，从而给订阅者造成很大的压力。当压力过大时，有可能造成订阅者本身的奔溃，所产生的级联效应甚至可能造成整个系统的瘫痪。负压的作用在于提供一种从订阅者到生产者的反馈渠道。订阅者可以通过 request() 方法来声明其一次所能处理的消息数量，而生产者就只会产生相应数量的消息，直到下一次 request() 方法调用。这实际上变成了推拉结合的模式。 Flux和MonoFlux 和 Mono 是 Reactor 中的两个基本概念（Java9 中也看到了类似的对象，可以理解为 Reactor = JDK8 Stream + JDK9 Reactive Stream）。Flux 表示的是包含 0 到 N 个元素的异步序列。在该序列中可以包含三种不同类型的消息通知： 正常的包含元素的消息 序列结束的消息 序列出错的消息 当消息通知产生时，订阅者中对应的方法 onNext(), onComplete() 和 onError() 会被调用。Mono 表示的是包含 0 或者 1 个元素的异步序列。该序列中同样可以包含与 Flux 相同的三种类型的消息通知。Flux 和 Mono 之间可以进行转换。对一个 Flux 序列进行计数操作，得到的结果是一个 Mono&lt;Long&gt; 对象。把两个 Mono 序列合并在一起，得到的是一个 Flux 对象。 Mono 实现了 org.reactivestreams.Publisher 接口，代表 0 到 1 个元素的发布者。 Flux 同样实现了 org.reactivestreams.Publisher 接口，代表 0 到 N 个元素的发表者。 Scheduler 表示背后驱动反应式流的调度器，通常由各种线程池实现。 在 Java 平台上，Netflix（开发了 RxJava）、TypeSafe（开发了 Scala、Akka）、Pivatol（开发了 Spring、Reactor）共同制定了一个被称为 Reactive Streams 项目（规范），用于制定反应式编程相关的规范以及接口。主要接口有：Publisher、Subscriber、Subcription。 直接消费的 Mono 或 Flux 的方式就是调用 subscribe 方法。如果在 Web Flux 接口中开发，直接返回 Mono 或 Flux 即可。Web Flux 框架会为我们完成最后的 Response 输出工作。 异步并不代表并行，如果需要并行，使用 zip 方法完成。使用反应式，任何环节都需避免阻塞。对于客户端是透明的。 Reactive Streams 是规范，Reactor 实现了 Reactive Streams。Web Flux 以 Reactor 为基础，实现 Web 领域的反应式编程框架。 使用Reactor创建 Flux，Reactor 提供了一系列的静态方法来创建 Flux 1234567891011121314151617181920// 可以指定序列中包含的全部元素。创建出来的 Flux 序列在发布这些元素之后会自动结束。Flux.just("Hello", "World").subscribe(System.out::println);// （还有 fromIterable 和 fromStream）可以从一个数组、Iterable 对象或 Stream 对象中创建 Flux 对象。Flux.fromArray(new Integer[] &#123;1, 2, 3&#125;).subscribe(System.out::println);// 创建一个不包含任何元素，只发布结束消息的序列。// 此外还有 error 和 neverFlux.empty().subscribe(System.out::println);// 创建包含从 start 起始的 count 个数量的 Integer 对象的序列。Flux.range(1, 10).subscribe(System.out::println);// 创建一个包含了从 0 开始递增的 Long 对象的序列。// 其中包含的元素按照指定的间隔来发布。// 除了间隔时间之外，还可以指定起始元素发布之前的延迟时间。Flux.interval(Duration.of(10, ChronoUnit.SECONDS)).subscribe(System.out::println);// 与 interval()方法的作用相同，只不过该方法通过毫秒数来指定时间间隔和延迟时间。Flux.intervalMillis(1000).subscribe(System.out::println); 上面的这些静态方法适合于简单的序列生成，当序列的生成需要复杂的逻辑时，则应该使用 generate() 或 create() 方法。 123456789101112131415161718192021222324// generate 方式Flux.generate(sink -&gt; &#123; sink.next("Hello"); sink.complete();&#125;).subscribe(System.out::println);final Random random = new Random();Flux.generate(ArrayList::new, (list, sink) -&gt; &#123; int value = random.nextInt(100); list.add(value); sink.next(value); if (list.size() == 10) &#123; sink.complete(); &#125; return list;&#125;).subscribe(System.out::println);// create 方式Flux.create(sink -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; sink.next(i); &#125; sink.complete();&#125;).subscribe(System.out::println); Mono 的创建方式与之前介绍的 Flux 比较相似。Mono 类中也包含了一些与 Flux 类中相同的静态方法。这些方法包括 just()，empty()，error() 和 never()等。除了这些方法之外，Mono 还有一些独有的静态方法。 12345Mono.fromSupplier(() -&gt; "Hello").subscribe(System.out::println);// 从一个 Optional 对象或可能为 null 的对象中创建 Mono。// 只有 Optional 对象中包含值或对象不为 null 时，Mono 序列才产生对应的元素。Mono.justOrEmpty(Optional.of("Hello")).subscribe(System.out::println);Mono.create(sink -&gt; sink.success("Hello")).subscribe(System.out::println); 和 RxJava 一样，Reactor 的强大之处在于可以在反应式流上通过声明式的方式添加多种不同的操作符。例如 buffer 和 bufferTimeout 这两个操作符的作用是把当前流中的元素收集到集合中，并把集合对象作为流中的新元素。还有 filter 、take、reduce 和 reduceWith、merge 和 mergeSequential、flatMap 和 flatMapSequential、消息处理、调度器相关的方法，这方面其实有很多内容，但是没细看，估计短时间内接触不到，有个印象等用的时候知道有这么个东西然后再查 API 好了。 使用WebFlux使用 WebFlux 与 Spring MVC 的不同在于，WebFlux 所使用的类型是与反应式编程相关的 Flux 和 Mono 等，而不是简单的对象。对于简单的 Hello World 示例来说，这两者之间并没有什么太大的差别。对于复杂的应用来说，反应式编程和负压的优势会体现出来，可以带来整体的性能的提升。 吞吐量为何会大幅提升？因为使用 WebFlux 后，容器的线程不会被阻塞，只会给业务代码一个回调函数（asyncContext.complete()），业务代码处理完了再通知我！这样就可以使用少量的线程处理更加高的请求，从而实现高吞吐量（结合负压不会造成过高的处理压力）。 类中的方法都以 Flux 或 Mono 对象作为返回值，这也是 WebFlux 应用的特征。Flux 类型的参数表示的是有多个对象需要处理。可以使用 doOnNext() 来对其中的每个对象进行处理。除了服务器端实现之外，WebFlux 也提供了反应式客户端，可以访问 HTTP、SSE 和 WebSocket 服务器端。分别对应：Web 的 HTTP、SSE、WebSocket，这里不再多说。 这里不贴代码了，参考我 Github 的这个模块。 服务器推送事件服务器推送事件（Server-Sent Events，SSE）允许服务器端不断地推送数据到客户端。相对于 WebSocket 而言，服务器推送事件只支持服务器端到客户端的单向数据传递。虽然功能较弱，但优势在于 SSE 在已有的 HTTP 协议上使用简单易懂的文本格式来表示传输的数据。作为 W3C 的推荐规范，SSE 在浏览器端的支持也比较广泛，除了 IE 之外的其他浏览器都提供了支持。在 IE 上也可以使用 polyfill 库来提供支持。在服务器端来说，SSE 是一个不断产生新数据的流，非常适合于用反应式流来表示。在 WebFlux 中创建 SSE 的服务器端是非常简单的。只需要返回的对象的类型是 Flux&lt;ServerSentEvent&gt;，就会被自动按照 SSE 规范要求的格式来发送响应，或者指定 MediaType。 1234567891011121314151617181920@RestController@RequestMapping("/sse")public class SseController &#123; @GetMapping("/randomNumbers") public Flux&lt;ServerSentEvent&lt;Integer&gt;&gt; randomNumbers() &#123; return Flux.interval(Duration.ofSeconds(1)) .map(seq -&gt; Tuples.of(seq, ThreadLocalRandom.current().nextInt())) .map(data -&gt; ServerSentEvent.&lt;Integer&gt;builder() .event("random") .id(Long.toString(data.getT1())) .data(data.getT2()) .build()); &#125; @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE) public Flux&lt;User&gt; streamAll() &#123; return userRepository.findAll(); &#125;&#125; SseController 是一个使用 SSE 的控制器的示例。其中的方法 randomNumbers() 表示的是每隔一秒产生一个随机数的 SSE 端点。我们可以使用类 ServerSentEvent.Builder 来创建 ServerSentEvent 对象。这里我们指定了事件名称 random，以及每个事件的标识符和数据。事件的标识符是一个递增的整数，而数据则是产生的随机数。 PS：我记得在我写的 SB2.x 的初尝试那篇文章中关于这个有个小例子。 WebSocketWebSocket 支持客户端与服务器端的双向通讯。当客户端与服务器端之间的交互方式比较复杂时，可以使用 WebSocket。WebSocket 在主流的浏览器上都得到了支持。WebFlux 也对创建 WebSocket 服务器端提供了支持。在服务器端，我们需要实现接口 org.springframework.web.reactive.socket.WebSocketHandler 来处理 WebSocket 通讯。接口 WebSocketHandler 的方法 handle 的参数是接口 WebSocketSession 的对象，可以用来获取客户端信息、接送消息和发送消息。 123456789@Componentpublic class EchoHandler implements WebSocketHandler &#123; @Override public Mono&lt;Void&gt; handle(final WebSocketSession session) &#123; return session.send( session.receive() .map(msg -&gt; session.textMessage("ECHO -&gt; " + msg.getPayloadAsText()))); &#125;&#125; EchoHandler 对于每个接收的消息，会发送一个添加了 “ECHO -&gt; “ 前缀的响应消息。WebSocketSession 的 receive 方法的返回值是一个 Flux&lt;WebSocketMessage&gt; 对象，表示的是接收到的消息流。而 send 方法的参数是一个 Publisher&lt;WebSocketMessage&gt; 对象，表示要发送的消息流。在 handle 方法，使用 map 操作对 receive 方法得到的 Flux&lt;WebSocketMessage&gt; 中包含的消息继续处理，然后直接由 send 方法来发送。 在创建了 WebSocket 的处理器 EchoHandler 之后，下一步需要把它注册到 WebFlux 中。我们首先需要创建一个类 WebSocketHandlerAdapter 的对象，该对象负责把 WebSocketHandler 关联到 WebFlux 中。 12345678910111213141516171819@Configurationpublic class WebSocketConfiguration &#123; @Autowired @Bean public HandlerMapping webSocketMapping(final EchoHandler echoHandler) &#123; final Map&lt;String, WebSocketHandler&gt; map = new HashMap&lt;&gt;(1); map.put("/echo", echoHandler); final SimpleUrlHandlerMapping mapping = new SimpleUrlHandlerMapping(); mapping.setOrder(Ordered.HIGHEST_PRECEDENCE); mapping.setUrlMap(map); return mapping; &#125; @Bean public WebSocketHandlerAdapter handlerAdapter() &#123; return new WebSocketHandlerAdapter(); &#125;&#125; 其中的 HandlerMapping 类型的 bean 把 EchoHandler 映射到路径 /echo。 参考代码：Github 函数式编程模型WebFlux 还支持基于 lambda 表达式的函数式编程模型。与基于 Java 注解的编程模型相比，函数式编程模型的抽象层次更低，代码编写更灵活，可以满足一些对动态性要求更高的场景。不过在编写时的代码复杂度也较高，学习曲线也较陡。开发人员可以根据实际的需要来选择合适的编程模型。目前 Spring Boot 不支持在一个应用中同时使用两种不同的编程模式。在函数式编程模型中，每个请求是由一个函数来处理的， 通过接口 org.springframework.web.reactive.function.server.HandlerFunction 来表示。HandlerFunction 是一个函数式接口，其中只有一个方法 Mono&lt;T extends ServerResponse&gt; handle(ServerRequest request)，因此可以用 labmda 表达式来实现该接口。接口 ServerRequest 表示的是一个 HTTP 请求。通过该接口可以获取到请求的相关信息，如请求路径、HTTP 头、查询参数和请求内容等。方法 handle 的返回值是一个 Mono&lt;T extends ServerResponse&gt; 对象。接口 ServerResponse 用来表示 HTTP 响应。ServerResponse 中包含了很多静态方法来创建不同 HTTP 状态码的响应对象。下面是一个简单的计算器实现来展示函数式编程模型的用法。 1234567891011121314151617181920212223242526272829303132333435363738@Componentpublic class CalculatorHandler &#123; public Mono&lt;ServerResponse&gt; add(final ServerRequest request) &#123; return calculate(request, (v1, v2) -&gt; v1 + v2); &#125; public Mono&lt;ServerResponse&gt; subtract(final ServerRequest request) &#123; return calculate(request, (v1, v2) -&gt; v1 - v2); &#125; public Mono&lt;ServerResponse&gt; multiply(final ServerRequest request) &#123; return calculate(request, (v1, v2) -&gt; v1 * v2); &#125; public Mono&lt;ServerResponse&gt; divide(final ServerRequest request) &#123; return calculate(request, (v1, v2) -&gt; v1 / v2); &#125; private Mono&lt;ServerResponse&gt; calculate(final ServerRequest request, final BiFunction&lt;Integer, Integer, Integer&gt; calculateFunc) &#123; final Tuple2&lt;Integer, Integer&gt; operands = extractOperands(request); return ServerResponse .ok() .body(Mono.just(calculateFunc.apply(operands.getT1(), operands.getT2())), Integer.class); &#125; private Tuple2&lt;Integer, Integer&gt; extractOperands(final ServerRequest request) &#123; return Tuples.of(parseOperand(request, "v1"), parseOperand(request, "v2")); &#125; private int parseOperand(final ServerRequest request, final String param) &#123; try &#123; return Integer.parseInt(request.queryParam(param).orElse("0")); &#125; catch (final NumberFormatException e) &#123; return 0; &#125; &#125;&#125; 上述代码给出了处理不同请求的类 CalculatorHandler，其中包含的方法 add、subtract、multiply 和 divide 都是接口 HandlerFunction 的实现。这些方法分别对应加、减、乘、除四种运算。每种运算都是从 HTTP 请求中获取到两个作为操作数的整数，再把运算的结果返回。在创建了处理请求的 HandlerFunction 之后，下一步是为这些 HandlerFunction 提供路由信息，也就是这些 HandlerFunction 被调用的条件。这是通过函数式接口 org.springframework.web.reactive.function.server.RouterFunction 来完成的。接口 RouterFunction 的方法 Mono&lt;HandlerFunction&lt;T extends ServerResponse&gt;&gt; route(ServerRequest request) 对每个 ServerRequest，都返回对应的 0 个或 1 个 HandlerFunction 对象，以 Mono&lt;HandlerFunction&gt; 来表示。 当找到对应的 HandlerFunction 时，该 HandlerFunction 被调用来处理该 ServerRequest，并把得到的 ServerResponse 返回。在使用 WebFlux 的 Spring Boot 应用中，只需要创建 RouterFunction 类型的 bean，就会被自动注册来处理请求并调用相应的 HandlerFunction。 123456789101112131415161718@Configurationpublic class Config &#123; @Bean @Autowired public RouterFunction&lt;ServerResponse&gt; routerFunction(final CalculatorHandler calculatorHandler) &#123; return RouterFunctions.route( RequestPredicates.path("/calculator"), request -&gt; request.queryParam("operator").map(operator -&gt; Mono.justOrEmpty(ReflectionUtils.findMethod( CalculatorHandler.class, operator, ServerRequest.class)) .flatMap(method -&gt; (Mono&lt;ServerResponse&gt;) ReflectionUtils.invokeMethod(method, calculatorHandler, request)) .switchIfEmpty(ServerResponse.badRequest().build()) .onErrorResume(ex -&gt; ServerResponse.status(HttpStatus.INTERNAL_SERVER_ERROR).build())) .orElse(ServerResponse.badRequest().build())); &#125;&#125; 上面的代码是相关的配置类 Config。方法 RouterFunctions.route 用来根据 Predicate 是否匹配来确定 HandlerFunction 是否被应用。RequestPredicates 中包含了很多静态方法来创建常用的基于不同匹配规则的 Predicate。如 RequestPredicates.path 用来根据 HTTP 请求的路径来进行匹配。此处我们检查请求的路径是 /calculator。 使用 ServerRequest 的 queryParam 方法来获取到查询参数 operator 的值，然后通过反射 API 在类 CalculatorHandler 中找到与查询参数 operator 的值名称相同的方法来确定要调用的 HandlerFunction 的实现，最后调用查找到的方法来处理该请求。如果找不到查询参数 operator 或是 operator 的值不在识别的列表中，服务器端返回 400 错误；如果反射 API 的方法调用中出现错误，服务器端返回 500 错误。 其他响应式数据流作为一种新的数据流规范应用于 Java 9 及其后续版本，并被多个供应商和技术企业采纳，这一规范的定位非常清晰，旨在提供同/异步数据序列流式控制机制，并在 JVM 上首先推广。该规范由 4 个 Java 接口，1 个 TCK 和一些样例组成。 响应式扩展，就是通常所说的 Rx，是一组定义良好的函数式 API，大规模扩展了观察者模式。Rx 模式支持响应式数据序列处理，主要的设计要点有： 使用回调链分离时间/延迟：仅当数据可用时才会回调 分离线程模型：用 Observable / Stream 来处理同步或异步 控制错误链/终止：数据载荷信号以及错误与完成信号都传递给回调链 解决各种预定义 API 中多重分散-聚合和构造问题 JVM 中响应式扩展的标准实现是 RxJava。它提供了强大的函数式 API，并将原始微软库中几乎全部的概念移植了过来。 响应式数据流和响应式扩展算是最近比较新的技术了，因为牵扯到异步非阻塞技术比较难理解，但是从 Spring5 的方向来看，这是未来，至于如何学习，我还在摸索那条路比较好。 Netty 示例相关参考：Github 参考https://www.ibm.com/developerworks/cn/java/spring5-webflux-reactive/index.htmlhttps://www.ibm.com/developerworks/cn/java/j-cn-with-reactor-response-encode/index.htmlhttps://www.jianshu.com/p/7ee89f70dfe5]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>WebFlux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[任性的语言-Golang]]></title>
    <url>%2F2020%2F05%2F12%2F%E4%BB%BB%E6%80%A7%E7%9A%84%E8%AF%AD%E8%A8%80-Golang%2F</url>
    <content type="text"><![CDATA[Go 现在名声越来越大，很多大型互联网公司都在过渡到 Go，也不用多介绍了，我对其的印象就是：Google 出品、21世纪编程语言、logo 非常萌、纯编译型、对标 C（C+Python=Go）、面向并发高效计算（毕竟出生的时候已经多核时代）、万物皆异步、函数一等公民（函数式编程，CSP 并发模型）、语法简洁（没有继承、泛型、异常处理和对象的概念，但是有接口即面向接口）、效率高（更少的存储空间和更少的内存写操作）、企业级编程语言（另一个是 Java）承诺保证向后兼容、完全开源。兴趣使然，简单看一下作为后来者到底解决了前人的那些痛点，取取经（我才发现原来 Python 的出生比 Java 早）。 还有个好处就是自带 GC，这一点可真是太贴心了，性能方面基本可以赶上 Java（Scala、JIT 优化后的代码），不要问为什么纯编译型语言还不如 Java，Java 慢是很久很久之前的观点了，硬要说是臃肿的第三方库（有利有弊，结果就是一个项目打个包就过百 M 了），另外 JVM 的 JIT 确实很厉害的，优化后与 C 基本无异；JIT 还帮你擦屁股（比如各种锁优化、逃逸分析、无效代码消除等等），优化人写出来的烂代码，这点 Go 是做不到的，你写成什么样就什么样了，所以你写的代码烂真的还不如 JVM 这种 runtime 的优化。Golang 国内镜像站：https://golang.google.cn/package 文档：https://golang.google.cn/pkg/另外，Go 已成为区块链的主流开发语言。 入门安装完成后使用 go env 可以检查环境，然后新建 GOPATH 环境变量，设置代码工作区，新版本支持多个，也可以不建，一般这个目录下会有三个文件夹，src、pkg、bin，分别对应源码、中间文件、可执行文件。然后把 $GOPATH/bin 添加到系统环境变量中，方便直接在命令行中执行。作为个人来说，src 目录下使用类似网址来区分项目即可，这里正序即可不需要像 Java 那样 com 倒序开头，文件命名使用小写，多单词使用下划线分割。常用命令（其他命令用到自行搜索）： 1234567891011121314151617181920212223# 编译go build# 其他路径下编译，编译后的可执行文件保存在当前目录下# go build &#123;src下的路径，到最后的文件夹&#125;go build github.com/xxx# 重命名go build -o demo# 运行./xx# 直接运行go run xx.go# 编译、复制到 bin 下go install# 交叉编译\跨平台编译GOOS=linux GOARCH=amd64 go build hello.go # linux 64 位CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build # win 64 位SET CGO_ENABLED=0 # 禁用 CGO，使用了 cgo 的代码是不支持跨平台编译的SET GOOS=linux # 目标平台是 linuxSET GOARCH=amd64 # 目标处理器架构是 amd64 入门代码： 1234567package mainimport "fmt"func main() &#123; fmt.Println("Hello, 世界")&#125; 需要注意的是，package 是必须的，如果包名是 main，代表是编译为可执行文件，需要有个 main 函数作为入口。函数外部无法使用语句，只能定义变量。Go 语言中，有 25 个关键字，37 个保留字，可以说是非常精简了。 go get单独把这个命令拿出来是因为用的非常频繁，go get 命令可以借助代码管理工具通过远程拉取或更新代码包及其依赖包，并自动完成编译和安装。整个过程就像安装一个 App 一样简单。这个命令在内部实际上分成了两步操作：第一步是下载源码包，第二步是执行 go install。使用 -d 参数可以只下载不安装；-x 显示详细过程；-u 强制从网络更新。因为 Go 语言代码托管在 Github，所以自然默认使用 git 作为版本控制工具，使用 go get 之前需要安装 git，虽然它支持 SVN 等其他工具和其他的类似 Github 的托管站点，但是主流一般都是 Github。 工具Go 语言提供了不少工具来高效率编程，如果使用 VSC 它会提示你安装这些工具，以 gofmt 来说，体现了 Go 对格式的强硬态度，这个工具是对代码进行格式化对，并且没有任何参数可以调整，这表示对编程风格的高度统一，不存在因为格式而撕逼对情况了。 风格Go 中单行语句结尾不需要分号，编译器会主动把换行符转换为分号，在某些情况下不会添加，例如括号、+ 等符号，也就意味着语句太长换行的时候是有规则的，不能在普通字符后换行。在方法调用等情况中，如果太长折行的话，一定是从左括号开始，否则因为自动添加分号会编译不通过，而括号作为特殊字符不会进行处理，同样为了保持风格，Go 允许最后以 , 结尾，逗号作为特殊符，后面也可以进行换行。其他的，例如注释也有相应的规范，包注释写在第一行，多个文件的话只写一个即可，文档工具会自行处理，Go 的规范网上可以搜到很多，例如这篇看一下即可，如果使用了 golint 等工具，不规范直接会给你提示。 对于错误或者异常，Go 语言习惯是在 if 中（if err != nil）处理并且直接返回（return），error 也一般是作为返回值返回，放在最后，另一个选择是 panic，相比之下用的不多。 变量与常量Go 既然是静态语言，变量的声明是要确定类型的，Go 中的声明标识符分为四类：var、const、type、func，先说前两个，用代码示例来展示一下用法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657var name string// 批量声明var ( age int isOk bool)var a, b int = 3, 4var c, d = "str", 5var desc string = "desc"// 函数变量/定义函数var fun1 = func() &#123; fmt.Println("func...")&#125;func main() &#123; // 短变量声明，只能在函数中使用 s1 := "Goooo" fmt.Println(s1) // 字符串、数值（任何类型）、增加双引号（%q） // 另外 %b %o %x %T %p，二、八、十六进制，类型，地址&amp; // 浮点数 %f %.2f fmt.Printf("%s, %v, %#v", s1, s1, s1)&#125;// 匿名变量x, _ = func1()// 声明常量const info = "MMP"const ( // 都是 1 code1 = 1 code2 code3)const ( c1 = iota // 0 c2 // 1 c3 // 2 _ c4 // 4 c5 = 100 // 100 c6 // 100 c7 // 7)const ( a1, a2 = iota + 1, iota + 2 // 1, 2 a3, a4 = iota + 1, iota + 2 // 2, 3)// 定义数量级const ( _ = iota KB = 1 &lt;&lt; (10 * iota) MB = 1 &lt;&lt; (10 * iota) GB TB) 变量的定义顺序比较『反人类』是先名字后类型，同时，变量声明后会进行默认初始化，文件也统一使用 UTF-8 编码，推荐使用驼峰命名；函数中声明的变量必须使用，否则编译不通过，go 文件必须使用 package 声明包名，可以与文件夹不一致；当想要忽略某个值的时候，可以使用匿名变量（_），匿名变量不占用命名空间，不会分配内存，所以也不存在重复声明。iota 是常量计数器，在 const 出现的时候重置为 0，每新增一行声明就加一，注意是从 0 开始，批量声明中使用，可以当作枚举来使用。常量的命名不要使用全大些，那个有另外的用途，总之像普通变量那样使用就好了。 经典数据类型这里并没有按照 Go 的分类（基础类型、复合类型、引用类型、接口类型），其中部分复杂的复合类型会单独介绍，基本数据类型方面与其他语言基本一致，也就是整数、浮点数、字符串、布尔这些，数字又分为有符号和无符号： 类型 描述 uint8 无符号 8 位整型 (0 到 255) uint16 无符号 16 位整型 (0 到 65535) uint32 无符号 32 位整型 (0 到 4294967295) uint64 无符号 64 位整型 (0 到 18446744073709551615) int8 有符号 8 位整型 (-128 到 127) int16 有符号 16 位整型 (-32768 到 32767) int32 有符号 32 位整型 (-2147483648 到 2147483647) int64 有符号 64 位整型 (-9223372036854775808 到 9223372036854775807) uint 32 位操作系统上就是uint32，64 位操作系统上就是uint64 int 32 位操作系统上就是int32，64 位操作系统上就是int64 uintptr 无符号整型，用于存放一个指针，大小根据操作系统而定 rune 可理解为字符型（char），解决多语言问题，大小是可变的 byte 其中，uint8就是我们熟知的byte型，int16对应C语言中的short型，int64对应C语言中的long型。 获取对象的长度的内建 len() 函数返回的长度可以根据不同平台的字节长度进行变化。实际使用中，切片或 map 的元素数量等都可以用 int 来表示。在涉及到二进制传输、读写文件的结构描述时，为了保持文件的结构不会受到不同编译目标平台字节长度的影响，不要使用int和 uint。 不显式声明的情况下，默认是 int 类型，而 x := int8(16) 就是显式使用 int8 类型；浮点数的话默认是 float64 类型。布尔类型在 Go 中是独立的，不能转换成 0 和 1。GO 中只有显式类型转换没有隐式类型转换（不包括字面量）。PS：Go 中还设置了实数部分与虚数部分的复杂数据类型（复数），可能是想进军科学计算领域，一般用不到。 字符串字符串在 Go 中的实现是 UTF-8 编码的，可以使用反引号进行多行字符串的输入；字符串相关处理： 12345678910111213141516171819func str() &#123; str := "Mps" fmt.Println(len(str)) fmt.Println(str + "拼接1") ss := fmt.Sprintf("%s%s", str, ",拼接2") s1 := strings.Split(ss, ",") fmt.Println(s1) // 遍历 - 中文 for i, c := range []rune(str) &#123; fmt.Println(string(c)) &#125; // 字符数统计 fmt.Println(utf8.RuneCountInString(str)) // 字符串转成 byte 数组 bs := []byte(str)&#125; 其中 len 是求的 byte 的数量，对于非英文文字，例如中文在 U8 中一般是 3 个字节（也可能是 4 个），Go 中使用了一种 rune 的类型（实际为 int32）来保存非 ASCII 字符，可以理解为其他语言中的 char 类型，对应的是 Unicode，所以 rune 大小是一致的，即使是 ASCII；另外，字符串是不允许修改的，底层存储使用的是 byte，非 ASCII 字符要转换成 []rune 然后再进行操作，一个 rune 可以由 1 个或者多个 byte 保存。使用 RuneCountInString 方法统计字符的时候，它会有一层转换或者说解码，先 byte 到 rune 之后再计算。字符串相关操作 API 在 strings 包下。 nilnil 不属于关键字，就像其他语言中的 null 也不是关键字（你甚至可以更改 nil 的值），在 Go 中 nil 是一个预声明标识符，使用频率很高，除了其他语言的 null 用途还有其他的特点，例如 Go 中的 nil 是非常安全的，即使是 nil 也可以调用方法，只要不牵扯具体的内部变量，是完全没问题的。PS：Go 的一个特点就是即使是使用 nil 值，也不会抛异常，因为它表示的是 zero value，或者说对象的默认值。 数组和切片数组定义同样也是反着的，大概是特色吧一般都是反着的，切片基于数组，或者看作是可以自动扩容对数组，可变长度意味着切片非常灵活，代码示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 定义数组var arr1 [3]intvar arr2 = [3]int&#123;1, 2, 3&#125;var arr3 = [...]int&#123;1, 2, 3&#125;var arr4 = [3]int&#123;1:1, 2:3&#125; // 指定索引初始化var arr5 = [...]&#123;99: -1&#125; // len = 100// [3]int 和 [5]int 是不同类型func arrEcho(arr [3]int) &#123; for _, v := range arr &#123; fmt.Println(v) &#125;&#125;// 使用指针，可以进行修改func editArr(arr *[3]int) &#123; arr[0] = 100&#125;// 切片var arr = [...]int&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;func sp()&#123; // 其他语法 [:4] [2:] [:] s := arr[2:4] // [2,3] s2 := s[2,4] // [4,5] 向后扩展 s = append(s, 233) fmt.Println("s / arr : ", s, arr) // 定义一个切片 var ss []int // 默认值 nil fmt.Println(ss == nil) ss = append(ss, 1, 2, 3) fmt.Println(ss) // 其他定义方式 sp2 := []int&#123;1, 2, 3&#125; sp3 := make([]int, 10) // len = 10 sp4 := make([]int, 10, 16) // len = 10, cap = 16 sp(sp2) sp(sp3) sp(sp4) // sp2 拷贝到 sp3 copy(sp3, sp2) fmt.Println(sp3) // 通过 append 来删除，也可以使用 copy 来删除 sp3 = append(sp3[:2], sp3[3:]...) fmt.Println(sp3)&#125;// 函数中使用切片func sp(s []int) &#123; fmt.Printf("len: %d, cap: %d", len(s), cap(s))&#125; 需要注意的是数组是值类型，意味着函数传递的时候会进行完全的拷贝，而不是传递地址（引用），当然你可以使用指针来传递，但是太麻烦了，Go 中一般不会直接使用数组，而是会用切片。切片在 Go 中使用 []T 定义，与数组很像，只是没有固定长度，如果接触过 python，那么应该很好理解，这里的切片和 python 中的切片还是很相似的，区间都是 [x,y)，对数组切片返回的是数组的视图，也就是如果切片修改，原来的数组也会跟着修改；也正是这个原因，我们直接往函数里传递切片，就相当于传递的引用了。使用下标获取超出切片大小的内容会报错，但是再次使用切片来处理却没有这个问题，可以说切片是可以向后扩展的（向前取不行），在切片的实现中，使用 ptr 来记录头部，使用一个 len 记录长度，所以使用下标是取不到的，会跟 len 做比较，但是使用切片会跟 cap 做对比，它记录了 ptr 到数组最后一个元素的长度，所以只要不是物理的越界，是可以进行切片的。如果使用 append 来向切片追加数据，在不超过 cap 的情况下会覆盖原数组的内容，如果 append 后超过了 cap，那么 Go 会新创建一个数组，并由 append 返回，旧数组如果没有引用，就会被 GC。 切片的底层就是一个数组对象，由三部分组成，指针、长度和容量，长度不能超过容量（数值上，数据饱可以自动扩容），多个切片之间可能有重叠部分，因为底层可能是指向了同一个数组；切片之间不能直接比较（毕竟是间接引用），只能一个个的来，虽然效率并不高，唯一合法的比较是跟 nil，判断切片是不是空应该用 len 函数而不是使用 nil。PS：上例使用了类似展开运算符的 sp... 语法来适配可变参数，相比其他语言同样也是倒着的；具体的扩容机制不展开说了，毕竟是隐式无法调整的，见拓展里的文章，在 1024 之前一般是 2 倍，之后是 1/4，不同类型的处理也不太一样。 MapMap 这个数据类型肯定不陌生，不过陌生的是 Go 中的定义方法。。。 123456789101112131415161718192021222324func main() &#123; m := map[string]string&#123; "k1": "v1", "k2": "v2", &#125; fmt.Println(m) m["k3"] = "v3" // 遍历 for k, v := range m &#123; fmt.Println(k, v) &#125; // 判断是否存在 if val, ok := m["name"]; ok &#123; fmt.Println("存在：", val, ok) &#125; else &#123; fmt.Println("不存在", val, ok) &#125; // 删除 delete(m, "k1") fmt.Println(m)&#125; 嵌套定义也一样，就是繁琐，或者你还可以使用 make 函数来创建（make(map[string]string)），创建出来的是个空 Map，而 var 定义出来的是 nil；无论是那个，都是可以直接运算不会抛出异常。map 也是 hash 实现，是无序的；如果需要排序需要加入到切片中，然后通过对 key 的排序来达到有序。当获取的 k 不存在时，获取的是 v 类型的默认值，字符串的话就是空串了；这种情况下我们需要通过返回的第二个参数手动判断；除 slice、map、func 的内建类型基本都可以作为 key，自定义的 Struct 类型只要不包含前面说的类型也可以作为 key，换句话说 key 必须是可以进行 == 比较运算的。使用 key 来获取 map 的内容，最好是通过 ok 来判断一下，大部分情况下即使是空也是安全的，因为空的话会默认取零值，或者说默认值。禁止对 map 进行取地址操作，因为随着内容的变化，地址也会随之变化，哈希实现就是如此。当 map 是 nil 的时候，添加元素会导致 panic，但是用 make 创建的空 map 就不会有这个问题。 Go 中没有提供 set 类型，但是使用 map 的 key 特性可以做到这一点，不过也有一定的限制，因为 key 要求是可比较类型，切片就不满足这个条件，你可以定义一个函数将其转换为 string。 流程控制编程语言我认为最核心的两块，数据类型和流程控制，流程控制基本就是指条件判断与循环，以及延伸的 switch 和 goto；因为 where 这个可以用 for 来替代，或者说 where 是 for 的语法糖，在 Go 中也没有 where，循环就是 for（三个部分都可省略，可完美替代 where）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func main() &#123; num := 12 if num &gt; 10 &#123; fmt.Println("Get") &#125; if n1 := 2; n1 &gt; 1 &#123; fmt.Println("Go") &#125; for i := 0; i &lt; 10; i++ &#123; fmt.Println("for - " + string(i)) if i == 4 &#123; break // continue // return &#125; &#125; // 无限循环 for &#123; fmt.Println("for...") &#125; // where 用法 x := funcName() for x.isTrue() &#123; // ... &#125; // range 遍历，v 不要的话可以省略，i 不要使用 _ 丢弃 str := "abc中文字符" for i, v := range str &#123; // fmt.Println(i, v) fmt.Printf("index: %d, val: %c\n", i, v) &#125; // switch switch f := 2; f &#123; case 1: fmt.Println("sw - 1") case 2, 3: fmt.Println("sw - 2,3") default: fmt.Println("sw - def") &#125; n := 3 switch &#123; case n &lt; 5: fmt.Println("sw - (&lt; 5)") &#125;&#125; 语法上确实不太一样，没有括号了，range 关键字跟其他语言的 foreach 类似，可以遍历可迭代的数据类型，返回两个值：坐标和具体的值，不过需要注意遍历字符串的时候 index 是按字节来的，虽然输出不会乱码，但是一个汉字会跳 3 个 index。switch 也不需要手动调用 break，符合匹配后它不会向下执行的（自动 break）；另外为了可读性，还是尽量少使用 goto，虽然在跳出多层循环中 goto 很好用，但是建议使用多层 break + if 来代替。 使用 for 进行迭代的时候，一定要注意声明域（使用 := 的要注意），它记录的是循环变量的内存地址， 而不是某一时刻的值，如果需要某一时刻的值，请声明（:=）一个临时变量进行保存。 函数函数作为 Go 语言的一等公民，这个肯定是重点，在返回值上面刚接触也有点『反人类』也就是返回值写在最后，这样倒是确实有好处，如果没有返回值直接可以省略，不需要 void 了，通过代码来解释就是： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657func eval(a, b int, op string) int &#123; switch op &#123; case "+": return a + b case "-": return a - b case "*": return a * b case "/": return a / b default: // 抛出异常 panic("unsupported") &#125;&#125;func eval2(a, b int, op string) (int, error) &#123; switch op &#123; case "+": return a + b, nil case "-": return a - b, nil case "*": return a * b, nil case "/": return a / b, nil default: return 0, fmt.Errorf("unsupported op %s", op) &#125;&#125;// 多返回值func div(a, b int) (q, r int) &#123; return a / b, a % b&#125;// 不推荐的方式func div2(a, b int) (q, r int) &#123; q = a / b r = a % b return&#125;// 使用// q, r := div2(3, 4)// 接收函数，或者直接将函数的定义写在参数上func apply(op func(int, int) int, a, b int) int &#123; return op(a, b)&#125;// 可变参数func sum(nums ...int) int &#123; sum := 0 for i := range nums &#123; sum += nums[i] &#125; return sum&#125; 函数的返回值定义在后面，同时支持多返回类型（参考 py），多返回类型可以起名，例如 div2 的方式，但是不推荐，如果不需要全部的返回值，可以使用 _ 匿名变量来丢弃；处理异常建议使用 Go 的推荐方式使用返回值（eval2），而不是直接抛出；函数只能与 nil 进行比较；既然是函数式编程，必然支持高阶函数，那么函数的方法参数也可以接收函数，只需要说明函数的签名就行，不需要写函数的参数名称，只需要类型；另一种就是类似其他语言的匿名类的方式，直接把函数定义在参数上。另外，Go 中的函数不支持重载，这样倒是简化了方法调度。 内建函数内建函数是上面说的保留字中的一部分，包括 new、make、len、cap 等函数，介绍常用的几个： new(T)它将创建一个指定类型 T 的匿名变量，初始化为对应的零值（默认值），然后返回地址，指针类型为 *T；它与普通的变量声明没有太大区别，它类似一个语法糖，不需要名字的匿名变量。一般情况下，new 用的并不是很多，并且它不是关键字，所以你甚至可以改变它的定义。 12345func main() &#123; num := new(int) fmt.Println(num) fmt.Println(*num)&#125; 使用的时候要注意，new 返回的是地址而不是值。 make([]T, len, cap)它会创建一个指定的类型、长度、容量的切片，容量可以省略，默认与 len 相等。同样，它创建的也是匿名变量，除了切片，其他类型也可以创建，例如 map make(map[string]string) ；make 只用于 slice、map 以及 channel 的初始化，并且返回的是引用本身；为了避免 panic，这三种类型尽量使用 make 来创建。 append用于向切片追加数据（可以是多个），容量饱和会自动扩容（通过复制），并返回。安全起见，请尽量将它的返回值赋予原变量。 copy(target, source)将一个类型复制到另一个相同的类型变量上，例如上面的 append 函数中。另外，在 IO 中用的也很频繁。 一览表： 内置函数 介绍 close 主要用来关闭channel len 用来求长度，比如string、array、slice、map、channel new 用来分配内存，主要用来分配值类型，比如int、struct。返回的是指针 make 用来分配内存，主要用来分配引用类型，比如chan、map、slice append 用来追加元素到数组、slice中 panic和recover 用来做错误处理 匿名函数与闭包匿名函数因为没有名字，所以无法显式调用，所以匿名函数要么保存到一个变量，要么就立即执行，多用于回调和闭包；Go 使用闭包技术实现函数值，很多人也将函数值称为闭包；闭包指的是一个函数和与其相关的引用环境组合而成的实体。简单来说，闭包=函数+引用环境。 12345678910111213func adder() func(int) int &#123; var x int return func(y int) int &#123; x += y return x &#125;&#125;unc main() &#123; var f = adder() fmt.Println(f(10)) //10 fmt.Println(f(20)) //30&#125; 变量 f 是一个函数，并且引用了外部变量 x，此时 f 就是一个闭包，在 f 有效的生命周期中，x 一直有效；闭包的概念与 js、py 中基本一致。 Deferred函数defer 语句会将其后面跟随的语句进行延迟处理；也就是说，先被defer的语句最后被执行，最后被defer的语句，最先被执行。 12345678910111213func main() &#123; fmt.Println("start") defer fmt.Println(1) defer fmt.Println(2) defer fmt.Println(3) fmt.Println("end")&#125;// out:// start// end// 3// 2// 1 正是因为这个特性，非常方便的处理资源释放问题。比如：资源清理、文件关闭、解锁及记录时间等。在 Go 语言的函数中 return 语句在底层并不是原子操作，它分为给返回值赋值和 RET 指令两步。而 defer 语句执行的时机就在返回值赋值操作后，RET 指令执行前。 PanicGo 没有异常机制，但是可以使用 panic/recover 模式来处理错误。 panic 可以在任何地方引发，但 recover 只有在 defer 调用的函数中有效。 123456789func funcB() &#123; defer func() &#123; // 如果程序出出现了 panic 错误,可以通过 recover 恢复过来 if err := recover(); err != nil &#123; fmt.Println("recover in B") &#125; &#125;() panic("panic in B")&#125; 当 panic 发生时，程序会中断执行，并立即执行该 goroutine （理解为线程）中被延迟的函数（defer） 指针Go 中的指针相比 C 已经简化了很多了，虽然进行了简化，但是在 Go 中还是非常重要的一个东西；在 Go 中，指针不能参与运算，只有值传递一种方式，不用特意的去理解，跟 Java 其实也没多大区别，地址也是值。 12345678910func swap(a, b *int) &#123; *a, *b = *b, *a&#125;// callswap(&amp;a, &amp;b)func editArr(arr *[3]int) &#123; arr[0] = 100&#125; 当然，上面的交换使用返回值的方式也可以做到，这里仅仅是单纯的演示指针。在 editArr 中接收的是数组的地址（指针），下面可以直接使用 arr 这个名字来进行操作，不需要在带着 * 这个还是非常方便的。简单来说，&amp; 就是取地址，* 就是获取指针指向的变量内容，例如指向 int 类型的指针的类型就是 *int，如果 p 指向的是一个地址（&amp;name）那么通过指针修改变量值就需要使用 *p。 结构体和方法这里本来想说面向对象的，但是 Go 它不是面向对象的语言，它只有封装，没有继承和多态，这也算是好消息吧；所以它没有 class 只有 struct，结构体是一个聚合类型，由零个或者任意多个任意类型组合的实体。 123456789101112131415161718192021222324252627282930313233343536373839404142func main() &#123; // 定义 var root treeNode fmt.Println(root) root = treeNode&#123;value: 3&#125; root.left = &amp;treeNode&#123;&#125; root.right = &amp;treeNode&#123;4, nil, nil&#125; // 指针可以直接用 . 调用 root.right.left = new(treeNode) root.print() root.setVal(233) root.print()&#125;// 工厂函数func createNode(val int) *treeNode &#123; return &amp;treeNode&#123;value: val&#125;&#125;// 定义结构体type treeNode struct &#123; value int left, right *treeNode&#125;// 定义结构体的方法func (node treeNode) print() &#123; fmt.Println(node.value)&#125;func (node *treeNode) setVal(val int) &#123; node.value = val&#125;// 因为经常使用指针操作结构体，一般定义func show()&#123; pp := &amp;treeNode&#123;3&#125; // 等价于 pp := new(treeNode) *pp = treeNode&#123;3&#125;&#125; 结构体中不需要有构造函数，就是指针类型也可以直接用『点』调用，使用 new 函数也可以创建一个空结构体返回地址（区别 make 是创建空间），对结构体指针可以直接使用点调用，其实是个语法糖，会自动转换为 (*var).xxx； 命名类型（type）可以让我们方便的给一个特殊类型一个名字，因为 struct 的声明非常长，一般使用 type 来取一个别名。 我们可以通过工厂函数来创建一个结构体，并且可以将地址返回给调用方用，至于对象是在堆还是栈分配，这个不需要太过在意，由编译器决定，当你返回一个地址，那么会被认为需要给其他地方用，就会在堆分配，接受 GC 的管理，反之可能就直接在栈分配。需要注意的是，结构体中的方法是定义中外面的，与普通函数相比，多了一个接受者参数，放在最前面；本质上与普通函数并没有什么区别；也正是这样，在修改的函数中，因为 Go 只有值传递（变量拷贝），所以需要定义参数为指针，其他的该怎么用就怎么用。如果结构体比较大，为了避免复制，应该考虑优先使用指针接收；结构体通常是使用指针处理，所以定义的时候一般会直接定义成地址，如代码的最后；结构体内的定义如果都是可以比较的，那么结构体本身也是可以比较的。 定义结构体时，是按行来定义内部类型；有特殊情况，就是可以省去名字，只写类型，这样就是匿名成员，匿名成员的特点就是可以直接点出其内部的元素（或者是叶子属性）而不需要给出完整路径，有点继承的感觉，也算是一个语法糖；因为匿名成员也会有一个隐式的名字，所以匿名成员类型只允许有一个；同时因为没有名字，权限也就根据子类型的定义来了。在初始化的时候，匿名成员就有问题了，不能直接通过名字或者字面值直接赋值，只能通过完整的类型名来赋值，代码参考 Github，当不同的匿名成员有相同的字段时，就需要使用完整的路径来指定。结构体的内存布局是连续的，空的结构体是不占空间的。 封装与扩展对于封装的一些规约，首字母大写表示是 Public 的，首字母小写表示 Private，命名都是采用驼峰。每一个目录只有一个包（package）其中 main 包比较特殊，它是程序的执行入口；为结构体定义的方法必须放在同一个包内，可以是不同的文件；跨包引用的时候用 包名.结构体 这样来调用，当然需要使用 import 关键字来进行导入。不过既然限制结构体方法定义必须同一个包，那么扩展别人的库就有点难受来，因为 Go 没有继承，解决方案是使用别名或者使用组合的方式。 1234567891011121314151617181920212223func main()&#123; // 调用扩展的结构体 extRoot := myTreeNode&#123;&amp;root&#125; extRoot.echo()&#125;// 组合的方式进行扩展 treeNodetype myTreeNode struct &#123; node *treeNode&#125;func (myNode *myTreeNode) echo() &#123; if myNode == nil &#123; return &#125; left := myTreeNode&#123;myNode.node.left&#125; right := myTreeNode&#123;myNode.node.right&#125; fmt.Println("扩展...") left.node.print() myNode.node.print() right.node.print()&#125; 组合就是设计模式中的组合方式，以上代码要对比结构体中的 treeNode 定义来看；而另一种别名就是使用 type 关键字 + 函数指针的方式来通过改变原始对象达到目的，省略相关代码。封装后也可以像 Java 一样提供 getter 和 setter，Go 中的习惯是 getter 方法会直接省略前面的 get；Go 的编程风格不禁止直接导出字段的行为，但是这样意味着你之后不能随意的删除，为了兼容。 import 关键字默认会从标准库（GOROOT）和 GOPATH 下面寻找依赖，所以导入之前要确认是否存在；定义包名要尽量短小，因为给别人用的时候需要使用包名来确定范围，参考 fmt。导入的包就必须要使用，如果不用编译会失败，这也体现出为了性能，真的是任性。 因为一个 package 可以有多个文件，默认按照文件名排序后依次初始化，因为函数外不允许进行逻辑计算，还可以在其中定义一个或多个 init 函数用来做逻辑初始化（类似其他语言的构造），它与普通的函数没有多大区别，仅仅是会在加载时自动调用，多个的话就按照定义的顺序来，并且 init 函数无法被手动调用和引用。还有就是 main 包总是在最后被加载。 接口Go 中既想要 python、c++ 中的灵活性（只需要有这个方法，就能调用），又想要 java 中的类型检查（不需要等到运行或者编译时才能确定是否具有相应的方法）；传统语言中的具体逻辑是由实现者来定义的，Go 中也反过来来，接口由使用者来定义，并且实现是隐式的，你只要有相应的方法就可以；同时，Go 是面向接口编程，Go 中的接口是一种类型，一种抽象的类型。 123456789101112131415161718192021222324252627// 使用者type Retriever interface &#123; // 接口中默认就是 func 声明 Get(url string) string&#125;func down(r Retriever) string &#123; return r.Get("https://bfchengnuo.com")&#125;func main() &#123; fake := mock.Retriever&#123;"Mps....."&#125; fmt.Println(down(fake))&#125;// 实现者package mocktype Retriever struct &#123; Content string&#125;func (r Retriever) Get(url string) string &#123; return r.Content&#125; 接口的定义是跟使用者一起的，而实现看起来跟一般的结构体并没有什么不同，所以说是隐式的；接口可以简单看作是一组方法的集合，是duck-type programming的一种体现，接口本来就是用来抽象的，看到一个接口，我们并不知道它具体是啥，但是知道它能干啥，这跟 Java 中的概念差不多；按照规范，接口的名称一般会以 er 结尾，参数名、返回值名可以省略，只写类型。值接收者实现的接口，具体的结构体可以是值类型也可以是指针，Go 会自动处理；而如果是指针接收者的实现，那么只能传入指针类型；不管选择那一种，都是实现者来决定的。同样，一个类型可以实现多个接口，接口也可以进行嵌套。同时还存在空接口的类型，使用空接口可以实现接收任意类型的函数参数；空接口作为 map 的值就可以实现保存任意值的字典。 如果使用接口参数，尤其是空接口，我们根本不知道具体是什么类型，类型断言就是解决这个问题的，类型断言用法：x.(T)，空接口使用的非常广泛； 12345678910func main() &#123; var x interface&#123;&#125; x = "Hello 沙河" v, ok := x.(string) if ok &#123; fmt.Println(v) &#125; else &#123; fmt.Println("类型断言失败") &#125;&#125; 如果需要多次断言，可以使用 switch 写法会舒服一点。只有当有两个或两个以上的具体类型必须以相同的方式进行处理时才需要定义接口。不要为了接口而写接口，那样只会增加不必要的抽象，导致不必要的运行时损耗。 类型-type在任何语言中，都会有这样的情况：一些变量有着相同的内部结构，但是表示的却是完全不同的概念，例如 int 可以表示循环的控制变量，也可以表示时间戳；而 type 就是用来分割不同的概念类型；使用方法为：type 类型名字 底层类型声明语句一般出现在包一级，如果使用大写字母开头，表示包外也可以访问。这样声明后，虽然两个类型可能有相同的底层数据结构，但是它们却是完全不同的数据类型，多用在结构体中；底层类型想要转换为包装的别名必须通过显式类型转换，类型转换可以使用 name(x) 的通用方式进行。经过类型定义后，你可以给这个类型添加方法，也可以使用它原始类型的特性，例如如果底层类型是 int，你就可以直接进行基本的逻辑运算。 另外，可以定义函数类型，只要符合签名就可以被赋值： 12345678type calculation func(int, int) intfunc add(x, y int) int &#123; return x + y&#125;var c calculationc = add 以上也体现了一定的动态性，可以对比接口来看。 在 1.9 版本出现了另一种用法，就是别名，区别于类型定义，别名只是换了个名称，本质还是原来的类型；定义：type TypeAlias = Type使用上与原始类型 Type 一样，仅仅是名字不同。 依赖管理最早的时候，Go 语言所依赖的所有的第三方库都放在 GOPATH 这个目录下面，这就导致了同一个库只能保存一个版本的代码。如果不同的项目依赖同一个第三方的库的不同版本，应该怎么解决？Go 语言从 v1.5 开始开始引入 vendor 模式，如果项目目录下有 vendor 目录，那么 go 工具链会优先使用 vendor 内的包进行编译、测试等。godep 是一个通过 vender 模式实现的 Go 语言的第三方依赖管理工具，类似的还有由社区维护准官方包管理工具 dep。 go module 是 Go1.11 版本之后官方推出的版本管理工具，并且从 Go1.13 版本开始，go module 将是 Go 语言默认的依赖管理工具。 所以接下来只说 go module，要启用 go module 支持首先要设置环境变量 GO111MODULE ，通过它可以开启或关闭模块支持，它有三个可选值：off、on、auto，默认值是auto。 GO111MODULE=off禁用模块支持，编译时会从GOPATH和vendor文件夹中查找包。 GO111MODULE=on启用模块支持，编译时会忽略GOPATH和vendor文件夹，只根据 go.mod下载依赖。 GO111MODULE=auto，当项目在$GOPATH/src外且项目根目录有go.mod文件时，开启模块支持。 简单来说，设置GO111MODULE=on之后就可以使用go module了，以后就没有必要在 GOPATH 中创建项目了，并且还能够很好的管理项目依赖的第三方包信息。使用 go module 管理依赖后会在项目根目录下生成两个文件go.mod（记录了项目所有的依赖信息）和go.sum（记录每个依赖库的版本和哈希值）。常用命令： 命令 作用 go mod download 下载依赖包到本地（默认为 GOPATH/pkg/mod 目录） go mod edit 编辑 go.mod 文件 go mod graph 打印模块依赖图 go mod init 初始化当前文件夹，创建 go.mod 文件 go mod tidy 增加缺少的包，删除无用的包 go mod vendor 将依赖复制到 vendor 目录下 go mod verify 校验依赖 go mod why 解释为什么需要依赖 因为 Go 是谷歌的，包管理也肯定在谷歌的服务器，所以对大陆肯定不友好，所以建议设置代理：目前公开的代理服务器的地址有： goproxy.io 开源，为 Go 模块而生 goproxy.cn 由国内的七牛云提供。 Windows 下设置 GOPROXY 的命令为：go env -w GOPROXY=https://goproxy.cn,directMacOS 或 Linux 下设置 GOPROXY 的命令为：export GOPROXY=https://goproxy.cn更改完毕可以使用 go env 查看，使用 go module 非常简单，只需要两步命令： 1234# 生成 go.mod 文件，如果不在 GOPATH 下需要指定完整的模块路径go mod init# 自动发现依赖go get 在项目中执行go get命令可以下载依赖包，并且还可以指定下载的版本（name@x.x），如果下载所有依赖可以使用 go mod download 命令。 包使用包的声明一般写当前文件夹名，一个文件夹下不允许有不同的 package 定义，同一个 package 的文件也不允许出现在多个文件夹下，导包的时候要写全路径（对于 GOPATH）；给包设置别名：import 别名 &quot;包的路径&quot;匿名导包（不使用，但是会被编译到可执行文件中）：import _ &quot;包的路径&quot; 更多内容请到 Github 仓库查看。 拓展Go语言学习之路Golang-100-Days（文档，未看）新版本Golang的包管理入门教程]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EffectiveJava后知后觉]]></title>
    <url>%2F2020%2F04%2F30%2FEffectiveJava%E5%90%8E%E7%9F%A5%E5%90%8E%E8%A7%89%2F</url>
    <content type="text"><![CDATA[这本书又叫高效 Java，书中的很多条目还是让我眼前一亮的，尤其并发和序列化，大概是因为这一块本来接触的就不多吧，但也有不少内容其实是早就知道的，所以作为我个人的记录，我并不会一条条的罗列出来，主要记录一些平时用的很少或者记得模糊的东西。这本书感觉如果是对入门不久的开发者来说，是非常有必要读一下的，但是如果你经常阅读 JDK 或者 Spring 等开源框架源码的老鸟，其实大部分你都可以跳着看了，因为从它们的源码中基本就能学到大部分的技巧。 创建和销毁对象对象的创建与销毁算是比较耗时的，在合适的时机创建对象在高性能的系统中是非常有必要的；频繁的 GC 也会让系统卡顿。 静态工厂代替构造器一个类允许客户端获取其实例的传统方式是提供一个公共构造方法。还有一种比较好的选择就是静态工厂，相比构造器，主要的优势有： 拥有名称，描述更为清晰。 可避免每次调用都创建一个新对象；可以确保是一个单例或者不可实例化的亦或者不可变的，这种类被称为实例受控的类。 可以返回原返回类型的子类型，可以用这种方式来隐藏实现类，适用于基于接口的框架。 返回对象的类可以根据输入参数的不同而不同（任何子类型都是允许的） 在编写包含该方法的类时，返回的对象的类不需要存在。这种灵活的方案构成了服务提供者框架的基础，例如 JDBC。 当然也是有缺点的： 类一般不含有公有或者受保护的构造器，所以就不能被子类化。这也鼓励开发者多使用组合而不是继承。 开发者很难找到它们。不过也有一些惯用名称，举例说明：from：类型转换方法，它接受单个参数并返回此类型的相应实例；of：聚合方法，接受多个参数并返回该类型的实例，并把他们合并在一起；valueOf：from 和 to 更为详细的替代方式；instance 或 getinstance：返回一个由其参数 (如果有的话) 描述的实例，但不能说它具有相同的值；create 或 newInstance：与 instance 或 getInstance 类似，除此之外该方法保证每次调用返回一个新的实例；getType：与 getInstance 类似，但是在工厂方法处于不同的类中的时候使用（Type 是工厂方法返回的对象类型）；newType：与 newInstance 类似，但是在工厂方法处于不同的类中的时候使用（Type 是工厂方法返回的对象类型）；type：getType 和 newType 简洁的替代方式。 例如下面一个 boolean 提升为包装的例子： 123public static Boolean valueOf(boolean b) &#123; return b ? Boolean.TRUE : Boolean.FALSE;&#125; 这里要区分与工厂模式的差异，此例中与工厂模式没有什么直接联系。如果你使用接口的静态方法，注意不能为私有，但是私有却是必要的，Java 9 允许私有静态方法，但静态字段和静态成员类仍然需要公开。 构造方法参数过多时使用builder模式静态工厂和构造器有个共同的局限性，不能很好的扩展到大量的可选参数。构建器的一般套路就是让客户端利用必要的参数来调用构造器或者静态工厂得到一个 builder 对象，然后客户端在这个对象上调用各种 setter 方法设置相关参数，最后客户端调用无参的 build 方法来生成不可变的对象。假设使用一个个的 setter 方法来设置，这时候对象就会出现不完整状态，单线程倒还无所谓。一般来说，构建器是可以使用调用链的，这种写法非常舒服，记得在 Android 中有大量使用。 当然，构建器也有它的局限性，要创建对象你得先搞出一个构建器来，创建对象需要开销啊，在非常注重性能的系统里就有点问题了。构建器显得更加冗长，因此只有在很多参数的时候才使用，比如 4 个以上，如果以后有可能会增加参数那么一开始就最好使用构建器。在参数众多，并且很多参数是可选的或者相同类型，那么 builder 模式是个很不错的选择。 关于单例单例真的是相当有料啊，常见的“饿汉式”基本可以保证单例，只能用大部分情况下，最简单的，使用反射可以轻易的调用私有的构造函数，对应的防范措施就是修改构造器，让它在创建的时候检查是否已经存在，如果已存在抛出异常。如果单例的对象需求为可序列化的，那么仅仅实现标记接口是不行的，这样还是可能会出现第二个；为了维护并保证单例，必须声明所有实例域都是瞬时（transient）的，并提供一个 readResolve 方法（具体实现可以是返回单例对象即可） 从 1.5 之后，实现单例还可以使用枚举，这种方式虽然跟上面的公有静态方法获取没啥区别，但是更简洁；无偿的提供了序列化机制，绝对防止多次实例化，这差不多已经成为最佳的实现单例方法，虽然没广泛使用。 然后说起工具类，一般都是静态方法组成的，也不需要实例化，为了防止实例化，可以把构造改为私有，但只有这样还不行，和单例不同，这样内部可以实例化，但是工具类内部也没有实例化的需求，所以在构造里直接抛一个异常是最简单的做法，只不过这样的代码看起来比较奇怪，写上注释比较好，当然它也就不可能有子类了。 避免创建不必要的对象当然，能重用就不要浪费资源去创建相同功能的新对象，如果对象是不可变的那么它始终可以被重用（使用单例模式）。对于同时提供了静态工厂方法和构造器的不可变类，通常使用静态工厂而不是构造器，以避免创建不必要的对象，例如 Boolean.valueOf(String) 和 Boolean(String)，后者在 Java9 中已被废弃。 有些情况下看起来并不是很明细，例如 Map 中的 ketSet，返回键值的 set 集合，第一次看上去会以为每次调用都会创建一个新的 set 返回，我们可以对这个 set 进行操作；实际上，他们返回的 set 都是同一个，因为对应的是一个 Map，可以称作为这个 Map 的一个视图（适配器），所以当改变这个 set 与之关联的都会变化。在使用正则匹配的时候如果多次使用也要使用 Pattern 实例，因为创建它是昂贵的。PS：自动装箱是非常耗费性能的！与基本数据类型尽量不要混用。 当然，小对象的构造器只做少量的工作，所以小对象的创建和回收动作是非常廉价的，通过附加对象提升程序的清晰性和简洁性是好事。 对象的公共方法重写 equals 方法看起来很简单，但是有很多方式会导致重写出错，其结果可能是可怕的。避免此问 题的最简单方法是不覆盖 equals 方法，在这种情况下，类的每个实例只与自身相等。什么时候需要重写 equals 方法呢?如果一个类包含一个逻辑相等(logical equality)的概念，此概 念有别于对象标识(object identity)，而且父类还没有重写过 equals 方法。这种情况多发生在值类型的情况，例如 String、Integer 之类。如果真的需要重写，请保证下面的几个约定： 自反性: 对于任何非空引用 x， x.equals(x) 必须返回 true。即：一个对象必须与自身相等 对称性: 对于任何非空引用 x 和 y，如果且仅当 y.equals(x) 返回 true 时 x.equals(y) 必须 返回 true。即：任何两个对象必须在是否相等的问题上达成一致 传递性: 对于任何非空引用 x、y、z，如果 x.equals(y) 返回 true， y.equals(z) 返回 true，则 x.equals(z) 必须返回 true。即：如果第一个对象等于第二个对象，第二个 对象等于第三个对象，那么第一个对象必须等于第三个对象 一致性: 对于任何非空引用 x 和 y，如果在 equals 比较中使用的信息没有修改，则 x.equals(y) 的多次调用必须始终返回 true 或始终返回 false。即：如果两个对象是相等的，除非一个(或两 个)对象被修改了， 那么它们必须始终保持相等 对于任何非空引用 x， x.equals(null) 必须返回 false。即：所有的对象都必须不等于 null 如果不遵守，程序可能就会出现各种奇奇怪怪的问题。对于类型为非 float 或 double 的基本类型，使用 == 运算符进行比较；对于对象引用属性，递归地调用 equals 方法；对于 float 基本类型的属性，使用静态 Float.compare() 方法（double 也是类似，避免使用 equals 装箱操作）；对于数组属性，将这些准则应用于每个元素。 如果数组属性中的每 个元素都很重要，请使用其中一个重载的 Arrays.equals 方法，对于可能存在空的情况，使用 Objects 的 equals 方法也许不错。equals 方法的性能可能受到属性比较顺序的影响。 为了获得最佳性能，你应该首先比较最可能不同 的属性，开销比较小的属性，或者最好是两者都满足(derived fields) 编写和测试 equals(和 hashCode)方法很繁琐，生的代码也很普通。替代手动编写和测试这些方 法的优雅的手段是，使用谷歌 AutoValue 开源框架，该框架自动为你生成这些方法，只需在类上添加一 个注解即可。虽然 IDE 也会提供相关功能，但是生成的代码很冗长，并且不能动态更新。 然而，大多数情况下，我们并不需要去重写 eq。 在每个类中，在重写 equals 方法的时侯，一定要重写 hashcode 方法。 如果不这样做，你的类违反了 hashCode 的通用约定，这会阻止它在 HashMap 和 HashSet 这样的集合中正常工作，相等的对象必须具有相等的哈希码。这在根据类的 equals 方法，两个不同的实例在逻辑上是相同的情况下显得很重要。例如 HashMap 就做了优化，缓存了与每一项(entry)相关的哈希码，如果哈希码不匹配，则不会检查对象是否相等了。哈希的生成你可能看到很多都用一个 31 这个数，因为它是一个奇数的素数，避免乘法溢出，并且它可以被 JVM 优化为位运算。可以看出 equals 和 hashCode 使用了大量递归，这两个方法还是很重的。如果一个类是不可变的，并且计算哈希码的代价很大，那么可以考虑在对象中缓存哈希码，而不是在每次请求时重新计算哈希码。 如果你认为这种类型的大多数对象将被用作哈希键，那么应该在创建实例时计算哈希码。最后，不要试图从哈希码计算中排除重要的属性来提高性能 对于 toString 的通用约定是：建议所有的子类重写这个方法，不过在静态工具类中编写 toString 方法是没有意义的，同样枚举中也完全没有必要。 虽然存在 Cloneable 这样的接口，不幸的是，它没有达到这个目的。它的主要缺点是缺少 clone 方法，而 Object 的 clone 方法是受保护的。你只能借助反射来调用，仅仅因为它实现了 Cloneable 接口，就调用对象上的 clone 方法这想法是行不通的。那么这个接口的意义是，如果一个类实现了 Cloneable 接口，那么 Object 的 clone 方法将返回该对象的逐个属性 (field-by-field) 拷贝；否则会抛出异常，这确实是一个不太好的设计。通常情况下，实现一个接口用来表示可以为客户做什么。但对于 Cloneable 接 口，它会修改父类上受保护方法的行为。 12345678@Overridepublic PhoneNumber clone() &#123; try &#123; return (PhoneNumber) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; throw new AssertionError(); // Can't happen &#125;&#125; 这是一个简单的例子，在没有引用变量的情况下是可行的，但是如果涉及引用变量，就得递归调用 clone，同时它用到了协变规范；这也牵扯到我们常说的深度克隆和浅度克隆。在数组上调用 clone 会返回一个数组，其运行时和编译时类型与被克隆的数组相同。这是复制数组的首选习语。事实上，数组是 clone 机制的唯一有力的用途。如果 elements 属性是 final 的，则以前的解决方案将不起作用，因为克隆将被禁止向该属性分配新的值，在克隆上真的是各种问题，并且 Object 的克隆方法是不同步的，也正是这样用的并不多。如果确实要用，实现 Cloneable 的所有类应该重写公共 clone 方法，而这个方法的返回类型是类本身。这个方法应该首先调用 super.clone，然后修复任何需要修复的属性。对象复制更好的方法是提供一个复制构造方法或复制工厂。考虑到与 Cloneable 接口相关的所有问题，新的接口不应该继承它，新的可扩展类不应该实现它。这个规则的一个明显的例外是数组，它最好用 clone 方法复制。这里给几个参考：Java对象的属性拷贝效率比较、一入Java深似海（评价 Java 中的 clone 方法？） 类和接口组合优于继承不要继承一个现有的类，而应该给你的新类增加 一个私有属性，该属性是现有类的实例引用，这种设计被称为组合(composition)，因为现有的类成为新类的组成部分。如果使用继承，那么当父类有所变化时，子类也会受影响，即使子类的代码并没有任何变化。这很像是装饰器模式，这里要区别与代理模式；包装类的缺点很少， 一个警告是包装类不适合在回调框架中使用，可能会存在 Self 问题（this 指向）；有些人担心转发方法调用的性能影响，以及包装对象对内存占用， 两者在实践中都没有太大的影响。只有在子类真的是父类的子类型的情况下，继承才是合适的。所以，从这个角度，JDK 中 Stack 类不应该继承 Vector 类，Properties 不应该继承 Hashtable，它们之间使用组合更加合适。继承是强大的，但它是有问题的，因为它违反封装。 只有在子类和父类之间存在真正的子类型关系时才适用。 接口的默认方法从 Java8 开始，接口可以有默认方法了，这确实解决了一些问题，但是也引入了一些新问题，跟继承类似，接口的实现者是感知不到接口中新增的默认方法，例如，一个将集合同步化的工具类，在接口新增了默认方法后并不会感知，这也导致这个方法不会被同步，如果作者不及时重写默认方法，可能就会导致问题出现。在默认方法的情况下，接口的现有实现类可以在没有错误或警告的情况下编译，但在运行时可能会失败。所以，应该避免使用默认方法向现有的接口添加新的方法，除非这个需要是关键的。 常量接口模式所谓的常量接口模式就是定义一个接口，里面用来存储常量；常量接口模式是对接口的糟糕使用，对类的用户来说，类实现一个常量接口是没有意义 的；事实上，它甚至可能使他们感到困惑，如果实现了常量接口，那么它的所有子类的命名空间都会被接口中的常量所污染，并且未来版本中如果不需要常量了，仍然需要实现接口，来保证二进制兼容。JDK 中也有部分实现，这些接口应该被视为不规范的，不应该被效仿。更好的方式应该是使用一个不可实例化的类来做，抽象类也不可以，容易被误解是来做继承的。总之，接口只能用于定义类型。 它们不应该仅用于导出常量。 关于内部类或者叫嵌套类， 如果一个嵌套类在其他一些情况下是有用的，那么它应该是一个顶级类。有四种嵌套类: 静态成员类，非静态成员类，匿名类和局部类。 除了第一种以外，剩下的三种都被称为内部类。如果嵌套类的实例可以与其宿主类的实例隔离存在，那么嵌套类必须是静态成员类：不可能在没有宿主实例的情况下创建非静态成员类的实例。静态成员类的一个常见用途是作为公共帮助类，类似：Aa.Bb.NAME，非静态成员类的一个常见用法是定义一个 Adapter，可参考 Map 中的各种视图。在非静态成员类中，每个实例都隐藏的引用了自己的宿主，存储这个引用就需要占用时间和空间，更严重的可能会导致 GC 无法回收宿主对象，就是因为引用是不可见的，很难被检测到。私有静态成员类的常见用法是表示由它们的宿主类表示的对象的组件，例如 Map.Entry；总结一下就是：如果一个嵌套的类需要在一个方法之外可见，或者太长而不能很好地适应一个方法，使用一个成员类。如果一个成员类的每个实例都需要一个对其宿主实例的引用，使其成为非静态的;；否则，使其静态。假设这个类属于一个方法内部，如果你只需 要从一个地方创建实例，并且存在一个预置类型来说明这个类的特征，那么把它作为一个匿名类；否则，把它变成局部类。 泛型泛型已经都用的很熟练了，编译时会自动强制转换，相当于运行时擦除，注意这里编译后的字节码还是保存着泛型的信息的，还有就是泛型不可以使用原始类型（没有任何类型参数的泛型类型的名称，简单说就是不用泛型，或者使用 ？通配符的情况），这样编译器没办法帮你检查错误。如果跟数组对比，会发现即有相似又有差异，最大的区别为数组是支持协变的，例如 String 数组可以赋值给 Object 数组，但是泛型不可能，也正是因为这个差异，导致不能使用泛型数组（尽管很少有用，创建无限定通配符类型的数组是合法的），所以作为替代方案一般我们都使用集合来替代，虽然牺牲了一些性能，但是安全。泛型这个东西解释起来不难，但是运用到你写的代码中很难，说白了就是一个设计问题。参数化不难，但是泛型化真的需要一定的水平。 编译器可能无法证明你的代码是安全的，所以会给你发出警告，但是你可以证明，这种情况下你可以使用 @SuppressWarnings 注解来进行标注，并使用注释来说明原因。使用 SuppressWarnings 的时候一定要小心，做到最小范围化，否则可能就出现吞掉其他警告的可能。 泛型的另一个难点是限定通配符的使用： 上限（&lt;? extends E&gt;）说明传入的可以是 E 或者 E 的子类 下限（&lt;? super E&gt;）说明传入的值可以是 E 或者 E 的父类 这个的用法其实有点误导，感觉有点别扭；另外需要注意的一点，可变参数也不可以和泛型同用，因为可变参数的本质就是数组，泛型数组是不可以用的。不过可变参数的情况下还是有点区别，它不会直接像数组那样给你一个错误，而是给你一个警告，这是因为像 T... 这样的用法是安全的，但是 List&lt;T&gt;... 这种就是不安全的，或者可以理解为如果仅仅是单纯的传递参数是安全的，总而言之，可变参数和泛型不能很好地交互。 枚举建议使用枚举类型来代替整型常量，使用枚举确实有很多好处，但是以我的观察，还是用 int 常量的多，我猜可能是因为这种入库的时候方便，如果 Entry 使用枚举定义，可能会有点复杂，并且与前端的约定也一般是 int 标识，还要处理序列化相关，当然如果不涉及入库操作，枚举肯定是首选。同时，枚举确实是非常灵活的，例如： 12345678910111213141516171819202122232425262728293031323334public enum Operation &#123; PLUS, MINUS, TIMES, DIVIDE; // Do the arithmetic operation represented by this constant public double apply(double x, double y) &#123; switch(this) &#123; case PLUS: return x + y; case MINUS: return x - y; case TIMES: return x * y; case DIVIDE: return x / y; &#125; throw new AssertionError("Unknown op: " + this); &#125;&#125;// Enum type with constant-specific class bodies and datapublic enum Operation &#123; PLUS("+") &#123; public double apply(double x, double y) &#123; return x + y; &#125; &#125;, MINUS("-") &#123; public double apply(double x, double y) &#123; return x - y; &#125; &#125;, TIMES("*") &#123; public double apply(double x, double y) &#123; return x * y; &#125; &#125;, DIVIDE("/") &#123; public double apply(double x, double y) &#123; return x / y; &#125; &#125;; private final String symbol; Operation(String symbol) &#123; this.symbol = symbol; &#125; @Override public String toString() &#123; return symbol; &#125; public abstract double apply(double x, double y);&#125; 第一种方案是有问题的，首先我们不喜欢有异常参与，主要是展示可以这样用 switch 语法。另外，永远不要从枚举的序号中得出与它相关的值; 请将其 保存在实例属性中（最好避免使 用 ordinal 方法）。 对于 int 类型的常量取值，参考 JDK 的位运算顺序是个不错的选择，即：1 &lt;&lt; 0 // 11 &lt;&lt; 1 // 21 &lt;&lt; 2 // 4使用位域的好处：TBD 枚举其实还是蛮复杂的，不过感觉我们平常用的真的很少，也许确实有一些复杂性在里面。 Lambdas和StreamsLambda 优于匿名类，一行代码对于 lambda 说是理想的，三行代码是合理的最大值。 如果违反这一规定，可能会严重损害程序的可读性。如果枚举类型具有难以理解的常量特定行为，无法在几行内实现，或者需要访问实例属性或方法，那么常量特定的类主体仍然是行之有效的方法。虽然 Lambda 是首选，但是 Lambda 并不能完全替代匿名类，其中还要注意 this 指向的问题。 方法引用优于 lambda 表达式，如果 lambda 变得太长或太复杂，它们也会给你一个结果；你可以从 lambda 中提取代码到一个新的方法中，并用对该方法的引用代替 lambda。 你可以给这个方法一个好名字，并把它文档记录下来（通常 IDE 会有智能提示）。还有就是在 JDK 中就包含了很多内置函数，优先使用这些内置的，没有合适的再自己定义，大部分情况下，自带的就足够了，自己定义的时候记得用 @FunctionalInterface 标记。 对于 Stream API， 该 API 提供了两个关键的抽象：流 (Stream)，表示有限或无限的数据元素序列，以及流管道 (stream pipeline)，表示对这些元素的多级计算。Stream pipeline 通常是惰性 (lazily) 计算求值；直到终结操作被调用后才开始计算，而为了完成终结操作而不需要的数据元素永远不会被计算出来。 这种惰性计算求值的方式，使得无限流成为可能。通常情况下，流管道会按顺序运行，如果要并行，只需要调用 parallel 方法，但是并不建议这么做。Stream API 具有足够的通用性，实际上任何计算都可以使用 Stream 执行，但是「可以」，并不意味着应该这样做；过度使用流会难以阅读和维护。使用 Lambda 过程中，因为没有显式类型声明，所以好的命名非常重要。流式处理有一个缺点是在多个阶段中一旦将值映射到其他值，原始值就丢失了。流与迭代那种方式好，要具体情况具体分析，通常来说迭代不适合并行化，也不适合计算场景。纯函数的结果仅取决于其输入；它不依赖于任何可变状态，也不更新任何状态，即所谓的无任何副作用。 对于流的并行化，看似非常简单，只需要调用一下 parallel，但是编写安全高效的并发代码还是一样的困难，你可能会遇到各种奇奇怪怪的现象，并且可能没有任何错误提示。所以，一定要慎用并行处理。 方法关于方法的设计，例如方法名的选取、参数不要过多、优先使用接口类型等等这些都已是业界规范，除此之外与布尔型参数相比，优先使用两个元素枚举类型。对于重载，也需要警惕，重载(overloaded)方法之间的选择是静态的，而重写 (overridden)方法之间的选择是动态的，这里关键是静态选择，无论你运行时实际是什么类型，都是根据你编写时候来确定走那个重载。可变参数和数组是一种，不可算重载。在数组和 Object 的重载中，null 会优先匹配数组类型，因为比 Object 更具体。很多时候重载不如另起一个更好的名称，例如 readInt、readLong 比使用重载更形象。像自动装箱、Lambda 之类进一步增加了重载的复杂性。开发者应当确保当传递相同的参数时，所有的重载行为都是一致的。在性能关键的情况下使用可变参数时要小心，每次调用可变参数方法都会导致数组分配和初始化。 尽量放回空数组或者集合，不要使用 null，编写客户端的程序员可能忘记编写特殊情况代码来处理 null 返回，如果有证据表明分配空集合会损害性能，可以通过重复返回相同的不可变空集合来避免分配，或者例如 Collections.emptySet 对象；不论是抛出异常还是返回 null，都不合适，前者代价太高，后者需要调用者单独处理；在 Java8 中的 Optional 或许是一个比较好的方案，跟 Guava 中的用法基本一致，容器类型，包括集合、映射、Stream、数组 和 Optional，不应该封装在 Optional 中。需要注意的是：返回包含已装箱基本类型的 Optional 的代价高得惊人，他们有专门的类似 OptionalInt 的包装。在集合或数组中使用 Optional 的键、值或元素几乎都是不合适的。除了作为返回值之外，不应该在任 何其他地方中使用 Optional，使用它也必然会带来一定的性能消耗。 通用程序设计从 Java 7 开始，就不应该再使用 Random。在大多数情况下，选择的随机数生成器现在是 ThreadLocalRandom。 它能产生更高质量的随机数，而且速度非常快。每个程序员都应该熟悉 java.lang、java.util 和 java.io 的基础知识及其子包，也就是不要重复造轮子。若需要精确答案就应避免使用 float 和 double 类型，二进制无法精确表示 0.1，就像十进制无法表示 1/3，要么进行倍数换算成整数运算，要么使用 BigDecimal，不过使用 BigDecimal 的时候一定要用字符串的构造，否则也是不精确的。BigDecimal 与原始算术类型相比很不方便，而且速度要慢得多；如果数值不超过 9 位小数，可以使用 int；如果不超过 18 位，可以使用 long。如果数量可能超过 18 位，则使用 BigDecimal。 警惕自动装箱的风险，在比较的时候 == 所带来的问题，以及包装类型默认值为 null，避免 NPE。 不要去计较效率上的一些小小的得失，在 97% 的情况下，不成熟的优化才是一切问题的根源。优化方面的准则：不要优化；在你还没有绝对清晰的未优化方案之前，请不要进行优化。努力编写好的程序，而不是快速的程序。这并不意味着在程序完成之前可以忽略性能问题，实现上的性能问题可以日后优化，对于架构缺陷，如果不重写系统，就不可能解决限制性能的问题，因此在设计时就要考虑性能。总结一下就是，不要努力写快的程序，要努力写好程序；速度自然会提高。但是在设计系统时一定要考虑性能，特别是在设 API、线路层协议和持久数据格式时。当你完成了系统的构建之后，请度量它的性能。如果足够快，就完成了。如果没有，利用分析器找到问题的根源，并对系统的相关部分进行优化。第一步是检查算法的选择：再多的底层优化也不能弥补算法选择的不足。根据需要重复这个过程，在每次更改之后测量性能，直到你满意为止。 在命名方面，不可实例化的实用程序类通常使用复数名词来命名（xxxxs），转换对象类型的实例方法用 to 开头，返回视图的方法用 as 开头；返回布尔类型的方法用 is 或者 has（比较少）开头；组件（包名）应该很短，通常为 8 个或更少的字符，鼓励使用有意义的缩写，例如 util 而不是 utilities。 异常只有在异常情况下才能使用异常，一切其他的骚操作都是不可取的，不仅模糊了代码意思，还降低性能，不要将它们勇于普通的控制流程。优先使用标准异常例如：IllegalArgumentException、IllegalStateException、UnsupportedOperationException。对于底层抛出的异常，直接抛出会让人摸不着头脑，通常会在高层进行转换，方便阅读，并且最好写清楚文档。 并发和序列化关于如何停止线程，stop 方法早已经废弃，主流是控制标志位的方法，但是请注意，标志位的变量一定要 volatile 保证可见性，因为 JVM 的优化（在非同步代码中可能会重排）和 Java 内存模型的关系，必须要进行 volatile 化；或者使用 synchronized 进行同步读写标志位的方法，注意，是读写都需要同步，单独独立到方法中去。同时，要避免过度同步，在一个被同步的区域内部，不要调用设计成要被覆盖的方法，这个类不知道该方法会做什么事情，也无法控制它，根据外来方法的作用，从同步区域中调用它会导致异常、死锁或者数据损坏（Effective Java 中的例子很经典）。应该将外来代码移出同步区，必要情况可以使用快照方式来避免 CME 等异常，例如使用 JUC 的 CopyOnWriteArrayList。当你不确定的时候，就不要同步类，而应该建立文档，注明它不是线程安全的，让调用者从外部同步；如果你在内部同步了类，就可以使用不同的方法来实现高并发性，例如分拆锁、分离锁和非阻塞并发控制。不建议使用困难的 wait 和 notify，也正是因为正确地使用 wait 和 notify 比较困难，就应该用更高级的并发工具来代替。最常用的同步器是 CountDownLatch 和 Semaphore，较不常用的是 CyclicBarrier 和 Exchanger ，功能最强大的同步器是 Phaser。Lock 字段应该始终声明为 final，对于延迟初始化，除非需要，否则不要这样做。 关于序列化，请优先选择 Java 序列化的替代方案，序列化的一个根本问题是它的可攻击范围太大，且难以保护，而且问题还在不断增多；当你反序列化一个你不信任的字节流时，你就会受到攻击。避免序列化利用的最好方法是永远不要反序列化任何东西。反序列化真的非常危险，你不应该接受来自不可信来源的 RMI 流量，但同时它确实又是必须的，或者说广泛使用的，比如 RMI（远程方法调用）、JMX（Java 管理扩展）和 JMS（Java 消息传递系统）。设计不良的序列化形式，可能会造成严重后果；序列化是一种用于创建对象的超语言机制，或者说『隐藏的构造函数』，readObject 方法实际上相当于另外一个公有的构造器，或者说是一个「用字节流作为唯一参数」的构造器，并且这个字节流可以伪造；如无必要不要实现 Serializable。为了防止字节流的伪造，建议重写 readObject 方法，在调用 defaultReadObject 之后进行手动校验，但这也只能防止部分攻击；自定义序列化规则的时候注意 transient 的运用，反序列化的时候进行手动恢复。当一个对象被反序列化的时候，对于客户端不应该拥有的对象引用，如果那个字段包含了这样的对象引用，就必须做 保护性拷贝，这是非常重要的。如果单例对象要序列化，请在 readObject 忽略任何序列化相关的逻辑，因为默认无论你做如何防范，它总会返回一个新对象。readResolve 的可访问性 (accessibility) 也十分重要，单例的防攻击可以使用枚举类限制。 PS：领先的跨平台结构化数据表示是 JSON 和 Protocol Buers，也称为 protobuf（二进制，效率更高）。如果没有显式声明 serialVersionUID，系统会自动使用 SHA-1 进行生成。建议是无论选择哪种序列化形式，都要在编写的每个可序列化类中声明显式的序列版本 UID，目的是为了版本兼容。建议使用序列化代理模式（为可序列化的类设计一个私有的静态嵌套类，精确地表示外围类的逻辑状态。这个嵌套类被称为序列化代理），序列化代理方式可以阻止伪字节流的攻击以及内部字段的盗用攻击，但是也有一定的局限性，例如性能方面的损耗。 其他如果 instanceof 的第一个操作数是 null，那么不管第二个操作数是那种类型，都返回 false； 函数式方法：每次执行返回一个新的实例，而不是修改这个实例。要善用不可变对象；缺点主要是每个不同的值都需要创建一个单独的对象，创建过程可能代价很高。 除非有充分的理由使类成为可变类，否则类应该是不可变的，唯一的缺点是在某些情况下可能会出现性能问题。 序列化攻击。 Java7 开始有了下划线语法，对于底数为 10 的数字，无论是整型还是浮点型的，都应该用下划线将数字分成三个数字组，表示一千的正负幂。 Java8 开始支持可重复注解，使用 @Repeatable； 标记接口类型的存在允许在编译时捕获错误，如果使用标记注解，则直到运行时才能捕获错误。标记接口对于标记注解的另一个优点是可以更精确地定位目标； 优先使用基本类型而不是基本类型的包装类。 在 Java 7 中添加的 Objects.requireNonNull 方 法灵活方便，因此没有理由再手动执行空值检查。 性能监控方面，JDK Micro Benchmark Framework：JMH]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TypeScript基础语法]]></title>
    <url>%2F2020%2F04%2F25%2FTypeScript%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[TypeScript 是 JavaScript 的一个超集，或者说 TS 是基于 JS 的，主要提供了类型系统和对 ES6 的支持，它由 Microsoft 开发，代码开源于 GitHub 上，它可以编译成纯 JavaScript；编译出来的 JavaScript 可以运行在任何浏览器上（不能直接运行 TS）。我对其简单的理解就是强类型的 JS，强类型可以给我们带来静态语言的一些好处，比如可读性和可维护性，也更加适合 IDE 做语法分析和代码提示，同时它也有强大的类型推断，缺点就是学习成本和开发成本（挺明显的）。这样看来 TS 对写后端项目可是极其友好，学 TS 也是只看与 JS 的不同点即可，对于 JS 和 Java 基础不错的人，在很多方面真的都是似曾相识，理解起来简单很多。现在的前端项目使用 TS 的越来越多，不管怎么样，还是了解一下为好。 安装需要 Node 的环境，并且 Node 无法直接运行 TS 文件，所以使用下面的命令安装 ts 工具： 123456npm install typescript -g# 安装便捷的 node 运行工具npm install ts-node -g# 监控文件变化，自动编译tsc -w 然后就可以使用 tsc、ts-node 这些命令了。 常用类型静态类型带来的好处不光是强约束，还有代码提示，因为类型固定，所以此类型的方法也都明确知道了，通过代码来说明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 基础类型const num: number = 123;const tag: string = 'tt';let temp: number | string;let tempArr: (number | string)[];// 元组const tupleArr: [number, string] = [1, 'a'];class Student &#123;&#125;// 对象类型，定义也可以独立出去，例如 studentconst Person: &#123; name: string; age: number;&#125; = &#123; name: 'mps', age: 12,&#125;;const arr: number[] = [1, 2, 3];const stu: Student = new Student();// 函数类型的两种定义const fun: () =&gt; number = () =&gt; &#123; return 2 ** 2;&#125;;// 可省略返回值类型使用自动推断const fun2 = () =&gt; &#123; return 2 ** 2;&#125;;// 自动推断返回值function getSum(x: number, y: number) &#123; return x + y;&#125;// 手动约束返回值function getSum2(x: number, y: number): number &#123; return x + y;&#125;// 解构语法赋值约束function getSum3( &#123; x, y &#125;: &#123; x: number; y: number &#125; ): number &#123; return x + y;&#125; 像类似 const num: number 的定义叫做类型注解，也就是我们告诉 TS 它是什么类型；如果直接不写类型直接写值，那么会自动推断出类型来，当无法推断的时候，只能使用类型注解的方式了。特殊的，如果返回值空，使用关键字 void 标注，最好还是标注一下，虽然有自动推断；其他特殊的有 never 表示永远不会执行到最后，例如抛出异常或者无限循环之类。对于元组类型，可以理解为固定长度，固定顺序的数组，需要你精确控制各个类型的顺序，例如读取 CSV 的时候。 接口概念不多说，跟静态语言里的基本一致，也是抽取共性： 1234567891011121314151617181920212223242526272829interface Point &#123; x: number; y: number; desc?: string; alert?(): void; alert2?: () =&gt; void; readonly name?: string; // 允许其他类型 [propName: string]: any;&#125;const p: Point = &#123; x: 2, y: 3,&#125;;// 接口函数类型interface echoFun &#123; (desc: string): string;&#125;const str: echoFun = (desc) =&gt; &#123; return desc;&#125;;// 接口函数『重载』interface Fun &#123; (): string; (param: string): string;&#125; 接口非常灵活，能用接口就用接口，实在做不了再考虑类型别名之类。其他的都差不多，都是给 class 实现用的，可以多实现，接口也可以继承。这里补充一下，当你使用结构赋值直接传递的时候 TS 是强校验，就是说多一个少一个都不行；但是你通过对象引用来传递的时候，就不会那么强了，只要不少就行，可以多；亦或者定义中允许其他类型，例如上面的接口。在 TS 编译过程中，接口会被消除，也就是接口只是在 TS 开发阶段帮助我们的工具。 类TypeScript 除了实现了所有 ES6 中的类的功能以外，还添加了一些新的用法。类在面向对象语言中非常常见，用来封装的好工具，继承、重写等概念之类的不多说，熟悉静态语言的都很熟了，在这里也都基本一样。 12345678910111213141516171819202122232425262728293031class Animal &#123; public desc: string = ''; private _name: string; static isAnimal(a) &#123; return a instanceof Animal; &#125; constructor(name: string, public tag: string) &#123; this._name = name; &#125; get name() &#123; return this._name; &#125; set name(name: string) &#123; this._name = name; &#125;&#125;let a = new Animal('Kitty', 'tag'); // setter: Kittya.name = 'Tom'; // setter: Tomconsole.log(a.name); // Tomconsole.log(a.tag); // tagAnimal.isAnimal(a); // true// 单例模式，这里使用普通的延迟加载也可以class Demo &#123; private static INSTANCE = new Demo(); private constructor() &#123;&#125; static getInstance() &#123; return this.INSTANCE; &#125;&#125; 相比 Java，getter 和 setter 方法定义有些许不同，调用的时候也不需要按照方法的形式，不过总体来说基本一致。静态方法可以直接调用，也可以通过实例调用，这一点上一致；可以定义抽象类，可以使用 public、private 和 protected 访问修饰符，不写的话默认是 public；并且可以通过 readonly 关键字定义只读属性。在构造方法中通过类似 public/private tag: string 的语法可以快速定义类似上例中 name 的属性，也就是省去了定义和赋值的代码。在 ES7 中，还加入了实例属性和静态属性，真是越来越熟悉了。 类型别名在定义比较复杂的类型声明时，可以使用类型别名抽取，常用于联合类型（叠加）： 1234567891011121314151617181920212223242526272829303132333435363738394041type Teacher = &#123; name: string; age: number;&#125;const t: Teacher= &#123; name: 'mps', age: 12,&#125;;// 联合类型type Name = string;type NameResolver = () =&gt; string;type NameOrResolver = Name | NameResolver;class Stu &#123; name: string = 'mps'; clazz: string = 'one';&#125;// 联合类型的转换function fn(data: Stu | Teacher) &#123; // 第一种方式 if (data instanceof Stu) &#123; (data as Stu).clazz; &#125; // 第二种方式，不需要 as 强转 if ('age' in data) &#123; data.name; &#125;&#125;const n = '123' as string;// 类型保护function add(x: number | string, y: number | string) &#123; if(typeof x == 'string' || typeof y == 'string') &#123; return `$&#123;x&#125;$&#123;y&#125;`; &#125; return x + y;&#125; 别名与接口的一个区别也就是类型别名可以做联合类型，另外例如上面的将 name 设置为 string 的别名，接口就做不到，这也很符合别名这个概念。 枚举和泛型枚举概念上与 Java 中类似，不过幸好远远没有 Java 中那么复杂；也终于看到了泛型，熟悉 Java 的我使用起来没有什么障碍： 1234567891011121314151617181920212223enum Status &#123; ONE, TWO = 3, THREE&#125;// 0 3 4console.log(Status.ONE)console.log(Status.TWO)console.log(Status.THREE)// 反查console.log(Status[0])// 泛型function Gen&lt;E, T&gt;(data: E): E &#123; return data;&#125;// 泛型中的 keyof 用法function Gen2&lt;E extends keyof Teacher&gt;(data: Teacher, key: E): Teacher[E] &#123; return data[key];&#125; 枚举里的项默认就是连续数字，默认从 0 开始，如果你手动赋值，之前的从 0 开始，之后就按你赋值的数依次加一；当然你也可以手动设置为字符串类型。TS 中的泛型类型推断要强得多，很多情况虽然可以不写，但是建议写上清晰一点；也可以使用 extends 和 supper 关键字，甚至可以这样写：T extends number | string，灵活性是更多一些。尤其注意 TS 中泛型的 keyof 用法，这相对来说是个新语法，就是展开对象的 key。 装饰器使用之前需要在配置文件中开启 experimentalDecorators，这是 ES7 的东西，好用是好用。首先，装饰器本身也是一个函数，毕竟 JS 中函数是一等公民；装饰器的参数是一个（作用在）构造函数，通过 @ 来调用，示例： 123456789101112131415161718192021222324252627282930313233343536// new 表示是构造函数，可简单理解为传入一个有构造的对象（类）返回一个装饰后的类function testDecorator&lt;T extends new (...args: any[]) =&gt; any&gt;(constructor: T) &#123; return class extends constructor &#123; echo() &#123; console.log('decorator'); &#125; &#125;;&#125;function testDecorator2(flag: boolean) &#123; if (flag) &#123; return testDecorator; &#125; return () =&gt; &#123;&#125;;&#125;function testDecorator3() &#123; return testDecorator;&#125;@testDecorator@testDecorator2(false)class Test &#123; constructor(private name: string) &#123;&#125;&#125;// 使用函数的方式装饰，避免调用不到装饰新增的方法const Test2 = testDecorator3()( class &#123; constructor(private name: string) &#123;&#125; &#125;);const tt = new Test('mps');const tt2 = new Test2('mps');tt2.echo(); 执行时机为类『加载』，同时只会执行一次，无论你 new 没 new 都会执行；一个类可以使用多个装饰器，顺序按照定义的顺序。上面介绍的是类装饰器，同样的，也有方法装饰器，作用在类的 prototype（如果是静态方法就是构造函数）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 方法装饰器，target 表示类的 prototype，静态方法为构造函数function echoDecorator(target: any, key: string, desc: PropertyDescriptor) &#123; // 禁止修改方法，通过 PropertyDescriptor 控制方法行为 desc.writable = false; console.log(target, key); // 异常处理 const fn = desc.value; desc.value = function () &#123; try &#123; fn(); &#125; catch &#123; console.log('error...'); &#125; &#125;;&#125;class MethodDemo &#123; constructor(private name: string) &#123;&#125; @echoDecorator echo() &#123; console.log(this.name); &#125;&#125;// 方法参数装饰器function paramDecorator(target: any, key: string, index: number) &#123; console.log(target, key, index);&#125;// @catchDecorator('msg')function catchDecorator(msg: string) &#123; return (target: any, key: string, desc: PropertyDescriptor) =&gt; &#123; // 异常处理 const fn = desc.value; desc.value = function () &#123; try &#123; fn(); &#125; catch &#123; console.log(msg); &#125; &#125;; &#125;;&#125; 对于方法装饰器的调用时机，跟类一样，在定义的时候就完成了，不需要等到实例化的时候。特别的，在 setter 和 getter 方法上，只允许一个有装饰器。属性的装饰器跟方法装饰器基本一致，只是不再有 PropertyDescriptor 这个参数了，在装饰器里修改也是改的原型上的值，并不能修改实例的值。而参数装饰器就是多个一个参数位置的参数，其他的基本一致， 我感觉跟注解挺像的。 命名空间假设在 Web 中运用 TS 编译后的文件，如果就最基本的写法，很多类、函数之类的都会变成全局变量，这肯定是混乱糟糕的。 123456789101112131415161718// 命名空间namespace Tese &#123; export class Demo &#123; tag: string = ''; &#125; // 子命名空间 export namespace Sub &#123; // ... &#125;&#125;///&lt;reference path='' /&gt;namespace Main &#123; export function main()&#123; return new Tese.Demo(); &#125;&#125; 如果不使用 export 导出，默认是调用不到的，当命名空间互相引用的时候，建议是写清楚注释，虽然这并不是必须的。但是可以看出这种方式并不优雅，为了可读性，建议使用 ES6 的模块化语法，这样就不需要使用 namespace 关键字了。在 Web 场景下使用 ES6 的模块化打包为一个 JS 文件，可能需要使用 amd 标准，但是浏览器不支持，有需要引入其他的支持库，语法也很繁琐，正是如此 Webpack、parcel 之类的打包工具才崛起了。在不同的文件中如果存在相同的命名空间，那么 TS 会做融合处理，也就是取并集。 附：配置文件使用 tsc --init 会在当前目录生成 TS 的编译配置文件，当运行 tsc 的时候自动读取，示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&#123; "compilerOptions": &#123; /* Basic Options */ "target": "es5" /* target用于指定编译之后的版本目标: 'ES3' (default), 'ES5', 'ES2015', 'ES2016', 'ES2017', 'ES2018', 'ES2019' or 'ESNEXT'. */, "module": "commonjs" /* 用来指定要使用的模块标准: 'none', 'commonjs', 'amd', 'system', 'umd', 'es2015', or 'ESNext'. */, "lib": ["es6", "dom"] /* lib用于指定要包含在编译中的库文件 */, "allowJs": true, /* allowJs设置的值为true或false，用来指定是否允许编译js文件，默认是false，即不编译js文件 */ "checkJs": true, /* checkJs的值为true或false，用来指定是否检查和报告js文件中的错误，默认是false */ "jsx": "preserve", /* 指定jsx代码用于的开发环境: 'preserve', 'react-native', or 'react'. */ "declaration": true, /* declaration的值为true或false，用来指定是否在编译的时候生成相应的".d.ts"声明文件。如果设为true，编译每个ts文件之后会生成一个js文件和一个声明文件。但是declaration和allowJs不能同时设为true */ "declarationMap": true, /* 值为true或false，指定是否为声明文件.d.ts生成map文件 */ "sourceMap": true, /* sourceMap的值为true或false，用来指定编译时是否生成.map文件 */ "outFile": "./", /* outFile用于指定将输出文件合并为一个文件，它的值为一个文件路径名。比如设置为"./dist/main.js"，则输出的文件为一个main.js文件。但是要注意，只有设置module的值为amd和system模块时才支持这个配置 */ "outDir": "./", /* outDir用来指定输出文件夹，值为一个文件夹路径字符串，输出的文件都将放置在这个文件夹 */ "rootDir": "./", /* 用来指定编译文件的根目录，编译器会在根目录查找入口文件，如果编译器发现以rootDir的值作为根目录查找入口文件并不会把所有文件加载进去的话会报错，但是不会停止编译 */ "composite": true, /* 是否编译构建引用项目 */ "incremental": true, /* Enable incremental compilation */ "tsBuildInfoFile": "./", /* Specify file to store incremental compilation information */ "removeComments": true, /* removeComments的值为true或false，用于指定是否将编译后的文件中的注释删掉，设为true的话即删掉注释，默认为false */ "noEmit": true, /* 不生成编译文件，这个一般比较少用 */ "importHelpers": true, /* importHelpers的值为true或false，指定是否引入tslib里的辅助工具函数，默认为false */ "downlevelIteration": true, /* 当target为'ES5' or 'ES3'时，为'for-of', spread, and destructuring'中的迭代器提供完全支持 */ "isolatedModules": true, /* isolatedModules的值为true或false，指定是否将每个文件作为单独的模块，默认为true，它不可以和declaration同时设定 */ /* Strict Type-Checking Options */ "strict": true /* strict的值为true或false，用于指定是否启动所有类型检查，如果设为true则会同时开启下面这几个严格类型检查，默认为false */, "noImplicitAny": true, /* noImplicitAny的值为true或false，如果我们没有为一些值设置明确的类型，编译器会默认认为这个值为any，如果noImplicitAny的值为true的话。则没有明确的类型会报错。默认值为false */ "strictNullChecks": true, /* strictNullChecks为true时，null和undefined值不能赋给非这两种类型的值，别的类型也不能赋给他们，除了any类型。还有个例外就是undefined可以赋值给void类型 */ "strictFunctionTypes": true, /* strictFunctionTypes的值为true或false，用于指定是否使用函数参数双向协变检查 */ "strictBindCallApply": true, /* 设为true后会对bind、call和apply绑定的方法的参数的检测是严格检测的 */ "strictPropertyInitialization": true, /* 设为true后会检查类的非undefined属性是否已经在构造函数里初始化，如果要开启这项，需要同时开启strictNullChecks，默认为false */ "noImplicitThis": true, /* 当this表达式的值为any类型的时候，生成一个错误 */ "alwaysStrict": true, /* alwaysStrict的值为true或false，指定始终以严格模式检查每个模块，并且在编译之后的js文件中加入"use strict"字符串，用来告诉浏览器该js为严格模式 */ /* Additional Checks */ "noUnusedLocals": true, /* 用于检查是否有定义了但是没有使用的变量，对于这一点的检测，使用eslint可以在你书写代码的时候做提示，你可以配合使用。它的默认值为false */ "noUnusedParameters": true, /* 用于检查是否有在函数体中没有使用的参数，这个也可以配合eslint来做检查，默认为false */ "noImplicitReturns": true, /* 用于检查函数是否有返回值，设为true后，如果函数没有返回值则会提示，默认为false */ "noFallthroughCasesInSwitch": true, /* 用于检查switch中是否有case没有使用break跳出switch，默认为false */ /* Module Resolution Options */ "moduleResolution": "node", /* 用于选择模块解析策略，有'node'和'classic'两种类型' */ "baseUrl": "./", /* baseUrl用于设置解析非相对模块名称的基本目录，相对模块不会受baseUrl的影响 */ "paths": &#123;&#125;, /* 用于设置模块名称到基于baseUrl的路径映射 */ "rootDirs": [], /* rootDirs可以指定一个路径列表，在构建时编译器会将这个路径列表中的路径的内容都放到一个文件夹中 */ "typeRoots": [], /* typeRoots用来指定声明文件或文件夹的路径列表，如果指定了此项，则只有在这里列出的声明文件才会被加载 */ "types": [], /* types用来指定需要包含的模块，只有在这里列出的模块的声明文件才会被加载进来 */ "allowSyntheticDefaultImports": true, /* 用来指定允许从没有默认导出的模块中默认导入 */ "esModuleInterop": true /* 通过为导入内容创建命名空间，实现CommonJS和ES模块之间的互操作性 */, "preserveSymlinks": true, /* 不把符号链接解析为其真实路径，具体可以了解下webpack和nodejs的symlink相关知识 */ /* Source Map Options */ "sourceRoot": "", /* sourceRoot用于指定调试器应该找到TypeScript文件而不是源文件位置，这个值会被写进.map文件里 */ "mapRoot": "", /* mapRoot用于指定调试器找到映射文件而非生成文件的位置，指定map文件的根路径，该选项会影响.map文件中的sources属性 */ "inlineSourceMap": true, /* 指定是否将map文件的内容和js文件编译在同一个js文件中，如果设为true，则map的内容会以//# sourceMappingURL=然后拼接base64字符串的形式插入在js文件底部 */ "inlineSources": true, /* 用于指定是否进一步将.ts文件的内容也包含到输入文件中 */ /* Experimental Options */ "experimentalDecorators": true /* 用于指定是否启用实验性的装饰器特性 */ "emitDecoratorMetadata": true, /* 用于指定是否为装饰器提供元数据支持，关于元数据，也是ES6的新标准，可以通过Reflect提供的静态方法获取元数据，如果需要使用Reflect的一些方法，需要引入ES2015.Reflect这个库 */ &#125; "files": [], // files可以配置一个数组列表，里面包含指定文件的相对或绝对路径，编译器在编译的时候只会编译包含在files中列出的文件，如果不指定，则取决于有没有设置include选项，如果没有include选项，则默认会编译根目录以及所有子目录中的文件。这里列出的路径必须是指定文件，而不是某个文件夹，而且不能使用* ? **/ 等通配符"include": [], // include也可以指定要编译的路径列表，但是和files的区别在于，这里的路径可以是文件夹，也可以是文件，可以使用相对和绝对路径，而且可以使用通配符，比如"./src"即表示要编译src文件夹下的所有文件以及子文件夹的文件"exclude": [], // exclude表示要排除的、不编译的文件，它也可以指定一个列表，规则和include一样，可以是文件或文件夹，可以是相对路径或绝对路径，可以使用通配符"extends": "", // extends可以通过指定一个其他的tsconfig.json文件路径，来继承这个配置文件里的配置，继承来的文件的配置会覆盖当前文件定义的配置。TS在3.2版本开始，支持继承一个来自Node.js包的tsconfig.json配置文件"compileOnSave": true, // compileOnSave的值是true或false，如果设为true，在我们编辑了项目中的文件保存的时候，编辑器会根据tsconfig.json中的配置重新生成文件，不过这个要编辑器支持"references": [], // 一个对象数组，指定要引用的项目&#125; 在官方的文档中，各个配置项有详细的说明。如果你使用 ts-node 工具，也会使用这个编译配置文件的。 其他我们约定使用 TypeScript 编写的文件以 .ts 为后缀，用 TypeScript 编写 React 时，以 .tsx 为后缀。 在 TS 中不能直接 import JS 库，如果需要，要额外安装对应的翻译库，这个 VSC 之类的 IDE 都有提示，会转换到一个 .d.ts 中间文件；这个文件仅仅是用 declare var 语法将对应库的 JS 语法合理化，也就是为了避免 IDE 的错误提示，即使不用也可以使用 parcel 之类正确的打包。 相比 Webpack，parcel 更快，也无需配置即可使用，做 demo 是很适合用的。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot编程思想之不求甚解]]></title>
    <url>%2F2020%2F04%2F18%2FSpringBoot%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3%E4%B9%8B%E4%B8%8D%E6%B1%82%E7%94%9A%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[在 Java 领域，SpringBoot 是目前最流行的微服务框架，将使用门槛大幅度的降低，达到开箱即用；那么这也必然使 SB 成为了一个黑盒，如果需要深度定制或者了解内部原理变得有点复杂，希望通过阅读本系列书籍来解开这个黑盒。最重要的部分就是 SB 的主要特性部分，自动装配和自动配置方面的理解；从 SB2.x 开始架构逐步稳定，现在可以尝试大规模使用了。微服务架构作为一种细粒度的 SOA，无论那种表达方式，只不过是名词之争，架构设计的好坏不在于理论和技术，而在于实施者对业务的理解和专业水平。想了一下，直接写到这一篇里了，不再分篇，超长文预警。 初探SpringBoot对于简单的使用没什么可说的，SB 是出了名的简单，这是对于使用来说，也有句话越简单的东西越复杂，关于基本的使用参考官方文档和 API 文档基本就可以了，我之前也写过两篇使用内容为主的：SpringBoot初尝试、SpringBoot进阶上面的两篇可能与本篇有重叠，但是总应该是互补的，侧重点不同。还可以参考我的一篇笔记深入SB还有一篇未完成的 Spring核心编程概述 待填坑。 特性官方列举了六大特性： 创建独立的 Spring 应用 嵌入 Tomcat, Jetty 或者 Undertow 等 Web 容器（不需要部署 War 文件） 提供固化的 starter 依赖，简化构建配置 当条件满足时，自动装配 Spring 或者第三方类库 提供运维特性，例如指标信息、健康检查、外部化配置 绝无代码生成，不需要 XML 配置 独立的Spring应用SpringBoot 除了构建我们熟悉的 Web 应用，在非 Web 应用上也是非常好用的，例如服务提供、调度任务、消息处理等场景。并且，在 Web 应用方面，除了传统的 Servlet 容器，2.x 版本实现了 Reactive Web 容器，也就是 Spring5.x 的 WebFlux。 按照一般的思路用 SB 构建的应用我们应该叫 SB 应用，然而实际上是 Spring 应用，因为在 SB 的构建过程中，主要的驱动核心是靠 SpringApplication 完成的，所以可以称为 Spring 应用。 Spring Web 时代是利用 ServletContext 生命周期构建 Web Root Spring 应用上下文（ContextLoaderListener）；结合 Servlet 生命周期构建 DispatcherServlet 的 Spring 应用上下文。 所以，在 Spring Web 时代都是被动的回调执行，没有完整的应用主导权，这在使用了嵌入式容器后才改善。 可执行JAR用 SB 构建的应用我们可以非常方便的使用 java -jar 来运行，但是默认的 Maven 打包出来是不支持的，这是因为使用了一个 Maven 插件：spring-boot-maven-plugin，并且一般情况不需要指定版本，因为父工程已经配置好了。可执行 JAR 又被称为 fat jars。PS：开发环境使用 mvn spring-boot:run 命令运行也是同样效果。 我们知道 jar 文件其实就是一个 zip 包，解压可执行 jar 文件会得到一些目录和文件： BOOT-INF/classes存放应用编译后的 class 文件 BOOT-INF/lib存放依赖的 jar 包 META-INF/存放相关元信息，例如 MANIFEST.MF org/存放 SB 相关 Class 文件 接下来就是分析为什么这个 jar 可以直接运行了，依赖于 spring-boot-loader；根据 Java 规范，如果使用 -jar 运行，引导配置必须放在 MANIFEST.MF 文件中，它必须在 META-INF 目录下；所以你查看这个文件基本就能了解了。 123456Main-Class: org.springframework.boot.loader.JarLauncher # Main 函数Start-Class: com.example.demo.DemoApplication # 启动类Spring-Boot-Classes: BOOT-INF/classes/ # 编译之后的 class 文件目录Spring-Boot-Lib: BOOT-INF/lib/ # 当前工程依赖的 jar 包目录Build-Jdk-Spec: 1.8 # 指定的 JDK 版本Spring-Boot-Version: 2.1.6.RELEASE # SpringBoot 版本 这里主要看 JarLauncher 这个类，相应的，打出来的可执行 war 包就是 WarLauncher；知道了核心类，那么我们就又了另一种启动方式，把 jar 包解压，在解压后的目录可以直接执行： 1java org.springframework.boot.loader.JarLauncher 这样也是可以正常启动 Spring 应用，可以得出 JarLauncher 装载执行了我们的启动类，如果你尝试直接运行启动类，很遗憾是不行的，原因是 lib 依赖库的原因。具体的原因可以查看 JarLauncher 的源码，很简单就不说了（深层次的例如对于 URL 协议的处理就很难理解），里面进行了处理所以才能 jar 或者解压后运行。一句话概括就是：JarLauncher 实际上是同进程内调用 Start-Class 的 main 方法，并在启动前准备好 CP（classpath），war 包亦是如此，不过目录会有所差别，毕竟要兼容 servlet 容器（仅关注 WEB-INF/classes、lib）运行，根据容器的特性就可以做到忽略非规范目录的冲突的包，当然 WebFluex 不行。 嵌入式Web容器从 2.x 开始，Netty 也作为容器加入，不同容器无法共存，Undertow 作为 JBoss 社区推出的新一代兼容 Servlet3.1+ 的容器，用的还是不太广泛。目前最新版本支持 HTTP/2 和 servlet4.0，核心 jar 体积只有 2.2MB，4.0 规范对应 Tomcat 和 Jetty 9.x，Undertow 2.x。因为 SpringFlux 基于 Reactor 框架实现，因此 Netty 容器属于 Reactor 和 Netty 的整合实现；其他的三种容器也能作为 Reactive Web Server，默认是 Netty，毕竟 Servlet3.1+ 也支持 Reactive 异步非阻塞的特性。另外需要说明的是，嵌入式容器并不是 SB 首创，各个容器早就支持，SB 不过是将整合做到了极致。 Apache Tomcat 官方很早就提供了相应的可执行 jar 的构建插件，但是运行时还是通过解压到临时目录的方式实现；对于 SB，它使用了零压缩模式，所以可以不解压直接读取运行，也正是这个所以重写 JAR 协议的 URL 实现。 同样，Jetty 天然的可插拔 API 对嵌入式容器开发更加友好，也是 Google 的 GAE 弃用 Tomcat 转为 Jetty 的一个原因；不仅想如果当时有 SB 这种框架或许迁移难度会大幅下降。 在嵌入式 Reactive Web 容器方面，Undertow 用的还是蛮多的，它也对这两种情况各有一个实现。当依赖中存在 WebFlex 的时候，容器的 Starter 就自动装配为 Reactive 容器了。 自动装配关于这点，一开始让我们感觉很神奇，文档也是轻描淡写『@EnableAutoConfiguration 和 @SpringBootApplication 两者选其一标注在 @Configuration 类上』，但是它没说明这个 @Configuration 是如何装配的。对于 Spring，我们熟悉常见的三种装配方式： &lt;context:component-scan&gt; 标签 @Import @ComponentScan 以上三种手段均需要 Spring 上下文引导，前一个可以使用 ClassPathXmlApplicationContext 加载，后者就需要 AnnotationConfigApplicationContext 加载；对于 SB 很显然是通过 SpringApplication 实现的，那么我们可以认为，主启动类承担了 @Configuration 的角色。 @SpringBootApplication这个注解肯定不陌生，它是很有料的，如果你点进去看源码，相关的分析其实在『Spring进阶』中已经说的差不多了，这里不再重复，简单说它相当于开启自动装配、标注配置类、开启包扫描；在包扫描上，它排除了一些特定类型，例如同时标注配置类和开启自动装配的，这就避免了使用多次时的冲突问题。PS：配置类源于 @Component 的派生，Spring 称为『模式注解』。 另外，在这个注解中，使用了大量的 @AliasFor 别名，这属于 JDK 的知识了，只是提醒一下。 @SpringBootApplication 也并不是一定要标注在引导类上，只要保证 run 方法传递的是标注这个注解的类即可，这一点的原因在进阶文章里也提到过。具体的将也不是非 @SpringBootApplication 不可，官方的说法 @EnableAutoConfiguration 也是没问题的（即使它不是 @Configuration），这里去看进阶篇就足够了，点到为止，之后还会深入分析，这里还不到时候。 作为 @Configuration 的派生类，也继承了 CGLIB 提生的特性，官方文档中有说明，传统的 @Bean 属于轻量模式（Lite）在 @Configuration 或者 @Component 下的 @Bean 就属于完全模式（Full），会执行 CGLIB 提升。 @SpringBootConfiguration这个注解如果你看的话跟 @Configuration 没什么不同，但是在 SB 中，官方建议多使用 @SpringBootConfiguration，在做注解元数据解析的时候会有帮助。 创建自动配置类编写自动配置类也是有固定套路的，一般创建名为 xxxAutoConfiguration 的配置类，这个类一般被标注了 @Configuration 和 @Import 注解；对于 @Import 注解，你可以选择导入一个配置类或者 ConfigurationSelector 的实现类。接下来，就是配置执行入口，也就是在 META-INF/spring.factories 文件中配置这个类： 123# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\ com.bfchengnuo.demo.xxxAutoConfiguration 因为都是注解，解析注解带来的时间成本肯定会影响应用的启动速度，所以在 Spring5.x 的时候加入了 @Indexed 注解，在编译的时候就会向 @Component 和其派生注解增加索引，从而减少运行时的消耗。 Production-Ready特性也就是为生产准备的特性，例如指标、健康检查、外部化配置。它是 DevOps 的立足点，当然这个词具体的定义就是仁者见仁智者见智了。对于 SpringBootActuator 国内大部分都不重视，其实还是很有用的，它用于监控和管理 Spring 应用，可以通过 HTTP Endpoint 或者 JMX Bean 与其交互。这里就不继续展开了，内容还是蛮多的，如果以后逐步都用起来了，估计还会单独写吧，个人感觉还是很有前途的。 下面说说外部化配置，在 SB 中有三种用途： Bean 的 @Value 注入 Spring Environment 读取 @ConfigurationProperties 的读取 上面说的是消费方，那么生产源呢，在官方文档中描述了 17 种 PropertySource 的顺序，然而我们平常用的并不多，也就是用用命令行指定来替代 application.yml 中的属性。因为 SB 的规约大于配置思想，让我们省去了大量的配置。 后记纵观 Spring 的一路演变，就是注解驱动之路，在 3.1 之后可谓大变化，增加了 @EnableXXX 模式，包扫描和 ConfigurationSelector（ImportSelector）；到了 4.x 有了 Condition 条件注解，接下来会更加详细的探讨。SB 的特性让其称为微服务的基础组件，也是 SC 的基础设施，或者将 SB 称为微服务的中间件，也是 SB 的出现让 Spring 社区焕发了第二春，简单再说 SC，对于分布式系统要解决的问题有： 分布式配置 服务注册和发现 路由 服务调用 负载均衡 熔断机制 分布式消息 Spring 官方最大的优势就是 API 设计能力，也可以说是抽象能力，云平台的现在，Java 已经派生语系处于垄断地位，SC 也慢慢成长（虽然 Netflix 进入维护模式，不过 Alibaba 崛起了）SpringCloudStream 也可以了解一下。 走向自动装配前面的自动装配只是简单的提了一下相当于是结果，这一部分讲述自动装配的发展过程以及其中涉及的一些编程模式。 注解驱动的发展在 Spring2.5 的时候我们常用的 @Autowired、@Component、@Order、@Service、@Qualifier、SpringMVC 相关注解才被加入，之前基本还是 XML 的天下，即使这个版本提供了大量的注解，但是还是离不开 &lt;context:component-scan&gt; 和 &lt;context:annotation-config&gt; 的噩梦。 对于 @Qualifier 这个注解，还有一种『逻辑类型』限定的使用（不带参数，用来缩小匹配范围），例如在 SC 的 @LoadBalanced 中就如此使用。即，你在 @Bean 的时候顺带加一个 @Qualifier，那么在 @Autowired 的时候也加一个 @Qualifier 它会自动找之前定义的时候也有 @Qualifier 的对象，就算有其他的相同的类型的 Bean 也会被过滤，并且这种方式不需要写值。另外，Spring 还有一种缺省机制，也可以说容错机制，在没用 @Qualifier 注解下，即缺省情况下使用 Bean 标识符（方法名、字段名）注入。 到了 3.x 时代，进入了注解驱动的黄金时代，井喷式增长，当然也跟 Java5 的特性有关，通过派生性，有了 @Configuration 这类注解，@Import 也出现了，并且提供 @ImportResource 来解决 XML 遗留问题。通过加入 @Bean 等一些列注解对标 XML 配置，决心替代 XML 了。那么最后的问题，谁来引导 SpringContextConfiguration 呢，不可能通过包扫描标签，那样又是依赖 XML，所以有了 AnnotationConfigApplicationContext，用这个来注册 @Configuration，并且通过前面的 @ImportResource 导入遗留的 XML 配置，虽然这种方式看起来有点别扭。终于，在 3.1 的时候引入 @ComponentScan 替换了包扫描标签，全面进入注解驱动时代；同时，也出现了 @Profile 这类条件化定义 Bean 能力的注解，虽然功能很弱；Web 方面也是突飞猛进，@RequestBody 之类的注解也都出现；并且 3.x 时代还新增了 Environment 之类的 API 接口，@PropertySource 的出现为外部化配置奠定了基础，其他的还有很多，例如缓存的抽象、异步的支持（@Schedule、异步 Web 处理）、校验注解（JSR-303）、@Enable 模块驱动（@EnableWebMvc 等）。所以，3.x 真的是黄金时代，看着都兴奋。 接下来的 4.x 进入完善时代，上面说过 @Profile 很鸡肋，所以加入了 @Conditional，通过编程的方式来解决 @Profile 灵活性不足的问题，SB 中的 @ConditionalOn* 就是这个的派生。尽管不是强制使用 Java8，但也巧妙的兼容了新的时间 API、@Repeatable 和参数名称发现；也正是有了 @Repeatable 所以 @PropertySource 提升为了可重复标注的注解（Java8 之前可以配合 @PropertySources 使用），@ComponentScan 也是同理；同时，4.2 新增了 @EventListener 作为 ApplicationListener 的另一个选择，@AliasFor 注解也解除了之前派生的一些限制（@GetMapping 一类注解就使用了这个特性），加入 @CrossOrigin 作为 CorsRegistration 的替换方案。其他的一些例如依赖查找 @Lookup 处在比较边缘化的地位，不需要太多了解。 最后，也就是现在的 5.x 版本，新增的 @Indexed 用来构建索引，加快包扫描，但是存在一定的缺陷，要避免模式注解忽略的问题（例如项目引入两个 jar，一个使用了一个没使用，并且包名相同，那么你在项目里只能扫到使用了 @Indexed 的 Bean，因为没使用的不会存在于 META-INT/spring.components 索引文件中，存在这个文件 Spring 就不会扫描包，只会寻找索引文件对应的 CandidateComponentsIndex 对象）；感兴趣的可以研究一下。同时引入了 JSR-305 适配注解 @NonNull 之类（Spring 中已经大量使用），为 Java 和 Kotlin 之间提供技术杠杆。 注解编程模型Java 语言规范规定，注解不能继承，没有派生子类的功能，所以 Spring 采用了元标注（注解上标注注解）的方式来实现『派生』；需要注意，直到 Spring4.x 才实现了多层次派生性，之前都是深度有限，4.x 版本中的 AnnotationAttributesReadingVisitor 使用了递归方式查找元注解，这个问题才得以解决，不过 SB 最低版本都是依赖 4.x+，所以不用担心。其他的，为了方便使用还添加了很多组合注解，例如 @TransactionalService 就是 @Transactional 和 @Service 的组合；那么如何感知是个问题，常规的思路肯定是反射手段解析元信息，但是 Spring 并没有这么做，使用的是抽象出 AnnotationMetadata 这个接口。为了提高效率，Spring 的类加载机制是通过 ASM 实现的，例如 ClassReader，相对于 ClassLoader 体系，它直接操作字节码，也便于进行字节码提升；这方面的内容很复杂，是一个大的体系，这里不再详细说明。 AnnotationMetadata 有两个实现类：AnnotationMetadataReadingVisitor 和 StandardAnnotationMetadata；前者使用 ASM 方式读取，涉及 AnnotationMetadata 和 ClassMetadata 等对象，丰富性肯定不如 Java 反射 API；后者使用 Java 反射进行读取，用的也很多。 那么 Spring 为什么要两套实现？除了效率的差距，还有一个是场景，如果使用 Java 反射 API，必然需要一个大前提，就是反射的 Class 必须被 ClassLoader 装载，但是在 Spring 的包扫描阶段，显然是不可能的，所以使用 ASM 的方式；之后装载之后就可以使用 Java API 了。如果需要进行反射相关操作，不妨试试 Spring 提供的反射工具类：ReflectionUtils；类似的 AnnotationUtils、AnnotatedElementUtils 的工具类 Spring 中也大量使用。同时，在使用元注解的时候要留意属性覆盖的情况，细分可以是显性覆盖（@AliasFor）和隐性覆盖；其中也有一些规则，这里不细说了。 注解驱动设计模式从 @Enable 模式开始说起（模块装配），Spring 中就存在很多，例如 Web Mvc、缓存、JMX、Async 模块等，这都是来自 Spring，并不是 SB 的特性，当然在 SB 和 SC 中也有新增，例如开启自动装配。这个模式简化了装配模式，做到了按需装配，但是缺点是必须手动开启。而想要自定义 @Enable 也很简单，你可以随便拿一个来参考，主要就是利用 @Import 和 @Configuration，当然你也可以试试接口编程的 ImportSelector 接口，其他的 ImportBeanDefinitionRegistrar 用的相对少一点。而原理，主要还是对这些注解的解析，因为这又是一个大的体系，在这也不想多说，简单提一提： 无论是 XML 还是注解驱动的场景，均是通过 AnnotationConfigUtils 的 registerAnnotationConfigProcessors 方法进行装载 ConfigurationClassPostProcessor 类，这个类是最高优先级的 BeanFactoryPostProcessor 实现。 解析 Spring BeanDefinition 的注解元信息最重要的组件是 ConfigurationClassParser，它的两个重载分为不同的实现，就是前面说过的 AnnotationMetadataReadingVisitor 和 StandardAnnotationMetadata；这里也会进行递归调用解析，还记得前面说的轻量模式和完全模式么，就是根据这个来区分进行 CGLIB 提升。 SpringWeb自动装配在 Spring 中除了模块装配，在 3.1 之后，也支持自动装配，仅限于 Web 场景；新引入的 WebApplicationInitializer 构建在 Servlet 3.0 的 ServletContainerInitializer 上，支持编程方式替代传统 web.xml。在 SpringSecurity 也有类似的实现 AbstractSecurityWebApplicationInitializer。 实现原理还是依赖 Servlet 3.0 的特性，大部分开发人员对 Servlet 规范还是陌生的，新规范带来的运行时插拔可是极大的灵活性（拓展知识 SPI），也不能说新了，毕竟很多年了，现在 4.0 的异步技术又有多少人关注呢。关于这一块的内容，如果感兴趣去稍微看下源码，其实还挺有意思的。 123AbstractDispatcherServletInitializer |-AbstractAnnotationConfigDispatcherServletInitializer |-AbstractContextLoaderInitializer 在 Web Mvc 中，DispatcherServlet 有专属的 WebApplicationContext，它继承了来自 Root WebApplicationContext 的所有 Bean，也就是我们常说的父子容器。无论哪一个容器，都是基于注解驱动的 Web 应用上下文实现的，一般情况我们选择最具体的就好。 条件装配说的就是 @Profile 和 @Conditional，对于 Profile 只支持简单的 @Profile({&quot;dev&quot;, &quot;prod&quot;})、@Profile(&quot;!dev&quot;) 这种形式，具体的处理原理不多说了，还是分析注解元数据那一套。由于 Profile 太过于局限性，现在基本都是 @Conditional 的天下，实现起来也不复杂。 1234@Conditional(&#123;ProfileCondition.class&#125;)public @interface Profile &#123; String[] value();&#125; 看到这个源码，还有什么疑问，肯定是 @Conditional 的天下了。当多个 Conditional 并存时，会使用 @Order 排序，需要注意一下；ConditionEvaluator 的评估有两个阶段，Bean 注册阶段和 Configuration Class 解析阶段。 SB自动装配在 SB 中的自动装配相比上面所说，一个区别就是应用依赖的 Jar 存在变化的可能，因为所在包的路径不确定，所以很多手段都不太合适，或许会想到使用 @ComponentScan 来全局扫描，但是官方文档中明确表示不鼓励这样扫描默认包，因为它会读取所有 Jar 中的类。SB 的自动装配是非侵占性的，对于失效自动装配有两种方式：代码配置（@EnableAutoConfiguration 的 exclude）和外部化配置（spring.autoconfigure.exclude），为了避免侵入性，外部化是优先选择。如果你想看 SB 自动装配的原理，可以进入注解发现是通过 AutoConfigurationImportSelector 来实现的，读一下这个的源码就能猜个差不多，从字面意思： 加载自动装配的元信息至于为什么需要加载元信息，因为 @Conditional 之类的注解处理时机较晚，所以根据元信息来匹配就减少了自动装配的时间，参考：spring-autoconfigure-metadata.properties 获取 @EnableAutoConfiguration 标注类的元信息 获取自动装配的候选类名集合使用 SpringFactoriesLoader 进行读取，采用 Spring 工厂机制的加载器，简单说就是读取 spring.factories 中的配置，合并成一个 Map。 移除重复对象利用的是 Set 集合去重：return new ArrayList&lt;&gt;(new LinkedHashSet&lt;&gt;(list)); 移除我们自己配的『失效自动装配』 再根据 autoConfigurationMetadata 过滤过滤 spring.factories 中那些当前 ClassLoader 不存在的 Class，可以说是检查是否合法 触发自动装配导入事件 SpringFactoriesLoader 在 SB 中大量的使用，这一块的内容确实不少，如果是做基础架构的，还是要深入了解，我这种打酱油的就先点到为止。关于事件，这里贴一个例子： 12345678910111213141516171819202122232425// 依赖于 spring.factories 配置文件：// org.springframework.boot.autoconfigure.AutoConfigurationImportListener=\// thinking.in.spring.boot.samples.auto.configuration.listener.DefaultAutoConfigurationImportListenerpublic class DefaultAutoConfigurationImportListener implements AutoConfigurationImportListener &#123; @Override public void onAutoConfigurationImportEvent(AutoConfigurationImportEvent event) &#123; // 获取当前 ClassLoader ClassLoader classLoader = event.getClass().getClassLoader(); // 候选的自动装配类名单 List&lt;String&gt; candidates = SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class, classLoader); // 实际的自动装配类名单 List&lt;String&gt; configurations = event.getCandidateConfigurations(); // 排除的自动装配类名单 Set&lt;String&gt; exclusions = event.getExclusions(); // 输出各自数量 System.out.printf("自动装配类名单 - 候选数量：%d，实际数量：%d，排除数量：%s\n", candidates.size(), configurations.size(), exclusions.size()); // 输出实际和排除的自动装配类名单 System.out.println("实际的自动装配类名单："); event.getCandidateConfigurations().forEach(System.out::println); System.out.println("排除的自动装配类名单："); event.getExclusions().forEach(System.out::println); &#125;&#125; SB 的事件分发相关在后面会说，事件的分发是一个非常重点的内容。 从生命周期来说，因为 AutoConfigurationImportSelector 实现了 DeferredImportSelector，从名字看这是延迟的（会有一个待处理队列），它在 @Configuration Bean 处理完毕后才会运作，Order 来看也是优先级接近最低的。拓展阅读：ConfigurationClassParser（实际执行 ImportSelector 的地方） 在顺序方面，可选择的有两种，固定和相对，例如 @AutoConfigureOrder 和 @AutoConfigureBefore，他们两个之间的顺序是先固定然后再根据相对顺序调整。PS：尽量使用 name 属性，别用 value，还是因为 SB 升级导致的破坏性 API 变更，每次升级或多或少都有些。 关于自动装配的 BasePackages 参考嵌套类：AutoConfigurationPackages.Registrar，为了避免重复导入，会根据名字判断 IoC 有没有，也就是它只会注册一次。因为方法执行在 Bean 初始化阶段，其 BeanDefinition 还有调整的机会，所以可以追加，借此达到夸大搜索范围的目的。 自定义SB自动装配从 spring.factories 这种加载机制可以看出与 SPI 非常像，那么接下来正式开始，首先从命名来看，遵循 SB 的套路使用 *AutoConfiguration 这样的规则，也要看一下他们的包结构，模仿着写；Spring 官方建议自动装配的代码写在 autoconfigure 模块下，starter 模块依赖该模块，并且附加其他需要的依赖，当然官方并没有坚持要分包（jar），开发者完全可以合并到一个模块（jar）。 Starter 的命名，官方建议是使用 ${module}-spring-boot-starter 的命名模式（或者拆分出一个 autoconfigure 模块），使用 spring-boot-configuration-processor 可以帮助 @ConfigurationProperties Bean 生成 IDE 辅助信息。PS：在设置 Key 命名空间的时候，注意不要跟官方冲突，要不然，可能会有奇奇怪怪的问题。 总体来说跟『创建自动配置类』差不多，毕竟那是基础，编写完自动配置类后将其加入 spring.factories 文件，然后就可以构建 Starter，其中你所依赖的 SB 相关依赖都要设置为 &lt;optional&gt;true&lt;/optional&gt; 不要传递依赖，因为用户使用的 SB 版本可能会与之发生冲突，由于不同环境的 ClassLoader 不确定性，最终导致 Class 文件二进制不兼容的情况，可能表现为 IDE 中正常线上不正常或者反之，例如常见的 NoClassDefFoundError、NoSuchMechanismException 等。 其中可以使用 @ConditionalOn* 之类条件注解来实现，由于 SB 的设计问题，在 2.0 时代真的是改来改去，各种破坏性升级，让人怀疑人生，一般情况为，name 属性是用于第三方库或者高低版本兼容的场景；value 属性用于物理路径非常稳定的情况，一般情况下，还是 name 用的多。 因为 ConditionEvaluator 在注册 Bean 阶段进行评估，所以 @ConditionalOnBean 和 @ConditionalOnMissingBean 的 Java doc 强烈建议开发人员仅在自动装配中使用该条件注解。这一对注解主要用来判断当前的 Spring 应用上下文是否存在该 Bean，如果存在就直接 autowired，如果不存在就 new。通常，它会和 OnClass 连用，先判断 CP 中是否存在，然后才有后来的容器中是否存在，还要考虑其他依赖是否装配了该 Bean，因为有个顺序问题。 对于属性条件注解 @ConditionalOnProperty，属性来源于 Spring Environment，典型代表就是 Java 系统属性和环境变量，application.yml 也是，都属于 PropertySource。剩下的 Resource 条件注解感兴趣的可以看一下，在书中进行了大量的源码分析，关键是要搞明白 ConditionEvaluator 和 ResourceLoader（ConditionEvaluator 关联的 ResourceLoader 来自 Spring 应用上下文），最终会发现 DefaultResourceLoader 实际上是 Spring 中的唯一 ResourceLoader 实现。个人感觉涉及 Resource 的东西，不说难吧就是很绕，很烦人，I/O 是个折磨人的东西，涉及 Stream、各种 URL 协议之类，Handle 和 Factory。 SB 自定义的 Condition 基本都是扩展的 SpringBootCondition 而不是直接实现 Condition 接口，可以借鉴下。 示例工程：Github 理解SpringApplication这一部分从 Spring 应用的生命周期来看，分为初始化阶段、运行阶段、结束阶段。也是很硬核的内容。 SpringApplication初始化阶段初始化阶段主要分为构造阶段和配置阶段，构造阶段当然是通过构造器来完成的，不过一般情况我们都用 run 这个静态方法了，它也是走的构造。无论那种，最终都需要传递一个 primarySource，也可以理解成标注了 @EnableAutoConfiguration 的类，最终会被 SpringApplication 的 primarySources 属性保存，接下来会依次执行 WebApplicationType.deduceFromClasspath、setInitializers、setListeners、deduceMainApplicationClass，可以理解为： 推断 Web 应用类型在此阶段上下文还没有准备，所以使用的是检查当前 ClassLoader 下的基准 Class 的存在性来判断；当 DispatcherHandler 存在，DispatcherServlet 不存在，也就是依赖 WebFlux 下，判断为 Reactive Web 应用；当 Servlet 和 ConfigurableWebApplicationContext 不存在时，非 Web 应用；当 Spring WebFlux 和 Spring Mvc 同时存在，按 Servlet Web 处理。 加载 Spring 应用上下文初始化器该方法返回所有 spring.factories 资源配置中的 ApplicationContextInitializer实现类名单。并不强制要求实现 Ordered 排序，排序后保存到 initializers 属性中；PS：在调用 run 之前，允许你使用 setter 方法进行覆盖性更新。 加载 Spring 应用事件监听器 推断应用引导类 构造阶段就算到此完成，接下来是配置阶段，该阶段是可选的，主要用于调整或者补充构造阶段的状态，以 SpringApplication 的 setter 方法为代表，是用于调整的相关；补充行为则以 add 方法为代表。推荐使用 SpringApplicationBuilder。大多数情况开发人员无需调整 SpringApplication 的默认状态，作为拓展可以看看 SC 或者 SC Data Flow。 123456new SpringApplicationBuilder(DemoApplication.class) .bannerMode(Banner.Mode.CONSOLE) .web(WebApplicationType.NONE) .profiles("dev") .headless(false) .run(args); 用的最多的也许是 Banner 相关吧。 关于配置源，1.x 和 2.x 差别较大，2.x 中构造函数由 Object 改为 Class，所以 XML 和 packages 就无法作为参数传递，只能通过 setSources 方法传递。PS：注意 sources 和 primarySources 属性。 其中，sources 属性使用的是 LinkedHashSet 说明具有去重功能，并且有序。 SpringApplication运行阶段这个阶段属于核心过程，围绕 run 方法展开，它会进一步完善所需要的资源准备，随后启动 Spring 应用上下文，伴随 SB 和 Spring 的事件分发，形成完整的 SpringApplication 生命周期。它可以再进行细分，准备、启动、启动后。 上下文准备阶段本阶段的范围是从 run 方法开始到 refreshContext 调用之前；挑主要的说。SpringApplicationRunListeners 属于组合模式的实现，内部关联了 SpringApplicationRunListener 的集合，按照字面意思，应该是 SB 运行时监听器（此处应有监听方法与运行阶段对应表） 结合 SpringFactoriesLoader 机制，可以从 spring.factories 中快速定位其内建实现；对于普通开发者，只需要根据 SpringFactoriesLoader 机制和 SpringApplicationRunListener 的要求就能对该接口进行扩展。 然后，可以得出，EventPublishingRunListener 是 SB 中的唯一内建实现，可以得出，根据 SpringApplication 所关联的 ApplicationListener 实例列表，动态的添加到 SimpleApplicationEventMulticaster 中；SimpleApplicationEventMulticaster 是 Spring 中实现 ApplicationEventMulticaster 接口的类，用于发布 Spring 应用事件（ApplicationEvent），所以可以看出它也充当了 SB 中事件发布者的角色。（此处应有 SB 事件与监听方法对照表）虽然 SB 与 Spring 事件有很大的关联性，但是他们还是有差异性的，主要体现再顺序和时机上，官方文档中也有提及。 拓展-理解Spring事件Spring 事件是 Spring 应用上下文 ApplicationContext 对象触发的，SB 事件的发布者则为 SpringApplication 关联的 SimpleApplicationEventMulticaster 类型，虽然它也是来自 Spring。Spring 中的事件也采用了 JDK 的观察者模式规范，不过进行了一定的扩展或者说增强。 对于如何监听具体的 ApplicationEvent 类型，在 3.0 得到改善，ApplicationListener 支持泛型监听，不再监听所有事件靠 instanceof 筛选，但是由于泛型的限制，无法同时监听不同的事件类型，如果继续使用 ApplicationEvent 做泛型，这就又回到了之前。所以，3.0 中引入了 SmartApplicationListener 接口，它通过 supports* 方法来过滤监听的事件类型和事件源类型，例如 ConfigFileApplicationListener。 上面说过 SB 的事件发布者 SimpleApplicationEventMulticaster 也是来自 Spring，并且是 ApplicationEventMulticaster 接口的实现类，该接口主要承担两个职责：关联 ApplicationListener 和广播 ApplicationEvent。PS：SB 的事件监听器都是经过排序了的。 看源码的朋友们，Spring 4.0 引入的 ResolvableType 是 Spring 为了简化 Java 反射 API 提供的组件，它能够轻松的获取泛型类型等。 SimpleApplicationEventMulticaster 虽然允许事件广播时 ApplicationListener 异步监听事件，但是无论时 Spring 还是 SB 均没有使用其来提升为异步执行，由于 EventPublishingRunListener 的封装，SB 事件监听器也无法异步执行。 关于 ApplicationEventMulticaster 与 ApplicationContext 的关系，官方文档提到过可以使用 ApplicationEventPublisher 发布 ApplicationEvent；查看 ApplicationContext 可以看到它扩展了 ApplicationEventPublisher，也就是说，无论那种 Spring 应用上下文，都具备发布 ApplicationEvent 的能力。PS：获取 ApplicationEventPublisher 可以通过 Aware 方式。仔细看还会发现拓展了 ResourceLoader，所以 ApplicationContext 也是 setResourceLoader 方法的常客。 我们可以简单的得出结论，ApplicationEventPublisher 的实例就是当前 ApplicationContext。SimpleApplicationEventMulticaster 既是 SB 事件广播的实现，又是 Spring 事件发布的实现。SimpleApplicationEventMulticaster 作为 Spring 中唯一的 ApplicationEventMulticaster 实现，无论是 Spring 还是 SB，都充当同步广播事件对象的角色，开发人员主要关注 ApplicationEvent 的类型和对应的 ApplicationListener 实现即可。此处应有 Spring 内建事件一览表 拓展-Spring内建事件当 ConfigurableApplicationContext 的 refresh 方法执行到 finishRefresh 方法时，Spring 应用上下文就会发布 ContextRefreshedEvent （上下文就绪）事件；此时应用上下文中的 Bean 已经完成初始化，并能投入使用，通常会使用 ApplicationListener&lt;ContextRefreshedEvent&gt; 监听，获取所需要的 Bean，防止出现 Bean 提早初始化。 剩下的 Spring 应用上下文启停事件不多说，SC 中还用了下，SB 不常见。只需要正确的理解上下文的 start、stop、close 方法之间的区别。相关类：Lifecycle； 因为事件源都是用的 ApplicationContext，所以称之为 Spring 上下文事件。 拓展-Spring应用上下文事件应用上下文 ApplicationContextEvent 与 Spring 事件 ApplicationEvent 的关系嘛，直接看就是继承关系（extends ApplicationEvent）；对于 Spring 事件的监听，4.2 开始可以使用 @EventListener 注解，毕竟开始进军注解驱动编程。 使用 @EventListener 的时候，注意要标注在 IoC 中的 Bean 上，并且需要 public 权限；单一类型监听中，虽然规范要求不能有返回值，但是即使返回值不为 void 也可以执行；在多类型监听中，需要特别处理，不能有返回值，也需要手动进行多个 ApplicationEvent 的过滤。 异步支持：需要先使用 @EnableAsync 开启，然后使用 @Async 注解。可以使用 @Order 控制顺序。对泛型 ApplicationEvent 支持方面，需要事件实现 ResolvableTypeProvider 接口。相关源码：EventListenerMethodProcessor（生命周期相关）、DefaultEventListenerFactory（适配相关） 因为关联了 ApplicationEventMulticaster 属性，在 close 的时候也没有进行销毁关系，所以即使在 close 后，依然可以发布 Spring 事件，但是因为关联的 ApplicationListener 已经被销毁，所以最终无法被监听。广播实现也很简单，就是通过 ApplicationEvent 找到关联的 ApplicationListener 列表，异步或者同步的调用即可。此处应当有这几种方式的对照表 Spring事件小结主要包括的是 Spring 事件、事件监听器、事件广播器，以及它们与 Spring 应用上下文的关系。涉及到的事件分为 Spring 事件与 Spring 应用事件，主要以事件源区分，并且开发者可以自定义 ApplicationEvent。进一步是泛型化与注解化。注解化依赖于适配器将其转换为 ApplicationListener，其中有 AOP 相关知识。其中我略去了大量的源码级讲解，没办法，源码看的我头大，目前真的不想再多弄了 (/▽＼)。 拓展-SB事件有了 Spring 的事件基础，再来看 SB 的事件就简单多了，毕竟都是一套体系，如果 SB 仅仅是复用 Spring 相关的 API 那么就更好了，他们各自为政互不干涉，对我们理解很友好。但是再 1.4 之前的版本，其实内部还是很混乱的，主要因为直接复用了 SimpleApplicationEventMulticaster，可能存在重复添加的情况，核心问题在于 spring.factories 中的实例列表是否有必要关联到 Spring 应用上下文中，这就导致 ApplicationListener 也可以监听到 SB 的事件，这其实并不好。 从 1.4 之后，SB 就进行了微调，SB 事件与 Spring 事件开始独立，互不干扰，也是从这开始核心 API 开始趋于稳定。具体改动可参考 EventPublishingRunListener 的 contextPrepared 方法，不再直接关联 SimpleApplicationEventMulticaster，但是实例依然会加入到 Spring 应用上下文中。也正是因为 SpringApplication 使用了独立的 ApplicationEventMulticaster 对象，虽然 SpringApplication 和 ApplicationContext 都还是使用的 SimpleApplicationEventMulticaster 实例，但不再是同一个对象。到这里就完成了隔离，SB 中可以监听 Spring 事件，反之不可，但是多数情况监听的还是 SB 事件。 在 SB 中，大量使用了 SmartApplicationListener，SB 中事件的事件源多用 SpringApplication。 拓展-SB内建事件在 SB 中，无论你监听 SB 事件还是 Spring 事件，都是通过在 spring.factories 中配置实现的（属性为 ApplicationListener 的实现类）。此处应当有 SB 内建事件一览表最熟悉的可能是 ConfigFileApplicationListener 和 LoggingApplicationListener，分别负责 SB 应用配置文件的加载和日志系统的初始化（日志框架识别和日志配置文件加载等）。SB 的内建事件根据 EventPublishingRunListener 的生命周期回调方法依次发布。可以理解为 SB 的事件/监听机制是继承于 Spring 的事件/监听机制，例如 SpringApplicationEvent 就继承自 ApplicationEvent。 其他的类似 ApplicationArguments、ConfigurableEnvironment （对应不同的 Web 类型）就战略性略过了（Spring 应用上下文运行前准备）。 上下文启动阶段本阶段有 refreshContext 方法实现，它首先调用 refresh，执行 ApplicationContext 的启动，然后注册 shutdownHook 线程（JVM shutdown hook 机制），实现优雅的 Spring Bean 销毁生命周期回调。在 1.4 版本之后，重构了 refreshContext 方法，随着这个方法的执行，Spring 应用上下文正式进入 Spring 生命周期，SB 的核心也随之启动，例如自动装配、嵌入式容器等特性，紧接着分发 ContextRefreshedEvent 事件。 上下文启动后阶段这里主要是 afterRefresh 方法，具体的实现在 2.0 也有略微调整，主要为不再具有执行 ApplicationRunner 或者 CommandLineRunner 的能力，不过并不会影响它们的执行，只是执行时机相对延后了，最后分发 ApplicationStartedEvent 事件。 同时 ApplicationStartedEvent 的语义也有所变化，1.5 中加入了 ApplicationStartingEvent 事件，虽然 ApplicationStartedEvent 标注为 2.0 加入，其实在 1.x 就存在，2.0 对其进行了调整，关联了 ConfigurableApplicationContext 对象。简单说就是，ApplicationStartingEvent 充当了之前 ApplicationStartedEvent 的角色，ApplicationStartedEvent 被延后触发。 SpringApplication结束阶段各个版本中此阶段的实现比较稳定，可分为正常结束和异常结束。当 ApplicationReadyEvent 事件触发后 SpringApplication 的生命周期进入尾声，除非发生异常，进入异常分支，这其中在不同版本中的实现都有细微变化，不过影响不大。而如果进入异常分支，基本就意味着 Spring 应用运行失败，可参考 SpringApplicationRunListener。 对于 SB 的异常处理，1.1 开始就替换为 Throwable，同时拥有故障分析器：FailureAnalysis，它会在上下文关闭之前执行错误分析并输出报告。其中 FailureAnalysis 仅分析故障，报告则由 FailureAnalysisReporter 负责（也是由工厂机制加载排序，默认仅存在一个）。当然你也可以自定义这两个的实现，一样配置到 spring.factories 中，1.x 与 2.x 有所不同，SC Data Flow 用户关注一下。 应用退出阶段这里主要是 ShutdownHook 线程，在 JVM 退出时能够确保完成 Spring 生命周期回调，进行资源释放，例如 JDBC 连接、会话状态等。这里也可以细分正常退出和异常退出，还有个退出码，不知道有没有人关注，用 IDEA 的时候会打印出来，非 0 就是异常退出，相关源码：ExitCodeGenerator。因为 IDEA 可以获取子进程的退出码，但是真实环境下基本不可用，除非你是用 SC Data Flow 之类。异常退出由 SpringApplication 的 handleRunFailure 方法负责，由于用的不多，不多展开了。 其他Spring IO Platform 项目是为了统一 Maven 管理的项目，2019 年后不再维护，被 spring-boot-dependencies 和 spring-boot-starter-parent 取代。 当相应的 starter 添加到 ClassPath 后，其关联的特性随应用的启动而自动装载，这种机制称为自动装配（AutomaticallyConfigure）。 使用 @HandlesTypes 来进行过滤，选择出自己关系的类型。 SB 根据 Web 类型推断来创建对应的 Spring 应用上下文，我们最常用的 Servlet 类型就是用 AnnotationConfigWebApplicationContext。WebApplicationType 还可以作为 ConfigurableEnvironment 对象具体类型的条件，所以 applicationContextClass 的属性设定后还需要对 webApplicationType 设置。 在 spring.factories 中声明的资源，可能存在重复执行的情况，所以建议凡是使用 Spring 工厂加载机制的场景，建议覆盖 hashCode 和 equals。 SB 的事件监听器均由 Spring 工厂加载机制加载并初始化，它们并非 Spring Bean，因此无法享受注解驱动和 Bean 生命周期管理回调接口的『福利』，不过这并不影响他们获得 Spring Bean，因为有关联的 ConfigurableApplicationContext 对象。这个要对比 ApplicationRunner 和 CommandLineRunner 看。 SB 引入 SpringApplication 大概是对 Spring 应用上下文的生命周期的补充。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的日志框架]]></title>
    <url>%2F2020%2F04%2F11%2FJava%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[很久之前我简单整理过相关笔记，刚开始认为日志框架只是很简单的存在，然而事实上并不简单，尤其在不同的依赖中使用不同的框架下，就是一团糟；再加上 SB 的黑盒，如果用的很杂其实还是蛮糟心的，各种冲突时不时跳出来恶心你一下，借此机会重新整理一下。 框架介绍我们主要讨论那些比较知名的日志框架，例如 commons-logging、log4j、slf4j、logback。在探讨之前，可以先了解下为什么需要日志框架，最早的框架是怎么演变的，这里推荐一篇文章：一个著名的日志系统是怎么设计出来的？ 在 log4j 被 Apache Foundation 收入门下之后，由于理念不合，log4j 的作者 Ceki 离开并开发了 slf4j 和 logback。 下面就一个个的来看。 CommonsLogging/JCLapache-commons-logging （之前称为 JakartaCommonsLogging 即 JCL）是 Apache 提供的一个通用的日志接口，可以理解为是一个规范。在 commons-logging 中，有一个 SimpleLogger 的简单实现，但是它功能很弱，所以使用 commons-logging ，通常都是配合着 log4j 来使用； commons-logging 会通过动态查找的机制，在程序运行时自动找出真正使用的日志库，并且尽可能找到一个”最合适”的日志实现类，如果判断有 Log4j 包，则使用 log4j，最悲观的情况下也总能保证提供一个日志实现 (SimpleLog) Log4j和Log4j2它是 Apache 的一个开放源代码项目，实现了输出到控制台、文件、 回滚文件、发送日志邮件、输出到数据库日志表、自定义标签等全套功能，且配置比较简单。 后来 Apache Logging 一直在关门憋大招，log4j2 在 beta 版鼓捣了几年，终于在 2014 年发布了 GA 版，不仅吸收了 logback 的先进功能，更通过优秀的锁机制、LMAX Disruptor、”无垃圾”机制等先进特性，在性能上全面超越了 log4j 和 logback。 log4j2 弃用了 properties 方式配置，采用的是 xml、json 或者 jsn 这种方式来做。 可以说 log4j2 是与 logback 来对标的。 slf4jslf4J，即简单日志门面（Simple Logging Facade for Java），不是具体的日志解决方案，它只服务于各种各样的日志系统。按照官方的说法，SLF4J 是一个用于日志系统的简单 Facade，允许最终用户在部署其应用时使用其所希望的日志系统。 可以这么说，slf4j 等于 commons-logging，是各种日志实现的通用入口，会根据 classpath 中存在下面哪一个 Jar 来决定具体的日志实现库；它只是一个 API，提供一个规范。因为 slf4j 用的很广泛，所以重点说说，提供一个架构图。 SLF4J (Simple logging Facade for Java) 不是一个真正的日志实现，而是一个抽象层（ abstraction layer），它允许你在后台使用任意一个日志类库。如果是在编写供内外部都可以使用的 API 或者通用类库，那么你真不会希望使用你类库的客户端必须使用你选择的日志类库。 如果一个项目已经使用了 log4j，而你加载了一个类库，比方说 Apache ActiveMQ 它依赖于于另外一个日志类库 logback，那么你就需要把它也加载进去。但如果 Apache Active MQ 使用了 SLF4J，你可以继续使用你的日志类库 (当前是 log4j) 而无需忍受加载和维护一个新的日志框架的痛苦。 slf4j 为各类日志输出服务提供了适配库，如 slf4j-log4j12（log4j 适配器），slf4j-simple（slf4j 简单实现），slf4j-jdk14（适配 JDK 的 Logger）等。一个 Java 工程下只能引入一个 slf4j 适配库，slf4j 会加载 org.slf4j.impl.StaticLoggerBinder 作为输出日志的实现类。这个类在每个适配库中都存在，当需要更换日志输出服务时（比如从 logback 切换回 log4j），只需要替换掉适配库即可。我们简单理解为，你使用 slf4j-api 来进行开发，其他人可以通过选用不同的适配器 + 对应的具体日志类的方式来进行各种组合。 slf4j 还推出了 jcl-over-slf4j 桥接库，能够把使用 JCL 的 API 输出的日志桥接到 slf4j 上，方便那些想要使用 slf4j 作为日志门面但同时又要使用 Spring 等需要依赖 JCL 的类库的系统。 logbacklogback 是由 log4j 创始人设计的又一个开源日志组件。logback 当前分成三个模块：logback-core、logback- classic 和 logback-access。 logback-core是其它两个模块的基础模块。 logback-classic它是 log4j 的一个 改良版本。此外 logback-classic 完整实现 SLF4J API 使你可以很方便地更换成其它日志系统如 log4j。 logback-access主要作为一个与 Servlet 容器交互的模块，比如说 tomcat 或者 jetty，提供一些与 HTTP 访问相关的功能。 logback 天然与 slf4j 适配，不需要额外引入适配库（毕竟是一个作者写的）想在 Java 程序中使用 Logback，需要依赖三个 jar 包，分别是 slf4j-api，logback-core，logback-classic。其中 slf4j-api 并不是 Logback 的一部分，是另外一个项目，但是强烈建议将 slf4j 与 Logback 结合使用。 补充在 Java 领域日志工具中，最早得到广泛使用的是 log4j。那么为啥有 commons-logging 的出现？上面已经介绍了 common-logging 只提供 log 的接口，其中具体的实现时动态绑定的，所以 common-logging 与 log4j 的结合比较多！ 但是随之也产生了一些问题，那就是 common-logging 的动态绑定有时候也会失败，在这样的背景下 slf4j 应运而生，slf4j 与 commons-logging 一样提供 log 接口，但是 slf4j 是通过静态绑定实现。 slf4j 唯独没有提供 log4j2 的适配库和桥接库，log4j-slf4j-impl 和 log4j-to-slf4j 都是 Apache Logging 自己开发的，看样子 Ceki 和 Apache Logging 的梁子真的很深啊……倒是 Apache 没有端架子，可能也是因为 slf4j 太火了吧 log4j2 和 logback 各有长处，总体来说，如果对性能要求比较高的话，log4j2 相对还是较优的选择 SB中的应用在 Spring 中使用的是 JCL 框架，SB 中根据 spring-boot-starter-logging 的依赖分析，可以得出： SpringBoot2.x 底层也是使用 slf4j+logback 或 Log4J 的方式进行日志记录。 SpringBoot 引入中间替换包把其他的日志都替换成了 SLF4J。 如果我们要引入其他框架、可以把这个框架的默认日志依赖移除掉。 至于 1.x 版本就不再讨论了，也可能是 JCL。根据不同的日志系统，你可以按如下规则组织配置文件名，就能被正确加载： Logback：logback-spring.xml, logback-spring.groovy, logback.xml, logback.groovy Log4j：log4j-spring.properties, log4j-spring.xml, log4j.properties, log4j.xml Log4j2：log4j2-spring.xml, log4j2.xml JDK (Java Util Logging)：logging.properties 门面框架虽然有 slf4j 和 jcl 两类，就目前肯定 slf4j 更受欢迎，那么之前用 jcl 的怎么办，例如 spring，这多亏了 jcl-over-slf4j 桥接器可以进行转换，这都不是事。 使用log4j也就是目前可以说最常见的组合 slf4j + log4j，先排除后添加： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;&lt;/dependency&gt; 可以根据日志情况来确认。 使用log4j2方法其实有很多种，那种好目前我也没评估，举例一种： 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt;&lt;/dependency&gt; 思路就是排除 SB 自带的日志框架，然后加入我们自己的（通过 starter 方式） 经典使用就目前来说，用的最广泛的还是 log4j ，毕竟即使后面的框架更好，项目已经进行好几年的情况下贸然换用可能会导致很多奇怪的问题，这也造成来现在的依赖日志框架都是乱七八糟，但总体以 log4j 为主，缺点可以忍受，没有大到需要换框架的地步。 log4j使用因为 log4j 用的很多，所以重点再说说。默认会读取资源目录下的 log4j.properties 文件，当然也可以自定义配置文件的位置。配置文件的基本格式： 1234567891011121314# 配置根 Loggerlog4j.rootLogger = [level], appenderName1, appenderName2, …# 配置日志信息输出目的地 Appenderlog4j.appender.appenderName = fully.qualified.name.of.appender.class log4j.appender.appenderName.option1 = value1 … log4j.appender.appenderName.optionN = valueN # 配置日志信息的格式（布局）log4j.appender.appenderName.layout = fully.qualified.name.of.layout.class log4j.appender.appenderName.layout.option1 = value1 … log4j.appender.appenderName.layout.optionN = valueN 输出级别关于 level 日志输出级别，共有五级： 标识 ID 描述 FATAL 0 适用于严重错误事件 ERROR 3 适用于代码存在错误事件 WARN 4 适用于代码会有潜在错误事件 INFO 6 适用于代码运行期间 DEBUG 7 适用于代码调试期间 除此之外还有两种状态就是 ALL： 打开所有日志；OFF：关闭所有日志； 输出目的地Appender 为日志输出目的地，Log4j 提供的 appender 有以下几种： 12345org.apache.log4j.ConsoleAppender（控制台），org.apache.log4j.FileAppender（文件），org.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件），org.apache.log4j.RollingFileAppender（文件大小到达指定尺寸的时候产生一个新的文件），org.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方） 日志输出格式Layout：日志输出格式，Log4j提供的layout有以下几种： 1234org.apache.log4j.HTMLLayout（以HTML表格形式布局），org.apache.log4j.PatternLayout（可以灵活地指定布局模式），org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串），org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等等信息） 打印参数 Log4J 采用类似 C 语言中的 printf 函数的打印格式格式化日志信息，如下: 123456789101112%m 输出代码中指定的消息%p 输出优先级，即 DEBUG，INFO，WARN，ERROR，FATAL %r 输出自应用启动到输出该log信息耗费的毫秒数 %c 输出所属的类目，通常就是所在类的全名 %t 输出产生该日志事件的线程名 %n 输出一个回车换行符，Windows 平台为 &quot;\r\n&quot;，Unix 平台为 &quot;\n&quot; %d 输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式， 比如：%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;，输出类似：2017-12-21 13:37:05 512%l 输出日志事件的发生位置，包括类目名、发生的线程，以及在代码中的行数。 举例：Testlog4.main(TestLog4.java: 10 )[%10p] 右对齐，最小宽度10[%-10p] 左对齐，最小宽度10 SSS 其实是毫秒的意思 配置参考具体项目具体对待，仅供参考： 1234567891011121314151617181920212223242526### set log levels ### log4j.rootLogger = debug, stdout, D, E ### 输出到控制台 ### log4j.appender.stdout = org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target = System.out log4j.appender.stdout.layout = org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern = %d&#123;ABSOLUTE&#125; %5p %c&#123; 1 &#125;:%L - %m%n ### 输出到日志文件 ### log4j.appender.D = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.D.File = logs/log.loglog4j.appender.D.Append = true## 输出DEBUG级别以上的日志log4j.appender.D.Threshold = DEBUGlog4j.appender.D.layout = org.apache.log4j.PatternLayoutlog4j.appender.D.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125; [ %t:%r ] - [ %p ] %m%n ### 保存异常信息到单独文件 ### log4j.appender.E = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.E.File = logs/error.loglog4j.appender.E.Append = true## 只输出ERROR级别以上的日志!!!log4j.appender.E.Threshold = ERRORlog4j.appender.E.layout = org.apache.log4j.PatternLayoutlog4j.appender.E.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125; [ %l:%c:%t:%r ] - [ %p ] %m% log4j2使用 log4j2 需要两个依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt;&lt;/dependency&gt; 配置文件示例，就是格式变成了 XML ，和上面其实也差不多： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;configuration status="error"&gt; &lt;!-- 先定义所有的appender --&gt; &lt;appenders&gt; &lt;!-- 这个输出控制台的配置 --&gt; &lt;Console name="Console" target="SYSTEM_OUT"&gt; &lt;!-- 控制台只输出level及以上级别的信息（onMatch），其他的直接拒绝（onMismatch）--&gt; &lt;ThresholdFilter level="trace" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;!-- 这个都知道是输出日志的格式 --&gt; &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;/Console&gt; &lt;!-- append 为TRUE表示消息增加到指定文件中，false 表示消息覆盖指定的文件内容，默认值是true --&gt; &lt;!-- 打印出所有的信息 --&gt; &lt;File name="log" fileName="log/test.log" append="false"&gt; &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;/File&gt; &lt;!-- 添加过滤器ThresholdFilter,可以有选择的输出某个级别以上的类别 onMatch="ACCEPT" onMismatch="DENY"意思是匹配就接受,否则直接拒绝 --&gt; &lt;File name="ERROR" fileName="logs/error.log"&gt; &lt;ThresholdFilter level="error" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd 'at' HH:mm:ss z&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;/File&gt; &lt;!-- 这个会打印出所有的信息，每次大小超过size，则这size大小的日志会自动存入按年份-月份建立的文件夹下面并进行压缩，作为存档 --&gt; &lt;RollingFile name="RollingFile" fileName="logs/web.log" filePattern="logs/$$&#123;date:yyyy-MM&#125;/web-%d&#123;MM-dd-yyyy&#125;-%i.log.gz"&gt; &lt;PatternLayout pattern="%d&#123;yyyy-MM-dd 'at' HH:mm:ss z&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;SizeBasedTriggeringPolicy size="2MB"/&gt; &lt;/RollingFile&gt; &lt;/appenders&gt; &lt;!-- 然后定义logger，只有定义了logger并引入的appender，appender才会生效 --&gt; &lt;loggers&gt; &lt;!-- 建立一个默认的root的logger --&gt; &lt;root level="trace"&gt; &lt;appender-ref ref="RollingFile"/&gt; &lt;appender-ref ref="Console"/&gt; &lt;appender-ref ref="ERROR" /&gt; &lt;appender-ref ref="log"/&gt; &lt;/root&gt; &lt;/loggers&gt; &lt;/configuration&gt; 配置其实都差不多，应该还是蛮好看懂的。 logback这里我也不打算详细说了，贴一个配置文件示例（logback.xml）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration scan="true" scanPeriod="60 seconds" debug="false"&gt; &lt;appender name="console" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;encoder&gt; &lt;pattern&gt;[%d&#123;HH:mm:ss.SSS&#125;][%p][%c&#123;40&#125;][%t] %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;DEBUG&lt;/level&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;appender name="mmall" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--&lt;File&gt;d:/mmalllog/mmall.log&lt;/File&gt;--&gt; &lt;File&gt;/developer/apache-tomcat-7.0.73/logs/mmall.log&lt;/File&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;/developer/apache-tomcat-7.0.73/logs/mmall.log.%d&#123;yyyy-MM-dd&#125;.gz&lt;/fileNamePattern&gt; &lt;append&gt;true&lt;/append&gt; &lt;maxHistory&gt;10&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;[%d&#123;HH:mm:ss.SSS&#125;][%p][%c&#123;40&#125;][%t] %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="error" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--&lt;File&gt;d:/mmalllog/error.log&lt;/File&gt;--&gt; &lt;File&gt;/developer/apache-tomcat-7.0.73/logs/error.log&lt;/File&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;/devsoft/apache-tomcat-7.0.73/logs/error.log.%d&#123;yyyy-MM-dd&#125;.gz&lt;/fileNamePattern&gt; &lt;!--&lt;fileNamePattern&gt;d:/mmalllog/error.log.%d&#123;yyyy-MM-dd&#125;.gz&lt;/fileNamePattern&gt;--&gt; &lt;append&gt;true&lt;/append&gt; &lt;maxHistory&gt;10&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;[%d&#123;HH:mm:ss.SSS&#125;][%p][%c&#123;40&#125;][%t] %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;logger name="com.mmall" additivity="false" level="INFO" &gt; &lt;appender-ref ref="mmall" /&gt; &lt;appender-ref ref="console"/&gt; &lt;/logger&gt; &lt;!-- geelynote mybatis log 日志 --&gt; &lt;logger name="com.mmall.dao" level="DEBUG"/&gt; &lt;!--&lt;logger name="com.ibatis.sqlmap.engine.impl.SqlMapClientDelegate" level="DEBUG" &gt;--&gt; &lt;!--&lt;appender-ref ref="console"/&gt;--&gt; &lt;!--&lt;/logger&gt;--&gt; &lt;!--&lt;logger name="java.sql.Connection" level="DEBUG"&gt;--&gt; &lt;!--&lt;appender-ref ref="console"/&gt;--&gt; &lt;!--&lt;/logger&gt;--&gt; &lt;!--&lt;logger name="java.sql.Statement" level="DEBUG"&gt;--&gt; &lt;!--&lt;appender-ref ref="console"/&gt;--&gt; &lt;!--&lt;/logger&gt;--&gt; &lt;!--&lt;logger name="java.sql.PreparedStatement" level="DEBUG"&gt;--&gt; &lt;!--&lt;appender-ref ref="console"/&gt;--&gt; &lt;!--&lt;/logger&gt;--&gt; &lt;root level="DEBUG"&gt; &lt;appender-ref ref="console"/&gt; &lt;appender-ref ref="error"/&gt; &lt;/root&gt;&lt;/configuration&gt; 具体用到再做补充。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticStack从入门到放弃]]></title>
    <url>%2F2020%2F03%2F19%2FElasticStack%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83%2F</url>
    <content type="text"><![CDATA[关于 ES 的基本使用之前已经写过了，然后 ELK 也许听的比较多，后来又有了一个 ELK Stack，指的就是 Elastic Stack。这一套技术用在搜索需求、日志分析非常好用，也有很多用来处理 Excel 数据的，就现在都发展来看，ES 真是绕不开都技术，早晚都是要学，ELK 一套带走就好了。毕竟这一全家桶内容很多，不打算过多深挖，很多东西都不细说了，提供关键词提点自己，用到来查到搜索方向后再 Google 具体使用就好了。但是，它东西真的太多了，并且枯燥，对于不喜欢数据库的我来说，以至于最终还是不能坚持，草草了事吧。 Elasticsearch是一个搜索和分析引擎（数据的存储、查询、分析） Logstash是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”中 Kibana可以让用户在 Elasticsearch 中使用图形和图表对数据进行可视化 Beats一系列轻量型的单一功能数据采集器 Logstash 和 Beats 的作用就是数据的收集与整理，只不过 Beats 是为了解决用户的 “我只想对某个文件进行 tail 操作”的需求。上面说过它们是采集数据的，范围很广，例如文件类的有日志、excel，数据库和 http 服务也可以，还支持自定义扩展。 ES 的服务端口默认 9300，Web 管理端口默认 9200.官网文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html 基本概念复习一下 ES 的基本概念，交互是靠 RESTful API，ES 中主要的就三类： 文档-Document最小的数据单元，内含多个字段 ，类似于数据库中的一行数据；由 Json Object （字段）构成，多种数据类型（可存二进制）；当然，还会存在一些元数据 类型-Type索引可以定义一个或多个类型，文档必须属于一个类型，相当于数据库中的 table；未来版本中因为索引调整为唯一，下面不再有多个类型，所以类型将会逐步消除。 索引-Index相同属性的文档集合，相当于数据库的 Database；再 6+ 的版本，官方已经禁止索引下创建多个类型，可以当作是 table 了。 节点-Node一个 ES 的运行实例，构成集群的基本单元。每一个 ES 实例可以看作就是一个 JVM 线程。 集群-Cluster多个节点构成的统一体 分布式中，必然还少不了分片和备份，类似 kafka 的概念了，一个索引可以分片成多个块，存放在不同的节点，这样既可以提高吞吐量，也可以方便的扩展，降低单节点的压力；备份就不多说了，分布在其他的节点上。 另外 ES 的特性还有：近实时，也就是秒级；分布式架构，方便扩展，简单说就是对 Lucene 的一个封装。 倒排索引与分词： 在说倒排索引之前，先要知道什么是正排索引，做比喻的话，就像一本书的目录，这就是正排索引；而倒排索引就是书的名词索引页。具体到 ES 的话，正排索引就是根据 ID 获取文档信息；倒排索引就是根据文档的内容（关键词）定位到是那个文档 ID； 而对于分词，这个也简单，毕竟对于搜索，分词后与之建立关系才能搜索。正排与倒排的关系就是 ID 到单词（或者完整信息，不分词也可）与单词到 ID 的关系（一般需要分词）。 倒排索引主要包含两部分： 单词词典（Term Dictionary）记录所有文档单词和列表关联关系，一般都比较大。实现一般是类似 B+ 树的结构。 倒排列表（Posting List）主要有 ID （获取原始信息）、单词频率（相关性算分）、位置（搜索词的前后）、偏移构成（高亮显示）。 我们从单词词典中拿到一个词，然后通过记录的偏移量快速定位到对应的倒排列表，然后就拿到了原始 ID（还会涉及相关性算分等等步骤），最后就可以返回了。PS：倒排索引是按字段来进行构建的。 分词说道分词，就得说分词器，组成如下： Character Filters针对原始文本进行处理，比如去除 html 特殊标记符；自带的有：HTML Strip、Mapping（字符串替换）、Pattern Replace（正则匹配替换） Tokenizer将原始文本按照一定规则切分为单词；自带的有 Standard、letter（非字符分割）、Whitespace、path_hierarchy 等。 Token filters针对 tokenizer 处理的单词就行再加工，比如转小写、删除或新增等处理；自带的有 lowercase（小写转换）、stop、Synonym 等。 在调用过程中就是按照上面都顺序执行，可以使用 analyze_api 来进行测试分词是否符合预期。ES 自带的分词器有： Standard（默认）按词切分，支持多语言，小写处理 Simple按照非字母拆分，小写处理 Whitespace按照空格切分 Stop相比 Simple 多了 Stop Word 处理，也就是语气助词等修饰词 Keyword不分词，当作一个单词输出 Pattern通过正则表达式自定义分词，默认 \w+ 即非字符符号 language提供常见的 30+ 语言分词器 说到中文，中文的分词是比较难的，因为中文的词之间没有明显的分隔符，并且很依赖上下文。常用的中文分词器有 IK、jieba （py 流行）等。另外的一些就是基于自然语言的分词系统，也就是根据上下文来切，例如 Hanlp、THULAC；如果这些分词器满足不了你，那就只能进行自定义了，就是自己写分词器的那三部分。 PS：只有查询与索引使用相同的分词器才能保证一致性，一般查询不需要指定分词器，默认就是一致的。 MappingMapping 相当于表结构定义，定义字段名、字段类型、倒排索引相关；获取可以使用 /index/_mapping API 查看。更多操作看文档就行了，不多说。Mapping 中的字段一旦确定，禁止直接修改，因为 Lucene 实现倒排索引后禁止修改，要改只能重新建立索引做 reindex 操作，所以要慎重。虽然禁止修改，但是你新增是没有问题的，这个行为也是可以通过 dynamic 来进行控制。Mapping 中还可以通过 copy_to 来将字段进行拼接处理；通过 index 来控制是否被索引，敏感信息可以加上这个，还省空间；对空值处理的 null_value，其他还有很多可配置的，参考官方文档。 ES 支持多字段特性 multi-fields，就是对一个字段采取不同的配置，例如拼音化。我们还可以通过使用 dynamic-template 动态模板来设置 ES 自动匹配的内容为什么类型，例如默认我们不希望让它识别为 text 分词，这样会占用空间，以及用 float 保存浮点数节约存储等；它是由上到下顺序匹配。类似的，也会有索引模板，便于索引的创建，使用 /_template API。 在数据建模过程中，要注意这些配置合理化，例如： enabled是否仅存储，不做搜索和聚合分析，节省空间 index是否构建倒排索引 index_options存储倒排索引的那些信息 norms如果不需要算分排序，仅用于过滤和聚合分析，可关闭 doc_values同上，要排序要分析就开，搜索过滤就没必要 fielddata是否为 text 类型启用排序和聚合分析等 store是否存储字段值，当内容很多时为了避免影响性能会设置为 false，独立出去；配合 _source 元信息。即使你使用 API 只要求返回限定字段，由于 ES 的原理，还是会拿到所有字段，只是在返回的时候给你过滤掉，所以这种治标不治本。 coerce是否开启类型自动转换 dynamicMapping 自动更新 其他的就是一些 date_detection 这种控制日期是否自动转换的配置，建模的过程也不简单，如果想设计的好。枚举类型一般设置为 keyword 因为不需要分词，也不需要计算，例如 HTTP 的状态码之类 类似传统数据库，在 ES 中也可以处理表关系，Nested Object 与 Parent/child ，不过对于我来说太复杂了，不多看了，我真的对数据库提不起兴趣。子元素更新频繁用 Parent，查询频繁用 Nested，因为 Parent 是分开存的，但是尽量用 Nested。 最后，保存好你的 Mapping 设计文档，做好版本管理。 查询ES 毕竟是一个数据库，并且是为搜索而生，主力还是在搜索 API 上，这一块太多了，之前的笔记也有常用的部分，还是以官网 API 文档为准，搜索的时候只要善用关键词，例如 AND、NOT、分组、+/- （记得 URL 编码）、区间之类会大大简化查询语句。尽量不要使用通配符、正则，尤其放在最前面，文档很大的时候会拖慢速度。相比传统数据库，它还多了近似度匹配的模糊查询，并且它有一个相关性算分的功能（结果排序），搜索引擎必备了；相关性算分需要算法引擎支持，目前默认的是 BM25（相比 TF/IDF 优化了词频 tf 过大的无限增长问题），可以通过 explain 来查看算分过程，算分过程是 shard 独立的，集群的需要注意。另外，在涉及日期搜索中，可以直接通过字符串计算，例如 now +1h ，总之就是很灵活的，不要受限于 SQL。text 类型的字段在搜索排序的时候不能直接用（可通过 `name.keyword 转换为整体进行检索），ES 中提供了 3 种分页方式，最常用的还是 from/size（快照方式 Scroll 非实时性，利用排序方式优化的 Search After 不能实现自由翻页），因为分片的原因，分页的时候并不能确定需要的记录在那个分片，只能每个分片都查一下，最后汇总后确定真正的条数。所以这种分页在遇到深度分页的时候会很耗资源，也就是越往后每个分片需要处理的数据就更大，就如同 Google 也不会把结果无限分页，如果前面都不是你想要的，后面更不会了。 由于倒排索引不可修改（确实有很多好处例如速度快可压缩等），新增数据的时候重新生成与修改，这样实时性肯定就无法保证，性能开销也很大，为了解决这个问题，可以单独为新文档构建倒排索引，查询的时候同时去两份倒排索引里翻， 当然需要有个文件来存储分块信息。为了达到实时性搜索，ES 做了很多优化，例如文件系统的缓存（refresh），在还未完全写入的时候就可以提供查询，当然有相应的保护机制（translog），毕竟 ES 被称为近实时的搜素。 想要存储大量数据，分片是必须的，但是分片就会遇到相关性算分不准的问题，这时候就需要用 DFS Query-then-Fetch（通过 RESTful API 指定 search-type）。 集群搭建过程就免了，在集群状态可视化上，可以常识下 cerebro 这个插件。集群中，分片与副本是关键，分片还有主副之分。新增节点后，原有对 index 不会增加数据容量，因为分片已经固定了；同样，只有新增节点才能增加吞吐量，增加副本是没用的。ES 提供了 API 来查看集群的状态，用颜色表示的话，绿色健康，黄色表示主分片分配正常，副本未正确分配，至于红色就是主分片未正确分配，会影响搜索，但是不会影响服务。既然是集群，肯定是要支持故障转移来实现高可用，这就牵扯到了 Master 选举。 文档的分片有几种方法，传统的随机在多节点下查找起来非常费劲，如果采用映射表的方式维护就是个难题（海量数据下），所以根据文档具有信息实时计算是不错的选择，就是利用哈希了，也正是因此，分片确定后就不可修改。这里请求数据的时候，『寻址』或者重定向是服务端做的，客户端只与服务端建立一次连接即可，即使目标数据不在这个节点，具体的方案可参考之前写的分布式集群相关，例如 Redis、ZK 都在用，至于脑裂问题也就是选举的问题，在 ZK 哪里也着重说过。 ES 的性能基本是按照性能扩展的，所以，你只要计算出了单个 shard 的指标，就能推断出所需要多少 shard。压测工具可以试试 esrally，官方还提供了一个 x-pack 插件来监控，Kibana 也有对应的可视化插件。就算日志场景一个 shard 也不要超过 50G，搜索场景不要超过 15G。 聚合分析为了便于统计，ES 提供了很多聚合查询功能（aggs），相比 hadoop 这类 T+1 的，它是实时返回的。聚合查询主要分为下面四类： Bucket分桶类型，类似 SQL 中的 GROUP BY 语法 Metric指标分析类型，如计算最大值、最小值、平均值等等；分为单值与多值类型（stats） Pipeline管道分析类型，基于上一级的聚合分析结果进行再分析 Matrⅸ矩阵分析类型，例如热力图 相比搜索，聚合还多了计算的能力，因为分片的存在，你不能要求它的计算结果是多么的精确，可以通过一些参数来调整精准度（show_term_doc_count_error 对应 doc_count_error_upper_bound），但是相应的也要付出性能。海量数据、精准度、实时性，这三个不能兼得，只能选其二，显然 ES 聚合分析（Cardinality 和 Percentile）放弃了精准度，例如 Hadoop 就放弃了实时性。 文档地址：https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html Logstash按照之前的介绍，它就是个过滤器，或者说适配器。 12345678910111213// codec.confinput &#123; // 标准输入 stdin &#123; codec =&gt; line &#125;&#125;output &#123; stdout &#123; codec =&gt; json &#125;&#125; 配置文件就是这样了（支持表达式），内部原理也是使用队列，一定程度可以代替 Kafka，默认使用的是内存队列，建议使用持久队列就行，性能损耗不是太大。主要涉及三类线程，读取、队列、消费，配置文件中可以对部分进行微调，这就是调优了，不多说。常用的 input 插件有：stdin、file、kafka，不需要担心继续读取、更新检查、重复读取的问题，都考虑到了。要想写的好，可能还需要学习 Grok 脚本。 Beats简单说可以理解为 Logstash 的轻量级版，了解过 Logstash 的应该知道配置非常复杂，也正是因为 Beats 是轻量级的，主要在数据的收集上，如果需要对数据进行复杂的处理，还是要接入 Logstash。 用于日志分析的话，基本都是用 Filebeat 这个插件，配置也蛮复杂的，还好有 Modules 可以有，达到开箱即用。 如果用于指标分析，就是 Metricbeat 这个轻量级采集器配合使用了，细分可分为系统指标类（CPU 使用等）与日志类（Redis、MySQL 提供的性能指标），不管怎么说，它都是在收集处理指标数据，供分析系统的状态。它相应的也有大量 Modules，系统的、Docker、Redis、MySQL 等等，用什么就装什么，相当于自动给你写好对应的配置文件了，你只需要改一下连接地址就能用。 网络数据包的分析使用 Packetbeat 插件，使用方式与上面一样，可以用来分析 DNS、HTTP、TLS、MySQL 连接等这些信息。对于 Linux 系统可使用 af_packet 模式获取更好的性能。 心跳检测，确认对方是否存活，使用 Heartbeat 插件，社区也有很多质量很高的插件。 Kibana为了让请求均匀分布，线上部署一般是采用一个 Coordinating Only ES Node 负责分发，这样 Kibana 只需要连接这个节点即可。因为是 Web UI 终于不跟之前的那些似得那么枯燥了，也仅仅是视觉上，操作还是离不开 ES 的那些语句，要配置好界面也不是一个容易的活。]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React核心使用总结]]></title>
    <url>%2F2020%2F03%2F01%2FReact%E6%A0%B8%E5%BF%83%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[既然 Vue 都学了，难道不顺便把 React 收了么？它的大名就不需要说了，全球最火的前端框架，也是个啥都能写的货。相比 Vue 它会更灵活，这就意味着它更复杂，对于主力后端的我来说，也就点到为止，那些深层次的东西就不挖的太深了，App （React Native）相关的构建也以后再说，这里主要是 Web 应用的开发。 准备环境创建一个 React App 就使用专业一点的 Create React App 脚手架吧，安装方式官网都有写，不多说。之前官方文档会让安装这个库，现在直接可以使用 npm 自带的 npx 指令（它是 npm 5.2+ 附带的 package 运行工具）来创建了。 下面来看一下创建出来的目录机构，相比 Vue，看着好像精简一点，默认使用 yarn 构建；webpack 相关的文件就不多说了，其中有用的就是 public 和 src 文件夹。public 文件夹很简单，放了一些 logo，还有一个网站入口文件 index.html，这个文件也非常简单，就是一个基本骨架。看文件夹名字，src 是主要战场，里面有个 index.js 是入口 js 文件，可以说是按照这个文件来一句一句执行代码。可以看到里面导入了一个 serviceWorker 的模块，这个是 PWA（Progressive Web App）是做 App 适配相关的，有个有意思的特点是加载后，断网情况下可查看已经加载过的内容。 PWA 的核心目标就是提升 Web App 的性能，改善 Web App 的用户体验。媲美 native 的流畅体验，将网络之长与应用之长相结合。 这 public 中的 manifest.json 文件就是来定义将此 Web App 添加快捷方式放到桌面上的图标等配置，用来尽量模拟原生 App 的体验。 不过 PWA 毕竟不是现在的重点，这个先放一放，到 React Native 再说。 另外，public 文件夹还有另一个作用，就是项目启动后，会自动把这个文件夹作为资源服务，所以这里的 html 也就是主入口了，所以你在这里面放 json 文件，可以作为接口的模拟。 组件React 中也是有组件的概念，就是一个个的模块，例如 App.js 就是一个模块，通过 index.js 来引入，我学习的时候使用的是最新的版本，但是资料是老的，为了兼顾老版本，尽量都写一下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// index.jsimport React from 'react';import ReactDOM from 'react-dom';import App from './App';// js 中的这种标签写法是 JSX 语法ReactDOM.render(&lt;App /&gt;, document.getElementById('root'));// App.jsimport React from 'react';import './App.css';function App() &#123; return ( &lt;div className="App"&gt; Mps~ &#123; this.state.list.map((item, index) =&gt; &#123; return &lt;div key=&#123;index&#125;&gt;&#123;item&#125;&lt;/div&gt; &#125;) &#125; &lt;/div&gt; );&#125;export default App;// 较老版本默认的 App.jsimport React, &#123; Component &#125; from 'react'; // Component 是 React 的子模块class App extends Component &#123; constructor(props) &#123; // 固定调用 super(props); this.state = &#123; val: '' &#125;; // 推荐在构造中改变 this 指向，如果用得到 this.method = this.method.bind(this) &#125; // render 函数返回的即最终组件内容，使用了 JSX render() &#123; return &#123; &lt;div&gt; MPS~ &lt;/div&gt; &#125; &#125;&#125; ReactDOM 这个模块是用来将我们写的组件挂载到 index.html 中的，至于为什么需要引入 React，因为使用到了 JSX。 无论你使用函数还是 class 定义，在 React 中是等效的，不过看起来函数更加的简洁，并且它还可以接受一个 props 对象来传递数据；不过嘛，虽然简洁了，但也牺牲了一些特性。如果是用的类定义方式，那么 props 的接收就要放到构造函数（constructor）里，效果是一样的。另外 props 是只读的，不可修改；另外，也不要直接修改 state 的内容，要改也是拷贝一份，然后使用 set 方法修改。 无状态组件为了更加方便管理，一般会将 UI 部分单独拆离出来，也就是把 render 函数单独搞出来，这样在上层的组件中 render 之间返回 &lt;xxxUI /&gt; 就可以了，但是 UI 中用到的变量怎么办，就是父子组件的传值了。在需要传递方法给子组件执行的时候，如果方法需要传递参数，应该怎么写呢，之间加括号显然不妥，只能使用箭头函数：onClick={() =&gt; {this.props.method(p)}} 像这种抽取出来的 UI 组件只有视图，那么就可以称为无状态组件，还记得脚手架给我们生成的默认文件的那个函数，那就是一个无状态组件了，它的性能更高。 JSX虽然 React 并不强制使用 JSX，但是基本所有的 React 都这用吧，毕竟是真的好用，目前只能说说用到的一些功能。 JSX 语法不仅仅是可以便捷的使用 HTML 标签，自定义的标签也是可以的，例如 &lt;App /&gt;不过需要注意，自定义的标签开头必须大写，也就是你 import 导入的时候命名就得符合这个规则。 JSX 规定返回的内容必须包裹这一个标签内，但是这样就会多了一层结构，对样式可能不友好，类似的，它也有相应的特殊标签来规避：Fragment 12345678910111213141516171819202122render() &#123; return ( &lt;React.Fragment&gt; &lt;ChildA /&gt; &lt;ChildB /&gt; &lt;input value=&#123;this.state.val&#125; ref=&#123;(input) =&gt; &#123;this.input = input&#125;&#125; onChange=&#123;this.handleChange.bind(this)&#125;/&gt; &lt;/React.Fragment&gt; );&#125;handleChange(e) &#123; // 必须通过 set 函数 dom 才会刷新 this.setState(&#123; // e.target = this.input val: e.target.value, // 展开运算符，添加新元素 list: [...this.state.list, newVal] &#125;)&#125; 可以看出，它其实也是一个组件，只不过是 React 自带的，如果嫌麻烦，可以在 import 的时候就把它导进来。上面的例子中，使用到了 ref 语法，虽然它能简化 e.target 这种写法，但是不推荐使用。JSX 的差值表达式是使用一个大括号来定义的，包括触发事件（驼峰格式）的函数引用，注意 this 的作用域，然后下面是一些 JSX 的语法补充。 1234567891011121314151617181920212223// 注释，利用 &#123;&#125; 里面是 JS 表达式的原理，仅开发可见&#123;/* xxxx */&#125;&#123; // xxxxx&#125;// 样式，避免与类定义混淆，使用 classNameimport './demo.css'function App() &#123; return ( &lt;h1 className='xxx'&gt;&lt;/h1&gt; )&#125;// 禁用转义function createMarkup() &#123; return &#123;__html: 'First &amp;middot; Second'&#125;;&#125;function MyComponent() &#123; return &lt;div dangerouslySetInnerHTML=&#123;createMarkup()&#125; /&gt;; // return &lt;div dangerouslySetInnerHTML=&#123;&#123;__html: 'First &amp;middot; Second'&#125;&#125; /&gt;;&#125; 在禁用转义中使用了双花括号，这里第二个花括号已经是 JS 的对象了，所以，并没有太大含义。由于语言的歧义，除了 class 用 className 代替，在 label 中的 for 要用 htmlFor 来替代。 即使你不使用 JSX 语法，React 提供来一个函数： 12345678React.createElement( type, [props], [...children])// e.g. React.createElement('div', &#123;&#125;, 'show'); 它与 JSX 的效果是一样的，也可以说 JSX 就是它的简写版本。 组件之间传值页面都是有一个个的组件组成的，那就肯定避免不了传值问题，父组件向子组件传值相对很简单，直接通过标签的自定义属性即可，子组件只需要使用 this.props.attrName 就可以获取到，但是是只读的。很多时候，我们需要在子组件中修改父组件的一些数据，当然这肯定是不能允许直接改的，一般是通过调用父组件的一个方法来实现数值的修改。虽然子组件调用父组件的函数直接想不好搞，但是可以曲线救国，把父组件的方法直接传给子组件不就得了，子组件拿到了就可以在子组件的函数里直接调用；在传递的时候记得父组件里使用 bind 改变一下 this 指向。 单向数据流保证了数据的安全稳定，否则都不好定位是谁改了然后又导致了什么。 类型校验因为不管是什么类型都可以往子组件传递，甚至方法，为了避免出错，还是校验一下比较好，方法就是： 12345678910111213141516import PropTypes from 'prop-types';// 前面省略了App.propTypes = &#123; n1: propTypes.string.isRequired, n2: propTypes.func, n3: propTypes.number, n4: propTypes.array, n5: propTypes.bool&#125;App.defaultProps = &#123; n1: 'null'&#125;export default App; 如果类型不对会给你一个警告，虽然并不影响程序的运行。更多高级的用法例如或者并且等判断参考官方文档：https://zh-hans.reactjs.org/docs/typechecking-with-proptypes.html Redux父子组件传值还算是简单，当项目复杂起来，不可能只是父子之间的传值，如果还用现在的方法就会很复杂，所以需要引入 Redux 来完成不同组件之间的传值。原理也很简单，把数据统一放到一个 Store 对象中统一存储，然后需要的组件去监控这里面某个变量的值，当发生变化时，做出相应的改变。 上图就是 Redux 的数据流，也是按照单向数据流设计，组件必须通过 Action 才可以操作 Store 的数据。第一步，创建一个 Store，例如 ./store/index.js &amp; reducer.js ： 12345678910111213141516171819202122232425262728// index.jsimport &#123; createStore &#125; from 'redux'import reducer from './reducer'// 有 Redux Dev 插件可用，需要在这配置const store = createStore(reducer)export default store// reducer.jsconst defaultState = &#123;&#125; // 具体存储export default (state = defaultState, action) =&gt; &#123; // state = 上一次的 state 数据 // 要根据 action 的内容来更新 state，switch....case return state&#125;// 用于将所有可枚举属性的值从一个或多个源对象复制到目标对象。它将返回目标对象。Object.assign(&#123;&#125;, state, &#123; todos: [ ...state.todos, &#123; text: action.text, completed: false &#125; ]&#125;) 仓库创建好了，下面就可以使用了： 12345678910111213141516171819import React, &#123; Component &#125; from 'react';import store from './store'; // ./store/index.jsclass App extends Component &#123; constructor(props) &#123; super(props); store.getState(); // store 数据发生改变后执行 store.subscribe(() =&gt; console.log(store.getState())) &#125; change(newVal) &#123; const action = &#123; type: 'change_value', value: 'newVal' &#125;; store.dispatch(action); &#125;&#125; 为了避免 Action#type 手误写错，可以专门起一个 js 文件来定义，类似枚举。同理，Action 方法也是类似。由于 reducer 中的 state 不能直接修改，每次根据 action 更新时需要拷贝一个，一不小心就容易漏掉，这个时候就可以使用 immutable-js 这个库来做，生成不可变对象，经过它的转换，就可以通过使用 get 方法获取或者 set 方法重新生成一个不可变对象。为了统一样式，可以使用 redux-immutable 库来进行处理。 PS：Redux = Reducer + Flux React-redux通过 React-redux 可以让我们更方便的使用 Redux，官方主页是这样写的：Official React bindings for Redux 123456789101112131415import React from 'react'import ReactDOM from 'react-dom'import &#123; Provider &#125; from 'react-redux'import store from './store'import App from './App'const rootElement = document.getElementById('root')ReactDOM.render( &lt;Provider store=&#123;store&#125;&gt; &lt;App /&gt; &lt;/Provider&gt;, rootElement) 这里使用 Provider 组件将 store 进行了传递，这样 Provider 下的所有子组件都能共享了。 12345678910111213141516171819202122232425import &#123; connect &#125; from 'react-redux'// class App ...// 将 store（state）的数据映射到 propsconst mapStateToProps = (state /*, ownProps*/) =&gt; &#123; return &#123; counter: state.counter &#125;&#125;// 将要调用 dispatch 的方法映射const mapDispatchToProps = (dispatch) =&gt; &#123; return &#123; change(e) &#123; const action = &#123;&#125;; dispatch(action); &#125; &#125;&#125;export default connect( mapStateToProps, mapDispatchToProps)(App) 子组件获取 store 使用的是 connect 这个函数，在 export 的时候使用。connect 函数需要传入两个参数，也就是如何做连接，就是做了两个映射，这样都省的订阅了，store 变化数据就实时刷新了。并且，这样组件基本不包含任何的逻辑代码了，可以做成无状态组件了；也可以这么理解，connect 函数将 UI 组件和逻辑部分进行了拼装，返回了容器组件。 State与Props当 State 或者 Props 发生改变时，render 函数就会执行，这样 Dom 就会被重新渲染。 在前面组件里我们说的是改变数据必须用 set 函数，使用的是传统的传入一个对象（）利用 K-V 的形式来进行更新；不过还有性能更好的方法就是异步函数： 12345678910function handleChange(e) &#123; let val = e.target.value; this.setState(()=&gt; &#123; return &#123;value: val&#125;; &#125;)&#125;// 如果只有一个 return，箭头函数还可以简写this.setState(()=&gt;(&#123;value: val&#125;)) 不过因为异步，在使用的时候，数值尽量固定化，也是因为异步，所以它还会有第二个参数，是回调，根据需要使用。在 setState 使用异步函数的时候，函数其实会传给你一个 prevState 参数，它就是原来的值。 123this.setState((prevState)=&gt;(&#123;value: prevState.value + newVal&#125;))// ES6 语法简写&#123;val: val&#125; == &#123;val&#125; 至于 setState 为什么建议使用异步，因为这样如果有多个修改并且间隔很短，React 就可以合并成一个操作，避免连续多次的虚拟 DOM 比对与渲染。 虚拟DOM不管是 React 还是 Vue，都是使用虚拟 DOM 来控制页面的渲染（刷新），原因就是 JS 渲染一次真实的 DOM 需要调用浏览器 API，性能损耗过高，如果频繁渲染，体验肯定不好。对于这一问题，有几个方案，现在采用的基本都是虚拟 DOM 数据改变，重新生成新的 DOM，替换老的 DOM 数据改变，重新生成新的 DOM，与老的 DOM 比对，只替换差异部分 加载时构建虚拟 DOM，数据改变，虚拟 DOM 随之改变，对比之前老的虚拟 DOM，确定差异部分，操作 DOM 更新差异。 这里的这个虚拟 DOM 可以理解为就是个 JS 对象，用这个 JS 对象来描述真实的 DOM，正是因为是 JS 对象，所以速度很快，性能就大幅提高了。在 React 中，真实的 DOM 是按照虚拟 DOM 来渲染的，也正是因为虚拟 DOM 的存在，所以 React Native 得以存在，虚拟 DOM 渲染为真实 DOM 就是浏览器，渲染为原生 App 组件，这就成移动端的应用了。 虚拟 DOM 的比对 Diff 使用的是同层比对，从上往下，只要发现一层中的某个节点不一样，此节点下面所有的结构都会被重新渲染，虽然下面可能都没有变，但是这样的比对速度会更快。之所以要设置 Key 值也是为了提高比对速度，当 key 是唯一的时候，那么就是 key 与 DOM 一对一的对应关系，当发现某个节点的 key 没有变时，可以直接复用了（先根据 key 拿到对应的节点的信息，然后校验是否一致，如果不一致说明 key 是不稳定的，这种缓存就没法用了），所以，用 index 当作 key 很不妥。 生命周期对于这类描述生命周期的内容，没有什么比图更直观了： 这两幅图基本已经描述的够清楚了（虽然版本有点老），还有一个是官方的图：http://projects.wojtekmaj.pl/react-lifecycle-methods-diagram/根据官方图，getDerivedStateFromProps 这个方法中 16.4 发生了变化，需要特别注意。以及 componentWillReceiveProps 与 componentWillUpdate、componentWillMount 已经被标注为过时。 在 React 中所有的组件都要继承 Component，大部分的生命周期函数中 Component 中都有默认实现，唯独 render 没有，所以这就是为什么你必须写 render 函数的原因（函数式写法有点特别）。因为当父组件的 render 被执行时，子组件的 render 也会被执行，即使这时候 props 并没有变化，这样就会带来不必要的性能开销，所以可以使用 shouldComponentUpdate 这个生命周期函数来优化（或者继承 PureComponent，它默认实现了这个方法，不过为了避免 bug，需要配合 immutable-js 使用）。 12345678910111213141516171819import axios from 'axios'class App extends React.Component &#123; shouldComponentUpdate(nextProps, nextState) &#123; return nextProps.xxx !== this.props.xxx; &#125; componentDidMount() &#123; axios.get('/index') .then((res) =&gt; &#123; if (res.data.ret &amp;&amp; res.data.data) &#123; // something &#125; &#125;) .catch() &#125; // ...&#125; 一般情况，我们发送 Ajax 请求的过程要放到 componentDidMount 中。建议使用 axios 当然需要安装（yarn add axios） 路由关于页面的路由，这里就使用 react-router-dom 这个模块来完成，文档参考这里 1234567891011121314151617181920212223import &#123;BrowserRouter, Route&#125; from 'react-router-dom'function App() &#123; if (this.props.loginStatus) &#123; return &lt;Redirect to='/' /&gt; &#125; return ( &lt;Provider store=&#123;store&#125;&gt; &lt;ResetStyle/&gt; &lt;GlobalStyle/&gt; &lt;Header /&gt; &lt;BrowserRouter&gt; &lt;Route path='/' exact component=&#123;Home&#125;&gt;&lt;/Route&gt; &lt;Route path='/detail/:id' exact render=&#123;() =&gt; &lt;div&gt;detail&lt;/div&gt;&#125;&gt;&lt;/Route&gt; &lt;/BrowserRouter&gt; &lt;/Provider&gt; );&#125;export default App; 简单说就是根据请求路径的不同来决定显示的内容，使用 exact 来进行完全匹配。老的版本可能不允许你 Provider 或者 BrowserRouter 有多个元素，那时候用 div 裹一下就好。 另外，它还提供了一个 Link 模块，可以用来替代 a 标签，做单页面应用，避免加载过多的 HTML。在进行匹配的时候，可以使用类似 /:id 来匹配变量，然后在组件里通过 this.props.match.params.id 获取，这就是动态路由了。另外一种是匹配 ？传值，一样的，只不过不需要写 /:id 了，如果有 ?id=xx 会自动进行填充，获取方法不太一样，要自己手动处理字符串 this.props.location.search 并不是很推荐。 Redux-thunkRedux-thunk 是 Redux 的一个中间件，用来『整理』Action 中的 Ajax 请求等处理，说明文档可以参考 Github 的主页 123456import &#123; createStore, applyMiddleware &#125; from 'redux';import thunk from 'redux-thunk';import rootReducer from './reducers/index';// Note: this API requires redux@&gt;=3.1.0const store = createStore(rootReducer, applyMiddleware(thunk)); 使用多个中间件，例如 Redux Dev Tools 具体方法参考 Github 的文档。使用 Redux-thunk 后在 action 中就不一定非要是 js 的对象了，可以返回一个函数： 1234567891011121314151617181920212223// beforeexport const initAction = (newVal) =&gt; (&#123; type: 'change_value', value: newVal&#125;);// afterexport const getAction = () =&gt; &#123; return (dispatch) =&gt; &#123; axios.get('/do') .then((res) =&gt; &#123; dispatch(initAction(res.data)); &#125;) &#125;&#125;// 使用componentDidMount() &#123; const action = getAction(); // 可以接受一个函数了 store.dispatch(action);&#125; 上方展示的也是封装后的 Action 写法，还是建议把 Action 封装到一个 js 文件，统一管理。看起来好像比之前麻烦了，不过以经验来看，当项目越来越大后，这种方式更加容易管理和测试。 Redux-saga与 Redux-thunk 类似，Redux-saga 也是一个类似的 Redux 中间件，做异步代码拆分的，他们两个可以互相替代，使用方法也可以参考一下 Github 的文档。它使用单独的 JS 文件来管理 Ajax 请求，然后 run 一下。 12345678910111213141516171819202122232425// sagas.js exampleimport &#123; put, takeEvery &#125; from 'redux-saga/effects'function* fetchUser(action) &#123; // 以下内容怕出错可以放到 try...catch 里 const res = yield axios.get('/do'); const action = initAction(res.data); // 执行完成后传给 reducer yield put(action);&#125;function* mySaga() &#123; // ES6 Generator 函数 yield takeEvery("ACTION_TYPE_NAME", fetchUser);&#125;export default mySaga;// 使用componentDidMount() &#123; // action 只是包含一个 type 类型的对象即可 const action = getAction(); store.dispatch(action);&#125; 也就是说当你 dispatch 时，mySaga 也会收到你的 action，然后如果类型匹配，就会执行对应的函数，例如 fetchUser。可以看出 Redux-saga 是更加复杂的，相应的功能也更强大，上面也是最基本的使用。 异步加载默认情况下，会把项目的所有 JS 打包成一个 JS，这样页面只需要加载一次 JS，但是，如果逻辑很多，第一次加载肯定很慢，对于大项目，更期望只加载用到的 JS，使用的是 react-loadable 这个组件。 12345678910111213// loadable.jsimport Loadable from 'react-loadable';import React from 'react'const LoadableComponent = Loadable(&#123; // 当前的 index.js 做异步 loader: () =&gt; import('./'), loading () &#123; return &lt;div&gt;正在加载...&lt;/div&gt; &#125;&#125;);export default () =&gt; &lt;LoadableComponent/&gt; 以上是基于官方示例做的简单修改，下面是使用： 1234567891011121314151617181920212223// import Detail from './pages/detail' 之前import Detail from './pages/detail/loadable'function App() &#123; return ( &lt;Provider store=&#123;store&#125;&gt; &lt;ResetStyle/&gt; &lt;GlobalStyle/&gt; &lt;BrowserRouter&gt; &lt;Route path='/detail/:id' exact component=&#123;Detail&#125;/&gt; &lt;/BrowserRouter&gt; &lt;/Provider&gt; );&#125;export default App;// 使用路由的情况下，需要对组件特殊处理import &#123; withRouter &#125; from 'react-router-dom';// ...export default connect(mapState, mapDispatch)(withRouter(Detail)); 可以看出，只需要把 import 语句改成引入编写的 loadable 文件即可，但是，如何使用了 BrowserRouter 路由，这样就会有问题，所以在 export 的时候需要使用 withRouter 包裹一下。 总结总结一下一般的模块开发套路，首先创建组件，如果需要较多的数据就要创建独立的 Store，然后使用 Redux + React-redux 来进行管理，子组件在 export 的时候使用 connect 函数连接 store。样式的编写可以采用 styled-components，这样数据全部来自相关的 store，使用 mapStateToProps 来映射到 props 中，相关的事件通过 mapDispatchToProps 映射到 props 中，这样改变数据就使用相应的 action 即可。为了将逻辑写在 action 中，例如异步请求，使用了 Redux-thunk 或者 Redux-saga 来将 action 变为一个函数，将数据处理好后直接调用 dispatch 即可，为了方便 action 管理，将所有 action 的创建抽取到 actionCreators.js 文件中，相应的，常量也应该抽取到一个单独的文件，便于 actionCreators.js 和 reducer.js 的使用。存储为了不必要的麻烦，使用 immutable-js 将其变为不可变对象，为了风格的统一使用 redux-immutable 将 store 的 state 也变为不可变对象。这样下来，store 文件夹里就有了不少文件，为了方便导入，可以建一个 index.js 做聚合。每一个模块都应该有自己的 store 数据仓储，所以把相关联的 store 文件夹放真自己的模块下，在主仓储使用 combineReducers 函数来组合。 因为数据变化 render() 就会执行，子组件也会重新渲染，即使子组件需要的数据没有变化，为了优化这个问题导致的页面重新渲染，除了在 shouldComponentUpdate 中判断外，React 自然也想到了这一点，所以它还有一个 PureComponent，只要继承它，就自动帮你实现了 shouldComponentUpdate 的内容，不过需要也使用 immutable-js，否则可能会有 bug。 其他关于动画，这个我是真的不想看，CSS 看着就头疼，想做的看下 react-transition-group 这个库吧。关于 UI 设计，可以看看 Ant Design 这个库。 正常情况下，如果你在 JS 中导入了 CSS，那么这个 CSS 是全局生效的，所以并不推荐这样使用，例如可以使用 styled-components 来管理，它使用 JS 来编写样式，这样在 WebStorm 不识别，可以安装他们的一个插件。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS布局之Flex布局]]></title>
    <url>%2F2019%2F11%2F26%2FCSS%E5%B8%83%E5%B1%80%E4%B9%8BFlex%E5%B8%83%E5%B1%80%2F</url>
    <content type="text"><![CDATA[某天在网上看到了一篇帖子有感而发，作为后端还是挺喜欢 JS 的，毕竟动态语言确实爽，不过 CSS 我就….心累，真的玩不起，我们称之为玄学。当我还以为盒子模型是主流时，评论区大呼 Flex 布局真香，而我就一脸懵逼了，CSS3 一直不怨触碰，但如果主流布局方式变了，那肯定是要学习一下的（当然还有一种是 grid 这个看着比较复杂，之后再说）。 Flex布局简介布局的传统解决方案，基于盒状模型，依赖 display 属性 + position 属性 + float 属性。它对于那些特殊布局非常不方便，比如，垂直居中就不容易实现。2009 年，W3C 提出了一种新的方案—- Flex 布局，可以简便、完整、响应式地实现各种页面布局。目前，它已经得到了所有浏览器的支持，这意味着，现在就能很安全地使用这项功能。 Flex 是 Flexible Box 的缩写，意为”弹性布局”，用来为盒状模型提供最大的灵活性。 123456789101112131415/* 任何一个容器都可以指定为 Flex 布局。 */.box&#123; display: flex;&#125;/* 行内元素也可以使用 Flex 布局。*/.box&#123; display: inline-flex;&#125;/* Webkit 内核的浏览器，必须加上 -webkit 前缀。 */.box&#123; display: -webkit-flex; /* Safari */ display: flex;&#125; 注意，设为 Flex 布局以后，子元素的 float、clear 和 vertical-align 属性将失效。 基本概念采用 Flex 布局的元素，称为 Flex 容器（flex container），简称”容器”。它的所有子元素自动成为容器成员，称为 Flex 项目（flex item），简称”项目”。 容器默认存在两根轴：水平的主轴（main axis）和垂直的交叉轴（cross axis）。主轴的开始位置（与边框的交叉点）叫做 main start，结束位置叫做 main end；交叉轴的开始位置叫做 cross start，结束位置叫做 cross end。项目默认沿主轴排列。单个项目占据的主轴空间叫做 main size，占据的交叉轴空间叫做 cross size。 容器的属性以下6个属性设置在容器上。 flex-direction flex-wrap flex-flow justify-content align-items align-content 第一次看到这些还是很陌生的，毕竟之前没接触过 Flex 布局，虽然它早就已经有了，下面就详细解释下这些属性。 flex-direction(主轴方向)flex-direction 属性决定主轴的方向（即项目的排列方向），它可能有 4 个值： row（默认值）：主轴为水平方向，起点在左端。 row-reverse：主轴为水平方向，起点在右端。 column：主轴为垂直方向，起点在上沿。 column-reverse：主轴为垂直方向，起点在下沿。 无论是横向还是纵向，默认都是紧凑着来（顶格），也就是如果想要它居中排列，还需要另外的设置。 flex-wrap(如何换行)默认情况下，项目都排在一条线（又称”轴线”）上。flex-wrap 属性定义，如果一条轴线排不下，如何换行， 它可能取三个值： nowrap（默认）：不换行。 wrap：换行，第一行在上方。 wrap-reverse：换行，第一行在下方。 简单说就是定义是否换行，和换行的方向，第一行在下面还是上面的问题。 flex-flow(聚合属性)flex-flow 属性是 flex-direction 属性和 flex-wrap 属性的简写形式，默认值为 row nowrap。就是把前面两个属性值写在一起了，CSS 中很常见，下一个！ justify-content(主轴对齐方式)justify-content 属性定义了项目在主轴上的对齐方式。 它可能取 5 个值，具体对齐方式与轴的方向有关。下面假设主轴为从左到右。 flex-start（默认值）：左对齐 flex-end：右对齐 center： 居中 space-between：两端对齐，项目之间的间隔都相等。 space-around：每个项目两侧的间隔相等。所以，项目之间的间隔比项目与边框的间隔大一倍。 如果主轴是从上到下，那么就不是左右对齐的问题了，就是上下了，属性也没用使用 left 和 right 这类词，而是 start 和 end，配合之前的 Flex 基本模型，还是非常好理解的；同时，也是垂直居中的一种方案。 align-items(交叉轴对齐方式)align-items 属性定义项目在交叉轴（默认主轴左右，交叉轴上下）上如何对齐。 它可能取 5 个值。具体的对齐方式与交叉轴的方向有关，下面假设交叉轴从上到下。 flex-start：交叉轴的起点对齐，即所有项目靠上。 flex-end：交叉轴的终点对齐，即所有项目靠下。 center：交叉轴的中点对齐，即所有项目排列在中间，垂直居中。 baseline: 项目的第一行文字的基线对齐。 stretch（默认值）：如果项目未设置高度或设为 auto，将占满整个容器的高度。 这个属性也是垂直居中的一种方案，它决定了项目在容器中垂直方向上的位置（默认情况下） align-content(多根轴线对齐方式)align-content 属性定义了多根轴线的对齐方式。如果项目只有一根轴线，该属性不起作用，该属性可能取 6 个值： flex-start：与交叉轴的起点对齐。 flex-end：与交叉轴的终点对齐。 center：与交叉轴的中点对齐。 space-between：与交叉轴两端对齐，轴线之间的间隔平均分布。 space-around：每根轴线两侧的间隔都相等。所以，轴线之间的间隔比轴线与边框的间隔大一倍。 stretch（默认值）：轴线占满整个交叉轴。 这个属性看似跟 align-items 没啥区别，区别就仅仅是 align-content 当项目只有一根轴线，该属性不起作用；也就是说，align-items 它是用来让每一个单行的容器居中而不是让整个容器居中；align-content 属性只适用于多行的 flex 容器，并且会把多行作为一个整体，它们之间没有间隙。简单讲，关键还是看是不是多行。 项目的属性以上的六个属性全部是用在容器上的，相应的项目也有六个可配属性： order flex-grow flex-shrink flex-basis flex align-self 同样下面详细解释一下这几个属性 order(项目排序)order 属性定义项目的排列顺序。数值越小，排列越靠前，默认为 0。 flex-grow(放大比例)flex-grow 属性定义项目的放大比例，默认为 0，即如果存在剩余空间，也不放大。如果所有项目的 flex-grow 属性都为 1，则它们将等分剩余空间（如果有的话）。如果一个项目的 flex-grow 属性为 2，其他项目都为 1，则前者占据的剩余空间将比其他项多一倍。 flex-shrink(缩放比例)flex-shrink 属性定义了项目的缩小比例，默认为 1，即如果空间不足，该项目将缩小。如果所有项目的 flex-shrink 属性都为 1，当空间不足时，都将等比例缩小。如果一个项目的 flex-shrink 属性为 0，其他项目都为 1，则空间不足时，前者不缩小。负值对该属性无效。 flex-basis(占位大小)flex-basis 属性定义了在分配多余空间之前，项目占据的主轴空间（main size）。浏览器根据这个属性，计算主轴是否有多余空间。它的默认值为 auto，即项目的本来大小。它可以设为跟 width 或 height 属性一样的值（比如 350px），则项目将占据固定空间。 flex(聚合属性)flex 属性是 flex-grow, flex-shrink 和 flex-basis 的简写，默认值为 0 1 auto。后两个属性可选。该属性有两个快捷值：auto (1 1 auto) 和 none (0 0 auto)。建议优先使用这个属性，而不是单独写三个分离的属性，因为浏览器会推算相关值。 align-self(独立对齐方式)align-self 属性允许单个项目有与其他项目不一样的对齐方式，可覆盖 align-items 属性。默认值为 auto，表示继承父元素的 align-items 属性，如果没有父元素，则等同于 stretch。该属性可能取 6 个值，除了 auto，其他都与 align-items 属性完全一致。 参考https://www.ruanyifeng.com/blog/2015/07/flex-grammar.html示例：http://www.ruanyifeng.com/blog/2015/07/flex-examples.html]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringSecurity笔记]]></title>
    <url>%2F2019%2F11%2F24%2FSpringSecurity%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这篇笔记其实很早就开始写了，毕竟 SpringSecurity 现在用的非常多，而我还是半吊子水平，不过因为时间和心情问题，断断续续搞了这么久，跨度得三个月左右了，相应的这篇笔记也非常长，我就不再分篇了直接一次性怼上，基本的原理与知识点应该是覆盖全了，除了基础的 SpringSecurity 知识点，另有对 OAuth2 对支持，使用 Social 对第三方社交登陆的支持，Session 处理相关，SSO 相关的提了一下，看完之后不是问题。 这篇笔记基于慕课上的一门课，个人认为还是不错的，反反复复看了好几遍，先跟着写了一遍，当然不是完全的照抄，对基础框架做了一点升级和自己的一些处理，之后发现内容太多，决定从头再来一遍，并且整理成这篇笔记。 SpringSecurity这一次我使用的版本是 SB2.x，集成 SpringSecurity 就不需要多说了，一个 starter 搞定，然后它的默认配置会将所有的接口保护起来，使用 http basic 来认证。 需要注意的是：security.basic.enabled 在 SB2.x 被废弃，如果需要禁用 SpringSecurity 请使用 exclude 的方式进行排除，例如:@SpringBootApplication(exclude = {SecurityAutoConfiguration.class, ManagementWebSecurityAutoConfiguration.class}) 接下来就是如何配置了，首先我们可以新建一个 java config 类，继承 WebSecurityConfigurerAdapter 这个适配器类，然后覆盖它的方法，下面是一个例子： 12345678910111213141516171819202122protected void applyPasswordAuthenticationConfig(HttpSecurity http) throws Exception &#123; // 使用表单登陆 http.formLogin() // 跳转认证的页面(默认 /login) .loginPage(SecurityConstants.DEFAULT_UN_AUTHENTICATION_URL) // 进行认证的请求地址（UsernamePasswordAuthenticationFilter） .loginProcessingUrl(SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_FORM) // 自定义登陆成功、失败后的处理逻辑 .successHandler(authenticationSuccessHandler) .failureHandler(authenticationFailureHandler) .and() // 设置授权要求 .authorizeRequests() .antMatchers(SecurityConstants.DEFAULT_UN_AUTHENTICATION_URL, "/user/register") // 以上匹配不需要认证 .permitAll() // 其他请求需要进行认证 .anyRequest() .authenticated() .and() .csrf().disable();&#125; 接着，我们来说一下 SpringSecurity 的原理，其实也不难猜，肯定是通过 Filter 实现的，它也确实是通过一组 Filter 链来做的，首先来看一下这个图： 很显然，UsernamePasswordAuthenticationFilter 这个过滤器就是来处理表单登陆的相关请求，BasicAuthenticationFilter 那就是来处理 http basic 登陆的相关请求；例如你使用表单提交，UsernamePasswordAuthenticationFilter 会从请求中拿到用户名密码，然后去做登陆校验，如果成功则标记为已认证（通过一个过滤器链的共享变量）；如果拿不到用户名密码就放行，进入到下一个过滤器再做其他方式的登陆校验。过滤链的最后一环是 FilterSecurityInterceptor，它通过那个标志位来判断前面是否已经通过了身份认证，然后根据我们 config 中配置的规则，来控制允不允许访问；如果不过，它会根据不同的原因来抛出不同的异常。那么，在 FilterSecurityInterceptor 前面的 ExceptionTranslationFilter 就是来接受它抛出的异常，然后根据不同类型的异常做出不同的处理，例如未登录的异常会根据前面的配置来引导用户进行登陆。PS：其中，绿色部分是我们可以控制是否开启的。 自定义用户认证拿到用户名密码后，如何判断是否是合法用户呢，这个需求每一个业务系统可能都不一样，所以肯定是可以自定义的，这个过程被抽象成了一个接口叫做 UserDetailsService，我们新建一个类，然后实现这个接口（只有一个需要实现的方法 loadUserByUsername），在这个方法中，我们可以拿到请求中的用户名，根据这个用户名如何获取用户信息就全靠我们自己了，使用 Mybatis 的 mapper 或者 JPA，最终只要返回一个 UserDetails 对象即可。说起 UserDetails 对象，我们既可以用 SpringSecurity 的默认实现 User，也可以继承 User 后进行增强，以默认的 User 对象来说，它有几个构造方法，满足我们日常的账号是否禁用、是否锁定、是否过期等等需求。它的判断逻辑也非常简单，你根据用户输入的用户名获取 UserDetails 对象并且返回，SpringSecurity 拿着你返回的这个对象中的秘密与用户输入的密码进行比对，如果错误则抛出用户名或密码错误的异常，如果这个对象的是否锁定为 true，则抛出用户已锁定的异常。最后的一个参数是权限，这个放在后面的鉴权里面说。 12345678910111213public class MyUserDetailsService implements UserDetailsService, SocialUserDetailsService &#123; private final PasswordEncoder passwordEncoder; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; // BCryptPasswordEncoder 每次生成的会不一样，应该在注册的时候保存，这里直接拿数据库保存的 String pwd = passwordEncoder.encode("123123"); System.out.println("PWD：" + pwd); // 简单实现 return new User(username, pwd, AuthorityUtils.commaSeparatedStringToAuthorityList("admin,ROLE_USER")); &#125;&#125; 关于密码的处理，这里使用了 BCryptPasswordEncoder，一般情况下，我们数据库中不可能存储明文密码，SpringSecurity 自然也考虑到了，所以它搞出来了个 PasswordEncoder 接口，这个接口主要定义了两个方法，一个是来处理原始密码的 encode，一个是来比较密码的 matches，你可以自己实现，也可以用它提供的几种，例如 BCryptPasswordEncoder。 这里稍微说下 BCryptPasswordEncoder，它根据 hash + salt 的方式生成密码，特点是即使是相同的密码，每次经过它编码后秘文是不一样的！但是他们 matches 后的结果会是 true，提高了安全性。 自定义认证界面我们最常用的就是表单认证，至于前端样式，肯定是各不相同，下面就来说如何自定义。然而最开始的配置里已经剧透了，就是通过 http.loginPage() 来指定登陆的 URL，然后你写你的 html 前端就行了，如果要兼容其他客户端例如 App，你可以写一个 Controller 来判断是不是请求的 html，来确定是返回 html 还是 json；最后只需要记得别忘了把这个地址放到白名单中就行。 123456789101112131415161718192021222324252627/** * SE 在认证时会将原请求缓存进 requestCache */private RequestCache requestCache = new HttpSessionRequestCache();/** * 重定向工具类 */private RedirectStrategy redirectStrategy = new DefaultRedirectStrategy();/** * 当需要身份认证时，执行此方法 * @return 如果是浏览器请求，重定向到认证页面；如果是其他返回 401 状态码提示 */@RequestMapping(SecurityConstants.DEFAULT_UN_AUTHENTICATION_URL)@ResponseStatus(code = HttpStatus.UNAUTHORIZED)public SimpleResponse reqAuth(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; SavedRequest savedRequest = requestCache.getRequest(request, response); if (savedRequest != null) &#123; String redirectUrl = savedRequest.getRedirectUrl(); if (StringUtils.endsWithIgnoreCase(redirectUrl, ".html")) &#123; redirectStrategy.sendRedirect(request, response, securityProperties.getBrowser().getLoginPage()); &#125; &#125; return new SimpleResponse("访问的服务需要授权");&#125; 当用户访问需要授权的接口时，如果检测到未登录，就会重定向到我们配置的登陆页面（浏览器），同时会将 URL 存起来，便于通过认证后再跳转回去，我们通过 RequestCache 这个工具就能拿到 SpringSecurity 存起来的 URL。 然后通过 http.loginProcessingUrl() 来指定表单的提交地址，也就是真正的后台处理登陆请求的地址，后面就是 UsernamePasswordAuthenticationFilter 那一套了。 自定义成功和失败Handler如何配置前面已经剧透了，想要自定义认证成功或者失败后的逻辑，只需要定义相关的 Handler 即可，例如： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 自定义登陆失败后处理逻辑 * 可以实现 &#123;@link AuthenticationFailureHandler&#125; 进行自定义； * 默认实现： &#123;@link SimpleUrlAuthenticationFailureHandler&#125; * * @author Created by 冰封承諾Andy on 2019/7/22. */public class MyAuthenticationFailureHandler extends SimpleUrlAuthenticationFailureHandler &#123; @Override public void onAuthenticationFailure(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, AuthenticationException e) throws IOException, ServletException &#123;&#125;&#125;/** * 自定义登陆成功后的行为，默认为跳转回原来的地址 * * 也可以选择实现 &#123;@link AuthenticationSuccessHandler&#125; 的方式来定制; * 默认规则为 &#123;@link SavedRequestAwareAuthenticationSuccessHandler&#125; * * @author Created by 冰封承諾Andy on 2019/7/22. */public class MyAuthenticationSuccessHandler extends SavedRequestAwareAuthenticationSuccessHandler &#123; @Resource private ObjectMapper objectMapper; @Resource private SecurityProperties securityProperties; private RequestCache requestCache = new HttpSessionRequestCache(); @Override public void onAuthenticationSuccess(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Authentication authentication) throws IOException, ServletException &#123; SavedRequest savedRequest = requestCache.getRequest(httpServletRequest, httpServletResponse); String redirectUrl = null; if (savedRequest != null) &#123; redirectUrl = savedRequest.getRedirectUrl(); &#125; // 当访问非 html 并且设置为 JSON 类型是返回 JSON 格式用户信息 if (!StringUtils.endsWithIgnoreCase(redirectUrl, ".html") &amp;&amp; LoginResponseType.JSON.equals(securityProperties.getBrowser().getLoginType())) &#123; httpServletResponse.setContentType("application/json;charset=utf-8"); httpServletResponse.getWriter().write(objectMapper.writeValueAsString(authentication)); &#125; else &#123; // 跳转回原来的 URL super.onAuthenticationSuccess(httpServletRequest, httpServletResponse, authentication); &#125; &#125;&#125; 其中，Authentication 对象封装了认证相关的信息，包括我们自定义的 UserDetails 对象，密码相关的敏感信息如果返回到前端会自动进行过滤。 基于表单的认证流程表单方式应该是我们用的最多的，所以就来看看它的处理过程，照例上一张图： 当认证请求进入 UsernamePasswordAuthenticationFilter 这个过滤器后，它会从请求取出用户名和密码信息，封装到一个 UsernamePasswordAuthenticationToken（未认证状态） 中，接下来就到了 AuthenticationManager，它本身不包含认证的逻辑，但它会从一堆 AuthenticationProvider 中选出一个最合适的来进行认证（校验），至于挑选的过程主要是根据 Authentication 的类型进行匹配；在这些 AuthenticationProvider 中，有一个 supports 方法，它会验证是否支持当前的 AuthenticationToken，如果支持，就进行后面的认证了（会调用我们 UserDetailsService 中的 loadUserByUsername 方法），不支持就跳过，进行下一次循环；认证过程会通过 UserDetailsService 来获取用户信息（UserDetails），然后进行比较和校验，如果顺利，就会把 UsernamePasswordAuthenticationToken 做一个”已认证”的标记，然后将信息保存到 session，最后的一个步骤会调用我们设置的 Handler，失败的处理流程也是类似。 那么关于请求直接信息共享，还记得在过滤器链最前端有一个 SecurityContextPersistenceFilter，它的作用简单说就是当请求进来的时候检查 session 是否有 Authentication 信息，如果有就将它取出来放到一个 ThreadLocal 里；当请求完成响应时，它检查 ThreadLocal 是否有认证信息，如果有就放到 session 中去。 12345678910111213141516171819/** * 获取当前登陆的用户信息 * * 可以通过 SecurityContextHolder.getContext().getAuthentication() 来进行获取 * 有条件可以直接注入 Authentication 的方式来获取； * 或者使用 @AuthenticationPrincipal 注解选择性的获取部分信息 * * SecurityContextHolder 可以简单理解为一个 ThreadLocal，通过最前端的 &#123;@link org.springframework.security.web.context.SecurityContextPersistenceFilter&#125; 过滤器， * 在每次请求到达时检查 session 是否有登陆信息，有则放到 SecurityContextHolder 中; * 在请求返回时，检查是否存在 SecurityContextHolder，如果存在则放到 session 中。 */@GetMapping("/me")public Object getCurrentUser(Authentication authentication, @AuthenticationPrincipal UserDetails user) &#123; Authentication authentication1 = SecurityContextHolder.getContext().getAuthentication(); // 获取的就是 UserService 中的对象 // User user = (User) SecurityContextHolder.getContext().getAuthentication().getPrincipal(); return user;&#125; 具体到代码，就是通过 SecurityContextHolder 这个类可以随时随地获取认证信息。 当然，还可以加一些自定义的过滤器，例如来做验证码的校验，示例参考 Github，往过滤链中添加我们自己的过滤器最简单的方案就是直接在文章开始的配置类调用 http 的 addFilterBefore 方法。现在流行的还有短信验证码，与普通的图形验证码最大的区别就是，使用短信验证码验证通过后是直接认证通过了，而不是单单一个验证码的校验，也可以说它其实是一种登陆方式。 记住我功能这个也是 Web 应用中常见的一个功能，也有不同的实现方式，在 Spring Security 中的方案是基于 Token 和数据库的，整理成流程图就是这样： 这个 RememberMeAuthenticationFilter 过滤器的位置处在靠后的位置，当前面的认证都无效时再进行“记住我”认证，表单中的 checkbox 控件固定名称是：remember-me。主要的 Java 配置为： 123456789101112131415161718192021222324@Beanpublic PersistentTokenRepository persistentTokenRepository() &#123; // 基于 JDBC 的 “记住我” 实现 JdbcTokenRepositoryImpl jdbcTokenRepository = new JdbcTokenRepositoryImpl(); jdbcTokenRepository.setDataSource(dataSource); // 自动执行建表语句 // jdbcTokenRepository.setCreateTableOnStartup(true); return jdbcTokenRepository;&#125;// 主配置protected void configure(HttpSecurity http) throws Exception &#123; http. // 记住我 配置 .rememberMe() .tokenRepository(persistentTokenRepository()) .tokenValiditySeconds(securityProperties.getBrowser().getRememberMeSeconds()) // 自定义密码处理 .userDetailsService(userDetailsService) .and() ....&#125; 既然是基于数据库，那么肯定会需要有一张表，你可以让他自己创建，或者进入到 JdbcTokenRepositoryImpl 的源码中，把里面定义的 SQL 手动执行一下。 自定义认证方式当了解完默认表单登陆的逻辑后，自定义其他登陆方式也就不那么难了，这里就以短信验证码登陆为例，用的也是蛮多的；首先是要明确的是，短信登陆与表单登陆是完全不同的一种方式，所以不可能在表单认证的 Filter 中搞，肯定是要自己搞一套，那么也就是需要一个拦截短信验证码的 Filter，以及一个进行校验的 Provider，然后校验通过把标识设置为 true 就好了，其他的跟表单方式基本一致。那么接下来就稍微整理下需要重写的主要类： 封装用户信息的 SmsCodeAuthenticationToken 参考 UsernamePasswordAuthenticationToken，去掉了密码部分，因为短信登陆不需要。 拦截 SMS 请求的过滤器 SmsCodeAuthenticationFilter，它负责从请求取出相关信息封装 SmsCodeAuthenticationToken 等必要的对象。 具体的处理实现 SmsCodeAuthenticationProvider，还记得之前说过 AuthenticationManager 会调用它的 supports 方法来根据 Token 对象的类型匹配是不是用它做认证。 具体做验证的统一 ValidateCodeFilter，处在最前端，仅负责校验验证码。 简单来说执行步骤是：SmsFilter –&gt; AuthenticationToken –&gt; AuthenticationManager –&gt; AuthenticationProvider –&gt; UserDetailsService –&gt; UserDetails –&gt; 获得 Authentication 已认证信息。既然所需要的类都准备好了，下一步就是将他们配置到 Spring 的环境中，可以定义以下配置类： 12345678910111213141516171819202122232425262728@Configurationpublic class SmsCodeAuthenticationSecurityConfig extends SecurityConfigurerAdapter&lt;DefaultSecurityFilterChain, HttpSecurity&gt; &#123; @Autowired private AuthenticationSuccessHandler authenticationSuccessHandler; @Autowired private AuthenticationFailureHandler authenticationFailureHandler; @Autowired @Qualifier("myUserDetailsService") private UserDetailsService userDetailsService; @Override public void configure(HttpSecurity httpSecurity) throws Exception &#123; // 设置 Filter 类似 ValidateCodeFilter，主类 SmsCodeAuthenticationFilter smsCodeAuthenticationFilter = new SmsCodeAuthenticationFilter(); smsCodeAuthenticationFilter.setAuthenticationManager(httpSecurity.getSharedObject(AuthenticationManager.class)); smsCodeAuthenticationFilter.setAuthenticationSuccessHandler(authenticationSuccessHandler); smsCodeAuthenticationFilter.setAuthenticationFailureHandler(authenticationFailureHandler); SmsCodeAuthenticationProvider smsCodeAuthenticationProvider = new SmsCodeAuthenticationProvider(); smsCodeAuthenticationProvider.setUserDetailsService(userDetailsService); // 添加配置，加入到认证之后 httpSecurity.authenticationProvider(smsCodeAuthenticationProvider) .addFilterAfter(smsCodeAuthenticationFilter, UsernamePasswordAuthenticationFilter.class); &#125;&#125; 然后，我们可以在文章最开始的那个配置类中，调用 http.apply() 方法让其生效即可。至此，我们再重新梳理一下流程，首先请求进入 ValidateCodeFilter 进行校验验证码（当然它也可以同时负责校验图形验证码），通过之后就继续过滤链，被 SmsCodeAuthenticationFilter 匹配捕获，然后封装 Token 信息，后面就是 Provider 拿着这些信息去 UserDetailsService 将用户信息获取出来，然后置为“已认证”状态了。 OAuth协议这里简单回顾一下 OAuth 协议，之前我就写过，连接在这里，虽然写的也是很简单就是了，这里再重新温习一下，OAuth 协议主要包含角色： 服务提供商（Provider） 认证服务器（Authorization Server） 资源服务器（Resource Server） 资源所有者（Resource Owner） 客户端（Client） 接下来说说运作的基本流程：资源所有者（也就是用户）去访问某个应用的服务（暂且称为客户端吧），然后需要获取其他应用（服务提供商）中的此用户数据，所以客户端会向用户请求授权，用户同意后，服务提供商会发放一个有有效期的令牌给客户端（用户同意操作在服务提供商完成）； 之后客户端可以拿着这个令牌去资源服务器获取资源，资源服务器会验证这个令牌的合法性，通过即可返回需要的资源。具体例子的话，可以想象任何网站的微信、微博登录功能，其中客户端指的就是你访问的那个网站，服务提供商就是微信或者微博，点击后是不是需要跳转到微信扫码或者微博授权页的那个地址呢。 其中，最重要的就是用户同意这个步骤了，至于同意后如何获取令牌，一般我们采用的也就是授权码模式（Authorization Code）和客户端模式（Client Credentials），其中授权码模式最为广泛（更加安全，因为不用将密码暴露给第三方）。 授权码模式的流程（其实跟上面那个基本是一致的，上面就是以授权码模式为例的）：需要授权时，客户端会将用户导向服务提供商的认证服务器，用户需要在认证服务器上完成同意授权，然后认证服务器会返回一个授权码，一般是将这个授权码返回到客户端的后台，然后客户端的后台根据这个授权码再去认证服务器换取令牌； 这样令牌就拿到了，整个过程需要两步，认证服务器也能通过导向的连接携带的参数来确定是那个客户端需要授权； 这样不直接返回令牌而是授权码大大加强来应用间的安全性。 SpringSocial 因为 social 项目调整，目前被标记为 in the Attic，在 Spring2.x 版本中，将相关源码进行了去除，如果使用到了相关到类，只能手动补全.参见：https://www.jianshu.com/p/e6de152a0b4e 基本原理： social 封装了绝大部分的 OAuth 协议步骤，会在过滤器链加入 Social 自己的过滤器，通过这个过滤器来简化我们的 OAuth 流程，其中根据令牌获取用户信息的实现各不相同只能由用户来提供，其中涉及的 URL 以及必要的参数也要由用户提供；，基本的组成部分可以概括为： ServiceProvider：服务提供商的一个抽象必须继承 AbstractOAuth2ApiBinding 类； 它包含 OAuth2Template 这个默认实现和 Api（AbstractOAuth2ApiBinding）因为每一个服务提供商定义的接口或者数据对象都可能不同，所以针对每一个服务提供商都应该提供一个抽象与之对应，这个当然就需要用户自己实现。示例：QQImpl、QQServiceProvider ConnectionFactory提供的默认实现包含 ServiceProvider 和 ApiAdapter（负责将服务提供商个性化的格式转换为 social 通用的格式） Connection UsersConnectionRepository最常用的就是 JDBC 的实现了，需要自己加入到 Spring 容器中，作用就是对数据库中 UsersConnection 表的 CRUD 操作 。 最终的目的就是获得某用户的 Connection，然后联合数据库来获取相关的信息； 想要得到 Connection 就需要 ConnectionFactory，而创建工厂需要 ServiceProvider 和 ApiAdapter； 其中有一些是 social 给我们提供了的，剩下的就需要我们自己实现了。 说到入口，因为 Social 是通过一个 SocialAuthenticationFilter 过滤器来进行操作的，在这个过滤器中默认拦截的是 /auth/{providerId} 请求，用户同意授权后也会再跳转回这个地址，不过是带着授权码 code 回来的，而这个过滤器就是通过是否携带授权码来区分是哪一个步骤的；当检测到是有 code 的时候，就会去自动的触发换取 token 的机制，如果失败默认的失败路径是 /signin，如果成功解析，它就会拿着获取到的 id 去数据库查相关信息，如果没有查到就跳转到 /signup 默认的注册请求，国内很多网站这里会让你绑定一个手机号（注册逻辑）或者绑定已有账号，但是你也可以让其自动注册然后自动登录（UsersConnectionRepository 会判断 ConnectionSignUp 是否为空，如果不为空会调用这个接口来获取一个 id，从而避免跳转），当然这些配置我们可以配，参考 SocialConfig。并且我们通过配置 ProviderSignInUtils 工具类，可以在自定义的注册或者绑定逻辑里拿到 Social 获取到的第三方用户信息，因为在跳转之前 SocialAuthenticationFilter 会将信息保存到 session 中，这个工具也已经封装了注册（绑定）的方法。简略流程可以参考： 本质与 SpringSecurity 没啥区别，都是获取用户信息，校验，最后置为“已认证”状态放到 Context 中，区别的就是获取用户信息与校验的不同。（蓝色的是 Social 提供的，橘色的是我们自己实现的） 绑定与解绑首先，要获取到当前用户的绑定情况，然后再判断是否进行绑定或者解绑；它默认提供了一个 地址，用来获取登陆用户的绑定信息，最终会跳转到一个叫 connect/status 的视图，一般情况都是会自定义这个视图的，拼装后返回符合我们期望的数据结构，参考 CustomizeConnectionStatusView 绑定与解绑 Social 也基本都替我们写好了，只需要在登陆状态下 POST 方式访问 /connect/{providerId} 就可以了，对应的它最后也会跳转到一个 /connect/{providerId}Connected 的视图，这个还是要自定义的，参考 CustomizeConnectView。解绑与绑定一致，只是请求方式换成了 DELETE，返回的视图名是 /connect/{providerId}Connect PS：集成 Social 后，UserDetailsService 记得实现 SocialUserDetailsService 接口，然后返回一个 SocialUserDetails，这个里面才有 id 关于 OAuth 的那些信息。 Session管理无论你用哪一种方式登陆，最终登陆成功后的用户信息默认是存在服务器的 session 中的，紧接着就会有超时或者说过期的问题，在集群环境下单机 session 更是一个问题。通常，在 SB 的配置文件中，可以设置超时时间，最低为 1 分钟，在 SecurityConfig 配置类中也可以进行一定的配置，比如失效后的跳转逻辑、在线数量、并发逻辑等。 要解决集群 session 的问题，只要把 session 不放在单独的服务器就行了，例如可以统一放在 Redis，SpringSession 可以简化这套流程的开发，只需要在配置文件中配置 spring.session.store-type 就可以了，基本上是透明的，非侵入，并且配置的 session 管理项也都是有效的。因为 session 读取会非常频繁，还具有时效性，所以放在 Redis 里是比较合适的。 SpringSecurityOAuth对于浏览器这种客户端，使用 Cookie-Session 的机制还算是方便，但是对于 App 这一类的客户端，再使用 Cookie-Session 的这种机制做认证就显得十分麻烦，难道每次都要手动保存和添加 cookie 到请求头？当然还有很多一些其他因素，这促使我们使用一种简单方便并且安全的机制来做认证，解决 HTTP 无状态的问题，然后 OAuth 就来了，简单说就是使用令牌来做认证。 OAuth 协议应该有所了解了，SpringSecurityOAuth 相当于简化了我们作为服务提供商的功能开发，服务提供商一般由认证服务器和资源服务器组成，这个前面说过，其中认证服务器最常见的是那四种授权模式，这个 Spring 已经帮我们实现了。 而资源服务器的角色，就是保护我们的资源（接口），他们两个物理上可以是一台机器，资源服务器简单说就是通过一个 Filter 来进行令牌的校验，跟前面所说的 SpringSecurity 原理差不多。 当然，自带的四种授权模式未必能满足我们，例如手机验证码登陆的需求，我们可以进行自定义授权模式，包括存储逻辑与令牌的生成也可以进行个性化。 认证服务器在 SpringSecurityOAuth 的加持下，实现一个认证服务器非常简单，只需要在 Java 配置类上增加一个 @EnableAuthorizationServer 注解即可。加入这个注解后，Spring 就会把四种基本的授权模式与 Token 的存储逻辑（默认内存存储）进行自动装配，直接就可以用了，下面以最常用的授权码模式与密码模式来简单说明。 授权码模式授权码模式的流程不在多说，上面已经讲过了，默认情况下开启注解会发现有这样两个地址映射： /oauth/authorize这一个就是默认提供的让用户确认授权的页面，类似你用第三方登录跳到的那个页面，让用户选择哪个账户、什么权限。访问这个地址是需要一些参数的，在 OAuth2 协议的官方文档有明确规定。PS：要想使用此功能，你系统的用户要有 ROLE_USER 权限，记得在你的 UserDetailsService 中进行配置。 /oauth/token这个就是用来换取 accessToken 的接口，如果使用授权码模式，就是用上一步得到的 code 来换取 accessToken。发送 POST 请求的时候记得要带你申请的 appid 之类的信息，一般通过请求头带 authorization 类型为 Base 的方式编码。 拿到 accessToken 之后就可以去资源服务器获取信息了，只需要在头带上 authorization，这里类型默认用 bearer，值就是 accessToken。它适合给第三方应用做授权，能避免第三方应用接触到我们应用用户的账号密码的风险。 密码模式密码模式比授权码模式要更简单，省去了获取 code 的步骤，这种方式用在同一家应用之间是没问题的，就算授权过程需要提供账户密码，但是因为都是自家的应用，还好，就相当于使用账号密码登录了。过程也没啥说的，参考授权码模式的第二步，仅仅是参数的变化，参数在 OAuth2 规范有明确定义。 其他补充无论使用哪种方式授权，最终获得的 accessToken 是一样的，Spring 会判断这个用户的 accessToken 是否过期，如果没过期，无论用哪种方式授权，得到的 accessToken 都是一致的。当然，服务器除了返回 accessToken 过期时间等必要信息，还会有一个刷新的 token，使用这个可以进行刷新 accessToken，使 accessToken 到期时用户可以无感知的“续期”令牌。 资源服务器资源服务器也是类似，一个 @EnableResourceServer 注解即可搞定，并且可以与认证服务器的注解写在一起，毕竟他们不需要物理上隔离，只是逻辑上的概念。按照前面所说，它其实是加了一个 Filter，坚持请求头的 authorization 的 bearer 是否合法。认证过程就区别于传统的 Session 方式了，更加自由一些。 流程解读大体的流程可以参考下面这张图： 其中绿色为类，蓝色为接口（括号里为默认实现）。 ClientDetailsService用来根据你传递的 client-id 信息读取客户端信息，内容会封装在 ClientDetails 实体。 TokenRequest它封装了第三方应用请求信息，就是 OAuth 协议中规定的那些请求参数都在这里面。同时，Spring 会把 ClientDetails 放进来。 TokenGranter它背后就是那四种标准的授权模式实现了（外加一种刷新令牌的实现），它会根据 TokenRequest 中的类型来选一个具体实现，然后执行授权。不管是哪一种授权，最后都会生成一个 OAuth2Request （TokenRequest 和 ClientDetails 的整合体）和 Authentication（包含授权用户的一些信息，那个用户在做授权，由 UserDetailsService 获得），他们两个最后会合并到 OAuth2Authentication 中去。 AuthorizationServerTokenServices这就是具体生成令牌的接口了，它根据 OAuth2Authentication 的信息就可以生成、存储令牌，并且允许通过 Enhancer 对令牌进行改造。 在授权码的方式中，权限是由用户最终决定的，你想获取用户的全部信息，用户未必会全部给你，当然需要支持用户勾选的情况下，所以在 TokenEndpoint 过程中，如果是授权码模式的第一阶段会把 Scope 置空，由第二步的用户来进行填充。创建令牌的过程中，会先检查当前用户是否已经创建过令牌、令牌是否过期、是否有刷新令牌，都没有后才会执行创建逻辑，创建的最后一步会判断你是否定义了增强类，如果有就按照你的逻辑来对令牌进行自定义。 自定义登陆自带的标准的四种方式并不一定会满足我们的业务需求，就像之前的短信验证码登陆，这里同样也需要自己实现。根据流程图可以知道，只需要调用 AuthorizationServerTokenServices 就可以产生令牌了，而构建它需要 OAuth2Authentication 对象，也就是 OAuth2Request 和 Authentication。然后在 SpringSecurity 中我们知道有个 handle 处理逻辑，只需要在成功的 handle 中调用一下这个 service 就可以了，并且 Authentication 对象是直接有现成的，那就只剩下 OAuth2Request 对象，而它可以从请求中提取数据拼装。示例可参考 MyAuthenticationSuccessHandler，经过它改造后，使用普通的表单登陆地址，只需要填入用户名和密码就可以拿到 assessToken 了。 对于 App 短信登陆，要解决的就是去 session 后验证码的校验，App 肯定不会携带 cookie 这一类用来标识，也就是服务器端不可能存在传统的 session 中进行验证；可以将验证码存在 Redis，以一个机器 id 作为 key 或者手机号作为 key，用户登陆的时候需要在请求头或者请求参数携带这个标识，用于校验，这也就要重写 SMS 验证码的校验逻辑。 对于 App 中的第三方登录，一般都是有专门的 SDK，最终用户同意后会拿到一个 openid ，而我们的系统需要提供一个接口，使用 openid 来换取 AssessToken，这样就算是登录了。当然也不一定第三方应用会给 openid，也可能会给一个授权码，需要服务器后台拿着授权码去换 token，这样的情况也需要单独处理，即在 App 的后端，要通过后处理器来处理授权码模式的请求，跟成功的 handle 一样，最后返回个 assessToken 就好了。 使用JWT首先简单说一下 JWT 的特点： 自包含 密签（签名，并不是加密） 可扩展 因为这些特点，它肯定也是比默认的 UUID 长不少，携带的信息越多自然就越长，我的话还是习惯用简洁一点的 UUID，虽然它并不能代表什么信息。要想使用 JWT，比较好的一种方案就是通过 TokenStore 将它默认的 UUID 进行转换和存储，示例参考 TokenStoreConfig 这个类。 SpringSecurity 默认是支持 SSO 的，不过这部分我没去细细研究，客户端只需要使用 @EnableOAuth2Sso 注解和一点配置就可以了，自动授权的相关类见 WhitelabelApprovalEndpoint，具体的示例参考见：https://github.com/jojozhai/security SpringSecurity授权简单说，就是控制你能干什么，不能干什么；在 SpringSecurity 中最简单的一种配置就是之前继承的 AbstractChannelSecurityConfig 类中的那个 http 对象，继续往下写： 12345678@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; // ... http.authorizeRequests() .antMatchers(HttpMethod.POST, "/user/*") .hasRole("Admin");&#125; 这样就需要你在 UserDetailsService 中赋予 “ROLEAdmin” 权限后才能访问此 URL，记得加 `ROLE` 前缀。 运行原理这里还是简单看一下它的执行过程，相信这张图还是很有印象的： 这次的焦点是在 AnonymousAuthenticationFilter，处于那一堆身份认证的最后一个，它的作用也很简单，当检测到到达这个过滤器时 Authentication 还是空的话，也就是前面那些都没有匹配到，就会创建一个 AnonymousAuthenticationToken，也就是代表匿名用户了（放进去的用户信息为 anonymousUser - ROLE_ANONYMOUS）。最终，由 FilterSecurityInterceptor （是个过滤器）来判断用户是否有足够的权限请求的资源，具体的执行逻辑会委托给 AccessDecisionManager，它有一个抽象实现 AbstractAccessDecisionManager 和几个具体实现： AffirmativeBased（默认），只要一个否定就否定 ConsensusBased，少数服从多数 UnanimousBased，只要有一个成功，即算作成功 简单说，它就是来统计投票的，具体投票是一堆 AccessDecisionVoter 对象（现在主要有 WebExpressionVoter 承担了，因为 3 之后的版本有了 Spring 表达式），根据选票与选择的 AccessDecisionManager，最终决定是否放行。如果投票不通过就会抛出一个异常，被前面的过滤器捕获，然后进行相应的处理。 权限表达式最终由 AccessDecisionVoter 评估的就是权限表达式，到达 AccessDecisionVoter 之前 Spring 会将权限信息转换成权限表达式的类型，之前在代码中写的 permitAll() 就是一个权限表达式，相信列表请参考这里，当然也可以使用多个，但是就需要手动写表达式了，例如： 1.access("hasRole('admin') and hasIpAddress('192.168.1.0/24')"); 当然也能使用自己写的方法来进行验证： 12345678910@Overridepublic boolean config(ExpressionUrlAuthorizationConfigurer&lt;HttpSecurity&gt;.ExpressionInterceptUrlRegistry config) &#123; config .antMatchers(HttpMethod.GET, "/fonts/**").permitAll() .antMatchers(HttpMethod.GET, "/**/*.html", "/resource").authenticated() .anyRequest() .access("@rbacService.hasPermission(request, authentication)"); return true; 其中 rbacService 指的是 Bean 的名字，hasPermission 是验证方法，返回一个布尔，后面跟上参数就 OK 了。 持久化存储这些权限的对应简单的话还好，如果非常复杂使用硬编码的方式就很蛋疼了，那肯定支持从数据库读取，建表就采用一般的 RBAC 基于角色的控制就行了，具体就是用户表、角色表、资源表、用户角色关系表、角色资源关系表。然后自定义你的 UserDetailsService 和 UserDetails 从数据库读取，加入该用户对应的权限信息，然后通过上面的自定义验证方法来进行校验。 拓展JWT（JSON Web Token）]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>安全</tag>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECMAScript常用语法整理]]></title>
    <url>%2F2019%2F09%2F14%2FECMAScript%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[首先集百家之介绍： ECMAScript 是一种由 Ecma 国际（前身为欧洲计算机制造商协会）通过 ECMA-262 标准化的脚本程序设计语言。这种语言在万维网上应用广泛，它往往被称为 JavaScript 或 JScript，但实际上后两者是 ECMA-262 标准的实现和扩展。 ECMAScript 6.0（以下简称 ES6）是 JavaScript 语言的下一代标准，已经在 2015 年 6 月正式发布了。它的目标，是使得 JavaScript 语言可以用来编写复杂的大型应用程序，成为企业级开发语言。 目前 ES 已经到达了 ES2018 版本，Google 的 V8 引擎支持率 100%，其他的并不友好，而我们常用 JavaScript 稳定版本的实现目前在 ES2016 版本，所以这里主要学习 ES6 的特性了。如果真的有什么原因不能使用 ES6 可以使用 Babel 将 ES6 语法转为 ES5.我会把实际中频繁用到的一些特性写出来，致力于用最优雅的写法写出更高质量的代码。 let和const使用 let 声明的变量只在它所在的代码块内有效： 123456789101112131415&#123; let a = 10; var b = 1;&#125;a // ReferenceError: a is not defined.b // 1var a = [];for (let i = 0; i &lt; 10; i++) &#123; a[i] = function () &#123; console.log(i); &#125;;&#125;a[6](); // 6，使用 var 则都是 10 例如 for 循环就合适使用 let 定义 i for 循环还有一个特别之处，就是设置循环变量的那部分是一个父作用域，而循环体内部是一个单独的子作用域。 var 命令会发生“变量提升”现象，即变量可以在声明之前使用，值为 undefined。这种现象多多少少是有些奇怪的，按照一般的逻辑，变量应该在声明语句之后才可以使用。为了纠正这种现象，let 命令改变了语法行为，它所声明的变量一定要在声明后使用，否则报错。 ES6 明确规定，如果区块中存在 let 和 const 命令，这个区块对这些命令声明的变量，从一开始就形成了封闭作用域。凡是在声明之前就使用这些变量，就会报错。ES6 规定，块级作用域之中，函数声明语句的行为类似于 let，在块级作用域之外不可引用。 总之，在代码块内，使用 let 命令声明变量之前，该变量都是不可用的。这在语法上，称为“暂时性死区”（temporal dead zone，简称 TDZ），不过应该提倡能用 let 的时候尽量别用 var，避免造成作用域的混乱。 const 声明一个只读的常量。一旦声明，常量的值就不能改变。这意味着，const 一旦声明变量，就必须立即初始化，不能留到以后赋值。const的作用域与let命令相同：只在声明所在的块级作用域内有效，不提升、存在暂时性死区。 const实际上保证的，并不是变量的值不得改动，而是变量指向的那个内存地址所保存的数据不得改动。对于简单类型的数据（数值、字符串、布尔值），值就保存在变量指向的那个内存地址，因此等同于常量。但对于复合类型的数据（主要是对象和数组），变量指向的内存地址，保存的只是一个指向实际数据的指针，const只能保证这个指针是固定的（即总是指向另一个固定的地址），至于它指向的数据结构是不是可变的，就完全不能控制了。 字符串在 ES6 中，对字符串进行了增强，尤其是模板字符串，真是非常的好用！ 模板字符串模板字符串（template string）是增强版的字符串，用反引号（`）标识。它可以当作普通字符串使用，也可以用来定义多行字符串，或者在字符串中嵌入变量或者调用函数。 1234567891011121314151617// 普通字符串`In JavaScript '\n' is a line-feed.`// 多行字符串`In JavaScript this is not legal.`// 字符串中嵌入变量let name = "Bob", time = "today";`Hello $&#123;name&#125;, how are you $&#123;time&#125;?`// 模板字符串之中还能调用函数function fn() &#123; return "Hello World";&#125;`foo $&#123;fn()&#125; bar`// foo Hello World bar 如果模板字符串中的变量没有声明，将报错。如果大括号中的值不是字符串，将按照一般的规则（toString）转为字符串。 新增方法ES5 字符串的实例方法很有限，基本就是 indexOf 了，在 ES6 新加入了一些： includes()：返回布尔值，表示是否找到了参数字符串。 startsWith()：返回布尔值，表示参数字符串是否在原字符串的头部。 endsWith()：返回布尔值，表示参数字符串是否在原字符串的尾部。 repeat()：返回一个新字符串，表示将原字符串重复 n 次。 在 ES2017 和 ES2019 又引入了 padStart() 用于头部补全，padEnd() 用于尾部补全和 trimStart() 和 trimEnd() 这两个方法。 函数ES6 允许为函数的参数设置默认值，即直接写在参数定义的后面。 123456789101112131415161718192021222324function log(x, y = 'World') &#123; console.log(x, y);&#125;log('Hello') // Hello Worldlog('Hello', 'China') // Hello Chinalog('Hello', '') // Hello// 与解构赋值默认值结合使用function foo(&#123;x, y = 5&#125;) &#123; console.log(x, y);&#125;foo(&#123;&#125;) // undefined 5foo(&#123;x: 1&#125;) // 1 5foo(&#123;x: 1, y: 2&#125;) // 1 2foo() // TypeError: Cannot read property 'x' of undefined// 如果没有提供参数，函数 foo 的参数默认为一个空对象function foo(&#123;x, y = 5&#125; = &#123;&#125;) &#123; console.log(x, y);&#125;foo() // undefined 5 ES6 引入 rest 参数（形式为 ...变量名），用于获取函数的多余参数，本质是个数组，跟 Java 很类似： 1234567891011function add(...values) &#123; let sum = 0; for (var val of values) &#123; sum += val; &#125; return sum;&#125;add(2, 5, 3) // 10 其次还有函数的 name 属性，返回该函数的函数名。 箭头函数ES6 允许使用“箭头”（=&gt;）定义函数。 1234567891011121314151617181920212223var f = v =&gt; v;// 等同于var f = function (v) &#123; return v;&#125;;var f = () =&gt; 5;// 等同于var f = function () &#123; return 5 &#125;;var sum = (num1, num2) =&gt; num1 + num2;// 等同于var sum = function(num1, num2) &#123; return num1 + num2;&#125;;// 正常函数写法[1,2,3].map(function (x) &#123; return x * x;&#125;);// 箭头函数写法[1,2,3].map(x =&gt; x * x); 怎么说呢，这个其实就是简化的匿名函数，用在回调的地方非常好用。箭头函数有几个使用注意点。 函数体内的 this 对象，就是定义时所在的对象，而不是使用时所在的对象。 不可以当作构造函数，也就是说，不可以使用 new 命令，否则会抛出一个错误。 不可以使用 arguments 对象，该对象在函数体内不存在。如果要用，可以用 rest 参数代替。 不可以使用 yield 命令，因此箭头函数不能用作 Generator 函数。 其中第一点尤其值得注意。this 对象的指向是可变的，但是在箭头函数中，它是固定的。 12345678910function foo() &#123; setTimeout(() =&gt; &#123; console.log('id:', this.id); &#125;, 100);&#125;var id = 21;foo.call(&#123; id: 42 &#125;);// id: 42 关于 this 的这个问题，版本对比为： 123456789101112131415161718192021222324// ES6 版本function foo() &#123; setTimeout(() =&gt; &#123; console.log('id:', this.id); &#125;, 100);&#125;// ES5 版本function foo() &#123; var _this = this; setTimeout(function () &#123; console.log('id:', _this.id); &#125;, 100);&#125;// 不适用情况// 对象不构成单独的作用域，导致 jumps 箭头函数定义时的作用域就是全局作用域。const cat = &#123; lives: 9, jumps: () =&gt; &#123; this.lives--; &#125;&#125; 实际原因是箭头函数根本没有自己的this，导致内部的this就是外层代码块的this。正是因为它没有this，所以也就不能用作构造函数。在 Vue 很多使用中，如果你使用箭头函数就不需要再在尾部来一个 .bind(this) 了。 其他补充使用 JSON.stringify() 方法可以将对象转为字符串类型的 json 格式。 关于 apply 和 call ，ECMAScript 规范给所有函数都定义了 call 与 apply 两个方法，它们的应用非常广泛，它们的作用也是一模一样，只是传参的形式有区别而已。apply 方法传入两个参数：一个是作为函数上下文的对象，另外一个是作为函数参数所组成的数组。call 方法第一个参数也是作为函数上下文的对象，但是后面传入的是一个参数列表，而不是单个数组。一般来说，它们的作用就是改变 this 的指向，或者借用别等对象的方法，那么它和 bind 什么区别呢？ 在 EcmaScript5 中扩展了叫 bind 的方法，在低版本的 IE 中不兼容。它和 call 很相似，接受的参数有两部分，第一个参数是是作为函数上下文的对象，第二部分参数是个列表，可以接受多个参数。 他们的主要区别就是： bind 方法不会立即执行，而是返回一个改变了上下文 this 后的函数。而原函数 func 中的 this 并没有被改变，依旧指向全局对象 window。在参数传递上，也有一些区别，看个例子： 123456789function func(a, b, c) &#123; console.log(a, b, c);&#125;var func1 = func.bind(null,'linxin');func('A', 'B', 'C'); // A B Cfunc1('A', 'B', 'C'); // linxin A Bfunc1('B', 'C'); // linxin B Cfunc.call(null, 'linxin'); // linxin undefined undefined call 是把第二个及以后的参数作为 func 方法的实参传进去，而 func1 方法的实参实则是在 bind 中参数的基础上再往后排。 数组的扩展扩展运算符（spread）是三个点（...）。它好比 rest 参数的逆运算，将一个数组转为用逗号分隔的参数序列。 12345678console.log(...[1, 2, 3])// 1 2 3console.log(1, ...[2, 3, 4], 5)// 1 2 3 4 5[...document.querySelectorAll('div')]// [&lt;div&gt;, &lt;div&gt;, &lt;div&gt;] 对于数组的克隆与合并，有了扩展运算符也变得简单多了： 123456789const a1 = [1, 2];// 写法一const a2 = [...a1];// 写法二const [...a2] = a1;// ES6 的合并数组[...arr1, ...arr2, ...arr3]// [ 'a', 'b', 'c', 'd', 'e' ] ES5 中只能使用 concat 函数间接达到目的。字符串也可以被展开：[...&#39;hello&#39;]，还可以用于 Generator 函数： 1234567const go = function*()&#123; yield 1; yield 2; yield 3;&#125;;[...go()] // [1, 2, 3] 对象扩展现在对象的属性有了更简洁的写法： 1234567891011const baz = &#123;foo&#125;;// 等同于const baz = &#123;foo: foo&#125;;function f(x, y) &#123; return &#123;x, y&#125;;&#125;// 等同于function f(x, y) &#123; return &#123;x: x, y: y&#125;;&#125; 简单说就是当 key 和 val 一样时，可以进行简写。其实，方法也可以进行简写： 123456789101112const o = &#123; method() &#123; return "Hello!"; &#125;&#125;;// 等同于const o = &#123; method: function() &#123; return "Hello!"; &#125;&#125;; 这种写法会非常的简洁，另外常用的还有 setter 和 getter，就是采用的这种方案： 1234567891011121314const cart = &#123; _wheels: 4, get wheels () &#123; return this._wheels; &#125;, set wheels (value) &#123; if (value &lt; this._wheels) &#123; throw new Error('数值太小了！'); &#125; this._wheels = value; &#125;&#125; 需要注意的一点就是简洁写法的属性名总是字符串。在对象定义上，也变得更加灵活了： 123456let propKey = 'foo';let obj = &#123; [propKey]: true, ['a' + 'bc']: 123&#125;; ES6 又新增了另一个类似的关键字super，指向当前对象的原型对象。 另外，对象也有扩展运算符，例如： 1234567let z = &#123; a: 3, b: 4 &#125;;let n = &#123; ...z &#125;;n // &#123; a: 3, b: 4 &#125;var ll = &#123;name:'loli', age: 12, getVal(val)&#123;console.log(val)&#125;&#125;var test = &#123;...User, dd:'dd'&#125;test.getVal(test.dd) 简单说就是把对象里的方法进行拷贝，Vuex 中的这种写法算是明白了吧，Vuex 中，我们经常用类似 ...mapState({xxx}) 的写法，很显然 mapState 函数返回的是一个对象，然后我们使用“展开运算符”将其展开了。 遍历变量数组或者对象，可以使用 forEach 这个函数（ES5 中也可使用）： 1234567[1, 2 ,3, 4].forEach(alert);[1, 2 ,3, 4].forEach((item, index) =&gt; &#123;console.log(item)&#125;)arr.forEach(function callback(currentValue, index, array) &#123; //your iterator&#125;[, thisArg]); 使用 forEach 函数进行遍历时，中途无法跳过或者退出；在 forEach 中的 return、break、continue 是无效的。 see：https://www.jianshu.com/p/bdf77ee23089 然后遍历除了基本的 fori，还有两种：for...in 和 for...of ，那么他们俩有啥区别呢？ 推荐在循环对象属性的时候，使用 for...in，在遍历数组的时候的时候使用 for...of。 for...in 循环出的是 key，for...of 循环出的是 value 注意，for...of 是 ES6 新引入的特性。修复了 ES5 引入的 for...in 的不足 for...of 不能循环普通的对象，需要通过和 Object.keys() 搭配使用 下面是一段示例代码： 123456789101112131415161718192021222324252627282930let aArray = ['a',123,&#123;a:'1',b:'2'&#125;]for(let index in aArray)&#123; console.log(`$&#123;aArray[index]&#125;`);&#125;// 结果：// a// 123// [object Object]for(let value of aArray)&#123; console.log(value);&#125;// 结果：// a// 123// &#123;a: "1", b: "2"&#125;// 可以使用 for...of 遍历 Map，它部署了 Iterator 接口const map = new Map();map.set('first', 'hello');map.set('second', 'world');for (let [key, value] of map) &#123; console.log(key + " is " + value);&#125;// first is hello// second is world 作用于数组的 for-in 循环除了遍历数组元素以外，还会遍历自定义属性。for...of 循环不会循环对象的 key，只会循环出数组的 value，因此 for...of 不能循环遍历普通对象，对普通对象的属性遍历推荐使用 for...in reduce某次，遇到一个做累加的需求，用传统的方式肯定是没问题，但是我想到既然是动态语言，就没有什么骚操作？结果搜了一下，确实有很多骚操作，还有直接用 eval 黑魔法的，不过，我觉得比较优雅的就是 reduce 方法了： 12var arr = [1,2,3]arr.reduce((prev, cur) =&gt; prev + cur, 0) 总感觉似曾相识，不知道在哪里用过，也许是 J8 的 Lambda 吧，这样看来 reduce 可以做的东西就多了。 forEach与mapMDN 上的描述： forEach()：针对每一个元素执行提供的函数 (executes a provided function once for each array element)。 map()：创建一个新的数组，其中每一个元素由调用数组中的每一个元素执行提供的函数得来 (creates a new array with the results of calling a provided function on every element in the calling array)。 forEach 方法不会返回执行结果，而是 undefined。也就是说，forEach() 会修改原来的数组。而 map() 方法会得到一个新的数组并返回。 123456789101112// 将数组中的数据翻倍let arr = [1, 2, 3, 4, 5];arr.forEach((num, index) =&gt; &#123; return (arr[index] = num * 2);&#125;);let doubled = arr.map(num =&gt; &#123; return num * 2;&#125;);// 结果都为： [2, 4, 6, 8, 10] 如果你习惯使用函数是编程，那么肯定喜欢使用 map()。因为 forEach() 会改变原始的数组的值，而 map() 会返回一个全新的数组，原本的数组不受到影响。总之，能用forEach()做到的，map()同样可以。反过来也是如此。一般来说，使用 map 速度会更快，测试地址：https://jsperf.com/map-vs-foreach-speed-test Class语法ES6 提供了更接近传统语言的写法，引入了 Class（类）这个概念，作为对象的模板。通过 class 关键字，可以定义类。基本上，ES6 的 class 可以看作只是一个语法糖，它的绝大部分功能，ES5 都可以做到，新的 class 写法只是让对象原型的写法更加清晰、更像面向对象编程的语法而已。 123456789101112131415161718192021222324// ES5function Point(x, y) &#123; this.x = x; this.y = y;&#125;Point.prototype.toString = function () &#123; return '(' + this.x + ', ' + this.y + ')';&#125;;var p = new Point(1, 2);// ES6class Point &#123; constructor(x, y) &#123; this.x = x; this.y = y; &#125; toString() &#123; return '(' + this.x + ', ' + this.y + ')'; &#125;&#125; 对于做静态语言后端的我，果然还是 ES6 的写法更舒服。 定义“类”的方法的时候，前面不需要加上 function 这个关键字，直接把函数定义放进去了就可以了。另外，方法之间不需要逗号分隔，加了会报错。 既然说 class 只是一个语法糖，那么我们就要深入一点看看了： 1234567891011121314151617181920212223242526272829303132333435363738class Point &#123; // ...&#125;typeof Point // "function"Point === Point.prototype.constructor // true/*******************分割线********************/class Point &#123; constructor() &#123; // ... &#125; toString() &#123; // ... &#125; toValue() &#123; // ... &#125;&#125;// 等同于Point.prototype = &#123; constructor() &#123;&#125;, toString() &#123;&#125;, toValue() &#123;&#125;,&#125;;class B &#123;&#125;let b = new B();b.constructor === B.prototype.constructor // truePoint.prototype.constructor === Point // truePoint.name // "Point" 类的数据类型就是函数，类本身就指向构造函数。构造函数的 prototype 属性，在 ES6 的“类”上面继续存在。事实上，类的所有方法都定义在类的 prototype 属性上面。类的内部所有定义的方法，都是不可枚举的（non-enumerable），这一点与 ES5 的行为不一致。生成实例对象如果忘记加上 new，像函数那样调用 Class，将会报错。类不存在变量提升（hoist），也就是没办法先使用后定义。此外还有很多需要注意的点，不过我认为我知道这一部分就足够了，了解更多就去看阮一峰的书吧。 Promise函数在 JavaScript 的世界中，所有代码都是单线程执行的。由于这个“缺陷”，导致 JavaScript 的所有网络操作，浏览器事件，都必须是异步执行，也就是要通过异步来处理。Promise 有各种开源实现，在 ES6 中被统一规范，由浏览器直接支持。 123456789101112131415161718192021222324252627282930313233343536function test(resolve, reject) &#123; setTimeout(function () &#123; // ... if (timeOut &lt; 1) &#123; log('call resolve()...'); resolve('200 OK'); &#125; else &#123; log('call reject()...'); reject('timeout in ' + timeOut + ' seconds.'); &#125; &#125;, timeOut * 1000);&#125;// 基本使用new Promise(test).then(function (result) &#123; console.log('成功：' + result);&#125;).catch(function (reason) &#123; console.log('失败：' + reason);&#125;);// 以下所有的 job 与 p 开头的都是 Promise 类型// 嵌套顺序调用job1.then(job2).then(job3).catch(handleError);// job1.then([return Promise]).then(...)// 同时执行p1和p2，并在它们都完成后执行then:Promise.all([p1, p2]).then(function (results) &#123; console.log(results); // 获得一个Array: ['P1', 'P2']&#125;);// 任意一个成功即可继续：Promise.race([p1, p2]).then(function (result) &#123; console.log(result); // 'P1'&#125;); 就我来说，它最重要的功能是来解决回调地狱问题，解决异步中回调的多层嵌套。还有一个，就是异步剥夺了 return 的权利，你用异步，return 基本就没啥意义，只能通过传入方法来执行，也是相当于回调了。 async函数异步操作是 JavaScript 编程的麻烦事，麻烦到一直有人提出各种各样的方案，试图解决这个问题。从最早的回调函数，到 Promise 对象，再到 Generator 函数，每次都有所改进，但又让人觉得不彻底，异步编程的最高境界，就是根本不用关心它是不是异步。一句话，async 函数就是 Generator 函数的语法糖。 1234567891011121314151617181920212223242526272829303132333435363738var fs = require('fs');var readFile = function (fileName)&#123; return new Promise(function (resolve, reject)&#123; fs.readFile(fileName, function(error, data)&#123; if (error) reject(error); resolve(data); &#125;); &#125;);&#125;;// beforevar gen = function* ()&#123; var f1 = yield readFile('/etc/fstab'); var f2 = yield readFile('/etc/shells'); console.log(f1.toString()); console.log(f2.toString());&#125;;// aftervar asyncReadFile = async function ()&#123; var f1 = await readFile('/etc/fstab'); var f2 = await readFile('/etc/shells'); console.log(f1.toString()); console.log(f2.toString());&#125;;// e.g.async function getStockPriceByName(name) &#123; var symbol = await getStockSymbol(name); var stockPrice = await getStockPrice(symbol); return stockPrice;&#125;getStockPriceByName('goog').then(function (result)&#123; console.log(result);&#125;); async 函数就是将 Generator 函数的星号（*）替换成 async，将 yield 替换成 await，仅此而已。 async 自带执行器，相比 Generator 也有更好的寓意，async 表示函数里有异步操作，await 表示紧跟在后面的表达式需要等待结果。await 命令后面的 Promise 对象，运行结果可能是 rejected，所以最好把 await 命令放在 try…catch 代码块中。await 命令只能用在 async 函数之中，如果用在普通函数，就会报错。 Module语法 历史上，JavaScript 一直没有模块（module）体系，无法将一个大程序拆分成互相依赖的小文件，再用简单的方法拼装起来。其他语言都有这项功能，比如 Ruby 的 require、Python 的 import，甚至就连 CSS 都有 @import，但是 JavaScript 任何这方面的支持都没有，这对开发大型的、复杂的项目形成了巨大障碍。 在 ES6 之前，社区制定了一些模块加载方案，最主要的有 CommonJS 和 AMD 两种。前者用于服务器，后者用于浏览器。ES6 在语言标准的层面上，实现了模块功能，而且实现得相当简单，完全可以取代 CommonJS 和 AMD 规范，成为浏览器和服务器通用的模块解决方案。 ES6 模块的设计思想是尽量的静态化，使得编译时就能确定模块的依赖关系，以及输入和输出的变量。CommonJS 和 AMD 模块，都只能在运行时确定这些东西。比如，CommonJS 模块就是对象，输入时必须查找对象属性。 12345678// CommonJS 模块let &#123; stat, exists, readFile &#125; = require('fs');// 等同于let _fs = require('fs');let stat = _fs.stat;let exists = _fs.exists;let readfile = _fs.readfile; 上面代码的实质是整体加载 fs 模块（即加载 fs 的所有方法），生成一个对象（_fs），然后再从这个对象上面读取 3 个方法。这种加载称为“运行时加载”，因为只有运行时才能得到这个对象，导致完全没办法在编译时做“静态优化”。ES6 模块不是对象，而是通过 export 命令显式指定输出的代码，再通过 import 命令输入。 123// ES6模块// 仅加载三个方法（函数）import &#123; stat, exists, readFile &#125; from 'fs'; 这种加载称为“编译时加载”或者静态加载，即 ES6 可以在编译时就完成模块加载，效率要比 CommonJS 模块的加载方式高。当然，这也导致了没法引用 ES6 模块本身，因为它不是对象。由于 ES6 模块是编译时加载，使得静态分析成为可能。有了它，就能进一步拓宽 JavaScript 的语法，比如引入宏（macro）和类型检验（type system）这些只能靠静态分析实现的功能。 export一个模块就是一个独立的文件。该文件内部的所有变量，外部无法获取。如果你希望外部能够读取模块内部的某个变量，就必须使用 export 关键字输出该变量。下面展示一下几种 export 的写法： 1234567891011121314151617181920// 第一种export var firstName = 'Michael';export var year = 1958;// 第二种（推荐）var firstName = 'Michael';var year = 1958;export &#123; firstName, year &#125;;// 输出函数或者类或者对象export function multiply(x, y) &#123; return x * y;&#125;;// 输出函数自定义名称function v2() &#123; ... &#125;export &#123; v2 as streamV2, v2 as streamLatestVersion&#125;; 它们的实质是，在接口名与模块内部变量之间，建立了一一对应的关系，所以你不能直接输出一个值，例如数字。 1234567891011121314151617// 报错export 1;// 写法一export var m = 1;// 写法二var m = 1;export &#123;m&#125;;// 报错function f() &#123;&#125;export f;// 正确export function f() &#123;&#125;;// 正确function f() &#123;&#125;export &#123;f&#125;; export 命令可以出现在模块的任何位置，只要处于模块顶层就可以。 import使用export命令定义了模块的对外接口以后，其他 JS 文件就可以通过import命令加载这个模块。 12345678910111213141516// 自定义函数名import &#123; lastName as surname &#125; from './profile.js';import &#123;a&#125; from './xxx.js'a = &#123;&#125;; // Syntax Error : 'a' is read-only;a.foo = 'hello'; // 合法操作（非常不建议）// 报错import &#123; 'f' + 'oo' &#125; from 'my_module';// 报错let module = 'my_module';import &#123; foo &#125; from module;// 整体导入// 通过 别名.函数名 调用import * as circle from './circle'; import 命令输入的变量都是只读的，因为它的本质是输入接口。注意，import 命令具有提升效果，会提升到整个模块的头部，首先执行，同时 .js 后缀可以省略。由于 import 是静态执行，所以不能使用表达式和变量，这些只有在运行时才能得到结果的语法结构。 export default为了给用户提供方便，让他们不用阅读文档就能加载模块（不需要知道名字），就要用到 export default 命令，为模块指定默认输出。其他模块加载该模块时，import命令可以为该匿名函数指定任意名字。 12345678export default function () &#123; console.log('foo');&#125;import customName from './export-default';// 导入默认与非默认方法import _, &#123; each, forEach &#125; from 'lodash'; 这时 import 命令后面，不使用大括号。显然，一个模块只能有一个默认输出，因此 export default 命令只能使用一次。所以，import 命令后面才不用加大括号，因为只可能唯一对应 export default命令。本质上，export default 就是输出一个叫做 default 的变量或方法，然后系统允许你为它取任意名字。正是如此所以： 123456789101112131415// 正确export var a = 1;// 正确var a = 1;export default a;// 错误export default var a = 1;// 正确export default 42;// 报错export 42; 静态化固然有利于编译器提高效率，但也导致无法在运行时加载模块。在语法上，条件加载就不可能实现。如果 import 命令要取代 Node 的 require 方法，这就形成了一个障碍。因为 require 是运行时加载模块，import 命令无法取代 require 的动态加载功能。 12const path = './' + fileName;const myModual = require(path); 因此，有一个提案，建议引入 import() 函数，完成动态加载，对于这个import 函数，我就不多进行了解了。 其他关于 a 标签默认行为（href 跳转）：常见的阻止默认行为的方式：&lt;a href=&quot;javascript:void(0);&quot; onclick= &quot;myjs( )&quot;&gt; Click Me &lt;/a&gt;函数 onclick 要优于 href 执行，而 void 是一个操作符，void(0) 返回 undefined，地址不发生跳转，使用 javascript:; 也是一样的效果。在 onclick 函数中，如果返回的是 true，则认为该链接发生了点击行为；如果返回为 false，则认为未被点击。 参考http://es6.ruanyifeng.com/#docs/class]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue核心使用总结]]></title>
    <url>%2F2019%2F06%2F02%2FVue%E6%A0%B8%E5%BF%83%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[将之前的 Vue 学习笔记进行了整理，即使仅是 Vue 核心的功能发现也还是比较有货的，跟后端的学法都是一样的，关键理解其设计思想，上手使用都不难，关键是能 get 到为什么这样设计，这样设计什么好处，这个有点玄学，全靠自己理解了 o(￣▽￣)ゞ))￣▽￣)o。 介绍vueJS 是一种轻量级的 MVVM 框架，它同时吸收了 react 和 angular 的优点，强调了 react 组件化的概念，可以轻松的实现数据和展现的分离。也吸收了 angular 灵活的指令和页面操作的一些方法。 Vue 引入建议放在头部，避免发生抖屏的现象。Vue 使用后不再需要任何 Dom 操作，Vue 接管了 Dom 的操作。Vue 只会处理挂载点下的内容 当数据发生变化时（比如被函数改变），Vue 会自动去更新页面的数据，整个过程不需要操作 Dom 关于 MVC、MVP、 MVVM 的介绍：http://www.ruanyifeng.com/blog/2015/02/mvcmvp_mvvm.html Vue基础首先，来复习下 Vue 最常用的一些东西，基本语法和模式 名词解释学习 Vue 首先接触的可能是挂载点、模板、Vue 实例等这些词，先来看一段示例代码： 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html lang="zh-Hans"&gt; &lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;Vue入门&lt;/title&gt; &lt;!-- 放在头部引用，避免抖屏现象 --&gt; &lt;script src="vue.js"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id="root"&gt; &lt;h1&gt;&#123;&#123;msg&#125;&#125;&lt;/h1&gt; &lt;p v-text="num" v-on:click="handleClick"&gt;&lt;/p&gt; &lt;/div&gt; &lt;script&gt; new Vue(&#123; el: "#root", // template: "&lt;h1&gt;&#123;&#123;msg&#125;&#125;&lt;/h1&gt;", data: &#123; msg: "Hello World!", num: 123 &#125;, methods: &#123; handleClick: function () &#123; // alert("test") this.msg = "Loli" &#125; &#125; &#125;) &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 这是标准的一个 HTML，其中 script 部分是 Vue 的主要施展的部分，通过 new Vue() 出的对象自然就称作是实例了。其中，那个 id 为 root 的 div 就可以称作是挂载点；也可以说是 Vue 实例中 el 所指向的元素。在挂载点内部的内容，都可以称作是模板内容；同时模板也可以写在 Vue 实例中，效果是一样的。 这里补充一个生命周期的图示： 数据指令及处理通过两对花括号的方式取值方式我们称之为“插值表达式”.在实例定义的方法中，可以直接通过 this.name 的方式来获取定义的数据对象（data 属性里的），相当于是个别名了，不需要太在意 this 指向，Vue 会进行进一步的处理的，简单理解为代表本组件即可。 指令： v-text标签的内容就是 v-text 指向的变量，例如：&lt;p v-text=&quot;num&quot;&gt;&lt;/p&gt;特殊字符会被转义 v-html和 v-text 基本一致，区别在于它不会转义那些 HTML 字符 v-on(@)绑定事件函数，比如点击事件（v-on:click），函数的定义可以写在实例的 methods 对象内。其中 v-on: 可以简写为 @ v-bind(:) 属性绑定，和双大括号类似，只不过是用在属性里的，例如：&lt;div v-bind:title=&quot;is + title&quot;&gt;Test&lt;/div&gt;同样，它可以简写为 : 使用了指令后，比如 v-xxx: 之类的形式，后面跟的是一个 js 表达式，也就是说可以使用 js 中的基本表达方式，比如 + 之类的连接符。 双向绑定上面一顿操作都是单向绑定的，也就是说实例的数据决定页面的显示，但是页面的显示不能改变实例中的数据，比如： 123456789101112131415&lt;body&gt; &lt;div id="root"&gt; &lt;input type="text" v-model="content"&gt; &lt;div&gt;&#123;&#123;content&#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;script&gt; new Vue(&#123; el: "#root", data: &#123; content: "is Content" &#125; &#125;) &lt;/script&gt;&lt;/body&gt; 这里就需要使用模板指令：v-mode，使用格式直接是：v-model=&quot;msg&quot; 计算属性与侦听器计算属性就是某一个属性的结果是其他几个属性值计算得出来的，并且在其他属性没有改变的情况下，再次使用会直接返回缓存值，避免重复计算。侦听器就是监听某一个数据的变化，一旦发生变化就执行相应的逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;body&gt; &lt;div id="cal"&gt; &lt;input type="text" v-model="firstName"&gt; &lt;input type="text" v-model="lastName"&gt; &lt;div&gt;&#123;&#123;fullName&#125;&#125;&lt;/div&gt; &lt;div&gt;&#123;&#123;count&#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;script&gt; new Vue(&#123; el: "#cal", data: &#123; firstName: "", lastName: "", count: 0 &#125;, computed: &#123; // 计算属性声明 fullName: function () &#123; // 再次使用 fullName 时，如果这两个变量没有改变会使用缓存值 // 避免重复进行计算 return this.firstName + " " + this.lastName &#125;, // get set 的方式 test: &#123; get () &#123; // do something &#125;, set (newVal) &#123; // do something &#125; &#125; &#125;, watch: &#123; // 侦听器，也会有缓存 firstName: function () &#123; this.count++ &#125;, lastName: function () &#123; this.count++ &#125; &#125; &#125;) &lt;/script&gt;&lt;/body&gt; PS：不要忘记使用 this，要不然找不到，Vue 会自动处理这个“别名”。计算属性 computed 可以使用 get 和 set，用来提供获取和设置的情况。 其他指令显示与隐藏来看 v-if 和 v-show 他们控制标签的显示和隐藏，当为 true 时就显示，false 时就隐藏，他们的区别在于，v-if 的表现形式是将标签直接删除，v-show 则是通过 display 来实现。 这可能带来了 dom 复用的问题，例如 input 框不会清空，这种情况下可以使用 key 值来绑定唯一，这样 Vue 就会不复用了。 性能上来说，频繁更改的话 v-show 更好，如果只是改一次那么 v-if 可能就更好了。其次还支持紧贴 v-if 的 v-else-if 和 v-else 遍历然后就是 v-for 用来遍历数据的，举个例子：&lt;li v-for=&quot;(item,index) of list&quot; :key=&quot;index&quot;&gt;&lt;/li&gt;list 就是定义的数组数据，item 是每次遍历的值，index 是索引，使用 :key 可以提高效率，但是要保证 key 的唯一；所以这里我加了个 index，如果 item 是唯一的那么可以直接使用 item：&lt;li v-for=&quot;item of list&quot; :key=&quot;item&quot;&gt;&lt;/li&gt;如果还要对其进行排序之类的操作，那么使用 index 也不是很合适了。 123456789101112131415161718192021222324252627&lt;body&gt; &lt;div id="root"&gt; &lt;button @click="myClick"&gt;Test&lt;/button&gt; &lt;div v-if="flag"&gt;&#123;&#123;text&#125;&#125;&lt;/div&gt; &lt;ul&gt; &lt;li v-for="(item,index) of list" :key="index"&gt;&#123;&#123;item&#125;&#125;&lt;/li&gt; &lt;!-- &lt;li v-for="item of list" :key="item"&gt;&#123;&#123;item&#125;&#125;&lt;/li&gt; --&gt; &lt;/ul&gt; &lt;/div&gt; &lt;script&gt; new Vue(&#123; el: "#root", data: &#123; text: "我是内容", flag: true, list: [1,2,3] &#125;, methods: &#123; myClick: function () &#123; this.flag = !this.flag &#125; &#125; &#125;) &lt;/script&gt;&lt;/body&gt; 样式控制关于样式的控制，可以使用 class 对象绑定： :class=&quot;{className: isActivated}&quot; 然后通过控制 isActivated 变量来控制 class 的显示或隐藏。还可以通过 :class=&quot;[activated, className]&quot; 这样通过 activated 这个变量来控制。如果是内联样式（:style），可以直接引用一个 js 对象，在对象里面定义 css 样式就行，同样也可以使用数组来挂载多个对象。 组件化当某一块的布局复杂后就需要抽取出来，形成了一个模板，模板又分为全局的和局部的，它们的使用也各不相同，这一个个的模板加上一下逻辑就可以当作是一个组件，在下面的代码中可以体现出来。因为模板中取不到外部的属性，所以使用了属性传值的方式来将数据传进去，在模板中接收一下就可以使用了。模板也可以看作是一个实例，可以说在一个 Vue 项目中，是由很多很多 Vue 实例（组件）组成的。根据“发布-订阅”模型，可以在子组件中向父组件发布消息，然后父组件可以监听子组件的自定义消息，然后调用相应的方法来进行处理： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;body&gt; &lt;div id="root"&gt; &lt;input type="text" v-model="content"&gt; &lt;button @click="mSubmit"&gt;提交&lt;/button&gt; &lt;ul&gt; &lt;li v-for="(item, index) of list" :key="index"&gt; &#123;&#123;item&#125;&#125; &lt;/li&gt; &lt;/ul&gt; &lt;hr&gt; &lt;ul&gt; &lt;!-- 使用组件 --&gt; &lt;!-- 根据发布-订阅，监听内部组件的自定义事件 --&gt; &lt;todo-item v-for="(item, index) of list" :key="index" :content="item" :index="index" @delete="handleDelete" &gt; &lt;/todo-item&gt; &lt;/ul&gt; &lt;/div&gt; &lt;script&gt; // 定义全局组件 Vue.component('todo-item', &#123; // 接收传入的属性 props: ['content', 'index'], template: '&lt;li @click="handleClick"&gt;&#123;&#123;content&#125;&#125;&lt;/li&gt;', // 因为模板也是一个实例可以定义事件 methods: &#123; handleClick: function () &#123; // 向外发布一个自定义事件（订阅-发布模型）, 并且将 index 传递过去 this.$emit('delete', this.index) &#125; &#125; &#125;) // 定义局部组件,必须要在 Vue 实例中声明才能用 var TodoItem = &#123; template: '&lt;li&gt;item&lt;/li&gt;' &#125; new Vue(&#123; el: "#root", // components: &#123; // // 声明局部组件，如果键和值相同可以写一个 // 'todo-item': TodoItem // &#125;, data: &#123; content: '', list: [] &#125;, methods: &#123; mSubmit: function () &#123; this.list.push(this.content) this.content = '' &#125;, handleDelete: function (index) &#123; this.list.splice(index, 1) &#125; &#125; &#125;) &lt;/script&gt;&lt;/body&gt; PS：模板中要求只能有一个根标签，所以在最外层一般都会套一个 div 总之，当年看到奇奇怪怪的 HTML 标签时，一般就是一个 Vue 组件了，一个组件一般会写在一个 xxx.vue 文件里，通过 ES6 的 import 语法导入进行使用。 我们导入的组件（component）起的名字一般是跟组件的 name 属性对应的，在 HTML 中使用组件直接用这个名字的标签即可，因为 HTML 对大小写不敏感，所以使用 - 进行转换，例如：name：todoList =&gt; &lt;todo-list&gt;&lt;/todo-list&gt; 下面是一个组件的基本 js 结构： 12345678910111213export default &#123; name: "basic", props: [], data() &#123; return &#123;&#125; &#125;, methods: &#123; &#125;, created() &#123;&#125;, mounted() &#123;&#125;, watch: &#123;&#125;&#125; 需要特别注意的是，组件里的 data 是一个函数，返回的是一个对象。 组件参数校验在父子组件之间传值都已经知道了，那么接下来就看看如果子组件要对父组件传递的数据进行校验要怎么办，使用的还是子组件里的 props 属性，只不过这里由本来的字符串数组变成了对象。 12345678910111213141516171819Vue.component('child', &#123; // 不是校验的话可以直接用字符串数组来标识 props: &#123; // content: [Number,String] content: &#123; type: String, required: true, default: 'def val', validator: function (value) &#123; return value.length &gt; 5 &#125; &#125; &#125;, template: "&lt;div&gt;&#123;&#123;content&#125;&#125;&lt;/div&gt;"&#125;)var vm = new Vue(&#123; el: "#root"&#125;) 补充一下这个 props 特性，也就是如果你在子组件的 props 中接收了传递的属性，那么 Vue 在视图渲染的时候就不会再在 HTML 中加上这个属性了。在子组件上绑定的事件默认都是自定义事件，也就是说原生的事件可能会失效，例如在子组件标签里使用 @click 是无效的，不过你可以在子组件的模板里来绑定，这样就不是自定义事件了。触发自定义事件就是手动的调用 emit 了；但是有些时候就想用子组件的原生事件，就想让它生效怎么办，也是有办法的，只需要一点点的改动：@click.native=&quot;fun&quot;。同时，为了解决手机端兼容问题，可以使用类似 @touchstart.prevent 的方式阻止事件的默认行为。 组件之间的传值这里主要说的就是非父子组件之间的传值，例如父与子的子、兄弟组件，虽然可以间接完成，但是过于麻烦，由于 Vue 的定义轻量级，它并不具备解决这个问题的能力，但是我们可以借助其他的方案。 Vue 官方推荐的 Vuex 框架。 发布-订阅模式，也就是总线机制，可以理解为是观察者模式。 一般情况下，都是使用 Vuex，非常简单的项目可以考虑下 bus 总线模式这里仅说一下总线的这种方式，其实就是在所有子组件上挂一个 Vue 实例，然后通过这个实例来进行事件的发送与处理。 1234567891011121314151617181920212223242526272829303132// 设置 busVue.prototype.bus = new Vue();Vue.component('child', &#123; // 子组件的 data 必须是函数 data: function () &#123; return &#123; selfContent: this.content &#125; &#125;, props: &#123; content: String &#125;, template: "&lt;div @click='handleClick'&gt;&#123;&#123;selfContent&#125;&#125;&lt;/div&gt;", methods: &#123; handleClick: function () &#123; // 向总线发送事件 this.bus.$emit("change", this.selfContent); &#125; &#125;, // 生命周期，挂载时触发 mounted: function () &#123; var this_ = this; this.bus.$on("change", function (val) &#123; this_.selfContent = val; &#125;) &#125;&#125;)var vm = new Vue(&#123; el: "#root"&#125;) 就是通过一个生命周期来完成的。 在 Vue 中，类似 vm.$xx 这种的调用，后面跟一个 $ 符号，意思是调用 Vue 实例的方法。 使用Vuex简单说 Vuex 就是一个单向数据的改变流，把需要改变的数据单独存储起来，然后通过指定的流程来进行更改。 一般情况下，我们在单独的一个 js 中设置 Vuex，例如： 12345678910111213141516171819202122232425import Vue from 'vue'import Vuex from 'vuex'Vue.use(Vuex)export default new Vuex.Store(&#123; state: &#123; name: localStorage.name || "xxx" &#125;, actions: &#123; changeName (ctx, name) &#123; // 调用 mutations ctx.commit('changeName', name) &#125; &#125;, mutations: &#123; changeName (state, name) &#123; state.name = name // 本地存储 (低版本浏览器或者隐身模式可能会抛异常) try &#123; localStorage.name = name &#125; catch (e) &#123;&#125; &#125; &#125;&#125;) 然后在 App 入口 js 中进行引用： 1234567new Vue(&#123; el: '#app', router, store, components: &#123; App &#125;, template: '&lt;App/&gt;'&#125;) 这样在任何子组件中就可以通过 this.$store.state 来获取 Vuex 中 state 的数据啦。 1234// 触发修改this.$store.dispatch('changeName', name)// 如果没有异步获取数据逻辑，可以直接调用 mutationsthis.$store.commit('changeName', name) 其中我们还使用了本地存储 localStorage。 实际中，大多会拆分 index.js 将 state、actions、mutations 单独放在一个文件中。 另外，Vuex 还提供了高级 API 允许我们更精简的写代码，例如： 123456789101112131415161718192021import &#123; mapState, mapActions &#125; from 'vuex'...methods: &#123; handleClick (name) &#123; // 使用 Vuex 改变全局数据 // this.$store.dispatch('changeName', name) this.changeName(name) // 或者可以直接调用 mutations // this.$store.commit('changeName', name) // 跳转回首页 this.$router.push('/') &#125;, // 展开运算符，可以直接调用 changeName 方法了 ...mapActions(['changeName'])&#125;,computed: &#123; // 使用 Vuex 的便捷映射, 数组、对象皆可 ...mapState(&#123; name: 'loli' &#125;)&#125; 基本的 Vuex 操作就是这些了。 使用插槽简单来说，当子组件有一部分内容是由父组件传递过来的 dom 来显示的时候，就可以使用插槽来处理。要解决的问题，之前： 12345678910111213// &lt;child content="&lt;p&gt;hello&lt;/p&gt;"&gt;Vue.component('child', &#123; props: ['content'], // ES6 语法 template: `&lt;div&gt; &lt;p&gt;Dear&lt;/p&gt; &lt;div v-html="this.content"&gt;&lt;/div&gt; &lt;/div&gt;`&#125;)var vm = new Vue(&#123; el: "#root"&#125;) 之后： 123456789101112131415161718192021&lt;div id="root"&gt; &lt;!-- &lt;child content="&lt;p&gt;hello&lt;/p&gt;"&gt;&lt;/child&gt; --&gt; &lt;child&gt; &lt;p&gt;hello&lt;/p&gt; &lt;/child&gt;&lt;/div&gt;&lt;script&gt; Vue.component('child', &#123; props: ['content'], // ES6 语法 template: `&lt;div&gt; &lt;p&gt;Dear&lt;/p&gt; &lt;slot&gt;默认内容&lt;/slot&gt; &lt;/div&gt;` &#125;) var vm = new Vue(&#123; el: "#root" &#125;)&lt;/script&gt; 可以看出子组件里的内容会被 slot 标签插入。如果需要将子组件里内容分片，那么也是可以的： 12345678910111213141516171819202122&lt;div id="root"&gt; &lt;child&gt; &lt;p slot="one"&gt;one&lt;/p&gt; &lt;p slot="two"&gt;two&lt;/p&gt; &lt;/child&gt;&lt;/div&gt;&lt;script&gt; Vue.component('child', &#123; props: ['content'], // ES6 语法 template: `&lt;div&gt; &lt;slot name="one"&gt;&lt;/slot&gt; &lt;p&gt;Dear&lt;/p&gt; &lt;slot name="two"&gt;&lt;/slot&gt; &lt;/div&gt;` &#125;) var vm = new Vue(&#123; el: "#root" &#125;)&lt;/script&gt; 就是稍微改变了下，进行了标识。 作用域插槽最后来看一下高级用法：作用域插槽，从例子开始： 12345678910111213141516171819202122232425262728&lt;div id="root"&gt; &lt;show&gt; &lt;!-- 固定写法，template 开始 --&gt; &lt;template slot-scope="props"&gt; &lt;h2&gt;&#123;&#123;props.item&#125;&#125;&lt;/h2&gt; &lt;/template&gt; &lt;/show&gt;&lt;/div&gt;&lt;script&gt; Vue.component('show', &#123; data: function () &#123; return &#123; list: [1,2,3,4] &#125;; &#125;, template: `&lt;div&gt; &lt;slot v-for="item of list" :item=item &gt;&lt;/slot&gt; &lt;/div&gt;` &#125;) var vm = new Vue(&#123; el: "#root" &#125;)&lt;/script&gt; 稍微解释一下，在 template 中，使用 for 来“循环插槽”，将每次循环的数据绑定到了 item 变量，然后视图中通过 slot-scope 来接收。 动态组件这里说的是动态的切换组件，可以手动实现，也可以通过 Vue 提供的 component 标签来实现，例子： 1234567891011121314151617181920212223242526272829&lt;div id="root"&gt; &lt;component :is='type'&gt;&lt;/component&gt; &lt;child-one v-if="type === 'child-one'"&gt;&lt;/child-one&gt; &lt;child-two v-if="type === 'child-two'"&gt;&lt;/child-two&gt; &lt;button @click="handleClick"&gt;change&lt;/button&gt;&lt;/div&gt;&lt;script&gt; Vue.component('child-one', &#123; template: '&lt;div v-once&gt;one&lt;/div&gt;' &#125;); Vue.component('child-two', &#123; template: '&lt;div v-once&gt;two&lt;/div&gt;' &#125;); var vm = new Vue(&#123; el: "#root", data: &#123; type: 'child-one' &#125;, methods: &#123; handleClick: function () &#123; this.type = this.type === 'child-one' ? 'child-two' : 'child-one'; &#125; &#125; &#125;)&lt;/script&gt; component 和下面使用 v-if 控制的标签是一样的，因为每次切换都需要销毁、重新创建，所以性能上会有点损耗，可以在模板上使用 v-once 来将实例放到内存中，这样就省去了创建、销毁的时间。 异步组件使用异步加载方式，避免所有的业务逻辑打包后在一个 js 文件中； 适用于打包后生成的 app.js 文件太大的情况，不然多发 http 请求反而更加不好。 以下内容建议阅读完『使用脚手架-使用路由』之后再阅读。 改用异步组件非常简单，在很多地方都可以用，例如 router 下的 index.js 123456789101112131415161718192021222324252627import Vue from 'vue'import Router from 'vue-router'Vue.use(Router)export default new Router(&#123; routes: [ &#123; path: '/', name: 'Home', component: () =&gt; import('@/pages/home/Home') &#125;, &#123; path: '/city', name: 'City', component: () =&gt; import('@/pages/city/City') &#125;, &#123; path: '/detail/:id', name: 'Detail', component: () =&gt; import('@/pages/detail/Detail') &#125; ], scrollBehavior (to, from, savedPosition) &#123; return &#123;x: 0, y: 0&#125; &#125;&#125;) 即，把 import Router from &#39;vue-router&#39; 更换为箭头函数 () =&gt; import(&#39;@/xxx&#39;) 使用脚手架新版 vue-cli3 的使用参考官方文档。相比 cli2 目录发生了一些变化，Webpack 从 3 升级到 4，支持 ui 创建，非常推荐，篇幅原因不再详细写，跟 2 差不多。初始化完成的项目一般我们还需要增加一些目录和配置，例如 views、api、vue.config.js 等，可参考这里。 对于老版 cli2 官方提供的安装和初始化： 123456789# 全局安装 vue-clinpm install --global vue-cli# 创建一个基于 webpack 模板的新项目vue init webpack my-project# 安装依赖，走你cd my-projectnpm run dev 因为是采用的 webpack 的方式，所以方便了很多，直接可以在浏览器中进行预览。这里就可以看出模块化了，其中 Vue 实例都集中放在了 .vue 结尾的文件中，vue 文件分成三部分，模板、js代码、样式。工程的入口是根目录下的 index.html 文件，其中有个 id 为 app 的 div，这就是 Vue 的挂载点了，然后看主要的代码在 src 中。在 main.js 中创建了 Vue 实例，也就是所谓的启动配置，并且定义了模板，导入了相应的子模板，这里就来修改下看看： 123456789101112// ES6 模块化语法import Vue from 'vue'import App from './App'Vue.config.productionTip = falsenew Vue(&#123; el: '#app', // 引入子模板 components: &#123; App &#125;, template: '&lt;App/&gt;'&#125;) 和上面写的差不多，创建了一个 Vue 实例，挂载到 app 这个 id 下面，模板采用的是实例内定义的，里面就是引用了个 &lt;App/&gt; ，也就是 App.vue 中定义的那一堆。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;template&gt; &lt;div&gt; &lt;input type="text" v-model="content"&gt; &lt;button @click="mSubmit"&gt;提交&lt;/button&gt; &lt;ul&gt; &lt;!-- 根据发布-订阅，监听内部组件的自定义事件 --&gt; &lt;todo-list v-for="(item, index) of list" :key="index" :content="item" :index="index" @delete="handleDelete" &gt;&lt;/todo-list&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt; import TodoItem from './components/TodoItem' export default &#123; // 相当于子模板的定义区 name: 'App', components: &#123; // 声明局部组件，如果键和值相同可以写一个 'todo-list': TodoItem &#125;, // vue 文件中 data 只能使用函数来定义 // ES6 简便写法 data () &#123; return &#123; content: '', list: [] &#125; &#125;, methods: &#123; mSubmit () &#123; this.list.push(this.content) this.content = '' &#125;, handleDelete (index) &#123; this.list.splice(index, 1) &#125; &#125; &#125;&lt;/script&gt;&lt;style scoped&gt;&lt;/style&gt; 在 vue-cli 中，当写一个实例的数据的时候使用的是函数方式，而不再是对象！！然后，App 这个模块中又引入了一个 TodoItem 的模板，就是曾经的子模板，完整的例子到 GitHub 仓库查看。再次说明，Vue 底层会处理 this 的指向，不需要太过担心，指的就是此实例. 使用路由使用路由来根据访问 url 动态切换的目的，在 App 主入口加入 &lt;router-view/&gt; 即可启用，在 src\router\index.js 进行配置路由规则即可，前提是安装了 vue-router 组件。其中路由规则的 url 映射可以使用变量来进行区分，例如：/detail/:id ，然后可以在子组件通过 this.$route.params.id 来获得变量。使用 &lt;router-link to=&quot;/list&quot;&gt;跳转&lt;/router-link&gt; 可进行单页应用的跳转，在跳转过程中不需要请求新的 HTML，但是首屏加载会慢一点，SEO 也不是很好。除了使用标签来路由，在 JS 环境下，可以使用编程式导航：this.$router.push(&#39;/&#39;) 为了防止路由调整后滚动条不会重置，官方文档的解决方案是在路由的 js 配置中加入下面的代码： 1234567891011121314151617import Vue from 'vue';import Router from 'vue-router';Vue.use(Router);export default new Router(&#123; routes: [ &#123; path: '/', name: 'Home', component: Home &#125; ], scrollBehavior (to, from, savedPosition) &#123; return &#123;x: 0, y: 0&#125; &#125;&#125;) 这样就解决了跳转后滚动条错乱的情况。最后记得在 main.js 中开启路由功能： 1234new Vue(&#123; render: h =&gt; h(App), router&#125;).$mount('#app') 其中 h =&gt; h(App) 是创建标签挂载组件的一个简写形式，参考这里。 使用Axios发送请求既然都用 Vue 了，那么基本是前后端分离的架构，于是 ajax 请求必然少不了，一般情况在 Vue 中处理请求都是用 Axios 这个库，直接 npm 安装后导入就可以用了。这里说下基本用法： 12345678910111213141516171819202122232425262728293031323334353637import Axios from 'axios'methods: &#123; getHomeInfo () &#123; Axios.get('/api/index.json?city=' + this.$store.state.city) .then(this.getHomeInfoCallback) .catch(err =&gt; &#123; console.log(err); &#125;) &#125;, getHomeInfoCallback (res) &#123; if (res.data.ret &amp;&amp; res.data.data) &#123; const data = res.data.data this.swiperList = data.swiperList &#125; &#125;&#125;axios.post('/user', &#123; firstName: 'Fred', lastName: 'Flintstone'&#125;).then(function (response) &#123; console.log(response)&#125;).catch(function (error) &#123; console.log(error)&#125;)axios(&#123; method: 'post', url: '/user/12345', data: &#123; firstName: 'Fred', lastName: 'Flintstone' &#125;&#125;).then(res =&gt; &#123; console.log(res)&#125;) 嗯，基本上就是这个样子了，then 表示才成功后回调，catch 表示失败后回调。注意下，成功或者失败的 res 都会给你包一层，res.data 才是真正响应的东西。 使用代理因为一般是前后端分离，有时候后端的接口还没开发完，这时候我们可以改 url，但是这样之后你还得再改回去，特别麻烦，webpack 提供了代理的工具，将我们 Axios 发送的请求按照指定规则进行转发，例如我们可以使用静态文件来模拟数据。一般在 webpack 的配置文件里这样写： 12345678910111213141516dev: &#123; // Paths assetsSubDirectory: 'static', assetsPublicPath: '/', proxyTable: &#123; // 添加 URL 的替换，转发为本地静态文件 '/api': &#123; target: 'http://localhost:8080', secure: false, // 如果是https接口，需要配置这个参数 changeOrigin: true, // 是否跨域 pathRewrite: &#123; '^/api': '/static/mock' &#125; &#125;, &#125;&#125; 主要就是将 url 重写，然后转发到你对应的地址。 其他补充这里列举下在项目中使用的一些特性或技术，除了动画，其他还是用的非常多的。 动画相关最简单的淡出淡然，可以使用 transition 标签进行包裹，作用简单说就是：在开始前，会给被包裹的标签增加两个 css 样式：v-enter 和 v-enter-active，当动画运行到第二帧的时候会去除 v-enter 这个样式，再增加一个 v-enter-to 样式；最后动画结束时去除所有样式。在使用 transition 标签的时候，如果加了 name 属性，那么样式就以你定义的名字作为前缀代替 v。上面说的是显示的动画，隐藏的过度也是类似，只需要把 enter 换成 leave 就可以了。如果想自定义 css 的名字，可以使用 enter-active-class 属性来定义，其他同理。 或者可以使用 animated.css 提供的样式，快速开发，使用起来非常简单，引入不要的 css 库，然后利用上面所说来自定义 css 名字，格式就是：animated 动画名 名字可以在官网找，其实就是封装了下 css3 的 @keyframes 特性。 PS：想要初始化的时候就展示动画需要使用 appear 属性来配合。 除了使用 css 来做动画，也可以使用 js 实现，在标签中通过 @before-enter、@enter、@after-enter 等来绑定方法，会传递一个参数过去，也就是包裹的 dom 元素 el。其中 @enter 会传递两个参数，第一个与上面一样，第二个是个函数引用 done，在动画完成后调用一下它告诉 Vue 动画结束，这样就会再继续执行下面的 after。如果嫌麻烦，可以使用像 Velocity 这样的 js 动画库。 对于列表动画，可以使用 transition-group 标签来包裹，其实它的作用就是将里面的循环每一个都包裹一个 transition 标签。 优化相关使用 &lt;keep-alive&gt; 标签 Vue 会自动帮你进行优化请求，例如 ajax 请求，如果启用了 &lt;keep-alive&gt; 标签，当第二次进行 ajax 请求时，会直接从内存里拿数据（mounted 函数不会执行）。一般就直接在 App.vue 中使用了： 1234567891011121314&lt;template&gt; &lt;div id="app"&gt; &lt;!--启用缓存,重新路由后不再发请求--&gt; &lt;keep-alive exclude="Detail"&gt; &lt;router-view/&gt; &lt;/keep-alive&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: 'App'&#125;&lt;/script&gt; 然后有的数据我们是不想进行缓存的，所以就可以使用 exclude 指定组件的名字（就是 name 属性）进行排除。或者可以使用生命周期函数 activated 来刷新数据（可能需要进行一定的逻辑判断·），这个函数在页面重新加载时执行。 默认情况下 webpack-server 是不支持 ip 访问的，如果就想 ip 访问，可以在 package.json 文件中的 dev 加一个配置：&quot;dev&quot;: &quot;webpack-dev-server --host 0.0.0.0 --inline --progress --config build/webpack.dev.conf.js&quot;以及可以在 devServer 下配置 host 为 0.0.0.0 的方式允许局域网访问。参考：https://github.com/bfchengnuo/MyRecord/issues/61 注意事项使用 v-for 无论是遍历数组还是遍历对象，直接使用下标增加、修改数组 View 不一定会刷新，想要视图跟着刷新就必须用方法来增加，例如数组的 pop、push 等方法；对于对象的属性增加，可以使用 Vue 的全局方法 set（Vue.set(obj, key, val) 或者使用实例的 set 方法，vm.$set(obj, key, val)），当然 set 方法也可以用来修改数组 key 就是下标了。 使用 v-for 的时候，为了不引入多余的 HTML 结构，可以使用 template 标签占位，在这个标签里使用 v-for 这样渲染后就没有痕迹了。 可以通过绑定 class 属性的方式来改变样式，支持对象、数组。 解决组件与 HTML5 规范冲突，可以使用 is 属性来标识其真正的组件，例如：&lt;tr is:&quot;row&quot;&gt;&lt;/tr&gt;。使用 Vue 提供的标签也是类似，例如：&lt;router-link tag=&quot;li&quot; :to=&quot;/index/&quot;&gt; 子组件里，data 属性必须是函数，可以是这个函数返回一个对象，里面包含一些属性；这样也就达到了多个子组件数据互不影响的目的。 必要的操作 Dom 时，通过 ref 属性来标识，在事件中可以 this.$refs.name 来获取 Dom 元素；如果 ref 加在了组件上，那么得到的就是这个组件的引用了。特殊情况下，如果 ref 和 v-for 连用，那么使用 :ref= 的形式，并且获取的是数组，需要 name[0] 使用。 子组件向父组件传值是通过事件的形式，一般来说在子组件中使用 this.$emit(&#39;name&#39;, data) 来进行手动触发；配合子组件的 HTML 标签中使用 @name=&quot;fun&quot; 来进行监听。 在导入语法中，使用 @ 来表示 src 目录；在组件样式编写的时候，如果不想影响到其他组件的样式，在 style 标签里加一个 scoped 即可。使用 @import 导入 css 变量域，~ 固定前缀：@import &#39;~@style/varibles.styl&#39;如果使用 stylus 语法，可以使用 &gt;&gt;&gt; 来做样式穿透。如果使用 less/sass 语法，可以使用 父样式名 /deep/ 子样式名 做穿透。 在 webpack 的配置文件里，可以使用 alias 来定义别名，快速引用文件夹，例如默认的 @ 表示 src 就是这样设置的。 使用定时器来截流某些场景，例如字母表滑动选择、实时搜索，这些绑定的事件会被频繁的调用，为优化效率，我们可以对其做削峰或者叫函数截流进行处理； 就是利用 setTimeout 的特性 1234567891011121314// 定义的局部变量来控制let timer// 避免第一次执行时的 NPEif (this.timer) &#123; // 当 timer 在指定的延迟内还没有执行时，又进行了调用，则会取消上次执行 // 如果已经执行，那....就执行了，也会先 clear，再进行下面的赋值 clearTimeout(this.timer)&#125;// 设置 timer 新的延迟执行方案this.timer = setTimeout(() =&gt; &#123; // do sth&#125;, 16)]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法之美-排序]]></title>
    <url>%2F2019%2F04%2F03%2F%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[排序对于任何一个程序员来说，可能都不会陌生。你学的第一个算法可能就是排序，大学里的 C 入门就是写的它吧？排序是非常重要的，但是排序算法太多了，有很多可能你连名字都没听说过，比如猴子排序、睡眠排序、面条排序等。我这里就只看最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。 排序算法 时间复杂度 是否基于比较 冒泡、插入、选择 O(n^2) 是 快排、归并 O(nlogn) 是 桶、计数、基数 O(n) 否 算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序（Sorted in place）就是特指空间复杂度是 O(1) 的排序算法，可以理解为不需要开辟很多空间的算法。 然后还有一个稳定性的概念，简单说就是排序前后，相等的元素还是原来的顺序不变，这个性质其实很重要，实际中我们都是对对象的某个属性排序，如果还能保证顺序就省事很多。比如说，我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。如果我们现在有 10 万条订单数据，我们希望按照金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。对于这样一个排序需求，我们怎么来做呢？借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的：我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。 冒泡排序冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。 实际上，刚讲的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作。 1234567891011121314151617181920private static void bubbleSort(int[] arr) &#123; int size = arr.length; if (size &lt;= 1) return; for (int i = 0; i &lt; size - 1; i++) &#123; // 提前退出冒泡循环的标志位 boolean flag = false; for (int j = 0; j &lt; size - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; int tmp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = tmp; // 表示有数据交换 flag = true; &#125; &#125; // 没有数据交换，提前退出 if (!flag) break; &#125;&#125; 冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。 为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。 对于一个给定的初始序列，移动操作的次数总是固定的。 插入排序描述下过程就是：首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间；初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。 1234567891011121314151617181920212223242526272829303132public static void insertionSort(int[] arr) &#123; int size = arr.length; if (size &lt;= 1) return; for (int i = 1; i &lt; size; i++) &#123; // 要插入的数 int value = arr[i]; // 有序区间的末尾 int j = i - 1; // 查找插入的位置 for (; j &gt;= 0; --j) &#123; if (arr[j] &gt; value) &#123; arr[j + 1] = arr[j]; // 数据移动 &#125; else &#123; break; &#125; &#125; // 插入数据 arr[j + 1] = value; &#125;&#125;// 熟练后 for 循环可以简化：while (j &gt;= 0 &amp;&amp; array[j] &gt; key) &#123; array[j + 1] = array[j]; j--;&#125;for (; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j-- ) &#123; arr[j + 1] = arr[j];&#125; 从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。 在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。 对于一个给定的初始序列，移动操作的次数总是固定的。 为什么插入更受欢迎把他们循环的最多次的代码拿出来比较下： 1234567891011121314// 冒泡排序中数据的交换操作：if (a[j] &gt; a[j+1]) &#123; // 交换 int tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; flag = true;&#125;// 插入排序中数据的移动操作：if (a[j] &gt; value) &#123; a[j+1] = a[j]; // 数据移动&#125; else &#123; break;&#125; 不要小看少的这两句语句，在数据量大的时候，就能带来显著的差距，所以尽量做到极致的话当然会选择插入排序，另外，如果喜欢插入排序，可以看看极致的改进版：希尔排序，不过它是非稳定的。 选择排序选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。 12345678910111213public static void selectionSort(int[] data) &#123; int temp; // 只循环到倒数第二个即可，最后一个没数可比 for (int i = 0; i &lt; data.length - 1; i++) &#123; for (int j = i + 1; j &lt; data.length; j++) &#123; if (data[i] &gt; data[j]) &#123; temp = data[i]; data[i] = data[j]; data[j] = temp; &#125; &#125; &#125;&#125; 这里要说明一下，选择排序是一种原地排序算法，但不是稳定的排序算法。 归并排序归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。这就是分治思想，分而治之，跟递归思想有点像。 然后重点说下将两个已经有序的数组拼装起来，我们可以先申请一个临时数组 tmp，大小与原数组 A 相同。我们用两个游标 i 和 j，分别指向 A[p…q] 和 A[q+1…r] 的第一个元素。比较这两个元素 A[i] 和 A[j]，如果 A[i]&lt;=A[j]，我们就把 A[i] 放入到临时数组 tmp，并且 i 后移一位，否则将 A[j] 放入到数组 tmp，j 后移一位。直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组 tmp 中的数据拷贝到原数组 A 中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class MergeSort &#123; // 归并排序算法, a是数组，n表示数组大小 public static void mergeSort(int[] a, int n) &#123; mergeSortInternally(a, 0, n - 1); &#125; // 递归调用函数 private static void mergeSortInternally(int[] a, int p, int r) &#123; // 递归终止条件 if (p &gt;= r) return; // 取p到r之间的中间位置q,防止（p+r）的和超过int类型最大值 int q = p + (r - p) / 2; // 分治递归 mergeSortInternally(a, p, q); mergeSortInternally(a, q + 1, r); // 将A[p...q]和A[q+1...r]合并为A[p...r] merge(a, p, q, r); &#125; private static void merge(int[] arr, int s1, int middle, int arrEnd) &#123; int start1 = s1; int start2 = middle + 1; // 申请一个大小跟a[p...r]一样的临时数组 int[] tmp = new int[arrEnd - s1 + 1]; // 临时数组的索引 int k = 0; while (start1 &lt;= middle &amp;&amp; start2 &lt;= arrEnd) &#123; if (arr[start1] &lt;= arr[start2]) &#123; tmp[k++] = arr[start1++]; &#125; else &#123; tmp[k++] = arr[start2++]; &#125; &#125; // 判断哪个子数组中有剩余的数据，默认设为第一个剩余 int start = start1; int end = middle; if (start2 &lt;= arrEnd) &#123; // 改变为第二个剩余 start = start2; end = arrEnd; &#125; // 将剩余的数据拷贝到临时数组tmp while (start &lt;= end) &#123; tmp[k++] = arr[start++]; &#125; // 将tmp中的数组拷贝回a[p...r] for (start1 = 0; start1 &lt;= arrEnd - s1; ++start1) &#123; arr[s1 + start1] = tmp[start1]; &#125; &#125;&#125; 它是稳定的算法，只要保证合并时稳定就好了。它不是原地排序算法，所以快排更加出名一些，即使它的最好情况下的时间复杂度要优于快排。 快速排序快排利用的也是分治思想，乍看和归并差不多，其实思想还是有差别的，快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。我们遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。当区间缩小为 1 的时候就可以认为它有序了。 12345678910111213141516171819202122232425262728293031323334353637383940public class QuickSort &#123; // 快速排序，a是数组，n表示数组的大小 public static void quickSort(int[] a, int n) &#123; quickSortInternally(a, 0, n - 1); &#125; // 快速排序递归函数，p,r为下标 private static void quickSortInternally(int[] a, int p, int r) &#123; if (p &gt;= r) return; int q = partition(a, p, r); // 获取分区点 quickSortInternally(a, p, q - 1); quickSortInternally(a, q + 1, r); &#125; private static int partition(int[] a, int p, int r) &#123; int pivot = a[r]; int left = p; for (int right = p; right &lt; r; ++right) &#123; if (a[right] &lt; pivot) &#123; if (left == right) &#123; ++left; &#125; else &#123; int tmp = a[left]; a[left++] = a[right]; a[right] = tmp; &#125; &#125; &#125; // 交换 pivot，返回缩小后区间的最后一个（新 pivot） int tmp = a[left]; a[left] = a[r]; a[r] = tmp; System.out.println("i=" + left); return left; &#125;&#125; 快排并不是一个稳定的算法，它是一个原地排序算法；可以发现，归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。 归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。我们前面讲过，归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。 快排 B 站有个视频讲的不错：https://www.bilibili.com/video/av39093184 桶排序桶排序，顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。 他的时间复杂度是 O(n)，只不过对数据的要求比较苛刻。首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。 比如说我们有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？ 我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名（00，01，02…99）。理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。 如果数据不能平均分，集中在一个桶里，为了避免时间复杂度的退化，那么你只能继续拆分。 计数排序计数排序其实是桶排序的一种特殊情况。当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。计数排序的算法思想就是这么简单，跟桶排序非常类似，只是桶的大小粒度不一样。 12345678910111213141516171819202122232425262728293031323334353637383940// 计数排序，arr 是数组，size 是数组大小。假设数组中存储的都是非负整数。public static void countingSort(int[] arr, int size) &#123; if (size &lt;= 1) return; // 查找数组中数据的范围 int max = arr[0]; for (int i = 1; i &lt; size; ++i) &#123; if (max &lt; arr[i]) &#123; max = arr[i]; &#125; &#125; // 申请一个计数数组 countArr，下标大小 [0,max]，初始化为 0 int[] countArr = new int[max + 1]; for (int i = 0; i &lt;= max; ++i) &#123; countArr[i] = 0; &#125; // 计算每个元素的个数，放入 countArr 中 for (int i = 0; i &lt; size; ++i) &#123; countArr[arr[i]]++; &#125; // 依次累加 for (int i = 1; i &lt;= max; ++i) &#123; countArr[i] = countArr[i - 1] + countArr[i]; &#125; // 临时数组 temp，存储排序之后的结果 int[] temp = new int[size]; // 计算排序的关键步骤 for (int i = size - 1; i &gt;= 0; --i) &#123; int index = countArr[arr[i]] - 1; temp[index] = arr[i]; countArr[arr[i]]--; &#125; // 将结果拷贝给 arr 数组 System.arraycopy(temp, 0, arr, 0, size);&#125; 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数（小数就先乘以倍数，负数就先加成正数）。 例如，对 50 万考生的成绩排名，假设满分是 900 最低是 0，数据范围很小，可以搞 901 个桶，然后将这些考试分布进这些桶里，并不需要排序，只需要扫描，时间复杂度是 O(n)。 基数排序我们再来看这样一个排序问题。假设我们有 10 万个手机号码，希望将这 10 万个手机号码从小到大排序，你有什么比较快速的排序方法呢？对于手机号，再用之前的桶排序就不合适了，因为它太大了，刚刚这个问题里有这样的规律：假设要比较两个手机号码 a，b 的大小，如果在前面几位中，a 手机号码已经比 b 手机号码大了，那后面的几位就不用看了。借助稳定排序算法，这里有一个巧妙的实现思路，先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。不过要保证在每一位的排序中，算法要是稳定的。每一位的排序就可以用上面的桶排序了~这样时间复杂度也是 O(n). 那么对于不等长的数据怎么排序（字符串为例）？我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”，因为根据 ASCII 码，所有字母都大于“0”，所以补“0”不会影响到原有的大小顺序。这样就可以继续用基数排序了。 基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序（O(n)）算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。 堆排序堆排序是一种原地的、不稳定的、时间复杂度为 O(nlogn) 的排序算法，甚至堆排序比快速排序的时间复杂度还要稳定，但是，在实际的软件开发中，快速排序的性能要比堆排序好。 什么是堆堆是一种特殊的树，只要满足这两点，它就是一个堆： 堆是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。 第一点，堆必须是一个完全二叉树。完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。 第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。实际上，我们还可以换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。 对于每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆”。 关于树的介绍，我之前写过一篇，点我跳转 实现一个堆完全二叉树比较适合用数组来存储，用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。为了方便计算，通常会把数组的 1 号索引作为根，也就是前面空一个，数组中下标为 i 的节点的左子节点，就是下标为 i∗2 的节点，右子节点就是下标为 i∗2+1 的节点，父节点就是下标为 i/2 的节点。 通过调整，让其满足堆的特性的过程就叫“堆化”，堆化实际上有两种，从下往上和从上往下；例如从下往上的，先插入到数组的最后，然后与父节点比较，不符合规则就交换，然后再跟父节点比较，直到合适为止。删除的话也有一个巧妙的方法，直接把最后一个替换到删除的元素上，然后进行调整。 实现堆排序首先，我们需要将数组原地建成一个堆，思路这里说一种，假设数组 size 是 10，编号就是 0-9：我们从后面开始，按照规律 8、9 所对应的父节点的索引是 4，所以我们比较 4 是不是比 8 和 9 的数大，如果小就交换最大的；然后继续找 6、7 对应的父节点 3，它们三个再比较，重复上面的步骤，最终就形成了符合规律的树，也就是堆。 12345678910111213141516171819202122232425262728293031323334// n 表示数据的个数，数组 a 中的数据从下标 1 到 n 的位置。// 需要注意，此算法数组需要空出第一个元素来public static void sort(int[] a, int n) &#123; buildHeap(a, n); int k = n; while (k &gt; 1) &#123; swap(a, 1, k); --k; heapify(a, k, 1); &#125;&#125;private static void buildHeap(int[] a, int n) &#123; for (int i = n / 2; i &gt;= 1; --i) &#123; heapify(a, n, i); &#125;&#125;private static void heapify(int[] a, int n, int i) &#123; while (true) &#123; int maxPos = i; if (i * 2 &lt;= n &amp;&amp; a[i] &lt; a[i * 2]) maxPos = i * 2; if (i * 2 + 1 &lt;= n &amp;&amp; a[maxPos] &lt; a[i * 2 + 1]) maxPos = i * 2 + 1; if (maxPos == i) break; swap(a, i, maxPos); i = maxPos; &#125;&#125;private static void swap(int[] arr, int a, int b) &#123; int temp = arr[a]; arr[a] = arr[b]; arr[b] = temp;&#125; 建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。有点类似删除操作，然后最大的就找出来了，依次往前，最终1就是个从小到大的有序数组了。 总结&amp;应用至于说为什么堆排序不如快排效率高，可以从两点来说：堆排序数据访问的方式没有快速排序友好，因为不是顺序访问，Cpu 缓存表示很无奈。对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序，就算是原本有序的数组，在堆化过程也会打乱的。 优先级队列在优先级队列中，就不是先进先出了，用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。 利用堆求 Top K我们可以维护一个大小为 K 的小顶堆，顺序遍历数组，从数组中取出取数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了。这样每次只需要跟堆顶数比较即可，因为它肯定是全堆最小的。 利用堆求中位数对于静态数据，当然数组求一下最好了，也不会变；如果是动态数据：我们需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。也就是说，如果有 n 个数据，n 是偶数，我们从小到大排序，那前 n/2 个数据存储在大顶堆中，后 n/2 个数据存储在小顶堆中。如果 n 是奇数，情况是类似的，大顶堆就存储 n/2+1 个数据，小顶堆中就存储 n/2 个数据。这样，大顶堆中的堆顶元素就是我们要找的中位数。 下面就是插入新数据了，如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；如果新加入的数据大于等于小顶堆的堆顶元素，我们就将这个新数据插入到小顶堆。这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况，这个时候，我们可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。例如将小顶堆的堆顶元素移动到大顶堆的堆顶。 举几个例子，合并有序小文件的时候，我们可以从很多小文件先取第一个，构建一个小顶堆，每次从堆顶取出最小的放入合并后的文件，再找到这个元素来自那个文件读取下一个。定时任务的时候，为了避免频繁扫描，可以根据时间来构造小顶堆，这样就知道最早执行的时间，避免多余的扫描。 PS：最后中位数说的那个约定：如果 n 是偶数，两个堆中的数据个数都是 n/2；如果 n 是奇数，大顶堆有 n/2+1 个数据，小顶堆有 n/2 个数据。 其他如果对小规模数据进行排序，可以选择时间复杂度是 O(n^2) 的算法；如果对大规模数据进行排序，时间复杂度是 O(nlogn) 的算法更加高效。所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数。 例如在 JDK8 中，排序函数并不是仅仅使用一种算法，在元素小于 47 的时候用插入排序，大于 47 小于 286 用双轴快排，大于 286 用 timsort 归并排序，并在 timesort 中记录数据的连续的有序段的的位置，若有序段太多，也就是说数据近乎乱序，则用双轴快排，当然快排的递归调用的过程中，若排序的子数组数据数量小，用插入排序。 娱乐时间下面就来给看看猴子排序、睡眠排序、面条排序（太蠢不说 2333），别笑。。。 睡眠排序构造 n 个线程，它们和这 n 个数一一对应。初始化后，线程们开始睡眠，等到对应的数那么多个时间单位后各自醒来，然后输出它对应的数。这样最小的数对应的线程最早醒来，这个数最早被输出。等所有线程都醒来，排序就结束了。能脑洞大开想出此算法的，绝壁天才啊。。。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485// 如果只需要输出public class SleepSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;1,4,7,3,8,9,2,6,5&#125;; //创建指定长度的线程数组 SortThread[] sortThreads = new SortThread[arr.length]; //指定每个线程数组的值 for (int i = 0; i &lt; sortThreads.length; i++) &#123; sortThreads[i] = new SortThread(arr[i]); &#125; //开启每个线程 for (int i = 0; i &lt; sortThreads.length; i++) &#123; sortThreads[i].start(); &#125; &#125; &#125;class SortThread extends Thread&#123; int s = 0; public SortThread(int s)&#123; this.s = s; &#125; public void run()&#123; try &#123; sleep(s*10+10); //睡眠指定的时间 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //输出该数 System.out.println(s); &#125; &#125; // 就算需要拿到顺序的数组，也有办法public class SleepSort &#123; public static void main(String[] args)&#123; int[] nums=&#123;9,7,2,6,15,8&#125;; SleepSort.sort(nums); for(int n:nums) System.out.printf("%d ",n); &#125; public static void sort(int[] nums)&#123; Sleeper.idx=0; Sleeper.output=new int[nums.length]; for(int i=0;i&lt;nums.length;i++) new Sleeper(nums[i]).start(); try &#123; // 主线程需要睡足够的时间，等他们都排好 // 当然可以使用其他 join、循环检查等方法 Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; for(int i=0;i&lt;nums.length;i++) nums[i]=Sleeper.output[i]; &#125;&#125;class Sleeper extends Thread&#123; public static int[] output; public static int idx; private int sleep_time; public Sleeper()&#123; this.sleep_time=0; &#125; public Sleeper(int sleep_time)&#123; this.sleep_time=sleep_time; &#125; @Override public void run()&#123; try&#123; Thread.sleep(this.sleep_time); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; output[idx++]=this.sleep_time; &#125;&#125; 猴子排序随机打乱数组，检查是否排好序，若是，则输出，否则再次打乱，再检查…最佳情况 O(n)，平均 O(n*n!)，最坏可执行直到世界的尽头。无限猴子定理 ：一只猴子随机敲打打字机键盘，如果时间足够长，总是能打出特定的文本，比如莎士比亚全集。用伪代码表示很简洁： 12while(! isOrdered(nums)) shuffle(nums);]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java性能监控与调优]]></title>
    <url>%2F2019%2F03%2F29%2FJava%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[想要有更强的竞争力，这部分一定是得学的，并且小公司的话，你也不太可能只仅仅开发，这些也要全要搞。多了解一些总不是坏事，想要走的更远的话。介绍下 JDK 或者第三方的一些工具来祝你优化你的程序！ JVM参数首先来了解下 JVM 参数的类型，一般来说，可以分为下面的三大类： 标准参数在 JVM 的各个版本中基本不变，比较稳定的。例如：-help、 -server、 -client、 -version、-cp、 -classpath X 参数非标准化参数，有可能会变，但是变化比较小。例如，解释执行：-Xint、第一次使用就编译成本地代码：-Xcomp 、混合模式（默认）：-Xmixed XX 参数使用的最多的一类参数，相对不稳定，主要用于 JVM 调优和 Debug。它还可以再分类，例如：布尔类型：-XX:[+-]&lt;name&gt; 表示启用或者禁用某个属性，启用 G1 垃圾收集器 -XX:+UseG1GC。非布尔类型，也就是 K-V 的形式：-XX:&lt;name&gt;=&lt;val&gt; 就是用来调整属性的。 我们见的最多的应该是 -Xms 和 -Xmx 了，然而它俩其实是 XX 参数，是一种简写形式。-Xms 等价于 -XX:InitialHeapSize ；-Xmx 等价于 -XX:MaxHeapSize ；-Xss 等价于 -XX:ThreadStackSize 查看JVM运行时参数使用到的参数有： -XX:+PrintFlagsInitial查看初始值 -XX:+PrintFlagsFinal查看最终值 -XX:+UnlockExperimentalVMOptions解锁实验参数，有些参数需要解锁后才可以设置。 -XX:+UnlockDiagnosticVMOptions解锁诊断参数 -XX:+PrintCommandLineFlags打印命令行参数 PrintFlagsFinal打印的值有两类，= 表示默认值；:= 表示被用户或者 JVM 修改后的值。可以在命令行中使用 java -XX:+PrintFlagsFinal -version 来体验一把。 jps专门用来查看 Java 进程的，跟 Linux 中的 ps 指令类似，可以使用 jps -l 来查看详细信息，更多的参数介绍可以在官方的文档中找到。 jinfo可以用来查看正在运行的 JVM 进程的参数，不过需要你知道参数的名字才行，例如：jinfo -flag MaxHeapSize [pid] 、查看垃圾回收器的：jinfo -flag [UseConcMarkSweepGC, UseG1GC] [pid] jstat可以查看 JVM 统计信息，例如类装载（-class）、垃圾收集（-gc）、JIT编译信息。举个例子：jstat -class [pid] 1000 10 后面两个是可选的，意思是每隔 1000ms 输出一次，一共输出 10 次。至于输出的是什么，文档里都有写，C 结尾表示的就是总量，U 结尾就是表示的已使用。 内存结构简单起见可以参考这张图： 非堆区也叫 Metaspace，是 JDK8+ 才有的，它移除了永久代的概念，使用堆外直接内存；其中的 CCS 不一定存在，当启用了指针压缩（64 -&gt; 32）才会有，CodeCache 是跟 JIT 编译相关的，还有一些其他的东西。之前也看到过 64 位的 JVM 跟 32 位的 JVM 其实变化很大的，指针膨胀就是个大问题。 关于内存溢出OOM 应该是常见的一种情况了，常见的分析思路就是看 Dump 文件，也就是内存镜像文件，发生 OOM 时自动导出这个文件可以这样配置：-XX:+HeapDumpOnOutOfMemoryError 、-XX:HeapDumpPath=./ 除了自动导出，还可以使用 jmap 命令来手动导出。例：jmap -dump:format=b,file=heap.hprof [pid] 、jmap -heap [pid] 使用MAT分析这是 Eclipse 的一个工具，非常好用，官方地址：https://www.eclipse.org/mat/载入 hprof 文件后，主页就会展示内存占用的分布情况，并且猜测那一块会有 OOM 的可能。常用的就两个功能，查看对象的数量和查看对象占用的内存，一般来说只看强引用就行。这个软件的详细用法 Google 一下。 死循环与死锁这里要介绍下 jstack 这个命令，它可以看到线程信息，当发现我们的 CPU 飙高，就有可能发生了死循环或者死锁的情况。使用 jstack 输出指定 pid 的情况，然后重定向到一个文件里，拿下来分析就好了。 PS：使用 top -p [pid] -H 命令可以查看某个进程里面的线程情况，使用 printf &quot;%x&quot; xxx 可以将十进制的 pid 转换为 16 进制。 jvisualvm可视化jvisualvm 是 JDK 自带的一个工具，使用它可以可视化的监控 Java 程序的运行情况，当然，远程的也是可以进行监控的，不过需要设置了 JMX 才行。然后，它可以安装第三方插件，推荐的两个插件是：VisualGC 和 BtraceWorkbench。 插件地址：http://visualvm.github.io/pluginscenters.html需要先把对应版本的源添加进配置里才可以下载安装。并且官网是有中文版的教程的，可以看一看。 Btrace调试它的作用是在我们的应用程序不重启不修改，正在运行的情况下动态修改字节码，达到监控调试的目的。使用步骤按照官方文档，设置个 BTRACE_HOME 的环境变量就可以了，运行方式有两种，一种直接命令行：btrace &lt;PID&gt; &lt;trace_script&gt; ，另一种可以使用 jvisualvm 插件来配合实现。脚本代码非常简单，跟我们的 Java 代码也很类似（拦截器），所以不要怕。 至于下载地址，在 Github 上搜就可以了，项目主页：https://github.com/btraceio/btrace 脚本示例： 1234567891011121314@BTracepublic class PrintArgSimple &#123; // 在那个类，那个方法，什么时候进行拦截 @OnMethod( clazz="com.imooc.monitor_tuning.chapter4.Ch4Controller", method="arg1", location=@Location(Kind.ENTRY) ) public static void anyRead(@ProbeClassName String pcn, @ProbeMethodName String pmn, AnyType[] args) &#123; BTraceUtils.printArray(args); BTraceUtils.println(pcn+","+pmn); BTraceUtils.println(); &#125;&#125; 编写脚本所依赖的几个 jar 包在你下载的安装包里都有，使用也非常简单，直接跟类名就行，他没有包的概念，并且跟程序是独立的。 使用 -cp 来指定额外的 classpath 依赖第三方的类库。 拦截方法包括构造方法也是可以的，使用的是字节码风格 &lt;init&gt;。默认是在入口的时候进行拦截，其他的拦截时机： ENTRY：入口（默认） RETURN：返回 THROW：异常 Line：行 如果你的代码有异常，但是被 try 给吃掉了，那么如何确定代码是否抛异常了呢，可以使用官方提供的一个代码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@BTrace public class PrintOnThrow &#123; // store current exception in a thread local // variable (@TLS annotation). Note that we can't // store it in a global variable! @TLS static Throwable currentException; // introduce probe into every constructor of java.lang.Throwable // class and store "this" in the thread local variable. @OnMethod( clazz="java.lang.Throwable", method="&lt;init&gt;" ) public static void onthrow(@Self Throwable self) &#123;//new Throwable() currentException = self; &#125; @OnMethod( clazz="java.lang.Throwable", method="&lt;init&gt;" ) public static void onthrow1(@Self Throwable self, String s) &#123;//new Throwable(String msg) currentException = self; &#125; @OnMethod( clazz="java.lang.Throwable", method="&lt;init&gt;" ) public static void onthrow1(@Self Throwable self, String s, Throwable cause) &#123;//new Throwable(String msg, Throwable cause) currentException = self; &#125; @OnMethod( clazz="java.lang.Throwable", method="&lt;init&gt;" ) public static void onthrow2(@Self Throwable self, Throwable cause) &#123;//new Throwable(Throwable cause) currentException = self; &#125; // when any constructor of java.lang.Throwable returns // print the currentException's stack trace. @OnMethod( clazz="java.lang.Throwable", method="&lt;init&gt;", location=@Location(Kind.RETURN) ) public static void onthrowreturn() &#123; if (currentException != null) &#123; BTraceUtils.Threads.jstack(currentException); BTraceUtils.println("====================="); currentException = null; &#125; &#125;&#125; 同样，你可以判断方法的某行代码是否执行： 1234567891011121314@BTracepublic class PrintLine &#123; @OnMethod( clazz="com.imooc.monitor_tuning.chapter4.Ch4Controller", // 支持使用正则表达式： /.*/ method="exception", location=@Location(value=Kind.LINE, line=20) ) public static void anyRead(@ProbeClassName String pcn, @ProbeMethodName String pmn, int line) &#123; BTraceUtils.println(pcn+","+pmn + "," +line); BTraceUtils.println(); &#125;&#125; 在处理方法参数类型的时候，你可以使用 AnyType 来接收，也可以使用确定的类型。 生产环境下可以使用，但是要注意，被修改的字节码是不会被还原的，除非重启 JVM。 tomcat调试使用 jpda 可以进行远程调试，相关的开启方法自行 Google，在配置文件中设置好端口后，在本地的 IDE 里直接填上就可以了。 tomcat 自带的管理界面比较简陋，更好的方案是使用 psi-probe 来监控，可以在 Github 上找到。 优化方面，主要着重的配置是： maxConnections猫能够接受和处理的最大连接数，在 8+ 版本，使用了 NIO 技术，多路复用提高了性能。使用 NIO 的情况下，默认是 10000 acceptCount当连接数超出了最大值，进入一个等待队列，这个属性控制队列的大小，默认 100 maxThreads配置工作线程的个数，默认是 200，同时并发处理的个数。 minSpareThreads最小空闲的工作线程数，不要太小。 enableLookups使用 request.getRemoteHost() 时进行 DNS 查询，建议禁用，8.5 默认禁用。 autoDeploy猫运行时，要不要周期性的检查是不是有新应用需要部署，需要开一个线程来周期性检测，生产环境要关闭，默认开启。 reloadable来监控 /WEB-INF/classes/ 和 /WEB-INF/lib 的变化，同理建议禁用，8.5 默认禁用。开发环境，用来支持热加载还是不错的。 protocol在 server.xml 文件中配置，8+ 版本默认的使用 NIO，如果是高并发可以尝试使用 APR ，它使用的是 native 方法，性能会有一定提升。 分布式情况下，如果使用了 SpringSession 类似的解决方案，建议禁用猫的 session，尤其是使用 JSP 的时候。 参考文档：docs/config/http.html、docs/config/host.html、docs/config/context.html nginx优化配置文件的解读就不说了，Google 很多，要注意的是，配置反向代理要关闭 selinux，setenforce 0。使用 nginx -V 可以查看编译参数。 使用 nginx 提供的 ngx_http_stub_status 配置来监控连接信息，要使用它需要将这个模块加入编译参数才行。 123456location = /nginx_status &#123; stub_status on; access_log off; allow 127.0.0.1; deny all;&#125; 还有一个好用的工具 ngxtop，使用 python 的 pip 包管理直接 install 就好。在 Github 上可以找到它的官方文档（不过这个项目已经不活跃了）。 12345678# 基本使用ngxtop -c /etc/nginx/nginx.conf# 查询状态是 200 的ngxtop -c /etc/nginx/nginx.conf -i 'status == 200'# 查询访问最多的 ipngxtop -c /etc/nginx/nginx.conf -g remote_addr 再来介绍另一款图形化的监控工具：Nginx-rrd，这个是基于 PHP 来做的，所以需要 PHP 的相关依赖，然后需要在 php-fpm 中跟 nginx 统一用户，具体的配置还蛮多的，可以去 Google 一下。它的原理是使用定时任务来扫描，每次都会存储成一张张的图片，最后使用 web 端来进行展示。 PS：这些监控工具都是基于 ngx_http_stub_status 来做的，所以上面那个 location 配置不能少。 一些常见的基本的 Nginx 优化： 增加工作线程数和并发连接数默认情况下，Nginx 只有一个工作线程，并发数为 1024。 启用长连接默认对客户端使用的是长连接，也可以对反向代理的后端使用长连接。 启用缓存、压缩 操作系统参数优化 下面见一个示例的配置文件： 123456789101112131415161718192021222324252627282930313233# 配置工作线程，受限于 CPU 的数量，一般配置与 CPU 的数量相等。worker_processes 1;events &#123; # 每一个线程打开的最大连接数，包含 nginx 与客户端和 nginx 与 upstream 之间的连接 # 受限于操作系统，需要修改操作系统参数，默认 1024 worker_connections 10240; # 可以一次建立多个连接 multi_accept on; use epoll;&#125;# 配置与后端的长连接示例server &#123; listen 80; server_name loli.com; # 配置反向代理的集群 upstream server_pool&#123; server localhost:8080 weight=1 max_fails=2 fail_timeout=30s; server localhost:8081 weight=1 max_fails=2 fail_timeout=30s; # 300个长连接 keepalive 300; &#125; # 配置反向代理 location / &#123; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; proxy_pass http://server_poll/; &#125;&#125; 然后可以使用 nginx -t 来测试下配置文件是否正确。开启缓存和压缩： 1234567891011121314151617181920212223242526272829303132333435363738# 开启gzipgzip on;# 启用gzip压缩的最小文件，小于设置值的文件将不会压缩gzip_min_length 1k;# gzip 压缩级别，1-10，数字越大压缩的越好，也越占用CPU时间，后面会有详细说明gzip_comp_level 2;# 进行压缩的文件类型。javascript有多种形式。其中的值可以在 mime.types 文件中找到。gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png font/ttf font/otf image/svg+xml;# 是否在http header中添加Vary: Accept-Encoding，建议开启gzip_vary on;# 禁用IE 6 gzipgzip_disable &quot;MSIE [1-6]\.&quot;;# 开启缓存location ~* ^.+\.(ico|gif|jpg|jpeg|png)$ &#123; access_log off; expires 30d;&#125;location ~* ^.+\.(css|js|txt|xml|swf|wav)$ &#123; access_log off; expires 24h;&#125;location ~* ^.+\.(html|htm)$ &#123; expires 1h;&#125;location ~* ^.+\.(eot|ttf|otf|woff|svg)$ &#123; access_log off; expires max;&#125;# 格式# expires 30s;# expires 30m;# expires 2h;# expires 30d; 操作系统参数优化： 123456789101112131415161718192021222324252627282930# /etc/sysctl.conf# ipv4 相关优化# 防止一个套接字在有过多试图连接到达时引起过载，默认为 0 表示关闭sysctl -w net.ipv4.tcp_syncookies = 1# os 的一个等待队列，默认 128，连接队列sysctl -w net.core.somaxconn = 1024# timewait 超时时间，在 tcp 的四次挥手时的状态，此状态下虽然已经断开，但是不能拿来用，没必要太长sysctl -w net.ipv4.tcp_fin_timeout = 10# os 直接使用 timewait 的连接，默认 0 表示关闭sysctl -w net.ipv4.tcp_tw_reuse = 1# timewait 回收禁用，默认 0，为了不必要的麻烦sysctl -w net.ipv4.tcp_tw_recycle = 0# 每个进程打开文件数量的限制# /etc/security/limits.conf* hard nofile 204800* soft nofile 204800* soft core unlimited* soft stack 204800# nginx 的 TCP 优化http &#123; # 减少文件在应用和内核之间拷贝 sendfile on; # 当数据包达到一定大小再发送 tcp_nopush on; # 有数据随时发送 tcp_nodelay off; ... ...&#125; 更多详细配置参考：https://wsgzao.github.io/post/sysctl/https://imququ.com/post/my-nginx-conf-for-wpo.htmlhttp://sfau.lt/b5DA5u 垃圾回收如何选择垃圾收集器呢？ 优先调整堆大小，让服务器自己来选择 如果内存小于 100M，使用串行收集器 如果是单核，并且没有停顿时间的要求，串行或者 JVM 自己选 如果允许停顿时间超过 1 秒，选择并行或者 JVM 自己选 如果响应时间很重要，并且不能超过 1 秒，使用并发收集器 几种垃圾收集器在之前的笔记里都有介绍，这里只作补充。对于并行的 GC，是有自适应的特性的，就是说你给定几个指标（吞吐量、停顿时间等）它会自动调整堆大小，但这不是最优的方案，因为动态调整也是消耗性能的。对应 Web 应用，我们还是比较关注停顿时间的，所以一般都是用并发的 GC，例如 CMS，这类收集器是对 CPU 敏感的，虽然跟用户线程并发执行，但是用户线程的 cpu 资源就少了，并且会产生浮动垃圾和空间碎片，在 G1 出现之前使用还是非常广泛的，在 J8 中官方推荐使用 G1。 G1 在 JDK7 开始提供，到 J8 已经比较成熟了，适用于大内存、低停顿的场景，在 J9 里 G1 已经成为默认的收集器，并且将 CMS 设置为废弃。在 G1 中，老年代新生代是逻辑上的称呼了，它将堆分为一个个的 Region，还有一些成为 H 区用来存放大对象（超过了 Region 的一半）。G1 中的 YoungGC 和传统的并没有什么区别，但是它没有了 FullGC，多了个 MixedGC，它回收所有 Young 和部分 Old，它也有并发标记的过程，默认堆的占有率达到 45% 就会触发。在每次 YGC 之后和 MGC 之前，会检查垃圾占比是否达到了某一个阀值，只有达到了才会发生 MGC。G1 的相关概念和参数还多得多，这里不再一一举例。 G1 最佳实践： 年轻代大小避免使用 -Xmn、 -XX:NewRatio 等显式设置，会覆盖暂停时间目标值 暂停时间目标时间不要太严苛，其吞吐量目标是 90% 的应用程序时间和 10% 的垃圾回收时间，太严苛会直接影响吞吐量。 当发生了下面的几种情况，可以考虑切换到 G1 了： 50% 以上的堆被存活对象占用 对象分配和晋升的速度变化非常大 垃圾回收时间特别长，超过了 1 秒 调优的过程就是在吞吐量和响应时间之间找平衡的过程，并且……在 J12 又带来了新的收集器 ZGC….。 参考资料：J8 JVM 规范J8文档J8 GC选择 日志分析既然是分析日志，那么首先得拿到日志，使用这些参数来开启，然后就有日志文件了，直接读也是可以的，不过可视化更方便，可以使用这个在线分析，或者使用 GCViewer（在 Github） 12345678-XX:+PrintGCDetails-XX:+PrintGCTimeStamps-XX:+PrintGCDateStamps-Xloggc:$CATALINA HOME/logs/gc.log## 下面的不用也可以-XX:+PrintHeapAtGC-XX:+PrintTenuringDistribution GC调优先来看并行的 ParallelGC，指导原则： 除非确定，否则不要设置最大堆内存 优先设置吞吐量目标 如果吞吐量目标达不到，调大最大内存，不能让 OS 使用 Swap，如果仍然达不到，降低目标 吞吐量能达到，GC 时间太长，设置停顿时间的目标 这些在 Oracle 官方文档里都有写的。关于 G1 的调优，参考上面的 G1 最佳实践规则就可以了。 调优过程要一个参数一个参数的设置，避免不知道是改的那个参数起的作用，然后这是个循序渐进的过程，不太可能一步到位的。这一块的内容，还是去慕课网看视频比较好，文字不太好描述。 其他JVM 是基于栈的架构，相比经典的基于寄存器的架构来说，它的指令更短，但是指令数量会更多。例如一个经典的问题，i++ 和 ++i 那个效率高，在 fori 循环里是很常用的，可以分析字节码，会发现这两种形式翻译的字节码是一样的，所以他们的效率其实是一样的。然后再看下面的一段代码： 12345678public String test()&#123; String s = "hello"; try&#123; return s; &#125;finally&#123; s = "ll" &#125;&#125; 那么到底返回的是那个呢，答案是 hello，通过字节码可以看出，因为字符串是不可变对象，会将 hello 和 ll 同时压入两个本地变量，在走到 return 的时候把第一个也就是 hello 返回，然后又将第二个 ll 赋值给了第一个本地变量。 我们知道在使用 + 进行字符串拼接时，默认会转换为 StringBuilder，那么这是绝对的么？ 12345678910111213public static void f1()&#123; final String s1 = "hello"; final String s2 = s1 + "Loli"; String s3 = s1 + s2; System.out.println(s3);&#125;public static void f2()&#123; final String s1 = "hello"; String s2 = s1 + "Loli"; String s3 = s1 + s2; System.out.println(s3);&#125; 可以去分析字节码，这里就直接说结论，当 String 类型为 final 时，是在编译阶段就直接确定了，不会再进行拼接。关于 String，还有一个很有趣的地方： 1234567891011public static void main(String[] args)&#123; String s = new String("1"); s.intern(); String s2 = "1"; System.out.print1n(s == s2); String s3 = new String("1") + new String("1"); s3.intern(); String s4 = "11"; System.out.print1n(s3 == s4);&#125; 在 JDK7- 和 JDK7+ 结果是不一样的，之前是两个 false，之后变成了 false 、 true，原因就是在 J7 之后字符串常量池移到了堆中，当使用 intern 方法并且常量池没有，堆中有的情况下，会将堆中的这个引用放到常量池中（常量池已经在堆中了），这样 s3 和 s4 就相等了。G1 还有个字符串去重的功能（需要手动开启），也就是我们程序中字符串常量是占了很大比重的，而堆中和常量池中都有的话是很浪费的，当堆中字符串生命周期很长（有个阀值）就会触发去重操作。 常见的代码优化： 尽量重用对象，不要循环创建对象，比如：for 循环字符串拼接 容器类初始化的时候指定长度（List、Map） 集合遍历尽量减少重复计算（例如条件是不确定的 size） 尽量使用 Entry 来遍历 Map 尽量使用基本类型而不是包装类型 及时消除过期对象的引用，防止内存泄露 尽量使用局部变量，减小变量的作用域 ThreadLocal 缓存线程不安全的对象 SimpleDateFormat（J8 可以直接用新的 API DateTimeFormat 它是线程安全的） 尽量使用延时加载，例如单例模式（内部静态类） 尽量减少使用反射，用的话也尽量加缓存 慎用异常，不要用抛异常来表示正常业务逻辑，因为抛异常是比较重的操作，但是也别为了这个而不用，有些地方该用就得用。 String 操作尽量少用正则 又一个有趣的例子： 1234567891011public void test()&#123; Integer a = 100; Integer b = 100; // true System.out.println(a == b); Integer c = 1000; Integer d = 1000; // false System.out.println(a == b);&#125; 这是因为 Integer 会自动缓存一个字节的数字，如果在缓存中直接返回，如果不在才 new，其他有的包装类型也有部分有缓存，比如 Long，不过 Double 这种肯定是没有的啦。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[体验ZooKeeper]]></title>
    <url>%2F2019%2F02%2F26%2F%E4%BD%93%E9%AA%8CZooKeeper%2F</url>
    <content type="text"><![CDATA[Apache ZooKeeper 是 Apache 软件基金会的一个软件项目，他为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册，是主要用于分布式中一致性处理的框架。ZooKeeper 曾经是 Hadoop 的一个子项目，但现在是一个独立的顶级项目。ZooKeeper 的架构通过冗余服务实现高可用性。ZooKeeper 节点将它们的数据存储于一个分层的命名空间，非常类似于一个文件系统或一个前缀树结构。客户端可以在节点读写，从而以这种方式拥有一个共享的配置服务。更新是全序的。 Zookeeper 从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些观察者做出相应的反应。 所谓的一致性，实际上就是围绕着“看见”来的。谁能看见？能否看见？什么时候看见？也对应着：强一致性、弱一致性、最终一致性。 ZK特点 一个领导者（Leader），多个跟随者（Follower）组成的集群。 集群中只要有半数以上（不包括一半）节点存活，Zookeeper 集群就能正常服务（所以集群最少 3 推荐奇数个）。 全局数据一致：每个 Server 保存一份相同的数据副本，Client 无论连接到哪个 Server，数据都是一致的。 更新请求顺序进行，来自同一个 Client 的更新请求按其发送顺序依次执行。 数据更新原子性，一次数据更新要么成功，要么失败。 实时性，在一定时间范围内，Client 能读到最新数据。 ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个节点称做一个 ZNode。每一个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过其路径唯一标识。 需要注意的是 ZK 的选举过程是非常复杂的，也就是耗时比较长，选举过程整个集群不可用，所以说 ZK 是 CP 的，相对的 Eureka 则是 AP 设计的，他们的比较看这里就够了。 为什么要保证半数以上？从概念上来说，ZooKeeper 它所做的就是确保对 Znode 树的每一个修改都会被复制到集合体中超过半数的机器上。如果少于半数的机器出现故障，则最少有一台机器会保存最新的状态，其余的副本最终也会更新到这个状态。 应用场景ZK 提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。 统一命名在分布式场景下，经常需要对应用/服务进行统一命名，便于识别，例如 ip 和域名的关系。 统一配置管理分布式中配置文件同步是常见的，一般要求在一个集群中，所有节点的配置是一致的。对某一个节点的配置修改后也会快速同步到其他节点上。因为客户端在监听，一旦 ZNode 中的数据变化，ZK 将通知各个客户端。 统一集群管理分布式中，实时掌握每个节点的状态是必要的，可以根据节点的状态做出一些调整。ZK 会将节点的相关信息一起写入到 ZNode 中。 服务器动态上下线客户端能够实时的洞察服务器的上下线情况。因为服务器注册的时候创建的是临时节点，服务器下线后会自动删除，ZK 会通知监听的客户端，然后客户端会去重新获取列表。 软负载均衡ZK 可以记录每台服务器的访问次数，让访问最少的来处理最新的客户端请求。 安装ZK本地安装windows 版的没啥好说的，就说说 linux 版，正常情况应该是搞集群的，不过测试用机器性能有限，开一个就行了。 1234567891011121314# 解压tar -zxvf zookeeper-3.x.tar.gz -C /opt/module/# 进入 conf 文件夹修改配置文件名# 进入配置文件将 dataDir= 修改到指定的目录，需要自己创建文件夹，例如 zkDatamv zoo_sample.cfg zoo.cfg# 启动bin/zkServer.sh start# 查看进程使用 jps# 查看状态bin/zkServer.sh status# 启动客户端，退出 quitbin/zkCli.sh 其他常用配置参数： tickTime通信心跳数，Zookeeper 服务器与客户端心跳时间，单位毫秒。它用于心跳机制，并且设置最小的 session 超时时间为两倍心跳时间。 initLimit集群中的 Follower（跟随者）服务器与 Leader（领导者）服务器之间初始连接时能容忍的最多心跳数（总时间就是它乘以 tickTime），用它来限定集群中的 Zookeeper 服务器连接到 Leader 的时限。 syncLimit同步通信时限，集群中 Leader 与 Follower 之间的最大响应时间单位，假如响应超过 syncLimit * tickTime，Leader 认为 Follwer 死掉，从服务器列表中删除 Follwer。 clientPort客户端连接的端口 客户端操作的常用命令： 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前 znode 中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建 -s 含有序列 -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 集群安装解压跟上面一样，如果安装了 rsync 远程同步工具，可以使用 xsync 命令同步过去，然后在设置的数据文件夹目录下创建一个 myid 的文件，在里面写上对应的编号。然后在所有的配置文件中加入： 1234567891011#######################cluster##########################server.2=hostname102:2888:3888server.3=hostname103:2888:3888server.4=hostname104:2888:3888# server.A=B:C:D# A 是一个数字，表示这个是第几号服务器，就是 myid 文件里写的那个数字，# Zookeeper 启动时读取此文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是哪个 server。# B 是这个服务器的 ip 地址；# C 是这个服务器与集群中的 Leader 服务器交换信息的端口；# D 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 最后分别启动就可以了，可以使用查看状态来查看。 结构体客户端使用 get 或者 ls2 命令获取到的信息解释： 1234567891011121314151617181920212223czxid - 创建节点的事务 zxid 每次修改 ZooKeeper 状态都会收到一个 zxid 形式的时间戳，也就是 ZooKeeper 事务 ID。 事务 ID 是 ZooKeeper 中所有修改总的次序。每个修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1 在 zxid2 之前发生。ctime - znode被创建的毫秒数(从1970年开始)mzxid - znode最后更新的事务zxidmtime - znode最后修改的毫秒数(从1970年开始)pZxid - znode最后更新的子节点zxidcversion - znode子节点变化号，znode子节点修改次数dataversion - znode数据变化号aclVersion - znode访问控制列表的变化号ephemeralOwner - 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。dataLength - znode的数据长度numChildren - znode子节点数量 主要留意最后两个就好了。 重点内容选举机制半数机制：集群中半数以上机器存活，集群可用。所以 Zookeeper 适合安装奇数台服务器。Zookeeper 虽然在配置文件中并没有指定 Master 和 Slave。但是，Zookeeper 工作时，是有一个节点为 Leader，其他则为 Follower，Leader 是通过内部的选举机制临时产生的。 选举机制简单说就是每个服务都首先选自己，如果超过了集群数量的半数以上，那么选举结果即确定，否则放弃选自己继而选择 id 比自己大的一个。假设集群里有五台服务器，id 编号 1-5，依次启动他们。 服务器 1 启动，此时只有它一台服务器启动了，它首先选择自己，但是发出去的报文没有任何响应，所以它的选举状态一直是 LOOKING 状态。 服务器 2 启动，它与最开始启动的服务器 1 进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以 id 值较大的服务器 2 胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是 3)，所以服务器 1、2 还是继续保持 LOOKING 状态。 服务器 3 启动，根据前面的理论分析，服务器 3 成为服务器 1、2、3 中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的 Leader。 服务器 4 启动，根据前面的分析，理论上服务器 4 应该是服务器 1、2、3、4 中最大的，但是由于前面已经有半数以上的服务器选举了服务器 3，所以只能作为 Follower，第五个也是类似。 真实的选举机制是非常复杂的，所以耗时长，Leader 选举是保证分布式数据一致性的关键所在，期间还需要对投票是否有效等进行检查，一般会进行多轮投票才会选出。 非初始化的情况下，通常那台服务器上的数据越新（ZXID 会越大），其成为 Leader 的可能性越大，也就越能够保证数据的恢复。如果 ZXID 相同，则 SID 越大机会越大。 Zookeeper 的核心是原子广播，这个机制保证了各个 Server 之间的同步。实现这个机制的协议叫做 Zab 协议（Zookeeper Atomic broadcast protocol）。Zab 协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。 当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 Server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 Server 具有相同的系统状态。 仅靠 Paxos 不能满足 ZooKeeper 需求。ZooKeeper 是一个树形结构，很多操作都要先检查才能确定能不能执行，例如创建 /a/b 之前要先确定 /a 的存在，我们就能从此看出 Paxos 的一致性达不到 ZooKeeper 一致性的要求，所以就有了 Zab 协议，它保证了： 同一个 leader 的发起的事务要按顺序被 apply，同时还要保证只有先前的 leader 的所有事务都被 apply 之后，新选的 leader 才能在发起事务。 一些已经 Skip 的消息，需要仍然被 Skip。 当 leader 崩溃或者 leader 失去大多数的 follower，这时候 zk 进入恢复模式，恢复模式需要重新选举出一个新的 leader，让所有的 Server 都恢复到一个正确的状态（包括数据的恢复）。Zk 的选举算法有两种：一种是基于 basic paxos 实现的，另外一种是基于 fast paxos 算法实现的；系统默认的选举算法为 fast paxos。选完 leader 以后，zk 就进入状态同步过程。 关于恢复模式。 选好新 leader 后它通过一个多数派获得老 leader 提交的最新数据；老 leader 重启后，可能还会认为自己是 leader，可能会继续发送未完成的请求，从而因为两个 leader 同时存在导致算法过程失败；解决办法是把 leader 信息加入每条消息的 id 中，Zookeeper 中称为 zxid 为一 64 位数字，高 32 位为 leader 信息又称为 epoch，每次 leader 转换时递增；低 32 位为消息编号，leader 转换时应该从 0 重新开始编号。通过 zxid，follower 能很容易发现请求是否来自老 leader，从而拒绝老 leader 的请求。新 leader 首先要获得大多数节点的支持，然后从状态最新的节点同步事务，完成后才可正式成为 leader 发起事务。 新 Leader 等待 server 连接； Follower 连接 leader，将最大的 zxid 发送给 leader； Leader 根据 follower 的 zxid 确定同步点； 完成同步后通知 follower 已经成为 uptodate 状态； Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了。 详细分析见：https://www.cnblogs.com/binyue/p/4270393.htmlhttp://www.cnblogs.com/leesf456/p/6107600.htmlhttps://www.cnblogs.com/sunddenly/p/4138580.htmlhttps://www.jianshu.com/p/e126bb01331c 节点类型节点的类型大体可分为两类： 持久（Persistent）：客户端和服务器端断开连接后，创建的节点不删除 短暂（Ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 其中每一种里又细分为两类，目录节点和顺序编号目录节点，就如同名字一类会在后面加一个递增的序号，由父节点负责维护，可以用于排序，记录事件的顺序。 监听器当 ZK 客户端运行后，会创建两个线程，一个负责网络连接通信（connect）一个负责监听（listener）；通过 connect 线程将注册监听事件发给 ZK，ZK 收到后会将其加入到监听列表，类似 ip:port:监听内容 这样的形式。满足条件后，ZK 就会按照监听列表告诉客户端，然后 listener 线程会回调 process 方法。监听一般分为两类，一种是数据变化的监听，一种是子节点变化的监听。 写数据流程首先 Client 向 ZooKeeper 的 Server1 上写数据，发送一个写请求。如果 Server1 不是 Leader，那么 Server1 会把接受到的请求进一步转发给 Leader，这个 Leader 会将写请求广播给各个 Server，各个 Server 写成功后就会通知 Leader。当 Leader 收到大多数 Server 数据写成功了，那么就说明数据写成功了。如果这里三个节点的话，只要有两个节点数据写成功了，那么就认为数据写成功了。认为写成功之后，Leader 会告诉 Server1 数据写成功了，Server1 会进一步通知 Client 数据写成功了，这时就认为整个写操作成功。 所以，你可以知道 Zookeeper 并不保证读取的是最新数据，也就是强一致性，它只能保证最终一致性，同时它具有原子性：更新操作要么成功要么失败，没有中间状态。而只需要一半以上写入完成即可认为成功也就好理解了，首先写操作只能由 Leader 发起，那么它肯定是最新的，后续可以同步给其他的 Follower，即使还没同步 Leader 就挂掉了，那么依然能保证新的 Leader 是有最新数据的，这就是靠 Zab 协议中的恢复模式了。 代码示例使用原生 API 的基本操作： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class SimpleZkClient &#123; private static final String connectString = "192.168.169.129:2181,192.168.169.130:2181,192.168.169.131:2181"; private static final int sessionTimeout = 2000; ZooKeeper zkClient = null; @Before public void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数 System.out.println(event.getType() + "---" + event.getPath()); try &#123; zkClient.getChildren("/", true); &#125; catch (Exception e) &#123; &#125; &#125; &#125;); &#125; // 获取子节点 @Test public void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren("/", true); for (String child : children) &#123; System.out.println(child); &#125; Thread.sleep(Long.MAX_VALUE); &#125; // 创建数据节点到 zk 中 @Test public void testCreate() throws KeeperException, InterruptedException &#123; // 参数1：要创建的节点的路径 // 参数2：节点的数据 // 参数3：节点的权限 // 参数4：节点的类型 String nodeCreated = zkClient.create("/servers", "hellozk".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; // 判断节点是否存在 @Test public void testExist() throws KeeperException, InterruptedException &#123; Stat stat = zkClient.exists("/java6", false); System.out.println(stat == null ? "not exist" : "exist"); &#125; // 获取节点的数据 @Test public void getData() throws KeeperException, InterruptedException &#123; byte[] data = zkClient.getData("/java6", false, null); System.out.println(new String(data)); &#125; // 删除节点 @Test public void deleteZnode() throws InterruptedException, KeeperException &#123; // 参数2：指定要删除的版本，-1 表示删除所有版本 zkClient.delete("/java6", -1); &#125; // 更新节点数据 @Test public void setData() throws KeeperException, InterruptedException &#123; zkClient.setData("/java6", "I Miss you".getBytes(), -1); byte[] data = zkClient.getData("/java6", false, null); System.out.println(new String(data)); &#125;&#125; 服务器节点动态上下线示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * 服务端代码 */public class DistributedServer &#123; private static final String connectString = "192.168.169.129:2181,192.168.169.130:2181,192.168.169.131:2181"; private static final int sessionTimeout = 2000; private static final String parentNode = "/servers"; ZooKeeper zkClient = null; public static void main(String[] args) throws Exception&#123; // 获取zk连接 DistributedServer server = new DistributedServer(); server.getConnect(); // 利用zk连接注册服务器信息 server.registerServer(args[0]); // 启动业务功能 server.handleBussiness(args[0]); &#125; /** * 创建到zk的客户端连接 */ public void getConnect() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123;&#125; &#125;); &#125; /** * 向zk集群注册服务器信息 */ public void registerServer(String hostname) throws Exception&#123; // 创建临时序号节点 String create = zkClient.create(parentNode + "/server", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname + "is online.." + create); &#125; /** * 业务功能 */ public void handleBussiness(String hostname) throws InterruptedException &#123; System.out.println(hostname + "start working....."); Thread.sleep(Long.MAX_VALUE); &#125;&#125;/** * 客户端代码 */public class DistributeClient &#123; private static String connectString = "hadoop102:2181,hadoop103:2181,hadoop104:2181"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = "/servers"; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 获取servers的子节点信息，从中获取服务器信息列表 client.getServerList(); // 业务进程启动 client.business(); &#125; // 创建到zk的客户端连接 public void getConnect() throws IOException &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 需要再次启动监听 try &#123; getServerList(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // 获取服务器列表信息 public void getServerList() throws Exception &#123; // 1.获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); ArrayList&lt;String&gt; servers = new ArrayList&lt;&gt;(); // 2.遍历所有节点，获取节点中的主机名称信息 for (String child : children) &#123; byte[] data = zk.getData(parentNode + "/" + child, false, null); servers.add(new String(data)); &#125; System.out.println(servers); &#125; // 业务功能 public void business() throws Exception&#123; System.out.println("client is working ..."); Thread.sleep(Long.MAX_VALUE); &#125;&#125; 对于集群来说，无论是服务器还是消费端，都是客户端；服务端就是向集群添加信息，消费端就是监听信息。 节点权限ZK 的节点有 5 种操作权限：CREATE、READ、WRITE、DELETE、ADMIN 也就是 增、删、改、查、管理权限，这 5 种权限简写为 crwda 。注：这 5 种权限中，delete 是指对子节点的删除权限，其它 4 种权限指对自身节点的操作权限。 身份的认证有 4 种方式：world：默认方式，相当于全世界都能访问auth：代表已经认证通过的用户(cli 中可以通过 addauth digest user:pwd 来添加当前上下文中的授权用户)digest：即 用户名:密码 这种方式认证，这也是业务系统中最常用的ip：使用 Ip 地址认证。 Java API 中，Ids.OPEN_ACL_UNSAFE ：默认匿名权限；Ids.READ_ACL_UNSAFE ：只读权限； CREATOR_ALL_ACL ：给创建该 znode 连接所有权限。 事实上这里是采用了 auth 验证模式，使用 sessionID 做验证，所以创建该 znode 的连接可以对该 znode 做任何修改。 关于服务发现相比 ZK，现在也有了不少其他优秀的选择，感兴趣的可以了解下 etcd、consul、Eureka。 Feature Consul zookeeper etcd euerka 服务健康检查 服务状态，内存，硬盘等 (弱)长连接，keepalive 连接心跳 可配支持 多数据中心 支持 — — — kv 存储服务 支持 支持 支持 — 一致性 raft paxos raft — CAP CA CP CP AP 使用接口(多语言能力) 支持http和dns 客户端 http/grpc http（sidecar） watch支持 全量/支持long polling 支持 支持 long polling 支持 long polling/大部分增量 自身监控 metrics — metrics metrics 安全 acl /https acl https支持（弱） — SpringCloud集成 已支持 已支持 已支持 已支持 真的是百家争鸣 o(￣▽￣)ゞ))￣▽￣)o 参考https://blog.51cto.com/zero01/2108483https://tonydeng.github.io/2015/10/19/etcd-application-scenarios/https://luyiisme.github.io/2017/04/22/spring-cloud-service-discovery-products/]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式中的那把锁]]></title>
    <url>%2F2019%2F01%2F25%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E7%9A%84%E9%82%A3%E6%8A%8A%E9%94%81%2F</url>
    <content type="text"><![CDATA[现在的面试，动不动就微服务、分布式、高并发、缓存、并发编程等，不管用不用得到，你反正得会才行，分布式锁也算是很重要的一块，之前我在 Github 的 issues 中写过，现在单独摘出来再总结下，因为真的问的太多了。 分布式锁，是控制分布式系统之间同步访问共享资源的一种方式。在分布式系统中，常常需要协调他们的动作。如果不同的系统或是同一个系统的不同主机之间共享了一个或一组资源，那么访问这些资源的时候，往往需要互斥来防止彼此干扰来保证一致性，在这种情况下，便需要使用到分布式锁。 传统实现分布式锁的方案一般是利用持久化数据库（如利用 InnoDB 行锁，或事务、乐观锁），大部分时候可以满足大部分人的需求。而如今互联网应用的量级已经几何级别的爆发，利用诸如 zookeeper、redis 等更高效的分布式组件来实现分布式锁，可以提供高可用的更强壮的锁特性，并且支持丰富化的使用场景。 开源实现已有不少比如 Redis 作者基于 Redis 设计的 Redlock、Redission 等。 常见的分布式锁的实现： Memcached 分布式锁利用 Memcached 的 add 命令。此命令是原子性操作，只有在 key 不存在的情况下，才能 add 成功，也就意味着线程得到了锁。 Redis 分布式锁和 Memcached 的方式类似，利用 Redis 的 setnx 命令。此命令同样是原子性操作，只有在 key 不存在的情况下，才能 set 成功。（setnx 命令并不完善，后续可能会介绍替代方案） Zookeeper 分布式锁利用 Zookeeper 的顺序临时节点，来实现分布式锁和等待队列。Zookeeper 设计的初衷，就是为了实现分布式锁服务的。 ChubbyGoogle 公司实现的粗粒度分布式锁服务，底层利用 Paxos 一致性算法。 Etcd后起之秀，从读写性能、可靠性、可用性、安全性和复杂度等方面综合考量，它完全媲美业界 “名宿” ZooKeeper，在有些方面，Etcd 甚至超越了 ZooKeeper。 这里也就说说他们实现的原理，具体的代码并不会完整的贴出来。 Memcached实现Memcached 是一个自由开源的，高性能，分布式内存对象缓存系统。Memcached 是一种基于内存的 key-value 存储，用来存储小块的任意数据（字符串、对象）。这些数据可以是数据库调用、API 调用或者是页面渲染的结果。Memcached 简洁而强大。它的简洁设计便于快速开发，减轻开发难度，解决了大数据量缓存的很多问题。它的 API 兼容大部分流行的开发语言。本质上，它是一个简洁的 key-value 存储系统。一般的使用目的是，通过缓存数据库查询结果，减少数据库访问次数，以提高动态Web应用的速度、提高可扩展性。分布式锁也就是用了 add 操作原子性的特点，用伪代码表示： 12345678910111213if (mc.Add("LockKey", "Value", expiredTime))&#123; //得到锁 try&#123; //do business function //检查超时 if (!CheckedTimeOut())&#123; mc.Delete("LockKey"); &#125; &#125; catch (Exception e)&#123; mc.Delete("LockKey"); &#125;&#125; 不过，现在大部分都用 Redis 来搞了。 与Redis比较看到这里就不得不说它和 Redis 的区别了： Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash, 有序&amp;无序列表 数据结构的存储。 Redis支持数据的备份，即 master-slave 模式的数据备份。 Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 Redis 可以实现主从复制，实现故障恢复。 Redis 的 Sharding 技术： 很容易将数据分布到多个 Redis 实例中 Redis 支持服务器端的数据操作 使用简单的 key-value 存储的话，Memcached 的内存利用率更高，而如果 Redis 采用 hash 结构来做 key-value 存储，由于其组合式的压缩，其内存利用率会高于 Memcached。 由于 Redis 只使用单核，而 Memcached 可以使用多核，单实例吞吐量极高，可以达到几十万 QPS，但是平均每一个核上 Redis 在存储小数据时比 Memcached 性能更高。 Memcached 是多线程，分为监听线程、worker 线程，引入锁，带来了性能损耗。Redis 使用单线程的 IO 复用模型，将速度优势发挥到最大，也提供了较简单的计算功能 。 Redis 中有不少好用的命令，例如 getset，比先 get 然后再 set 来回的网络开销不知道好了多少倍。不过还是要根据实际情况来选择使用。 为什么 Redis 采用单核单线程？ 因为 CPU 不是 Redis 的瓶颈。Redis 的瓶颈最有可能是机器内存或者网络带宽。既然单线程容易实现，而且 CPU 不会成为瓶颈，那就顺理成章地采用单线程的方案了。PS：普通笔记本轻松处理每秒几十万的请求 如果万一 CPU 成为你的 Redis 瓶颈了，或者，你就是不想让服务器其他核闲置，那怎么办？ 那也很简单，你多起几个 Redis 进程就好了。Redis 是 key-value 数据库，又不是关系数据库，数据之间没有约束。只要客户端分清哪些 key 放在哪个 Redis 进程上就可以了。redis-cluster 可以帮你做的更好。 Redis实现 参见笔记地址：MyRecord 使用 Redis 实现分布式锁首先要先知道几个 Redis 的命令，分布式锁就是通过这几个命令来实现的 setnx只有不存在的时候，setnx 才会设置值成功；可以理解为是否存在和设置值这两条命令的集合版，不过是原子性的。 getset先 get 再 set，也是两条命令的整合，具有原子性。 expire设置有效期 del删除 实现原理-流程首先使用 setnx 存入一个值，key 为锁名，val 为当前的时间戳加一个超时时间，这是为了防止死锁。 仔细看这个架构好像有点问题，因为我们设置的 val 根本没用，也没有任何的防死锁措施，只是实现比较简单而已，更完善的第二版在这： 当获取锁失败时，为了防止死锁，我们还需要进行一些判断，只要判定时间已经超时，就可以认为可以尝试去得到锁，然后接下来判断新的值写进去了没，只有新的时间戳写进去了才能认为是得到锁了，这样基本就不会出现死锁的情况了，下面来看看具体的代码。 第一版按照有瑕疵的第一张流程实现： 123456789101112131415161718192021222324252627282930313233public void closeOrderTaskV1()&#123; log.info("关闭订单定时任务启动"); long lockTimeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout","5000")); Long setnxResult = RedisShardedPoolUtil .setnx( Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK, String.valueOf(System.currentTimeMillis()+lockTimeout) ); if(setnxResult != null &amp;&amp; setnxResult.intValue() == 1)&#123; // 如果返回值是 1，代表设置成功，获取锁 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else&#123; log.info("没有获得分布式锁:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; log.info("关闭订单定时任务结束");&#125;private void closeOrder(String lockName)&#123; // 有效期5秒，防止死锁 RedisShardedPoolUtil.expire(lockName,5); log.info("获取&#123;&#125;,ThreadName:&#123;&#125;", Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK, Thread.currentThread().getName()); int hour = Integer.parseInt(PropertiesUtil .getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); // 删除 key，释放锁 RedisShardedPoolUtil.del(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); log.info("释放&#123;&#125;,ThreadName:&#123;&#125;", Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK, Thread.currentThread().getName());&#125; 很显然，这个防不了死锁，我们设置的超时时间也没用到，当执行到 closeOrder 方法之前宕掉的话，那么因为这个 key 没有设置有效期，就会到期其他模块一直进不去。closeOrder 中的设置有效期和执行后的删除键（释放锁）也是双重防死锁，这个有效期需要根据线上运行的实际情况来得出一个合理的时间。 第二版循序渐进，来看看如何解决死锁问题： 123456789101112131415161718192021222324252627282930313233@Scheduled(cron="0 */1 * * * ?")public void closeOrderTaskV2()&#123; log.info("关闭订单定时任务启动"); long lockTimeout = Long.parseLong(PropertiesUtil.getProperty("lock.timeout","5000")); Long setnxResult = RedisShardedPoolUtil.setnx( Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK, String.valueOf(System.currentTimeMillis()+lockTimeout)); if(setnxResult != null &amp;&amp; setnxResult.intValue() == 1)&#123; closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else&#123; //未获取到锁，继续判断，判断时间戳，看是否可以重置并获取到锁 String lockValueStr = RedisShardedPoolUtil.get(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); if(lockValueStr != null &amp;&amp; System.currentTimeMillis() &gt; Long.parseLong(lockValueStr))&#123; String getSetResult = RedisShardedPoolUtil.getSet( Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK, String.valueOf(System.currentTimeMillis()+lockTimeout)); // 根据返回的旧值，判断是否可以获取锁 if(getSetResult == null || (getSetResult != null &amp;&amp; StringUtils.equals(lockValueStr,getSetResult)))&#123; //已真正获取到锁 closeOrder(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125;else&#123; log.info("没有获取到分布式锁:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125;else&#123; log.info("锁未失效，没有获取到分布式锁:&#123;&#125;",Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); &#125; &#125; log.info("关闭订单定时任务结束");&#125; 这样看上去基本就是万无一失了，前半段并不需要修改，我们在 else 后做了一个超时判断，来觉得是否可以重置锁，这个判断可是不简单呢。首先通过 get 方法来获取 val，用这个 val 和当前时间的时间戳来判断是否超时，然后我们使用 getset 方法重新获取老值，并且重新设置超时时间（原子操作）；根据返回的旧值，判断是否可以获取锁，这里会有三种情况： 当 key 没有旧值时，即 key 不存在时，返回 nil 对应 Java 中的 Null 这说明其他分布式程序已经执行完使用 del 删除了键（释放了锁）或者过了 Redis 的生存时间； 这时可以安全获取锁。 当 key 有旧值，并且旧值和之前获取的一致的情况下 这说明这段时间没有程序操作这把锁，并且因为 getset 之后重新设置了有效期，可以保证现在也是安全的，可以获取锁。 当 key 有旧值，并且旧值和之前获取的不一致的情况下 这说明在程序执行期间有其他的分布式模块也操作了这把锁，并且对方比较快，先执行了 getset 这就导致两个旧值对不起来，这种情况下只能放弃，等待下次获取。 使用Redisson先来看看基本的介绍： Redisson 是架设在 Redis 基础上的一个 Java 驻内存数据网格（In-Memory Data Grid）。充分的利用了 Redis 键值数据库提供的一系列优势，基于 Java 实用工具包中常用接口，为使用者提供了一系列具有分布式特性的常用工具类。使得原本作为协调单机多线程并发程序的工具包获得了协调分布式多机多线程并发系统的能力，大大降低了设计和研发大规模分布式系统的难度。同时结合各富特色的分布式服务，更进一步简化了分布式环境中程序相互之间的协作。Redisson 采用了基于 NIO 的 Netty 框架，不仅能作为 Redis 底层驱动客户端，具备提供对 Redis 各种组态形式的连接功能，对 Redis 命令能以同步发送、异步形式发送、异步流形式发送或管道形式发送的功能，LUA 脚本执行处理，以及处理返回结果的功能，还在此基础上融入了更高级的应用方案。Redisson 生而具有的高性能，分布式特性和丰富的结构等特点恰巧与 Tomcat 这类服务程序对会话管理器（Session Manager）的要求相吻合。利用这样的特点，Redisson 专门为 Tomcat 提供了会话管理器（Tomcat Session Manager）。在此不难看出，Redisson 同其他 Redis Java 客户端有着很大的区别，相比之下其他客户端提供的功能还仅仅停留在作为数据库驱动层面上，比如仅针对 Redis 提供连接方式，发送命令和处理返回结果等。像上面这些高层次的应用则只能依靠使用者自行实现。 可以看出 Redisson 对分布式一些工具做了很好的封装，如今分布式盛行的年代下，越来越多的项目使用 Redisson 作为 Redis 的客户端，使用它可以更方便的使用 Redis 分布式锁，来看第三版： 12345678910111213141516171819202122232425262728public void closeOrderTaskV3()&#123; RLock lock = redissonManager .getRedisson() .getLock(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK); boolean getLock = false; try &#123; if(getLock = lock.tryLock(0,50, TimeUnit.SECONDS))&#123; log.info("Redisson获取到分布式锁:&#123;&#125;,ThreadName:&#123;&#125;", Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK, Thread.currentThread().getName()); int hour = Integer .parseInt(PropertiesUtil.getProperty("close.order.task.time.hour","2")); orderService.closeOrder(hour); &#125;else&#123; log.info("Redisson没有获取到分布式锁:&#123;&#125;,ThreadName:&#123;&#125;", Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK, Thread.currentThread().getName()); &#125; &#125; catch (InterruptedException e) &#123; log.error("Redisson分布式锁获取异常",e); &#125; finally &#123; if(!getLock)&#123; return; &#125; lock.unlock(); log.info("Redisson分布式锁释放锁"); &#125;&#125; 这段代码中使用了 Redisson 提供的 RLock 对象来获取、释放锁，这其实是一种可重入锁，Redisson 还提供了其他的多种锁，就不多说了；用这个来实现分布式锁原理其实是一样的，只不过被 Redisson 封装后更加的简单了。使用 RLock 的 tryLock 方法来尝试获取锁，可以使用三个参数的构造，第一个是最多等待时间（超时就直接过了），第二个是自动解锁时间，第三个是时间单位。这里的等待时间如果预估不准可以写 0，否则就会出现同时获得锁的情况，也就是程序执行的太快，还没超过等待时间所以又被第二个拿到了。 其他另外，关掉 Tomcat 的时候如果你不是直接 kill 掉，而是温柔的杀死他，使用 shutdown，那么可以使用这个注解来保证在它死之前执行 del 删除锁来避免死锁，虽然这很不现实，如果方法执行时间过长很多人也不能忍受。 1234@PreDestroypublic void delLock()&#123; RedisShardedPoolUtil.del(Const.REDIS_LOCK.CLOSE_ORDER_TASK_LOCK);&#125; 还有类似的好用注解，例如 @PostConstruct 标注 init 方法，会在构造完成后执行这个初始化。 数据库实现分布式锁常见的实现方式又分两种，但总的来说并不常用，因为用数据库的话比较费资源，效率也不高： 完全基于数据库表的 基于数据库排它锁 参见：http://www.hollischuang.com/archives/1716 基于数据库表要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。当我们要锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。创建这样一张数据库表： 12345678CREATE TABLE `methodLock` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键', `method_name` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的方法名', `desc` varchar(1024) NOT NULL DEFAULT '备注信息', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成', PRIMARY KEY (`id`), UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的方法'; 当我们想要锁住某个方法时，执行以下SQL： 1insert into methodLock(method_name,desc) values (‘method_name’,‘desc’) 因为我们对method_name做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。当方法执行完毕之后，想要释放锁的话，需要执行以下Sql: 1delete from methodLock where method_name ='method_name' 上面这种简单的实现有以下几个问题： 这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。 这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。 这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。 当然，我们也可以有其他方式解决上面的问题。 数据库是单点？搞两个数据库，数据之前双向同步。一旦挂掉快速切换到备库上。 没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。 非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功。 非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。 基于数据库排他锁除了可以通过增删操作数据表中的记录以外，其实还可以借助数据中自带的锁来实现分布式的锁。我们还用刚刚创建的那张数据库表。可以通过数据库的排他锁来实现分布式锁。 基于 MySql 的 InnoDB 引擎，可以使用以下方法来实现加锁操作： 12345678910111213public boolean lock()&#123; connection.setAutoCommit(false) while(true)&#123; try&#123; result = "select * from methodLock where method_name=xxx for update"; if(result==null)&#123; return true; &#125; &#125;catch(Exception e)&#123;&#125; sleep(1000); &#125; return false;&#125; 在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁。 这里再多提一句，InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给 method_name 添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。 我们可以认为获得排它锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，再通过以下方法解锁： 123public void unlock()&#123; connection.commit();&#125; 通过connection.commit()操作来释放锁。这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。 阻塞锁？for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。 锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。 但是还是无法直接解决数据库单点和可重入问题。 这里还可能存在另外一个问题，虽然我们对 method_name 使用了唯一索引，并且显示使用 for update 来使用行级锁。但是，MySql 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。。。 还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆 总结总结一下使用数据库来实现分布式锁的方式，这两种方式都是依赖数据库的一张表，一种是通过表中的记录的存在情况确定当前是否有锁存在，另外一种是通过数据库的排他锁来实现分布式锁。 优点直接借助数据库，容易理解。 缺点会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。操作数据库需要一定的开销，性能问题需要考虑。使用数据库的行级锁并不一定靠谱，尤其是当我们的锁表并不大的时候。 关于其他的各种锁，参加 issues，整理中… Zookeeper实现基于 zookeeper 临时有序节点可以实现的分布式锁。大致思想即为：每个客户端对某个方法加锁时，在 zookeeper 上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点，操作完成后断开自动删除。判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个（非阻塞情况下，直接判断有没有节点就好了）。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。来看下 Zookeeper 能不能解决前面提到的问题。 锁无法释放？使用 Zookeeper 可以有效的解决锁无法释放的问题，因为在创建锁的时候，客户端会在 ZK 中创建一个临时节点，一旦客户端获取到锁之后突然挂掉（Session 连接断开），那么这个临时节点就会自动删除掉。其他客户端就可以再次获得锁。 只能是非阻塞锁？使用 Zookeeper 可以实现阻塞的锁，客户端可以通过在 ZK 中创建顺序节点，并且在节点上绑定监听器，一旦节点有变化，Zookeeper 会通知客户端，客户端可以检查自己创建的节点是不是当前所有节点中序号最小的，如果是，那么自己就获取到锁，便可以执行业务逻辑了。 不可重入？使用 Zookeeper 也可以有效的解决不可重入的问题，客户端在创建节点的时候，把当前客户端的主机信息和线程信息直接写入到节点中，下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。如果和自己的信息一样，那么自己直接获取到锁，如果不一样就再创建一个临时的顺序节点，参与排队。 单点问题？使用 Zookeeper 可以有效的解决单点问题，ZK 是集群部署的，只要集群中有半数以上的机器存活，就可以对外提供服务。 可以直接使用 zookeeper 第三方库 Curator 客户端，这个客户端中封装了一个可重入的锁服务。 123456789101112131415161718192021public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; try &#123; return interProcessMutex.acquire(timeout, unit); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true;&#125;public boolean unlock() &#123; try &#123; interProcessMutex.release(); &#125; catch (Throwable e) &#123; log.error(e.getMessage(), e); &#125; finally &#123; executorService.schedule(new Cleaner(client, path), delayTimeForClean, TimeUnit.MILLISECONDS); &#125; return true;&#125; Curator 提供的 InterProcessMutex 是分布式锁的实现。acquire 方法用户获取锁，release 方法用于释放锁。 需要注意的问题使用 ZK 实现的分布式锁好像完全符合了我们对一个分布式锁的所有期望。但是，其实并不是，Zookeeper 实现的分布式锁其实存在一个缺点，那就是性能上可能并没有缓存服务那么高。因为每次在创建锁和释放锁的过程中，都要动态创建、销毁瞬时节点来实现锁功能。ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同不到所有的 Follower 机器上。 其实，使用 Zookeeper 也有可能带来并发问题，只是并不常见而已。考虑这样的情况，由于网络抖动，客户端可能和 ZK 集群的 session 连接断了，那么 zk 以为客户端挂了，就会删除临时节点，这时候其他客户端就可以获取到分布式锁了。就可能产生并发问题。这个问题不常见是因为 zk 有重试机制，一旦 zk 集群检测不到客户端的心跳，就会重试， Curator 客户端支持多种重试策略。多次重试之后还不行的话才会删除临时节点。所以，选择一个合适的重试策略也比较重要，要在锁的粒度和并发之间找一个平衡。 总结 优点有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。 缺点性能上不如使用缓存实现分布式锁。需要对 ZK 的原理有所了解。 方案比较上面几种方式，哪种方式都无法做到完美。就像 CAP 一样，在复杂性、可靠性、性能等方面无法同时满足，所以，根据不同的应用场景选择最适合自己的才是王道。 从理解的难易程度角度（从低到高）数据库 &gt; 缓存 &gt; Zookeeper 从实现的复杂性角度（从低到高）Zookeeper &gt;= 缓存 &gt; 数据库 从性能角度（从高到低）缓存 &gt; Zookeeper &gt;= 数据库 从可靠性角度（从高到低）Zookeeper &gt; 缓存 &gt; 数据库 目前来说，一提到分布式锁很多人第一反应就是 Redis，但是分布式锁本质是一个 CP 需求，基于 Redis 的实现的是一个 AP 需求，不过脱离业务场景来谈架构都是耍流氓。例如，业务是金融交易这种需要强锁的情况下，Redis 就不太行了，需要 CP 的实现，例如 etcd 等。 一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistency）等同于所有节点访问同一份最新的数据副本 可用性（Availability）每次请求都能获取到非错的响应，但是不保证获取的数据为最新数据 分区容错性（Partition tolerance）以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。 根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。 参考其他没有说到的就自己搜索探寻吧！ 想了解 etcd 的点这想了解 Chubby 的点这]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>分布式</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初探SpringCloud]]></title>
    <url>%2F2018%2F09%2F23%2F%E5%88%9D%E6%8E%A2SpringCloud%2F</url>
    <content type="text"><![CDATA[如期而至，关于什么是 SpringCloud 这里不多说，之前的笔记已经把 SpringCloud 生态的主要技术都做了解释，但是这里对笔记进行补充一下，关于微服务、微服务架构，以及 SpringBoot 与 Cloud、Dubbo 之间微妙的关系做个简单的解释~然后就是实践部分，先搞一个简单的 Demo 出来试试~ 微服务：一种架构风格，重点在个体，拆分为单个的模块，具体的一个服务，仅关注自己负责的，比如现在流行使用 SpringBoot 来构建。微服务架构：重点在整体，关注各个微服务之间的关系，如何治理好这些微服务，她要求各个微服务独立部署，能拥有“独立的空间（例如独立的数据库）”，现在流行使用 SpringCloud 提供的一站式解决方案构建。微服务目前业内还没有一个准确的概念，上面的是我瞎说的 o(￣▽￣)ゞ))￣▽￣)o说到微服务架构就必须要分布式了，其中涉及的还有服务注册发现中心、服务调用方式（轻量级网络协议交互，REST、RPC）、服务监控、断路器、服务网关、分布式配置、服务跟踪、服务总线、数据流、批量任务等等。 可以看出，SpringCloud 作为全局的服务治理框架，它依赖于 SpringBoot，而与 Dubbo 的最显著区别就是 SpringCloud 使用 REST；Dubbo 使用 RPC。使用 REST 更加灵活，并且语言无关，但是没有 RPC 的效率高，同时 RPC 也存在一些自身的问题。 当前由于 RPC 协议，注册中心元数据不匹配等问题，在面临微服务基础架构选型时，Dubbo 和 SpringCloud 只能二选一，所以才会出现两者的比较。Dubbo 负责人表示之后会积极寻求适配到 SpringCloud 生态 如果 Dubbo 不停更 5 年的话，说不定又是另一番景象呢；然而现在 SCNetflix 又开始维护模式了，反而 SCAlibaba 活跃起来了。 微服务搭建这个不是重点，但是确实前提条件，所以需要先用 SpringBoot 搭出至少两个微服务，一个做服务提供，一个做服务消费，然后在这个基础上加 SpringCloud。关于 SpringCloud 的生态圈涉及的技术太多了，看了不少视频和资料，大部分都是对主要的几个技术来做介绍，实际上也大部分都是用这些技术，其他的也就不多说了，感兴趣的可以去官方或者中文网逛逛，挺全的。 Eureka使用 Eureka 来实现服务的注册与发现，介绍之前说过了不多说，它分为客户端和服务端，一般会新建一个项目（微服务）作为服务端，这里就需要加入 SpringCloud 的依赖管理来负责做版本仲裁，然后也需要加入 EureKa 服务端的依赖，注意是以结尾的。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 然后在启动类使用注解开启功能，这个是 SpringCloud 的通用套路，先加依赖，然后在启动类添加 @EnableXXX 开启相关配置。 12345678// 声明这是一个Eureka服务@EnableEurekaServer@SpringBootApplicationpublic class EurekaServer &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServer.class, args); &#125;&#125; 最后，配置一下相关参数就可以启动测试了： 123456789101112server: # 服务端口 port: 6868eureka: client: # 是否将自己注册到 Eureka 服务中，本身就是,所以无需注册 registerWithEureka: false # 是否从Eureka中获取注册信息 fetchRegistry: false # Eureka客户端与Eureka服务端进行交互的地址 serviceUrl: defaultZone: http://127.0.0.1:$&#123;server.port&#125;/eureka/ 访问一下就可以看到相关的系统信息了，下面就可以把之前创建的微服务服务方注册到 Eureka 中了，如果某个微服务确定没有其他的微服务依赖它，那可以不用注册进来；方法和之前的套路一样，加入 SpringCloud 的依赖管理，加入 Eureka 的依赖（可以是客户端也可以是服务端，推荐客户端），然后关键的地方就是配置文件的修改了： 12345678910111213141516171819202122server: port: 8081spring: application: # 指定服务名，非常重要 name: microService-itemeureka: client: # 是否将自己注册到 Eureka 服务中，默认为 true registerWithEureka: true # 是否从Eureka中获取注册信息，默认为true fetchRegistry: true # Eureka 客户端与 Eureka 服务端进行交互的地址 serviceUrl: defaultZone: http://127.0.0.1:6868/eureka/ # 将自己的 ip 地址注册到 Eureka 服务中 instance: prefer-ip-address: true # 可以手动指定地址，可以可以通过表达式来获取 # $&#123;spring.application.name&#125;:$&#123;server.port&#125; ip-address: 127.0.0.1 最后，在主启动类上加入 @EnableDiscoveryClient 注解，表名这是个客户端即可。另一个作为消费端的微服务也是一样，唯一不同的是配置文件里就不需要将自己注册到 Eureka 服务中了，也不需要设置了 instance 了。具体使用： 1234567891011121314151617181920@Servicepublic class ItemService &#123; @Autowired private RestTemplate restTemplate; @Autowired private DiscoveryClient discoveryClient; public Item queryItemById(Long id) &#123; String serviceId = "microService-item"; List&lt;ServiceInstance&gt; instances = this.discoveryClient.getInstances(serviceId); if(instances.isEmpty())&#123; return null; &#125; // 为了演示，在这里只获取一个实例 ServiceInstance serviceInstance = instances.get(0); String url = serviceInstance.getHost() + ":" + serviceInstance.getPort(); return this.restTemplate.getForObject("http://" + url + "/item/" + id, Item.class); &#125;&#125; 如果导入的是服务端依赖，某些版本的 SpringCloud 会响应 XML 格式的数据，而我们希望它是 JSON，破坏了 SpringMVC 的默认配置，可以在 eureka server 的依赖中排除 jackson-dataformat-xml。对于 eureka 来说，这两个微服务都属于客户端，所以还是建议只导客户端依赖就好。另外，你还可以开启 Eureka 的身份认证，需要导入相应的依赖，一旦开启，需要在客户端配置好凭证。 搭建集群Eureka 的集群非常好搭建，为了避免单点故障，集群是很有必要的，只要启动多个 Eureka 服务并且让这些服务之间彼此进行注册即可实现。 123456789101112131415161718192021222324252627282930313233343536373839server: port: 6868spring: application: name: microService-eurekaeureka: client: # 是否将自己注册到 Eureka 服务中,这次（集群）选择 true registerWithEureka: true # 是否从Eureka中获取注册信息（集群选择 true） fetchRegistry: true # Eureka 客户端与 Eureka 服务端进行交互的地址，选择另一台 Eureka 服务端 serviceUrl: defaultZone: http://loli:pwd@127.0.0.1:6869/eureka/############## 第二台服务端 ################server: port: 6869spring: application: name: microService-eurekaeureka: client: # 是否将自己注册到 Eureka 服务中,这次选择 true registerWithEureka: true # 是否从Eureka中获取注册信息 fetchRegistry: true # Eureka 客户端与 Eureka 服务端进行交互的地址，选择另一台 Eureka 服务端 serviceUrl: defaultZone: http://127.0.0.1:6868/eureka/security: basic: # 开启基于 HTTP basic 的认证 enable: true user: name: loli password: pwd 他们的 defaultZone 互相指向对方，通过端口来区分，而微服务名字都是保持一致的，这样服务端的集群就搭建好了，而客户端注册的时候需要同时向这两台来注册，地址之间使用逗号分割。搭建集群的时候尽量不要再配 prefer-ip-address 了，默认是 hostname。 自我保护机制之前说过，Eureka 和 ZK 的一个区别，ZK 是按照 CP 原则来构建的，而 Eureka 是 AP 来做的。默认情况下，如果 Eureka Server 在一定时间内（默认90秒）没有接收到某个微服务实例的心跳，Eureka Server 将会移除该实例。但是当网络分区故障发生时，微服务与 Eureka Server 之间无法正常通信，而微服务本身是正常运行的，此时不应该移除这个微服务，所以引入了自我保护机制。当 Eureka Server 节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。一旦进入该模式，Eureka Server 就会保护服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。当网络故障恢复后，该 Eureka Server 节点会自动退出自我保护模式。Eureka Server 在运行期间会去统计心跳失败比例在 15 分钟之内是否低于 85%，如果低于 85%，Eureka Server 会将这些实例保护起来，让这些实例不会过期，但是在保护期内如果服务刚好这个服务提供者非正常下线了，此时服务消费者就会拿到一个无效的服务实例，此时会调用失败，对于这个问题需要服务消费者端要有一些容错机制，如重试，断路器等。我们在单机测试的时候很容易满足心跳失败比例在 15 分钟之内低于 85%，这个时候就会触发 Eureka 的保护机制，一旦开启了保护机制，则服务注册中心维护的服务实例就不是那么准确了，此时我们可以使用eureka.server.enable-self-preservation=false来关闭保护机制，这样可以确保注册中心中不可用的实例被及时的剔除（不推荐）。 也因为 AP 的特性，会导致其注册慢的问题，也就是 Client 可能延迟注册（30s），Server 的响应缓存（30s），Server 刷新缓存（30s）极端情况加起来就是 90s。 常用配置这里再以 properties 为例： 1234567891011121314151617181920212223242526272829303132333435363738#================================服务端==============================#应用名称spring.application.name=eureka-server-v1#应用端口server.port=7000#=======eureka中心配置=======#主机名eureka.instance.hostname=localhost# 注册时显示ip#eureka.instance.prefer-ip-address=true#是否注册为服务eureka.client.register-with-eureka=false#是否检索服务eureka.client.fetch-registry=false#eureka默认空间的地址eureka.client.service-url.defaultZone=http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/#关闭自我保护(生产时打开该选项)eureka.server.enable-self-preservation=false#扫描失效服务的间隔时间（缺省为60*1000ms）eureka.server.eviction-interval-timer-in-ms=5000#================================客户端==============================#端口号server.port=8081#服务名spring.application.name=produce-service-v1#=======eureka配置========#注册到eureka中心，获取到配置服务eureka.client.service-url.defaultZone=http://localhost:7000/eureka/#设置实例的ID为ip:porteureka.instance.instance-id=$&#123;spring.cloud.client.ipAddress&#125;:$&#123;server.port&#125;#========续约配置=========# 心跳时间，即服务续约间隔时间（缺省为30s）eureka.instance.lease-renewal-interval-in-seconds=5# 发呆时间，即服务续约到期时间（缺省为90s）eureka.instance.lease-expiration-duration-in-seconds=10# 开启健康检查（依赖spring-boot-starter-actuator）eureka.client.healthcheck.enabled=true 这些应该够用了吧….. Ribbon使用 Ribbon 实现客户端负载均衡，说到负载均衡，可以简单分为两类： 集中式消费方和服务方中间使用独立的 LB 设施，例如 F5、nginx 这类就是。 进程内一般集成到消费方，Ribbon 就是如此。 使用前的老一套不说了，导入依赖（eureka-server/client 中已经包含了 Ribbon 的依赖），在主启动类使用 @RibbonClient （简单使用可以不加）进行配置工具类。然后，在 Config 创建 RestTemplate 对象上设置 @LoadBalanced 注解就表示已经启用负载均衡啦！ 开启后，在执行请求前会经过 org.springframework.cloud.client.loadbalancer.LoadBalancerInterceptor 这个拦截器，并且通过 org.springframework.cloud.netflix.ribbon.RibbonLoadBalancerClient 的时候会根据 serverId 查找服务地址，然后在去做真正的请求；所以 RestTemplate 请求的 URL 可以直接使用服务名，而不需要手动获取地址了。 Spring Cloud Ribbon 虽然只是一个工具类框架，它不像服务注册中心、 配置中心、 API 网关那样需要独立部署， 但是它几乎存在于每一个Spring Cloud 构建的微服务和基础设施中。 因为微服务间的调用，API 网关的请求转发等内容实际上都是通过 Ribbon 来实现的，包括后续我们将要介绍的 Feign, 它也是基于 Ribbon 实现的工具（即 Feign 已经集成 Ribbon，所以注解 @FeignClient 的类，默认实现了 ribbon 的功能）。SpringCloud 服务调用的方式一般就是两种： Ribbon + RestTemplate Feign 常用 IRule 默认实现有以下几种： RoundRobinRule（默认）轮询规则，默认规则。同时也是更高级 rules 的回退策略 AvailabilityFilteringRule可用性敏感策略，这个负载均衡器规则，会先过滤掉以下服务： 由于多次访问故障而处于断路器跳闸状态 并发的连接数量超过阈值 然后对剩余的服务列表按照 RoundRobinRule 策略进行访问 WeightedResponseTimeRule权重轮询策略，根据平均响应时间计算所有服务的权重，响应时间越快，服务权重越重、被选中的概率越高。刚启动时，如果统计信息不足，则使用 RoundRobinRule 策略，等统计信息足够，会切换到 WeightedResponseTimeRule。 RetryRule在选定的策略基础上进行重试，先按照 RoundRobinRule 的策略获取服务，如果获取服务失败，则在指定时间内会进行重试，获取可用的服务 BestAvailableRule最小并发策略，此负载均衡器会先过滤掉由于多次访问故障而处于断路器跳闸状态的服务，然后选择一个并发量最小的服务 RandomRule随机获取一个服务 更改默认的策略也很简单，可以使用 JavaConfig 的方式也可以使用配置文件的方式： 123456789101112@EnableEurekaClient@SpringBootApplicationpublic class ConsumerApplication &#123; @Bean public RandomRule createRule() &#123; return new RandomRule(); &#125; public static void main(String[] args) &#123; SpringApplication.run(ConsumerApplication.class, args); &#125;&#125; 配置文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 设置负载均衡策略 eureka-provider 为调用的服务的名称eureka-provider: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule# 禁用 Eureka 支持，禁用后 Ribbon 不会获取 Eureka 中的服务列表ribbon: eureka: enabled: false# 禁用 Eureka 后手动配置服务地址ribbon-demo: ribbon: listOfServers: localhost:8081,localhost:8083# 其他常见配置ribbon: # 请求连接的超时时间 ConnectTimeout: 2000 # 请求处理的超时时间 ReadTimeout: 5000 # 最大连接数 MaxTotalConnections: 500 # 每个host最大连接数 MaxConnectionsPerHost: 500 #Ribbon更新服务注册列表的频率 ServerListRefreshInterval: 2000 # 预加载配置,默认为懒加载 eager-load: enabled: true clients: mima-cloud-producer,xxx# 这里使用服务提供者的instanceNamemima-cloud-producer: ribbon: # 代表Ribbon使用的负载均衡策略 NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule # 每台服务器最多重试次数，但是首次调用不包括在内，默认 0 MaxAutoRetries: 1 # 同一个服务其他实例的最大重试次数，不包括第一次调用的实例。默认值为 1 MaxAutoRetriesNextServer: 1 # 无论是请求超时或者socket read timeout都进行重试 # 配置为 true, 则任何请求方法都进行重试 # 配置为 false，GET请求方式也会进行重试，非GET方法只有在连接异常时才会进行重试。 OkToRetryOnAllOperations: true # Interval to refresh the server list from the source ServerListRefreshInterval: 2000 # Connect timeout used by Apache HttpClient ConnectTimeout: 3000 # Read timeout used by Apache HttpClient ReadTimeout: 3000 禁用 Eureka 以后 LoadBalanced 的根据服务名查找 ip 就会失效，可以通过指定 来达到点对点直连测试。关于重试的配置要格外小心，避免重复数据。 自定义Ribbon 自带了七中负载均衡的算法。默认轮询，当想选择其他算法时，在配置类里使用 @Bean 声明需要的官方提供的 IRule 其他实现类即可，这是 SpringBoot 自动配置的知识了。也正是因为这个原因，如果我们想自己实现负载均衡算法，除了需要在启动类使用 @RibbonClient 注解指定服务名和负载均衡算法具体实现类（需要在 @Configuration 下）外，还要求这个类不能在包扫描范围内。首先，可以自定义负载均衡规则，可以在配置文件里设置也可以使用注解： 123microService-consumer: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 在启动类上设置： 12345@SpringBootApplication@EnableEurekaClient// 在启动该微服务的时候就能去加载我们的自定义 Ribbon 配置类，从而使配置生效@RibbonClient(name="MICROSERVICECLOUD-DEPT",configuration=MySelfRule.class)public class Main&#123;&#125; 然后就是实现自己的负载均衡算法： 12345678910@Configurationpublic class MySelfRule&#123; @Bean public IRule myRule()&#123; // 随机使用 RoundRobinRule(); return new TestLB(); &#125;&#125;public class TestLB extends AbstractLoadBalancerRule&#123;&#125; 一般来说，默认的轮询就已经够用了。自定义的可以用于灰度发布等场景。常用组件：IPing（探测服务存活状态），其他概念：allServerList、upServerList。 SCLoadBalancer可以看出 SC 在逐步去 Netflix 化，Ribbon 宣布进入维护阶段，SC 早年也发布了自己的负载均衡框架 SpringCloudLoadBalancer。不过它并不是一个独立的项目，而是 spring-cloud-commons 其中的一个模块。 项目中用了 Eureka 以及相关的 starter，想完全剔除 Ribbon 的相关依赖基本是不可能的，Spring 社区的人也是看到了这一点，通过配置去关闭 Ribbon 启用 Spring-Cloud-LoadBalancer。 12345spring: cloud: loadbalancer: ribbon: enabled: false 关闭 ribbon 之后，Spring Cloud LoadBalancer 就会加载成为默认的负载均衡器。在使用 API 上他们两个基本一致，不过嘛，这个项目的活跃度不算高，也不如 Ribbon 完善，目前阶段还是老老实实用 Ribbon 比较好。 OpenFeign使用 Feign 实现声明式的 REST 调用，是为了简化 RestTemplate 的使用，让我们的代码更优雅，添加依赖就不多说了，主启动类加上 @EnableFeignClients 注解，这也是加在客户端（消费端）的。然后声明一个接口，然后可以像写 SpringMVC 哪样来定义这个接口的方法啦！ 12345678910111213141516// 声明这是一个Feign客户端，并且指明服务 id// 测试需要的情况，可以通过 url 参数指定地址// Path 属性可以抽取 url 的公共部分@FeignClient(value = "microService-item") public interface ItemFeignClient &#123; // 这里定义了类似于SpringMVC用法的方法，就可以进行RESTful的调用了 @RequestMapping(value = "/item/&#123;id&#125;", method = RequestMethod.GET) public Item queryItemById(@PathVariable("id") Long id); // 多参数构造-1 @RequestMapping(value = "/get", method = RequestMethod.GET) public User get1(@RequestParam("id") Long id, @RequestParam("username") String username); // 多参数构造-2 @RequestMapping(value = "/get", method = RequestMethod.GET) public User get2(@RequestParam Map&lt;String, Object&gt; map);&#125; Feign 使用，简单说：创建一个接口（使用 @FeignClient 标注），在上面使用 SpringMVC 注解即可；相当于封装了 RestClient。只需要定义接口就可以完成调用，很显然是用了动态代理，在 FeignClient 中的定义方法以及使用了 SpringMVC 的注解，Feign 就会根据注解中的内容生成对应的 URL，然后基于 Ribbon 的负载均衡去调用 REST 服务。 Feign 最开始是 Netflix 的一套独立的框架，也并不能与 SpringMVC 的注解配合使用，它有自己的注解，SC 团队为了方便使用和整合，基于它开发了 OpenFeign，现在基本都是用的 OpenFeign，支持 SpringMVC 注解，SC 维护。 本质来说，Feign 就是一个 Http 客户端，并且整合了 Ribbon 和 Hystrix。老版本中不支持 SpringMVC 的新组合注解，例如 @GetMapping，高版本没问题。再说 Feign 中的 http 客户端，Feign 就相当于是 http 客户端的一层包装，为了更方便使用，默认它支持三种： JDK HttpURLConnnection（默认） Apache HttpClient启用：feign.httpclient.enabled=true Square OkHttp开启：feign.okhttp.enabled=true 换用其他 http 客户端时候，需要加入 feign-okhttp 或者 feign-httpclient（io.github.openfeign） 依赖；相关自动配置参考 FeignRibbonClientAutoConfiguration。其中还有个比较特殊的 LoadBalancerFeignClient 类，它内部先使用 Ribbon 负载均衡算法计算 server 服务器，然后使用包装的 delegate 客户端实例，去完成 HTTP URL 请求处理，所以它具备了负载均衡的能力。以及可以进行压缩处理，在服务之间的调用上显然不能使用 Nginx 了，Feign 的压缩就派上用场了，只需要在配置中开启即可，小数据量其实没有必要，可以配最小阈值。为了避免重复定义，可以把 Feign 相关的定义抽取到单独的 Api 模块，可以在新模块中继承使用。 Hystrix使用 Hystrix 的熔断机制保护应用，在微服务架构中通常会有多个服务层调用，基础服务的故障可能会导致级联故障，进而造成整个系统不可用的情况，这种现象被称为服务雪崩效应。它是在服务端（服务提供方）使用的技术，使用起来非常简单，除了必要的依赖、在主启动类使用 @EnableHystrix 注解开启功能，只需要在 Service 的方法上加上 @HystrixCommand(fallbackMethod = &quot;fallbackMethod&quot;) 注解就可以了，其中 fallbackMethod 是具有类似方法签名的备用方法，当此方法的调用不可用时就会走这个备用方法，如果确实需要抛出异常，可以通过注解属性来配置忽略那些异常（或者抛出 HystrixBadRequestException 异常，当抛出这个异常时，Hystrix 不会处理）。但是这里会有一个问题，如果一个核心方法对应一个备用方法，很容易就会造成方法膨胀，耦合性还很高，这样也太不优雅了，所以可以使用 AOP 的思想来解决这个问题嘛~其实这样可以做服务降级，配合 Feign 来使用： 12345678910111213141516171819202122232425262728293031323334// 该接口下哪个方法抛异常，会调 fallbackFactory// 或者直接使用 fallback 属性指定一个其实现类，当作备用@FeignClient(value = "MICROSERVICECLOUD-DEPT",fallbackFactory=MyFallbackFactory.class)public interface DeptClientService&#123;&#125;@Component // 不要忘记添加public class MyFallbackFactory implements FallbackFactory&lt;DeptClientService&gt;&#123;&#125;// 使用 Hystrix 原生注解@HystrixCommand(groupKey = "PoiInfoServiceCommand", commandKey=“getStagedPoiBase”, fallbackMethod = "getStagedPoiBaseFallBack", commandProperties = &#123; //指定多久超时，单位毫秒。超时进fallback @HystrixProperty(name = "execution.isolation.thread.timeoutInMilliseconds", value = "1000"), //判断熔断的最少请求数，默认是10；只有在一个统计窗口内处理的请求数量达到这个阈值才会进行熔断与否的判断 @HystrixProperty(name = "circuitBreaker.requestVolumeThreshold", value = "10"), //判断熔断的阈值，默认值50，表示在一个统计窗口内有50%的请求处理失败，会触发熔断 @HystrixProperty(name = "circuitBreaker.errorThresholdPercentage", value = "50"), //熔断多少毫秒后开始尝试请求 默认5000ms @HystrixProperty(name = "circuitBreaker.sleepWindowInMilliseconds" , value = "10000") &#125;, threadPoolProperties = &#123; @HystrixProperty(name = "coreSize", value = "8"), @HystrixProperty(name = "maxQueueSize", value = "100"), @HystrixProperty(name = "keepAliveTimeMinutes", value = "2"), @HystrixProperty(name = "queueSizeRejectionThreshold", value = "15"), @HystrixProperty(name = "metrics.rollingStats.numBuckets", value = "12"), @HystrixProperty(name = "metrics.rollingStats.timeInMilliseconds", value = "1440") &#125; )public List&lt;PoiBaseTf&gt; getStagedPoiBase(List&lt;Long&gt; poiIds) throws Exception&#123; //todo&#125; 这样就可以达到容错的目的了，说到这里，不得不提涉及的两个概念，服务降级和服务熔断。 服务熔断主逻辑因短期内多次失败（也有可能是由于自我保护机制），而被暂时性的忽略，不再尝试使用，这种叫熔断。相关概念：时间窗口、错误率（包含超时）、滑动窗口；例如，根据 1s 内的统计（请求、超时、错误数等）封装为一个 block，取 10 个 block 计算平均值，达到错误率阈值后熔断（同时满足设定的错误率和请求数），这 10 个block 是滑动选取，前面去掉一个，后面就加一个。 服务降级:主逻辑失败采用备用逻辑的过程叫做降级（也就是服务降级发生在服务熔断之后）。当整体资源快消耗殆尽的时候（例如内存、CPU等），将某些服务临时关掉一大部分以释放资源（一般留下一个来维持运行返回给用户友好的提示），减轻主模块的压力，待资源恢复可用再开启。 看上去熔断和降级是非常相似的，都是调用失败后调用备用方法；但是他们的着重点是不同的。不管是服务降级还是熔断，他们的目的都是为了保证程序的健壮性，对于一些非核心服务，如果出现大量的异常，可以通过技术手段，对服务进行降级并提供有损服务，保证服务的柔性可用，避免引起雪崩效应。开启熔断之后，如何实现自动恢复？每隔一段时间（默认 5s），会释放一个请求到服务端进行探测，如果后端服务已经恢复，则自动恢复，这也称为半开状态。 此外，Hystrix 还具有服务监控的功能，它提供了准实时的调用监控 HystrixDashboard，是可视化界面，可以进行实时监测。这个监控使用非常简单，新建一个微服务，加入相关依赖，在启动类上加上 @EnableHystrixDashboard 就可以用啦！ Hystrix 的执行流程非常简单，它基本是业务无关性，在调用服务之前判断是否命中缓存、是否熔断、是否限流，都不满足才真正去调用，然后还会判断是不是调用失败了（超时、错误），失败就执行兜底方法，所有都正常才返回原始的结果。 根据架构图，扩展的 HystrixCommand 中主要的四个方法分别对应单次处理（同步、异步）、多次处理（阻塞、非阻塞（回调））的方式去执行（run 方法），其中 observe 称为 hot 处理，toObserve 称为 cold 处理（每次订阅都是一个新 Command 对象，后执行回调），主要区别是顺序问题。两种 Command 的执行区别为一个新线程执行，一个本线程执行。Hystrix 隔离规则有两种，线程隔离和信号量（轻量级），官方推荐使用线程隔离；线程隔离可以保证其中一个失败不会影响其他线程的执行。可配置项里有线程池的相关设置。目前，Hystrix 也是维护模式。 Zuul因为之前笔记有，介绍咱们还是略过，主要负责请求转发和请求过滤，简单理解为它相当于是一系列的过滤器或者拦截器就好了，继续新建一个微服务，导入相关依赖，在主启动类加入 @EnableZuulProxy 注解。服务网关是微服务架构中一个不可或缺的部分。通过服务网关统一向外系统提供 REST API 的过程中，除了具备服务路由、均衡负载功能之外，它还具备了权限控制等功能，将权限控制这些较重的非业务逻辑内容迁移到服务路由层面，使得服务集群主体能够具备更高的可复用性和可测试性。从上面的介绍可以看出，它还需要 Eureka 等依赖加强功能，让后面的微服务专心做自己的事情。 12345678910111213141516171819202122232425262728293031323334server: port: 6677spring: application: name: microservice-api-gatewayzuul: ignored-services: "*" prefix: /atguigu routes: # 名字可以随意起，或者使用微服务的名称 item-service: # 配置请求 URL 的请求规则 path: /item-service/** # 真正的微服务地址，可以使用 url ，也可以指定 Eureka 注册中心中的服务 id # url: http://127.0.0.1:8081 serviceId: microservice-itemeureka: client: # 是否注册自己，默认为true registerWithEureka: true fetchRegistry: true serviceUrl: defaultZone: http://127.0.0.1:6868/eureka/ instance: # 将自己的ip地址注册到Eureka服务中 prefer-ip-address: trueinfo: app.name: microcloud company.name: www.bfchengnuo.com build.artifactId: $project.artifactId$ build.version: $project.version$ 匹配规则：? 单个字符、 任意数量字符、* 匹配多级目录；同时，Zuul 还支持正则匹配、拦截器、路由前缀等功能，基于 Servlet 来实现，进阶可以看看自定义 Filter （实现 ZuulFilter 接口）相关知识。网关中也可以使用 Hystrix 做负载均衡，只不过写起来有点麻烦，还是习惯于在业务层里进行处理。Zuul 会默认过滤一些头信息，例如认证相关，如果不想让它过滤，可以配置 sensitive-headers 为空。 SpringCloudGatewaySpring 官方最终还是推出了自己的网关组件：Spring Cloud Gateway ，相比之前我们使用的 Zuul 是基于 Servlet，使用阻塞 API，它不支持任何长连接，如 WebSockets，Spring Cloud Gateway 使用非阻塞 API，支持 WebSockets，支持限流等新特性。 SpringCloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。目的为替代 Zuul。 它基于 WebFlux 框架实现的，而 WebFlux 框架底层则使用了高性能的 Reactor 模式通信框架 Netty。它不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。 学习它需要先了解三个术语： Filter（过滤器）和 Zuul 的过滤器在概念上类似，可以使用它拦截和修改请求，并且对上游的响应，进行二次处理。过滤器为 org.springframework.cloud.gateway.filter.GatewayFilter 类的实例。 Route（路由）网关配置的基本组成模块，和 Zuul 的路由配置模块类似。一个 Route 模块由一个 ID，一个目标 URI，一组断言和一组过滤器定义。如果断言为真，则路由匹配，目标 URI 会被访问。 Predicate（断言）：这是一个 Java 8 的 Predicate，可以使用它来匹配来自 HTTP 请求的任何内容，例如 headers 或参数。断言的输入类型是一个 ServerWebExchange。 因为也是基于过滤链，流程简单来看都差不多，也很简单：客户端向 Spring Cloud Gateway 发出请求；如果 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler；Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。配置路由可以使用 yml 配置文件，也可以使用 Java 配置，不过还是推荐使用 yml： 123456789101112131415161718192021222324server: port: 8080spring: cloud: gateway: routes: - id: neo_route uri: https://bfchengnuo.com predicates: - Path=/amy -id: message-provider-route uri: lb://message-provider predicates: -Path=/message-provider/** application: name: cloud-gatewayeureka: instance: prefer-ip-address: true client: service-url: defaultZone: http://localhost:8888/eureka/ 以上配置的解释： id：我们自定义的路由 ID，保持唯一 uri：目标服务地址，可以结合负载均衡、配置中心 ID 使用 predicates：路由条件，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。 filters：过滤规则，本示例暂时没用。可以配置对应的过滤处理 url 或者使用 Hystrix 熔断降级。 所以效果就是当访问 localhost:8080/amy/1 的时候，会自动转发到 https://bfchengnuo.com/1 这个地址；基于 JavaConfig 的配置： 123456789101112131415@SpringBootApplicationpublic class GatewayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GatewayApplication.class, args); &#125; @Bean public RouteLocator customRouteLocator(RouteLocatorBuilder builder) &#123; return builder.routes() .route("path_route", r -&gt; r.path("/amy") .uri("https://bfchengnuo.com")) .build(); &#125;&#125; SpringCloudGateway 的匹配规则非常丰富，请求路径、参数、时间、host、cookie 等等，并且支持组合使用。 SpringCloudConfig使用 SpringCloudConfig 统一管理微服务的配置，可以让我们统一管理配置文件，以及实时同步更新，并不需要重新启动应用程序，默认使用 Git 存储配置文件内容。同样，它也分为客户端和服务端，服务端可以新建一个微服务，加入相应的依赖，在主启动类加上 @EnableConfigServer 注解就可以使用了，当然还是要写一点配置的。 123456789101112server: port: 6688spring: application: name: microservice-config-server cloud: config: server: git: uri: http://172.16.55.138:10080/bfchengnuo/config-server.git #username: loli #password: 123456 还是推荐使用 SSH 密钥认证的方式，这样就可以通过 SpringCloudConfig 直接访问 Git 上的配置文件，同时它支持 properties 和 yml 的互相转换，通过请求地址的后缀实现。客户端的使用也是类似，导入没有 server 后缀的依赖，另外为了避免地址的硬编码，可以将服务端使用 @EnableDiscoveryClient 也注册到 Eureka 中，然后在客户端使用服务名来访问。需要新建配置文件：bootstrap.yml 1234567891011121314spring: cloud: config: # 对应的配置服务中的应用名称 name: microservice # uri: http://127.0.0.1:6869/ # 对应配置服务中的 &#123;profile&#125; profile: dev label: master discovery: # 启用发现服务功能 enabled: true # 指定服务名称 service-id: microservice-config-server 因为 bootstrap.yml 优先与 application.yml 加载（先读取了配置才能启动啊），所以把发现服务配置在 bootstrap 里。然后你可以使用 @Value 来注入配置，就是和配置文件在本地是一样使用。为了能够让配置自动更新，还需要为 Config Client 添加 refresh 支持，就是导入一个 spring-boot-starter-actuator 依赖，然后在配置类对应的实体类上加上 @RefreshScope 注解（测试可以临时把 actuator 安全认证关掉 management.security.enabled）。然后就可以使用 post 请求 /refresh 地址来更新配置内容了。更新后还需要手动访问下这个地址未免太麻烦了，所以，可以借助 git 的 webhook（web 钩子）实现自动更新。 SpringCloudBus消息总线 Spring Cloud Bus 也是很重要的，例如它可以更优雅的完成自动更新配置文件，简单的你可以理解为它就是个消息的交换机，所有的微服务模块都监听它，所以可以实现配置、缓存等的更新。以 RabbitMQ 为例，就先在 ConfigServer 中来加入吧，导入 spring-cloud-starter-bus-amqp 依赖，在 application.yml 添加 rabbitmq 的配置： 123456789101112131415161718192021eureka: client: serviceUrl: defaultZone: http://127.0.0.1:6868/eureka/spring: cloud: config: name: microservice # uri: http://127.0.0.1:6869/ profile: dev label: master discovery: # 启用发现服务功能 enabled: true service-id: microservice-config-server # RabbitMQ 相关的配置 rabbitmq: host: 127.0.0.1 port: 5672 username: guest password: guest 然后，会自动注册一个 /bus/refresh 的请求，这个请求就是由消息总线来处理的，那么我们可以设置当配置更新后 web 钩子向统一配置服务器发送 /bus/refresh 请求，然后配置服务器会将消息发送到 springCloudBus 的交换机，由于其他微服务的队列也绑定到交换机，所以也就都获取到了更新的通知，然后去 ConfigServer 获取最新的数据。需要注意，其他的微服务（客户端）这个 bus 配置是要写在 bootstrap.yml 中的，保证优先加载。 其他因为 Eureka 很早之前就宣布进入维护模式，目前可以选择使用 Alibaba 套件替代或者使用 Spring Cloud Consul。此外，SC 的其他一些项目也不错，例如 Spring Cloud Security / OAuth2。参考：https://github.com/macrozheng/springcloud-learning 参考https://www.jianshu.com/p/3463571febc2]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式中数据一致性探索]]></title>
    <url>%2F2018%2F09%2F15%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[关于这个，在面试中好像出境频率挺高的，现在都搞微服务、分布式（至于项目适不适合搞，嘿嘿）不懂一点确实说不过去，关于这点其实以前看过，但是当别人问起还是一脸萌逼，查了查资料感觉以前看过，然后过段时间再问，再懵逼…..这就必须得做笔记了！这篇基本都是概念性的东西，原理懂了下面的就好办了，基础最重要嘛~ CAP定理首先，来看一下著名的 CAP 定理： 在理论计算机科学中，CAP 定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistency）等同于所有节点访问同一份最新的数据副本 可用性（Availability）每次请求都能获取到非错的响应，但是不保证获取的数据为最新数据 分区容错性（Partition tolerance）以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。 根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。 如果我们选择的是可用性(Availability，简称 A)， 就是系统能提供服务就好， 数据不一致可以忍受。在网络节点之间无法通信的情况下, 和数据复制相关的功能， 要么选择可用性(A) , 要么选择一致性(C)， 不能同时选择两者。如果选择了可用性(A) + 分区容错性(P) , 就要放弃一致性(C)。如果选在一致性(C) + 分区容错性(P) , 就得放弃可用性(A) 。在选择 CP 的情况下，虽然系统的有些功能是不能使用的（放弃了 A）， 因为需要等待数据的同步， 但是那些和数据同步无关的功能还是可以访问的 ， 相当于系统做了功能的降级。既然有 AP 和 CP, 会不会出现仅仅是 CA（一致性+可用性）这种组合呢？ 就是没有分区容错性， 只保留可用性和一致性？ 仔细想想， 这种情况其实就退化成了单机应用， 没有意义了。然后，既然要做分布式，P 是肯定的，那么只能从 C 和 A 里再选择一个了，常见的有：其中，常用的 Zookeeper 保证的是 CP，而 Eureka 则是 AP 当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接 down 掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是 zk 会出现这样一种情况，当 master 节点因为网络故障与其他节点失去联系时，剩余节点会重新进行 leader 选举。问题在于，选举 leader 的时间太长，30 ~ 120s, 且选举期间整个 zk 集群都是不可用的，这就导致在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得 zk 集群失去 master 节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。 Eureka 看明白了这一点，因此在设计时就优先保证可用性。Eureka 各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而 Eureka 的客户端在向某个 Eureka 注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台 Eureka 还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)。除此之外，Eureka 还有一种自我保护机制，如果在 15 分钟内超过 85% 的节点都没有正常的心跳，那么 Eureka 就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况： Eureka 不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 Eureka 仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用 当网络稳定时，当前实例新的注册信息会被同步到其它节点中 因此， Eureka 可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像 zookeeper 那样使整个注册服务瘫痪。Eureka 作为单纯的服务注册中心来说要比 zookeeper 更加“专业”，因为注册服务更重要的是可用性，我们可以接受短期内达不到一致性的状况。不过 Eureka 目前 1.X 版本的实现是基于 servlet 的 java web 应用，它的极限性能肯定会受到影响。期待正在开发之中的 2.X 版本能够从 servlet 中独立出来成为单独可部署执行的服务（虽然闭源了）。 XA规范XA 是由 X/Open 组织提出的分布式事务的规范。XA 规范主要定义了（全局）事务管理器（Transaction Manager）和（局部）资源管理器（Resource Manager）之间的接口。XA 接口是双向的系统接口，在事务管理器（Transaction Manager）以及一个或多个资源管理器（Resource Manager）之间形成通信桥梁。XA 引入的事务管理器充当上文所述全局事务中的“协调者”角色。事务管理器控制着全局事务，管理事务生命周期，并协调资源。资源管理器负责控制和管理实际资源（如数据库或 JMS 队列）。目前，Oracle、Informix、DB2、Sybase 和 PostgreSQL 等各主流数据库都提供了对 XA 的支持。 JTA Java 事务 API（Java Transaction API，简称 JTA ） 是一个 Java 企业版 的应用程序接口，在 Java 环境中，允许完成跨越多个 XA 资源的分布式事务。JTA 是在 Java 社区过程下制定的规范，编号 JSR 907。 作为 Java 平台上事务规范 JTA（Java Transaction API）也定义了对 XA 事务的支持，实际上，JTA 是基于 XA 架构上建模的。在 JTA 中，事务管理器抽象为 javax.transaction.TransactionManager 接口，并通过底层事务服务（即 Java Transaction Service）实现。像很多其他的 Java 规范一样，JTA 仅仅定义了接口，具体的实现则是由供应商(如 J2EE 厂商)负责提供，目前 JTA 的实现主要有以下几种： J2EE 容器所提供的 JTA 实现(如JBoss)。 独立的 JTA 实现：如 JOTM（Java Open Transaction Manager），Atomikos。这些实现可以应用在那些不使用 J2EE 应用服务器的环境里用以提供分布事事务保证。 JTA 的使用示例（伪代码）： 1234567891011121314151617181920UserTransaction userTx = null;Connection connA = null;Connection connB = null;try&#123; userTx = getContext().lookup("java:comp/userTransaction"); // connA = 从数据库 A 获取连接 // connB = 从数据库 B 获取连接 // 启动分布式事务 userTx.begin(); // 在数据库 A 中执行操作 // 在数据库 B 中执行操作 // 提交事务 userTx.commit();&#125; catch(SQLException e) &#123; // 回滚 userTx.rollback();&#125; JTA 并没有取得像 JDBC 那样的广泛应用，分布式事务伴随着大量节点的通信交换， 协调者要确定其他节点是否完成， 加上网络带来的超时，导致 JTA 性能低下， 在高并发和高性能的场景下举步维艰。很多应用服务器 Websphere , Weblogic 等都支持 JTA， 可是使用者确是寥寥无几， 都快成摆设了。这个标准太理想化，完全不符合实情，总是想着让两个数据库保证实时的一致性（强一致性）， 为了达到这个目标，JTA 付出的代价太高了。 二阶段提交事务我们非常熟悉，特性 ACID，回滚非常的好用，但是一般只对单个数据库有用，如果需要夸数据库就会比较尴尬。 在计算机网络以及数据库领域内，二阶段提交（英语：Two-phase Commit）是指，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol)。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括为： 参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。 需要注意的是，二阶段提交(英文缩写：2PC)不应该与并发控制中的二阶段锁(英文缩写：2PL)进行混淆。wiki：https://zh.wikipedia.org/wiki/%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4 两阶段提交协议：由于涉及到多个分布式的数据库， 我们特设一个全局的事务管理器，它来负责协调各个数据库的事务提交， 为了实现分布式事务，需要两个阶段： 阶段一全局的事务管理器向各个数据库发出准备消息。 各个数据库需要在本地把一切都准备好，执行操作，锁住资源， 记录 redo/undo 日志， 但是并不提交。总而言之，要进入一个时刻准备提交或回滚的状态， 然后向全局事务管理器报告是否准备好了。 阶段二如果所有的数据库都报告说准备好了， 那全局的事务管理器就下命令： 提交！这时候各个数据库才真正提交，由于之前已经万事具备，只欠东风，只需要快速完成本地提交即可。如果有任何一个数据库报告说没准备好， 事务管理器就下命令： 放弃！这时候各个数据库要执行回滚操作， 并且释放各种在阶段1锁住的资源。 阶段1就是让大家都准备好，阶段2就是迅速提交。但是，一旦涉及到分布式，事情就不会那么简单，任何地方都有失败的可能。比如在第二阶段，那个事务管理器要是出了问题怎么办？ 人家各个数据库还在等着你发命令呢？ 你迟迟不发命令，大家都阻塞在那里，不知所措，到底是提交呢？还是不提交呢， 我这里还锁着资源呢， 迟迟不能释放，多耽误事啊 ！还是第二阶段，事务管理器发出的提交命令由于网络问题，数据库１收到了，数据库２没收到，这两个数据库就处于不一致状态了， 该怎么处理？ 拓展：X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型，或者叫 XA 规范， X/Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。二阶提交协议和三阶提交协议就是根据这一思想衍生出来的。 三阶段提交 三阶段提交（英语：Three-phase commit），也叫三阶段提交协议（英语：Three-phase commit protocol），是在计算机网络及数据库的范畴下，使得一个分布式系统内的所有节点能够执行事务的提交的一种分布式算法。 三阶段提交是为解决两阶段提交协议的缺点而设计的。 与两阶段提交不同的是，三阶段提交是“非阻塞”协议。三阶段提交在两阶段提交的第一阶段与第二阶段之间插入了一个准备阶段，使得原先在两阶段提交中，参与者在投票之后，由于协调者发生崩溃或错误，而导致参与者处于无法知晓是否提交或者中止的“不确定状态”所产生的可能相当长的延时的问题得以解决。 与 2PC 想比，3PC 主要有两个改动点： 引入超时机制。同时在协调者和参与者中都引入超时机制。 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的 第一阶段CanCommit这一阶段和 2PC 的准备阶段很像： 事务询问协调者向参与者发送 CanCommit 请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。 响应反馈参与者接到 CanCommit 请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回 Yes 响应，并进入预备状态。否则反馈 No 第二阶段PreCommit协调者根据参与者的反应情况来决定是否可以执行事务的 PreCommit 操作。根据响应情况，有以下两种可能。假如协调者从所有的参与者获得的反馈都是 Yes 响应，那么就会执行事务的预执行。 发送预提交请求协调者向参与者发送 PreCommit 请求，并进入 Prepared 阶段。 事务预提交参与者接收到 PreCommit 请求后，会执行事务操作，并将 undo 和 redo 信息记录到事务日志中。 响应反馈如果参与者成功的执行了事务操作，则返回 ACK 响应，同时开始等待最终指令。 假如有任何一个参与者向协调者发送了 No 响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 发送中断请求协调者向所有参与者发送 abort 请求。 中断事务参与者收到来自协调者的 abort 请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 第三阶段doCommit该阶段进行真正的事务提交，也可以分为以下两种情况。执行提交： 发送提交请求协调接收到参与者发送的 ACK 响应（第二阶段发送的），那么他将从预提交状态进入到提交状态。并向所有参与者发送 doCommit 请求。 事务提交参与者接收到 doCommit 请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 响应反馈事务提交完之后，向协调者发送 ACK 响应。 完成事务协调者接收到所有参与者的 ACK 响应之后，完成事务。 中断事务 协调者没有接收到参与者发送的 ACK 响应（可能是接受者发送的不是 ACK 响应，也可能响应超时），那么就会执行中断事务。 发送中断请求协调者向所有参与者发送 abort 请求 事务回滚参与者接收到 abort 请求之后，利用其在阶段二记录的 undo 信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 反馈结果参与者完成事务回滚之后，向协调者发送 ACK 消息 中断事务协调者接收到参与者反馈的 ACK 消息之后，执行事务的中断。 在 doCommit 阶段，如果参与者无法及时接收到来自协调者的 doCommit 或者 rebort 请求时，会在等待超时之后，会继续进行事务的提交。其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了 PreCommit 请求，那么协调者产生 PreCommit 请求的前提条件是他在第二阶段开始之前，收到所有参与者的 CanCommit 响应都是 Yes。一旦参与者收到了 PreCommit，意味他知道大家其实都同意修改了；所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到 commit 或者 abort 响应，但是他有理由相信：成功提交的几率很大。 与2PC的区别相对于 2PC，3PC 主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行 commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。了解了 2PC 和 3PC 之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby 的作者 Mike Burrows 说过： there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos. 意即世上只有一种一致性算法，那就是 Paxos，所有其他一致性算法都是 Paxos 算法的不完整版。 最终一致性知道了上面的知识，我们可以退一步，也就是我们可以忍受一段时间的不一致，只有最终一致就行。 比方说 A 给 B 转 100 元， A 中的钱已经扣除， 但是 B 中不会实时地增加，过段时间能保证增加就行了。 “ 假设两个账户（吕秀才和郭芙蓉）在两个独立的数据库中， 我们原来设计的 JTA 是要求从吕秀才账户减去 100 两银子， 然后在郭芙蓉账户加上 100 两银子， 这两个操作要么全部做完，要么全部不做， 但是在网络的环境下， 这是不大容易做到的， 或者说在高并发的情况下做到的代价太高。” 我们想从吕秀才账户转 100 两银子给郭芙蓉， 需要在数据库 1 发起一个事务， 从吕秀才账户扣除 100 两， 然后还得向消息队列插入一条给郭芙蓉账号增加 100 两的消息， 然后这个数据库 1 的事务就结束了！消息队列中的消息会在某一刻被读取出来，进行处理，给郭芙蓉的账号增加 100 两。至于这个加钱的操作什么时候执行，这个时间不确定， 就看具体怎么实现了， 比如有个后台程序定期运行，读取消息来处理，消息队列的数据都是持久化到硬盘上的， 不用怕宕机会丢失。假设数据库 2 down 机了， 对郭芙蓉有两种选择：一种是由于系统原因，转账操作完全不能操作；另外一种是可以转账，但是钱稍后到账；你说郭芙蓉会选哪一种？这就是最终一致性，数据在某些时候看起来不一致，但是同步内容都在消息队列中暂存着，等数据处理完成，数据还是一致的。第一种情况是完全不可用， 第二种只是是部分可用，对于高并发的场景，转账的时候扣完钱， 向消息队列插入消息，事务就结束了，根本不用什么两阶段提交，性能很好。 上面的方案看似非常完美，但是，还隐藏有一个重大问题： 这个事务同时操作了数据库和一个消息队列， 这两个东西是完全不同的， 怎么实现？难道再用 JTA？JTA 不仅仅可以支持数据库， 只要是支持XA协议的数据源都可以。 解决方案： 在这里，我们可以添加一个“事件表”， 转账开始的时候，把吕秀才的 100 两银子扣除， 同时还向事件表插入一行记录： 需要向郭芙蓉转 100 两， 由于这两个表是在同一个数据库中，所以直接使用本地事务就行。不用什么分布式事务。而那个定时运行程序就是个定时任务，它会定期从事件表中取出记录， 向 MQ 写入消息， 然后把记录的状态改成 “DONE”， 这样下次就不用再去取去处理了。这里还有一个坑，读数据后，向消息队列写入了消息， 如果还没来得及把事件表的 status 改为 “DONE” 就崩溃了，等到定时运行程序重启以后，会再次读取， 再次向 MQ 写入消息，这样整个系统就不一致了。所以必须要做到幂等性。 当你对一个事物操作的时候，可以一直重复地操作，那个事物不受影响， 例如对郭芙蓉的账号你查询一千次，一万次，账户余额还是那么多，不会变化。转账操作就不是一个幂等性操作，每次操作都会导致账号的变化。简单理解：read 是幂等的， modify 不是幂等的 也就是那个定时运行的程序可以出错，可以向消息队列写入多次重复消息 ， 但是消费方那边在执行的时候， 肯定也要判断之前是否执行过了， 如果没有的话就执行， 如果执行过了就简单的抛弃这个消息即可。所以消费方在判断是否已经执行过的时候（可以设置一个 id 来进行区分），也需要查询之前的执行记录， 这就意味着之前执行过的也需要用一个表保存下来才行。 有人说，可以消息内容直接为账户值 + 100，这样，无论任务操作多少次，郭芙蓉账户都是账户值 +100。但是这样首先要保证顺序，并且还是得标记这个事儿已经做过了，并且对于日志不是很友好，采用的不是很多吧。 如果遇到是对方账号输入错误，钱老是转不进去，也就是消息无法消费，只能使用补偿机制了（手动 or 自动？）。这些都是建立在 MQ 是稳定、高可用的前提下。。。。这种方法也就是 BASE 模型，BASE 模型反 ACID 模型，完全不同 ACID 模型，牺牲高一致性，获得可用性或可靠性。 Paxos算法首先来看一段解释，虽然未必看得懂….. Paxos算法是莱斯利·兰伯特（英语：Leslie Lamport）于1990年提出的一种基于消息传递且具有高度容错特性的一致性算法。为描述 Paxos 算法，Lamport 虚拟了一个叫做 Paxos 的希腊城邦，这个岛按照议会民主制的政治模式制订法律，但是没有人愿意将自己的全部时间和精力放在这种事情上。所以无论是议员，议长或者传递纸条的服务员都不能承诺别人需要时一定会出现，也无法承诺批准决议或者传递消息的时间。但是这里假设没有拜占庭将军问题（Byzantine failure，即虽然有可能一个消息被传递了两次，但是绝对不会出现错误的消息）；只要等待足够的时间，消息就会被传到。另外，Paxos 岛上的议员是不会反对其他议员提出的决议的。 主要目的是通过这个算法，让参与分布式处理的每个参与者逐步达成一致意见。用好理解的方式来说，就是在一个选举过程中，让不同的选民最终做出一致的决定（少数服从多数）。paxos 是个分布式一致性协议，它的事件需要多个节点共同参与，一个事件完成是指多个节点上均完成了自身负责的单机子事件(就让我门把这样的事件称为”分布式事件”)，这样的分布式事件可以看作是多个单机子事件的复合，但是即不能从两个分布式事件的先后推导出某个节点上它们的单机子事件的先后，也不能根据某个节点上两个单机子事件的先后断言它们对应的分布式事件的先后。自 Paxos 问世以来就持续垄断了分布式一致性算法，Paxos 这个名词几乎等同于分布式一致性。Google 的很多大型分布式系统都采用了 Paxos 算法来解决分布式一致性问题，如 Chubby、Megastore 以及 Spanner 等。开源的 ZooKeeper，以及 MySQL 5.7 推出的用来取代传统的主从复制的 MySQL Group Replication 等纷纷采用 Paxos 算法解决分布式一致性问题。然而，Paxos 的最大特点就是难，不仅难以理解，更难以实现。 Paxos 算法运行在允许宕机故障的异步系统中，不要求可靠的消息传递，可容忍消息丢失、延迟、乱序以及重复。它利用大多数 (Majority) 机制保证了 2F+1 的容错能力，即 2F+1 个节点的系统最多允许 F 个节点同时出现故障。一个或多个提议进程 (Proposer) 可以发起提案 (Proposal)，Paxos 算法使所有提案中的某一个提案，在所有进程中达成一致。系统中的多数派同时认可该提案，即达成了一致。最多只针对一个确定的提案达成一致。Paxos 将系统中的角色分为提议者 (Proposer)，决策者 (Acceptor)，和最终决策学习者 (Learner): 倡议者（Proposer）：倡议者可以提出提议（数值或者操作命令）以供投票表决 接受者（Acceptor）：接受者可以对倡议者提出的提议进行投票表决，提议有超半数的接受者投票即被选中 学习者（Learner）：学习者无投票权，只是从接受者那里获知哪个提议被选中 花费一晚上来看 Paxos ，本来想简单写写这个算法的原理，但是发现太难了！并且我也不确定是否真正看懂了，不过找到了两篇感觉比较好的文章放在了最后，有能力的请去看原作者论文。。。 Raft算法不同于 Paxos 算法直接从分布式一致性问题出发推导出来，Raft 算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制。Raft 实现了和 Paxos 相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。同时，Raft 算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。 参考：https://zhuanlan.zhihu.com/p/32052223 NWR模型引用一句话： 要想让数据有高可用性，就得写多份数据。写多份的问题会导致数据一致性的问题。数据一致性的问题又会引发性能问题这就是软件开发，按下了葫芦起了瓢。 一致性模型： Weak 弱一致性：当你写入一个新值后，读操作在数据副本上可能读出来，也可能读不出来。 Eventually 最终一致性：当你写入一个新值后，有可能读不出来，但在某个时间窗口之后保证最终能读出来。 Strong 强一致性：新的数据一旦写入，在任意副本任意时刻都能读到新值。 NWR 模型是 Amazon Dynamo 系统中提出的一个概念，非常的有意思，也值得学习分布式系统的人进行好好的思考。所谓 NWR 模型，N 代表 N 个备份，W 代表要写入至少 W 份才认为成功，R 表示至少读取 R 个备份。配置的时候要求 W+R&gt;N。因为 W+R&gt;N，所以 R&gt;N-W，这指的是读取的份数一定要比总备份数减去确保写成功的倍数的差值要大。也就是说，每次读取，都至少读取到一个最新的版本。当我们需要高可写的环境的时候，我们可以配置 W=1，这个时候只要写任何节点成功就认为成功，但是读的时候必须从所有的节点都读出数据。如果我们要求读的高效率，我们可以配置 W=N，R=1。这个时候任何一个节点读成功就认为成功，但是写的时候必须写所有三个节点成功才认为成功。NWR 模型的一些设置会造成脏数据的问题，因为这很明显不是像 Paxos 一样是一个强一致的东西，所以，可能每次的读写操作都不在同一个结点上，于是会出现一些结点上的数据并不是最新版本，但却进行了最新的操作。所以，Amazon Dynamo 引入了数据版本的设计。也就是说，如果你读出来数据的版本是 v1，当你计算完成后要回填数据后，却发现数据的版本号已经被人更新成了 v2，那么服务器就会拒绝你。版本这个事就像“乐观锁”一样。但是，对于分布式和 NWR 模型来说，版本也会有恶梦的时候 — 就是版本冲的问题，不过这里不多探讨了。Amazon Dynamo 的 NWR 模型，把 CAP 的选择权交给了用户，让用户自己选择 CAP 中的哪两个。 如果 W+R&gt;N ，是可以保证强一致性的。因为 W+R &gt; N， 所以 R &gt; N-W，什么意思呢？就是读取的份数必须要大于未成功写的份数，这样至少能读到一份最新值。 如果 W+R&lt;=N，则能够保证最终一致性。 如果我们要高可写的环境，我们可以配置 W=1，R=N。这个时候只要写任何节点成功就认为成功，但是读的时候必须从所有的节点都读出数据。 如果我们要求读的高效率，我们可以配置 W=N，R=1。这个时候任何一个节点读成功就认为成功，但是写的时候必须写所有三个节点成功才认为成功。 优化写性能(AP)当我们需要优化写性能（写多读少）的时候，可以配置 W = 1 （写完一个副本就成功，其他的副本就异步去慢慢复制）；如果 N=3，那么根据公式 W+R&gt;N，则 R = 3（读取数据的时候需要读 3 个副本以判断数据是否有冲突）。这种情况只要写任何节点成功就认为成功，但是读的时候必须从所有的节点都读出数据。 优化读性能(CP)当我们需要优化读性能（读多写少）的时候，可以配置 W=N（写完所有的副本才成功，只能同步复制）；根据公式 W+R&gt;N，则 R=1（只需读一个副本即可）。这种情况任何一个节点读成功就认为成功，但是写的时候必须写所有三个节点成功才认为成功。 平衡读写性能(AC)当我们数据不多，单台能搞定，且不需要容错和扩展性的时候，可以配置 N=1（只有一份数据）；根据公式 W+R&gt;N，则 W=1，R=1。这种情况就简化为单机问题了。 数据库中的事务要谈到事务的实现方式，最重要的要属 Undo 日志了，简单来聊一聊就是：事务开始后，在做具体的操作之前，会先写入日志，例如来一个经典的转账例子： [事务T1, 旺财原有余额 ， 200][事务T1, 小强原有余额， 50 ] 如果事务执行到一半，就断电了，那数据库重启以后可根据 undo 的日志文件来恢复。并且，记录的日志是可以做到幂等性的，也就是可以一直做恢复，恢复过程中断电也不怕，只要把恢复做完就行。恢复数据的时候， 那你怎么才能知道一个事务没有完成呢？ [开始事务 T1][事务T1, 旺财原有余额，200][事务T1, 小强原有余额，50][提交事务 T1] Undo 日志文件中不仅仅只有余额， 事务的开始和结束也会记录，如果我在日志文件中看到了 【提交事务 T1】，或者 【回滚事务 T1】， 我就知道这个事务已经结束，不用再去理会它了， 更不用去恢复。 如果我只看到 【开始事务 T1】, 而找不到提交或回滚，那我就得恢复。特别是，恢复以后， 需要在日志文件中加上一行 【回滚事务 T1】 ， 这样下一次恢复就不用再考虑 T1 这个事务了。 Undo 日志文件会面临和数据文件一样的问题， 都是需要加载到内存才能读写， 要不然会太慢。 那要是连日志文件还没写好就断电了呢？要解决这个问题，也不难，只需要遵守两条规则就好： 操作之前要把对应的日志写入硬盘的日志文件 像“提交事务”这样的日志，一定要在操作完成后写入到硬盘 通常把日志记录也放到了内存的 Undo 日志缓冲区，伺机写入硬盘。 参考https://mp.weixin.qq.com/s/J1WH4ZYyVWGgXx9g2siocwhttps://mp.weixin.qq.com/s/59iztoTssmIVri7UkZeGzwhttps://mp.weixin.qq.com/s/92SghOorf10dm3pM0DWzIghttp://www.hollischuang.com/archives/681http://blog.xiaohansong.com/2016/09/30/Paxos/https://segmentfault.com/a/1190000013478009http://lemon0910.github.io/%E7%B3%BB%E7%BB%9F/2016/04/29/distributed-summary.html]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot进阶]]></title>
    <url>%2F2018%2F08%2F26%2FSpringBoot%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[看了下之前的做的 SpringBoot 笔记连入门都算不上，顶多是个体验，然后外加现在 SpringBoot 这么火，还是有记录一下的价值的，也是为了更进一步了解 SpringBoot，同时也是为之后的 SpringCloud 做铺垫；这次的笔记基于 1.x 的版本，后续打算会跟进 2.x 版本，对于这一点，SpringBoot 比较任性，2.x 和 1.x 的版本有很大的改动，虽然原理是差不多的，但是方法说删就删…..之后有机会再总结吧，在那篇体验里也介绍过一些 2.x 的特性，慢慢来~ 主程序入口使用 SpringBoot 必须在 pom 文件中配置父工程，父工程中定义了大量的 JavaEE 常用库的版本号（用来做“版本仲裁”），这个大家都知道，就不多说了；然后我们知道在启动类上标注 @SpringBootApplication 注解，然后在 main 方法中运行：SpringApplication.run(HelloWorldMainApplication.class,args); 就可以让 web 工程跑起来（当然需要在 pom 中配置相关依赖，比如各种方便好用的各种 starter） 为简化部署，SpringBoot 提供了 spring-boot-maven-plugin 的 Maven 插件，使用后可以直接通过 Java -jar 命令来运行 jar 包。 SpringBoot 要求 run 方法第一个参数必须是 @SpringBootApplication 注解标注的类，既然这样就来看看这个注解是如何定义的： 12345678910@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123;...&#125; 一个一个来看，首先是 @SpringBootConfiguration 这个注解，从名字可以看出是 SpringBoot 的配置类，它其实继承了 @Configuration 注解，也就间接的继承了 @Component 注解，官方建议在 SpringBoot 应用中优先使用 @SpringBootConfiguration 注解。再来看 @EnableAutoConfiguration 这个注解，从名字来看是开启自动配置，自动配置应该是 SpringBoot 的一大核心了： 123@AutoConfigurationPackage@Import(EnableAutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123;&#125; 此注解继承了 @AutoConfigurationPackage ，也就是自动配置包，它里面重要的一句代码是：@Import(AutoConfigurationPackages.Registrar.class)： ，Spring 的底层注解 @Import 应该很熟悉了，主要是给容器中导入一个组件，这个类如果读源码的话，主要的作用就是将主配置类（@SpringBootApplication 标注的类）的所在包及下面所有子包里面的所有组件扫描到 Spring 容器，这就可以解释一些问题了！然后接下来看导入的 EnableAutoConfigurationImportSelector 这个类，从名字来看是来决定导入那些组件的选择器，它会将所有需要导入的组件以全类名的方式返回；这些组件（其实是自动配置类）就会被添加到容器中；经过上面的操作，将会给容器中导入非常多的自动配置类（xxxAutoConfiguration）；他们的作用就是根据当前环境的依赖配置好这些组件。有了自动配置类，免去了我们手动编写配置注入功能组件等的工作。Spring Boot 在启动的时候从类路径下的 META-INF/spring.factories 中获取 EnableAutoConfiguration 指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，在特定的环境下帮我们进行自动配置工作； 配置文件号称无配置的 SpringBoot 其实就是给我们做了常见的自动配置，如上面所解释，避免淹没在无尽的配置文件中，但自动配置不可能符合每个项目的需求，所以 SpringBoot 必定要提供定制的方法，如果继续采用传统的 XML 文件来配置，那显得还是太复杂了，properties 是个不错的选择，同时，还支持一种新型的流行配置语法 yaml！YAML 以数据为中心，比 json、XML 等更适合做配置文件。 YAML语法基本语法：形如 K:(空格)V 这样的形式。以空格的缩进来控制层级关系，空格多少无所谓，只要左对齐就行 ，同时，它的属性和值是大小写敏感的。对于值的写法，可分为下面几种形式： 字面量数字、字符串、布尔 直接写就可以了；特殊的，双引号和单引号，双引号内的特殊字符会转义，比如 \n ；单引号内的字符串不会被转义。 对象、Map另起一行写属性，例如： 123456friends: name: zhangsan age: 20# 还支持行内写法friends: &#123;name: zhansan,age: 20&#125; 两种写法效果一样，看个人喜好咯。 集合用 - 表示数组中的一个元素，例如： 123456pets: - cat - dog# 行内写法pets: [cat,dog] 两种写法的效果也是一致的。 配置文件的值注入将配置文件中配置的属性映射到 bean 中，使用 @ConfigurationProperties(prefix=&quot;&quot;) 注解实现。需要注意的是，这个 bean 必须在 spring 容器中才行；其支持松散绑定，也就是说你可以使用驼峰、下划线分割（测试日期格式使用 2018/08/12 的格式是可正确注入），都会正确的识别，还支持 JSR303 校验规则，可以使用相关的校验注解，只需要在类上加个 @Validated 就好。这个注解默认从全局配置文件中获取值。要使用 @ConfigurationProperties 最好导入这个依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 当时测试如果在 @Configuration 标注的类上无法注入，原因就是缺少这个依赖，当然在普通 Bean 是没有问题的，迷….你甚至可以直接把它标注在 @Bean 的方法上，比如数据源，可以直接注入到返回的 Bean 中，不用在调用那么多 setter 方法了。除了使用 @ConfigurationProperties 注解，还可以使用 @Value 注解注入单个值，类似我们 xml 中 bean 标签里的 property 的 Value，所以它支持几种写法： 字面量 ${配置文件属性} SPEL 表达式：{spel} 还可以做一些简单的运算，可以说定制性很高了，至于他们的比较： - @ConfigurationProperties @Value 功能 批量注入配置文件中的属性 一个个指定 松散绑定（松散语法） 支持 不支持 SpEL 不支持 支持 JSR303 数据校验 支持 不支持 复杂类型封装 支持 不支持 不管配置文件是 yml 还是 properties 他们都能获取到值；如果说，我们只是在某个业务逻辑中需要获取一下配置文件中的某项值，使用 @Value；如果说，我们专门编写了一个javaBean来和配置文件进行映射，我们就直接使用 @ConfigurationProperties； 有时候，我们并不希望把所有配置都写在主配置文件中，而是希望指定从那个配置文件中加载，那么就可以使用 @PropertySource 注解了。 123@PropertySource(value = &#123;"classpath:person.properties"&#125;)@Component@ConfigurationProperties(prefix = "person") 通常，他们都是搭配使用的。 1.5 以前的版本，那么可以通过 ConfigurationProperties 注解的 locations 指定 properties 文件的位置 ；但是 1.5 版本后就没有这个属性了，需要添加 @Configuration 和 @PropertySource()后才可以读取 SpringBoot 还有另一个导入配置文件的注解 @ImportResource：导入Spring 的配置文件，让配置文件里面的内容生效；这个导的是原始 Spring 的 XML 配置文件，可以写在 SpringBoot 配置类上，比如 SpringBoot 的启动类，但是官方是不推荐的，建议使用 Java 配置的方式（@Configuration）。 配置文件占位符在 SpringBoot 的配置文件中，是可以使用 ${xx} 这种表达式的，比如可以使用它来获取随机数：${random.value}、${random.int}、${random.long}、${random.int(10)}、${random.int[1024,65536]} ；也可以使用它引用之前配置的值：${person.hello:hello} 通过冒号可以设置默认值。 多环境在 Maven 中是支持多环境的，操作有点繁琐，SpringBoot 默认就集成了这个功能，它以文件名进行区分不同的环境：application-{profile}.properties/yml .如果使用的是 yaml 文件，还可以使用 --- 来进行划分： 123456789101112131415server: port: 8081spring: profiles: active: prod---server: port: 8083spring: profiles: dev---server: port: 8084spring: profiles: prod #指定属于哪个环境 至于激活那个环境，除了在主配置文件里配置，还有很多方式，比如命令行参数（--spring.profiles.active=dev）、JVM 参数（-Dspring.profiles.active=dev） 配置文件的加载springboot 启动会扫描以下位置的 application.properties 或者 application.yml 文件作为 Spring boot 的默认配置文件： 当前项目下的 Config 文件夹（file:./config/） 当前项目下（file:./） 类加载路径下的 Config 文件夹（classpath:/config/） 类加载路径下（classpath:/） 优先级由高到底，高优先级的配置会覆盖低优先级的配置；互补配置 ；另外，我们还可以通过在命令行指定 spring.config.location 来改变默认的配置文件位置，同样也是互补配置。至于加载顺序，以及会扫描加载那些外部配置，官方定义了很多路径，这里就不说了，有需要的可以看官方文档，地址在这：官方文档 配置文件能配的属性全都在这了：官网直达 自动配置关于自动配置，简单来讲，通过前面的主程序入口解析，我们知道 SpringBoot 在启动的时候会加载包下指定文件夹下的文件，然后导入了一堆的自动配置类；这些自动配置类都是一样的套路，与之配套的还有一个 xxxProperties 类，这个类的作用就是通过 @ConfigurationProperties 注入配置文件中配置的属性，然后自动配置类中就可以使用这些值了；当然自动配置类还有一些 Conditional 注解来控制根据当前环境加载某些配置，最后就通过默认配置创建出了一个个的 Bean，而不需要我们再显式的声明了。虽然文件中指定加载了一堆的自动配置类，但是很多的自动配置类都需要一些条件才能生效，所以并不是所有的功能都会生效的。 关于日志SpringBoot 中默认使用的日志是 SLF4J + logback，然而 Spring 使用的是 JCL，日志统一是个问题。关于 SLF4J 的使用，应该是都比较熟悉了，SpringBoot 中的 spring-boot-starter 中默认导入了一个 spring-boot-starter-logging ，它的作用就是来处理日志框架不统一的问题，使用各种 over 来转换成 SLF4J。正是因为有它，所以 SpringBoot 能自动适配所有的日志，而且底层使用 slf4j+logback 的方式记录日志，引入其他框架的时候，只需要把这个框架依赖的日志框架排除掉即可。并且，SpringBoot 会给我们默认配置日志的输出格式，也可以在配置文件中微调，或者直接将配置文件复制到 Resources 文件夹下。 1234567891011121314# 设置等级，可以具体到包logging.level.com.bfchengnuo=trace# 不指定路径在当前项目下生成springboot.log日志# 可以指定完整的路径；#logging.file=G:/springboot.log# 在当前磁盘的根路径下创建spring文件夹和里面的log文件夹；使用 spring.log 作为默认文件logging.path=/spring/log# 在控制台输出的日志的格式logging.pattern.console=%d&#123;yyyy-MM-dd&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n# 指定文件中日志输出的格式logging.pattern.file=%d&#123;yyyy-MM-dd&#125; === [%thread] === %-5level === %logger&#123;50&#125; ==== %msg%n SpringBoot 默认设置的日志等级是 info，滚动输出，最大文件 10M logging.file logging.path Example Description (none) (none) - 只在控制台输出 指定文件名 (none) my.log 输出日志到my.log文件 (none) 指定目录 /var/log 输出到指定目录的 spring.log 文件中 如果使用配置文件，多种文件名都可以被识别，例如 logback-spring.xml 和 logback.xml ，后者直接被日志框架所识别，而前者是由 Spring 来进行处理，所以它可以支持一些更强大的功能，例如 springProfile 标签： 123456789101112131415&lt;springProfile name="staging"&gt; &lt;!-- 可以指定某段配置只在某个环境下生效 --&gt;&lt;/springProfile&gt;&lt;!-- 下面举个例子 --&gt;&lt;appender name="stdout" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;layout class="ch.qos.logback.classic.PatternLayout"&gt; &lt;springProfile name="dev"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; ----&gt; [%thread] ---&gt; %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;/springProfile&gt; &lt;springProfile name="!dev"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; ==== [%thread] ==== %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;/springProfile&gt; &lt;/layout&gt;&lt;/appender&gt; 并且，你还可以切换日志框架，例如像从 slf4j 切换到 log2j2，只需要导入 spring-boot-starter-log4j2 这个依赖即可；因为和 spring-boot-starter-logging 是二选一的关系，所以记得排除依赖。使用 spring-boot-starter-log4j2 也会有相应的默认配置，官方文档中写的还是很详细的。 Web开发如果看 SpringBoot web 的自动配置，会发现默认的静态资源映射支持 webjars，就是将所有 /webjars/** 的请求映射到 classpath:META-INF/resources/webjars 下。 webjars 简单说就是可以将 js、css 等前端使用的库通过 jar 包的方式导入到项目中，支持使用 Maven 管理，默认打包在 classpath:META-INF/resources/webjars 文件夹下。 当请求没人处理时，会交给一个 /** 全局映射，默认从下面几个路径中寻找： 12345&quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;,&quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; &quot;/&quot;：当前项目的根路径 还贴心的设置了欢迎页：静态资源文件夹下的所有叫 index.html 的页面；被 “/“ 映射。所有的 /favicon.ico 都是在静态资源文件下找，可以来设置自己喜欢的网站图标。相关代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// WebMvcAuotConfiguration@Overridepublic void addResourceHandlers(ResourceHandlerRegistry registry) &#123; if (!this.resourceProperties.isAddMappings()) &#123; logger.debug("Default resource handling disabled"); return; &#125; Integer cachePeriod = this.resourceProperties.getCachePeriod(); if (!registry.hasMappingForPattern("/webjars/**")) &#123; customizeResourceHandlerRegistration( registry.addResourceHandler("/webjars/**") .addResourceLocations( "classpath:/META-INF/resources/webjars/") .setCachePeriod(cachePeriod)); &#125; String staticPathPattern = this.mvcProperties.getStaticPathPattern(); //静态资源文件夹映射 if (!registry.hasMappingForPattern(staticPathPattern)) &#123; customizeResourceHandlerRegistration( registry.addResourceHandler(staticPathPattern) .addResourceLocations( this.resourceProperties.getStaticLocations()) .setCachePeriod(cachePeriod)); &#125;&#125;//配置欢迎页映射@Beanpublic WelcomePageHandlerMapping welcomePageHandlerMapping( ResourceProperties resourceProperties) &#123; return new WelcomePageHandlerMapping(resourceProperties.getWelcomePage(), this.mvcProperties.getStaticPathPattern());&#125;//配置喜欢的图标@Configuration@ConditionalOnProperty(value = "spring.mvc.favicon.enabled", matchIfMissing = true)public static class FaviconConfiguration &#123; private final ResourceProperties resourceProperties; public FaviconConfiguration(ResourceProperties resourceProperties) &#123; this.resourceProperties = resourceProperties; &#125; @Bean public SimpleUrlHandlerMapping faviconHandlerMapping() &#123; SimpleUrlHandlerMapping mapping = new SimpleUrlHandlerMapping(); mapping.setOrder(Ordered.HIGHEST_PRECEDENCE + 1); //所有 **/favicon.ico mapping.setUrlMap(Collections.singletonMap("**/favicon.ico", faviconRequestHandler())); return mapping; &#125; @Bean public ResourceHttpRequestHandler faviconRequestHandler() &#123; ResourceHttpRequestHandler requestHandler = new ResourceHttpRequestHandler(); requestHandler .setLocations(this.resourceProperties.getFaviconLocations()); return requestHandler; &#125;&#125; 上面的代码就处理了静态文件的映射规则。当然我们也可以自定义路径规则，使用 spring.resources.static-locations=classpath:/hello/,classpath:/test/ ，但是这样 SpringBoot 的那些默认配置就失效了。 模板引擎ThymeleafSpringBoot 推荐的 Thymeleaf 虽然效率上经常被人黑，也确实很低，不过对于前后端解耦是比较友好的，要使用首先要引入依赖，对于 SpringBoot 直接加一个 starter 就好：spring-boot-starter-thymeleaf另外，你可以指定你要引入的版本： 123456&lt;properties&gt; &lt;thymeleaf.version&gt;3.0.9.RELEASE&lt;/thymeleaf.version&gt; &lt;!-- 布局功能的支持程序 thymeleaf3主程序 layout2以上版本 --&gt; &lt;!-- thymeleaf2 layout1--&gt; &lt;thymeleaf-layout-dialect.version&gt;2.2.2&lt;/thymeleaf-layout-dialect.version&gt;&lt;/properties&gt; 在 Thymeleaf 的自动配置中，设置的默认前缀是 classpath:/templates/ 默认后缀是 .html ，也就是说只要我们把 HTML 页面放在这个路径下，thymeleaf 就能自动渲染。然后在 HTML 导入名称空间（为了有代码提示）：&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;如何使用参考官方文档例子很详细，我也考虑写一个笔记放在 Github。简单来说就是他支持 OGNL 表达式，可以直接使用 th:任意html属性 替换原生 html 属性，如果直接打开，定义的这些属性就不会解析，如果使用模板引擎就替换为了 th:text 中的变量，前后端非常和谐。 SpringMVC的自动配置在 SpringBoot 的官方文档中有比较详细的描述，地址在这。自动配置的关键就在 WebMvcAutoConfiguration 这个类中，根据文档描述，主要做了下面几件事情： 自动配置了 ViewResolver （视图解析器，根据返回值得到具体的视图对象，视图对象决定如何渲染，例如是转发还是重定向） 使用 ContentNegotiatingViewResolver 组合所有的视图解析器，只要在容器中配一个试图解析器，就会自动组合进来。 静态资源文件夹路径。比如上面所说的 webjars、静态首页、图标等。 自动注册了 Converter, GenericConverter, Formatter 等 bean.转换器：请求参数与实体类之间的类型转换使用的就是 Converter；格式化器：例如日期格式化的注解，自己添加的格式化器转换器，我们只需要放在容器中即可； 自动注册消息转换器例如 HttpMessageConverter 将实体对象转换成 json 等，自定义的方式也和上面一样。另外还有定义错误代码生成规则的 MessageCodesResolver 等。 相应扩展 SpringMVC 的配置，只需要编写一个配置类（@Configuration），是 WebMvcConfigurerAdapter 类型（继承它）；不标注 @EnableWebMvc（加上了就不会进行默认配置了，也就是说全面接管 MVC）之前我们通常在 SpringMVC中 中配置 HiddenHttpMethodFilter 来使其支持 RESTful，现在 SpringBoot 也给自动配置好了，只需要在前台创建一个 input 项（一般设置为隐藏），name=&quot;_method&quot; 值就是我们指定的请求方。 修改SpringBoot默认配置一般套路为： SpringBoot 在自动配置很多组件的时候，先看容器中有没有用户自己配置的（@Bean、@Component）如果有就用用户配置的，如果没有，才自动配置；有些组件（例如 ViewResolver）可以将用户配置的和自己默认的组合起来； 在SpringBoot中会有非常多的 xxxConfigurer 帮助我们进行扩展配置 在 SpringBoot 中会有很多的 xxxCustomizer 帮助我们进行定制配置 错误处理SpringBoot 有默认的错误处理机制，在浏览器访问的时候返回的是错误页面，其他客户端返回的是 JSON 格式的错误信息。至于原理，其实是根据请求头来不同的处理，可以在 ErrorMvcAutoConfiguration 这个错误处理的自动配置类中看看具体是怎么实现的。它主要给容器添加了下面几个组件： DefaultErrorAttributes主要是帮我们在页面共享信息，通过一个 getErrorAttributes 方法来组装了错误页面需要的信息： 12345678910@Overridepublic Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; Map&lt;String, Object&gt; errorAttributes = new LinkedHashMap&lt;String, Object&gt;(); errorAttributes.put("timestamp", new Date()); addStatus(errorAttributes, requestAttributes); addErrorDetails(errorAttributes, requestAttributes, includeStackTrace); addPath(errorAttributes, requestAttributes); return errorAttributes;&#125; ErrorAttributes 我们可以进行自定义。 BasicErrorController它处理默认 /error 请求，我们可以通过 server.error.path 来自定义，它会根据请求头信息，来决定走那个方法，相关的代码： 12345678910111213141516171819202122232425@Controller@RequestMapping("$&#123;server.error.path:$&#123;error.path:/error&#125;&#125;")public class BasicErrorController extends AbstractErrorController &#123; // 产生html类型的数据；浏览器发送的请求来到这个方法处理 @RequestMapping(produces = "text/html") public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) &#123; HttpStatus status = getStatus(request); Map&lt;String, Object&gt; model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); //去哪个页面作为错误页面；包含页面地址和页面内容 ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView == null ? new ModelAndView("error", model) : modelAndView); &#125; @RequestMapping @ResponseBody //产生json数据，其他客户端来到这个方法处理； public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; body = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.ALL)); HttpStatus status = getStatus(request); return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(body, status); &#125; ErrorPageCustomizer系统出现错误以后，让其来到 error 请求进行处理，可以说是错误的入口类了。 DefaultErrorViewResolver可以说，它是来觉定走那个视图的，源码写的很明白： 1234567891011121314private ModelAndView resolve(String viewName, Map&lt;String, Object&gt; model) &#123; //默认SpringBoot可以去找到一个页面？ error/404 String errorViewName = "error/" + viewName; //模板引擎可以解析这个页面地址就用模板引擎解析 TemplateAvailabilityProvider provider = this.templateAvailabilityProviders .getProvider(errorViewName, this.applicationContext); if (provider != null) &#123; //模板引擎可用的情况下返回到errorViewName指定的视图地址 return new ModelAndView(errorViewName, model); &#125; //模板引擎不可用，就在静态资源文件夹下找errorViewName对应的页面 error/404.html return resolveResource(errorViewName, model);&#125; 模板引擎中可以使用 OGNL 之类的表达式来取值，静态资源（例如 static 文件夹下）就不行啦 下面来总结下：一但系统出现 4xx 或者 5xx 之类的错误；ErrorPageCustomizer 就会生效（可定制错误的响应规则）；默认就会来到 /error 请求；就会被 BasicErrorController 处理；响应去哪个页面是由 DefaultErrorViewResolver 解析得到的，有模板引擎的情况下；error/状态码【将错误页面命名为 错误状态码.html 放在模板引擎文件夹里面的 error 文件夹下】，发生此状态码的错误就会来到对应的页面。我们可以使用 4xx 和 5xx 作为错误页面的文件名来匹配这种类型的所有错误，精确优先。页面能获取的信息有： timestamp：时间戳 status：状态码 error：错误提示 exception：异常对象 message：异常消息 errors：JSR303 数据校验的错误都在这里 模板引擎和静态资源文件夹都没有错误页面，就是默认来到 SpringBoot 默认的错误提示页面。 定制错误数据统一处理异常还是用 SpringMVC 的知识，首先写一个类： 1234567891011@ControllerAdvicepublic class MyExceptionHandler &#123; @ResponseBody @ExceptionHandler(UserNotExistException.class) public Map&lt;String,Object&gt; handleException(Exception e)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put("code","user.notexist"); map.put("message",e.getMessage()); return map; &#125;&#125; 但是这种呢，没有自适应效果（不能区分浏览器和其他客户端），然后改进了一下： 12345678910@ExceptionHandler(UserNotExistException.class)public String handleException(Exception e, HttpServletRequest request)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); // 传入我们自己的错误状态码 4xx 5xx，否则就不会进入定制错误页面的解析流程，因为forward默认 200 request.setAttribute("javax.servlet.error.status_code",500); map.put("code","user.notexist"); map.put("message",e.getMessage()); // 转发到 /error return "forward:/error";&#125; 这样自适应是有了（靠 SpringBoot 来实现），但是我们自定义的数据如何传递过去又是个问题了，错误请求最终会被 BasicErrorController 处理，响应出去可以获取的数据是由 getErrorAttributes 得到的（是AbstractErrorController（ErrorController）规定的方法，所以我们可以编写一个 ErrorController 的实现类【或者是编写 AbstractErrorController 的子类】，放在容器中。但是重新编写实现类太麻烦了，收集这些信息是通过 DefaultErrorAttributes.getErrorAttributes() 这个方法完成的，所以有更简便的方法就是写一个 ErrorAttributes。 12345678910//给容器中加入我们自己定义的 ErrorAttributes@Componentpublic class MyErrorAttributes extends DefaultErrorAttributes &#123; @Override public Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; Map&lt;String, Object&gt; map = super.getErrorAttributes(requestAttributes, includeStackTrace); map.put("company","xxx"); return map; &#125;&#125; 当我们自定义了 ErrorAttributes 后，SpringBoot 就不再加载默认的 ErrorAttributes 而是使用容器中已存在的，这样就可以取到我们自定义的数据了。 嵌入式Servlet容器SpringBoot 默认使用 Tomcat 作为嵌入式的 Servlet 容器，我们可以通过配置文件来进行定制： 12345678server.port=8081server.context-path=/crudserver.tomcat.uri-encoding=UTF-8# 通用的Servlet容器设置server.xxx# Tomcat的设置server.tomcat.xxx 还可以编写一个EmbeddedServletContainerCustomizer 嵌入式的 Servlet 容器的定制器来进行定制： 12345678910@Bean //一定要将这个定制器加入到容器中public EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer()&#123; return new EmbeddedServletContainerCustomizer() &#123; //定制嵌入式的Servlet容器相关的规则 @Override public void customize(ConfigurableEmbeddedServletContainer container) &#123; container.setPort(8083); &#125; &#125;;&#125; 其实配置文件的方式 ServerProperties 本质也是 EmbeddedServletContainerCustomizer。你也可以换用其他容器，步骤就是先把 Tomcat 排除，然后导入相关的依赖即可，支持 Jetty （长连接比较好）和 Undertow （NIO，并发不错）。嵌入式容器默认并不支持 JSP，并且定制性复杂，还是要视情况而定。如果使用外置 Servlet 容器，除了打包方式改成 war，将内置的 Tomcat 排除后（可使用 provided），必须编写一个 SpringBootServletInitializer 的子类，并调用 configure 方法： 1234567public class ServletInitializer extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) &#123; //传入SpringBoot应用的主程序 return application.sources(SpringBoot04WebJspApplication.class); &#125;&#125; 这样就会把 SpringBoot 应用给带起来了，这多亏了 servlet3.0 规范的支持。 注册三大组件SpringBoot 给我们提供了简便的方法注册三大组件： 12345678910111213141516171819202122// 注册 servlet@Beanpublic ServletRegistrationBean myServlet()&#123; ServletRegistrationBean registrationBean = new ServletRegistrationBean(new MyServlet(),"/myServlet"); return registrationBean;&#125;// 注册 Filter@Beanpublic FilterRegistrationBean myFilter()&#123; FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(new MyFilter()); registrationBean.setUrlPatterns(Arrays.asList("/hello","/myServlet")); return registrationBean;&#125;// 注册监听器@Beanpublic ServletListenerRegistrationBean myListener()&#123; ServletListenerRegistrationBean&lt;MyListener&gt; registrationBean = new ServletListenerRegistrationBean&lt;&gt;(new MyListener()); return registrationBean;&#125; 自动配置的 SpringMVC 也是这样配置前端控制器的，看源码可得知： 12345678910111213141516@Bean(name = DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME)@ConditionalOnBean(value = DispatcherServlet.class, name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)public ServletRegistrationBean dispatcherServletRegistration( DispatcherServlet dispatcherServlet) &#123; ServletRegistrationBean registration = new ServletRegistrationBean( dispatcherServlet, this.serverProperties.getServletMapping()); // 默认拦截： / 所有请求；包静态资源，但是不拦截jsp请求； /*会拦截jsp // 可以通过server.servletPath来修改SpringMVC前端控制器默认拦截的请求路径 registration.setName(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME); registration.setLoadOnStartup( this.webMvcProperties.getServlet().getLoadOnStartup()); if (this.multipartConfig != null) &#123; registration.setMultipartConfig(this.multipartConfig); &#125; return registration;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ApacheCommons常用工具类使用]]></title>
    <url>%2F2018%2F07%2F24%2FApacheCommons%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Apache Commons 项目包含了很多工具，并且非常好用，能提高我们的开发效率，在很多开源框架中使用广泛，所以….如果项目中引入了这些依赖，能用就用吧，起码 Apache 的应该是比自己写的强多了。。另外，Google 的 Guava 也是非常好用的，也做了笔记，地址在这里。常用工具一览表： 组件 功能介绍 BeanUtils 提供了对于JavaBean进行各种操作，克隆对象,属性等等. Betwixt XML与Java对象之间相互转换. Codec 处理常用的编码方法的工具类包 例如 DES、SHA1、MD5、Base64 等. Collections java集合框架操作. Compress java提供文件打包压缩类库. Configuration 一个java应用程序的配置管理类库. DBCP 提供数据库连接池服务. DbUtils 提供对jdbc 的操作封装来简化数据查询和记录读取操作. Email java 发送邮件对 javamail 的封装. FileUpload 提供文件上传功能. HttpClien 提供 HTTP 客户端与服务器的各种通讯操作. 现在已改成 HttpComponents IO io 工具的封装. Lang Java 基本对象方法的工具类包 如：StringUtils，ArrayUtils 等等. Logging 提供的是一个 Java 的日志接口. Validator 提供了客户端和服务器端的数据验证框架. BeanUtils提供了对于 JavaBean 进行各种操作，比如对象，属性复制等等。 克隆对象： Person person2 = (Person)BeanUtils.cloneBean(person); 需要注意的是，cloneBean 方法拷贝对象只是浅拷贝，如果想深拷贝，可参考下面的代码 经测试，这种克隆效率很低，并且不是很稳定，推荐优先使用 Spring 的 BeanUtils 通过属性 copy 的方式来“克隆对象”。 12345678910111213141516171819public class JavaBeanCopier &#123; /** * Returns a deeply cloned java bean. * * @param fromBean java bean to be cloned. * @return a new java bean cloned from fromBean. */ public static Object copy(Object fromBean) &#123; ByteArrayOutputStream bos = new ByteArrayOutputStream(); XMLEncoder out = new XMLEncoder(bos); out.writeObject(fromBean); out.close(); ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); XMLDecoder in = new XMLDecoder(bis); Object toBean = in.readObject(); in.close(); return toBean; &#125;&#125; 将一个 Map 对象转化为一个 Bean：首先这个 Map 对象的 key 必须与 Bean 的属性相对应： 123456789101112131415161718Map map = new HashMap();map.put("name","tom");map.put("email”,”1@1.com");map.put("age”,”21");//将map转化为一个Person对象Person person = new Person();BeanUtils.populate(person,map);// web 中示例Enumeration params = request.getParameterNames();while (params.hasMoreElements())&#123; String name = (String) params.nextElement(); map.put(name, request.getParameter(name));&#125;// bean 转 mapMap map = BeanUtils.describe(person); 使用场景嘛…..如果不使用 web 层框架，还记得被 request.getParameter 支配的恐惧…. bean 的属性拷贝： 在拼装 VO 或者 DTO 的时候非常有用吧：BeanUtils.copyProperties(source, target); 需要注意的是，除了属性名要一致，属性拷贝同时也会将值为 null 的属性拷贝，所以，这行代码放的位置很重要！ 其他常用： 123456789101112Person p = new Person(new Book());// 使用beanUtils给对象的属性赋值BeanUtils.setProperty(p, "username", "张三");// 使用beanUtils获取对象的属性值System.out.println(BeanUtils.getProperty(p, "username"));// beanUtils 支持属性链赋值与获得值,不过赋值前 book 要先实例化BeanUtils.setProperty(p, "book.name", "历史小说");System.out.println(BeanUtils.getProperty(p, "book.name"));System.out.println(p.getBook().getName()); Codeccommons-codec是 Apache 开源组织提供的用于摘要运算、编码的包。在该包中主要分为四类加密：BinaryEncoders、DigestEncoders、LanguageEncoders、NetworkEncoders。最常用的类有两个： DigestUtils主要包括 MD5、SHA1、SHA256 算法等实现静态方法 Base64主要包含 Base64 编码和解码静态方法 测试代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class CodecTest&#123; /** * MD5散列算法实现(长度有16位和32位,常用32位的) */ @Test public void testMD5()&#123; String data = "hello"; String md5String = DigestUtils.md5Hex(data); System.out.println(md5String); &#125; /** * SHA1散列算法实现(长度为40位) */ @Test public void testSHA1()&#123; String data = "hello"; String sha1String = DigestUtils.sha1Hex(data); System.out.println(sha1String.length()); &#125; /** * SHA256散列算法实现(长度为64位) */ @Test public void testSHA256()&#123; String data = "hello"; String sha256String = DigestUtils.sha256Hex(data); System.out.println(sha256String.length()); &#125; /** * 摘要算法 */ @Test public void testDigestAlgorithms()&#123; // 摘要算法在 MessageDigestAlgorithms 下有列出 // public static final String MD2 = "MD2"; // public static final String MD5 = "MD5"; // public static final String SHA_1 = "SHA-1"; // public static final String SHA_256 = "SHA-256"; // public static final String SHA_384 = "SHA-384"; // public static final String SHA_512 = "SHA-512"; // 备注:都是基于 HASH 算法实现的 &#125; /** * 使用Base64类进行编码和解码,注意其可以转换二进制数据到字符串(比如图片转字符串) * * 如果是小图片的话,可以使用Base64编码来存储,只是可以并不是推荐使用。 */ @Test public void testBase64()&#123; String encodeString = Base64.encodeBase64String("hello".getBytes()); System.out.println(encodeString); byte[] bytes = Base64.decodeBase64(encodeString); System.out.println(new String(bytes)); &#125; /** * 使用Base64将图片编码为字符串 */ @Test public void image2String() throws Exception&#123; FileInputStream inputStream = new FileInputStream("e:/test.jpg"); // 借助Commons IO 组件的IOUtils静态方法将输入流转为子节数组 byte[] imageBytes = IOUtils.toByteArray(inputStream); String imageString = Base64.encodeBase64String(imageBytes); System.out.println(imageString); &#125; /** * 使用Base64将字符串解码为图片 */ @Test public void string2Image() throws Exception&#123; FileInputStream inputStream = new FileInputStream("e:/test.jpg"); // 借助Commons IO 组件的IOUtils静态方法将输入流转为子节数组 byte[] imageBytes = IOUtils.toByteArray(inputStream); String imageString = Base64.encodeBase64String(imageBytes); FileOutputStream outputStream = new FileOutputStream("e:/testCopy.jpg"); byte[] bytes = Base64.decodeBase64(imageString); // 借助Commons IO 组件的IOUtils静态方法将字节数组转为输出流 IOUtils.write(bytes, outputStream); &#125;&#125; 然后再补充一个 URL 的编码和解码： 123456789@Testpublic void testURLCodec() throws Exception &#123; System.out.println("==============URLCodec================"); URLCodec codec = new URLCodec(); String data = "啦啦啦"; String encode = codec.encode(data, "UTF-8"); System.out.println(encode); System.out.println(codec.decode(encode, "UTF-8"));&#125; Collections对 Java 中的集合类进行了一定的补充，定义了一些全新的集合，当然也是实现了 Collection 接口的，比如Bag，BidiMap。同时拥有新版本的原有集合，比如 FastArrayList。最后，更为重要的是一系列 utils 类，提供了我们常用的集合操作，可以大大方便我们的日常编程。 BagBag 定义了一种集合：收集一个对象出现的次数。例如 Bag：{a,a,b,c} 调用 bag.getCount(a) 返回 2，意味着里面有 2 个 a。 调用 bag.uniqueSet() 则返回一个 set，值为 {a,b,c}。 12345678910// 例子HashBag bag = new HashBag();bag.add("rabbit",1);bag.add("fox",1);bag.add("rabbit",2);// rabbit countSystem.out.print(bag.getCount("rabbit"));// how many animalsSystem.out.print(bag.uniqueSet().size()); 除了 HashBag，还有 SynchronizedBag、TreeBag，自行了解哈…. BidiMapBidiMap 定义了一种 map，不仅可以通过 key 得到 value，还可以通过 value 得到 key。Bidi 意思是 bidirectional，双向使用的 map。除了传统 Map 的操作，还加入了一些新”技能“： 12345678BidiMap bidi = new DualHashBidiMap();bidi.put("k1","v1");bidi.put("k2","v2");bidi.get("k2"); // return v2bidi.getKey("v2"); // return k2bidi.inverseBidiMap(); // 反转 bidi，原先的 value 作为 key 作为代价，BidiMap 必须要求 k 和 v 是一一对应的，在上述代码之后，无法做到 bidi.put(&quot;k2&quot;,&quot;v1&quot;);，因为这样就无法实现响应操作。 现实中如学号和身份证号做对应就是这样一种关系，可以视情况使用。同样，除了 DualHashBidiMap，还有 TreeBidiMap 等 有用的工具类这是 collections 包中最有价值的一个部分，介绍 ListUtils 和 CollectionUtils。 ListUtils 列表工具类 ListUtils.intersection(list1, list2)取交集； ListUtils.subtract(list1, list2)返回 list1 和 list2 的差。这里和 list1.removeAll(list2) 的差别在于：前者不改变任何一个集合；如果 list1 中有 2 个 a，list2 中有一个a：removeAll 会将 list1 中所有的 a 都抹去，而 subtract 结果 list1 仍然会剩下一个 a。 ListUtils.union(list1, list2)取并集； ListUtils.removeAll(list1, list2)不改变 list 的情况下做 removeAll CollectionUtils 通用的集合工具类 CollectionUtils.union(c1, c2)，CollectionUtils.intersection(c1,c2)不再解释 CollectionUtils.disjunction(c1, c2)返回两者的不相交部分的并集，没想到一个现实场景。。 CollectionUtils.containsAny(c1, c2)只要 c1 包含任何一个 c2 的元素，返回 true CollectionUtils.find(c, predicate)重要方法：借助 Predicate 达到神一般的效果，从此减少一半 for 循环。返回第一个找到的元素 CollectionUtils.filter(c, predicate)重要方法：同上，直接改变容器 c。 CollectionUtils.transform(c, transformer)重要方法：还是神器，但是在 jdk8 中等同于 foreach 方法效果。如果 jdk&lt;8，可以用这个方法代替 CollectionUtils.countMatches(c,predicate)根据 predicate 返回有多少元素满足预言，返回值 int。 CollectionUtils.select(c,predicate)根据 predicate 找出满足的元素，组成新的 collection 并返回 CollectionUtils.select(c,predicate,outputCollection)根据 predicate 找出满足的元素，加入到 outputCollection 中。 CollectionUtils.isEmpty(c)简单实用，是否为 null 或者空集合 补充：predicate既然上面用到了还是说一说，感觉自从 JDK8 来了后，能省一大部分工具类了。。。预言，这个类主要结合 CollectionUtils.find,CollectionUtils.filter 来使用。他的作用类似于『断言』，其中只有一个方法：public boolean evaluate(Object object);这个方法用于判断一个对象是否满足某种标准，感觉和 JDK8 中的 stream + map 差不多呢~ 1234567891011121314151617181920// 一个例子Predicate predicate = new Predicate&#123; @override public boolean evaluate(Object object)&#123; return PropertyUtils.getSimpleProperty(object,"age") &gt;50 ; &#125;&#125;Predicate predicate2 = new Predicate&#123; @override public boolean evaluate(Object object)&#123; return PropertyUtils.getSimpleProperty(object,"id") == 12306 ; &#125;&#125;//删除不满足条件的结果CollectionUtils.filter(list,predicate);//返回第一个满足的元素Object obj = CollectionUtils.find(list,predicate2);// new AndPredicate(predicate1,predicate2); 同时，Predicate 可以进行谓词连接，借助于：AndPredicate、OrPredicate、AnyPredicate、NotPredicate 这些类。 Long提供了很多安全操作和工具类，避免我们编码校验各种的 null；JDK8+ 后，对空值处理有了加强，但是嘛，国内用的人….. ArrayUtils:数组工具类，提供数组拷贝、查找、反转等功能 StringUtils:提供字符串操作，对 null 是安全的，字符串查找、替换、分割、去空格等操作；isEmpty 和 isBlank 的区别在于 isEmpty 不会忽略空格，而isBlank会认为是空,isBlank更常用 ObjectUtils:对 null 进行安全处理 RandomUtils:随机数工具类，获得随机整数、小数、字符串等 NumberUtils:数值工具类，数值类型转换等操作 DateUtils:日期工具类 EnumUtils:枚举工具类 ClassUtils判断是否有内部类、是否可以转型、获取包名、类名等 ReflectionToStringBuilder/ToStringBuilder:重写 toString 方法 EqualsBuilder/HashCodeBuilder:提供了方便的方法来覆盖 equals() 和 hashCode() 方法 然后给一些示例代码吧，比如 Builder 系列，虽然一般我们都是工具自动生成： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455//ToStringBuilder @Override public String toString() &#123; return new ToStringBuilder(this).append(this.getId()) .append(this.getUsername()).toString(); &#125; @Override public String toString() &#123; return ToStringBuilder.reflectionToString(this); &#125; // 以上输出格式为 Test@1270b73[&lt;null&gt;,&lt;null&gt;] // HashCodeBuilder @Override public int hashCode() &#123; return HashCodeBuilder.reflectionHashCode(this); &#125; @Override public int hashCode() &#123; return new HashCodeBuilder(this).append(this.getId()) .append(this.getUsername()).hashCode(); &#125;// EqulasBuilder @Override public boolean equals(Object obj) &#123; if (this == obj) &#123; return true; &#125; if (obj.getClass() == Test.class) &#123; Test test = (Test) obj; return new EqualsBuilder().append(this.getId(), test.getId()) .append(this.getUsername(), test.getUsername()).isEquals(); &#125; return false; &#125;@Override public boolean equals(Object obj) &#123; return EqualsBuilder.reflectionEquals(this, obj); &#125; // CompareToBuilder @Override public int compareTo(Test o) &#123; return CompareToBuilder.reflectionCompare(this, o); &#125; @Override public int compareTo(Test o) &#123; return new CompareToBuilder().append(this.getId(), o.getId()) .append(this.getUsername(), o.getUsername()).toComparison(); &#125; 除了上面列举的，还有很多工具类，大部分也不说了，看一下方法基本就会用了，非常简单，看例子可去参考的第二个链接。然后是日期，稍微提一下，也不常用： 1234567891011121314151617public class TestMain &#123; public static void main(String[] args) throws IllegalAccessException &#123; Date day1 = new Date(); /* * 由于 Aache 的 DateUtils 和 DateFormatUtils 并没有 Joda 强大, * 所以在这里只作简单的示例 */ // 增加一天 DateUtils.addDays(day1, 1); // 减少一年 DateUtils.addYears(day1, -1); // 格式化时间,第三参数为国际化,表示按美国时间显示 DateFormatUtils.format(day1, "yyyy-MM-dd", Locale.UK); &#125; &#125; IO看名字也知道，这是用来操作文件的工具类，工具类包括 FileUtils、IOUtils、FilenameUtils 和 FileSystemUtils，前三者的方法并没有多大的区别，只是操作的对象不同；故名思议：FileUtils 主要操作 File 类，IOUtils 主要操作 IO 流，FilenameUtils 则是操作文件名，FileSystemUtils 包含了一些 JDK 没有提供的用于访问文件系统的实用方法。当前，只有一个用于读取硬盘空余空间的方法可用。果然还是例子最能说明问题： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200public class FileUtilsTest &#123; private String basePath = null; @Before public void setUp() &#123; basePath = System.getProperty("user.dir") + "\\file\\"; &#125; /** * 拷贝文件 */ @Test public void testCopy() throws IOException &#123; File srcFile = new File(basePath + "a.txt"); File destFile = new File(basePath + "b.txt"); FileUtils.copyFile(srcFile, destFile); &#125; /** * 删除文件 */ @Test public void testDelete() throws IOException &#123; File delFile = new File(basePath + "b.txt"); FileUtils.forceDelete(delFile); //FileUtils.forceMkdir(delFile); &#125; /** * 比较文件内容 */ @Test public void testCompareFile() throws IOException &#123; File srcFile = new File(basePath + "a.txt"); File destFile = new File(basePath + "b.txt"); boolean result = FileUtils.contentEquals(srcFile, destFile); System.out.println(result); &#125; /** * 移动文件 */ @Test public void testMoveFile() throws IOException &#123; File srcFile = new File(basePath + "b.txt"); File destDir = new File(basePath + "move"); FileUtils.moveToDirectory(srcFile, destDir, true); &#125; /** * 读取文件内容 */ @Test public void testRead() throws IOException &#123; File srcFile = new File(basePath + "a.txt"); String content = FileUtils.readFileToString(srcFile); List&lt;String&gt; contents = FileUtils.readLines(srcFile); System.out.println(content); System.out.println("******************"); for (String string : contents) &#123; System.out.println(string); &#125; &#125; /** * 写入文件内容 */ @Test public void testWrite() throws IOException &#123; File srcFile = new File(basePath + "a.txt"); FileUtils.writeStringToFile(srcFile, "\nyes文件", true); &#125;&#125;public class FileSystemUtilsTest &#123; /** * 获取磁盘空余空间 */ @SuppressWarnings("deprecation") @Test public void testFreeSpace() throws IOException &#123; // 以字节为单位 System.out.println(FileSystemUtils.freeSpace("c:\\") / 1024 / 1024 / 1024); System.out.println(FileSystemUtils.freeSpace("d:\\") / 1024 / 1024 / 1024); // 以k为单位 System.out.println(FileSystemUtils.freeSpaceKb("e:\\") / 1024 / 1024); System.out.println(FileSystemUtils.freeSpaceKb("f:\\") / 1024 / 1024); &#125;&#125;// 行迭代器public class LineIteratorTest &#123; private String basePath = null; @Before public void setUp() throws Exception &#123; basePath = System.getProperty("user.dir") + "\\file\\"; &#125; /** * 测试行迭代器 */ @Test public void testIterator() throws IOException &#123; File file = new File(basePath + "a.txt"); LineIterator li = FileUtils.lineIterator(file); while (li.hasNext()) &#123; System.out.println(li.nextLine()); &#125; LineIterator.closeQuietly(li); &#125;&#125;// 文件过滤器public class FileFilterTest &#123; private String basePath = null; @Before public void setUp() throws Exception &#123; basePath = System.getProperty("user.dir") + "\\file\\"; &#125; /** * 空内容文件过滤器 */ @Test public void testEmptyFileFilter() throws IOException &#123; File dir = new File(basePath); String[] files = dir.list(EmptyFileFilter.NOT_EMPTY); for (String file : files) &#123; System.out.println(file); &#125; &#125; /** * 文件名称后缀过滤器 */ @Test public void testSuffixFileFilter() throws IOException &#123; File dir = new File(basePath); String[] files = dir.list(new SuffixFileFilter("a.txt")); for (String file : files) &#123; System.out.println(file); &#125; &#125;&#125;// 文件比较器public class ComparatorTest &#123; private String basePath = null; @Before public void setUp() throws Exception &#123; basePath = System.getProperty("user.dir") + "\\file\\"; &#125; /** * 文件名称比较器 */ @Test public void testNameFileComparator() throws IOException &#123; File f1 = new File(basePath + "a.txt"); File f2 = new File(basePath + "c.txt"); int result = NameFileComparator.NAME_COMPARATOR.compare(f1, f2); System.out.println(result); &#125; /** * 文件路径比较器 */ @Test public void testPathFileComparator() throws IOException &#123; File f1 = new File(basePath + "a.txt"); File f2 = new File(basePath + "c.txt"); int result = PathFileComparator.PATH_COMPARATOR.compare(f1, f2); System.out.println(result); &#125; /** * 组合比较器 */ @SuppressWarnings("unchecked") @Test public void testCompositeFileComparator() throws IOException &#123; File dir = new File(basePath); File[] files = dir.listFiles(); for (File file : files) &#123; System.out.println(file.getName()); &#125; CompositeFileComparator cfc = new CompositeFileComparator( DirectoryFileComparator.DIRECTORY_COMPARATOR, NameFileComparator.NAME_COMPARATOR); cfc.sort(files); System.out.println("*****after sort*****"); for (File file : files) &#123; System.out.println(file.getName()); &#125; &#125;&#125; 最常用的还是刚上来工具类的那些，精简一下，最多的就是复制文件/文件夹： 123456789101112131415161718192021222324252627282930// 复制文件夹（文件夹里面的文件内容也会复制），file1和file2平级。// 参数1：文件夹； 参数2：文件夹void copyDirectory( file1 , file2 ); // 复制文件夹到另一个文件夹。 file1是file2的子文件夹.// 参数1：文件夹； 参数2：文件夹void copyDirectoryToDirectory( file1 , file2 );// 复制文件夹，带有文件过滤功能void copyDirectory(File srcDir, File destDir, FileFilter filter);// ***********复制文件**************// 复制文件到另外一个文件void copyFile(final File srcFile, final File destFile);// 复制文件到输出流void long copyFile(final File input, final OutputStream output);// 复制文件到一个指定的目录void copyFileToDirectory( file1 , file2);// 把输入流里面的内容复制到指定文件void copyInputStreamToFile( InputStream source, File destination);// 把URL 里面内容复制到文件。可以下载文件。// 参数1：URL资源 ； 参数2：目标文件void copyURLToFile(final URL source, final File destination);// 把URL 里面内容复制到文件。可以下载文件。// 参数1：URL资源 ； 参数2：目标文件；参数3：http连接超时时间 ； 参数4：读取超时时间void copyURLToFile(final URL source, final File destination, final int connectionTimeout, final int readTimeout); 先暂时贴这么多吧，更多参考：https://blog.csdn.net/zhaoyanjun6/article/details/54972773 参考https://www.jianshu.com/p/c3c3ab2bad8dhttp://www.voidcn.com/article/p-ahnjkqaf-wo.htmlhttps://blog.csdn.net/u011179993/article/details/46743521]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo学习笔记]]></title>
    <url>%2F2018%2F07%2F07%2FDubbo%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[关于 Dubbo 以前也用过几次，都是浅度学习，也没做笔记，今天看了下官网竟然改版了，手册更新了，借这个契机来复习下，并且做下笔记，内容大部分来自官方手册。官网：https://dubbo.incubator.apache.org旧版的用户使用手册：https://www.gitbook.com/book/dubbo/dubbo-user-book官方文档开始的背景介绍写的挺不错的，可以去参考下，曾经的 ORM、MVC，到现在的分布式 RPC 和 SOA。 架构个人感觉 Dubbo 的架构设计是很不错的，官网的文档也解释的很详细： 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 Dubbo 架构具有以下几个特点，分别是连通性、健壮性、伸缩性、以及向未来架构的升级性。我认为 Dubbo 适用于比较大、复杂的服务调用需求的系统，一般的中小型使用通过配置服务的 URL 地址进行调用，通过 F5 等硬件进行负载均衡（或者通过 RMI 或 Hessian 等工具，简单的暴露和引用远程服务 ）这样就足够了。 连通性 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示 服务提供者向注册中心注册其提供的服务，并汇报调用时间到监控中心，此时间不包含网络开销 服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，同时汇报调用时间到监控中心，此时间包含网络开销 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 健壮性 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 伸缩性 注册中心为对等集群，可动态增加机器部署实例，所有客户端将自动发现新的注册中心 服务提供者无状态，可动态增加机器部署实例，注册中心将推送新的服务提供者信息给消费者 快速入门仿照官方文档，最简单的代码示例放在了我 Github 上的 Java_lean 仓库里，自取…..配置了 xml 版和注解版（推荐），Dubbo 采用全 Spring 配置方式，能基于 Spring 的 Schema 扩展进行加载，Dubbo 宣称能与 Spring（SpringBoot） 无缝整合，不能浪费。就是添加了 Dubbo 的依赖，根据传递性，就自动导入了 Spring 相关依赖，所以测试的话只需要一个 Dubbo、zookeeper 、zkclient 和你定义的接口就可以了！另外就是 Dubbo 底层用的是 netty 做通讯，所以效率会较高。在搭建测试的时候终究还是踩了不少坑，记录在这里附上了解决方案。在使用注解方式时，尤其注意 &lt;dubbo:annotation&gt; 在 2.5.8 之后的版本不再支持了！请使用 @Configuration 大法解决。 XML配置关于 XML 的基本配置，主要是服务的提供方和消费方，配置非常类似：服务提供方（remote-provider.xml）： 12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd"&gt; &lt;!-- 提供方应用信息(当前应用名称)，用于计算依赖关系 --&gt; &lt;dubbo:application name="dubbo-provider"/&gt; &lt;dubbo:registry address="zookeeper://127.0.0.1:2181"/&gt; &lt;!-- 使用 dubbo 协议，在 20880 端口暴露服务 --&gt; &lt;dubbo:protocol name="dubbo" port="20880"/&gt; &lt;!-- 配置需要提供远程服务的对象 --&gt; &lt;bean id=“xxxService” class=“com.xxx.XxxServiceImpl” /&gt; &lt;!-- 增加暴露远程服务配置 --&gt; &lt;dubbo:service interface=“com.xxx.XxxService” ref=“xxxService” /&gt;&lt;/beans&gt; 服务消费方（remote-consumer.xml）： 1234567891011&lt;!-- 不是匹配条件，不要与提供方一样 --&gt;&lt;dubbo:application name="dubbo-consumer"/&gt;&lt;!-- 使用zookeeper广播注册中心暴露发现服务地址 --&gt;&lt;dubbo:registry address="zookeeper://127.0.0.1:2181" /&gt;&lt;!-- 增加引用远程服务配置 --&gt;&lt;dubbo:reference id=“xxxService” interface=“com.xxx.XxxService” /&gt;&lt;!-- 和本地服务一样使用远程服务 --&gt;&lt;bean id=“xxxAction” class=“com.xxx.XxxAction”&gt; &lt;property name=“xxxService” ref=“xxxService” /&gt;&lt;/bean&gt; 以上就是最简单的纯 XML 方式使用 Dubbo，用的不多了吧，毕竟 XML 太繁琐了。另外就是所有标签都支持自定义参数（通过子标签 &lt;dubbo:parameter key=&quot;queue&quot; value=&quot;your_queue&quot; /&gt;），用于不同扩展点实现的特殊配置 。引用缺省是延迟初始化的，只有引用被注入到其它 Bean，或被 getBean() 获取，才会初始化，如需立即实例化，可配置：&lt;dubbo:reference ... init=&quot;true&quot; /&gt;对于相同的属性，例如 timeout 查找（优先级）顺序是： referenceMethod --&gt; serviceMethod --&gt; reference --&gt; service --&gt; consumer --&gt; provider 注解配置需要 2.5.7 及以上版本支持，使用注解的方式配置：使用 javaconfig 形式配置公共模块： 123456789101112131415161718192021222324252627@Configurationpublic class DubboConfiguration &#123; @Bean public ApplicationConfig applicationConfig() &#123; // 相当于 &lt;dubbo:application&gt; ApplicationConfig applicationConfig = new ApplicationConfig(); applicationConfig.setName("provider-test"); return applicationConfig; &#125; @Bean public RegistryConfig registryConfig() &#123; // 相当于 &lt;dubbo:registry&gt; RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setAddress("zookeeper://127.0.0.1:2181"); registryConfig.setClient("curator"); return registryConfig; &#125; // 对于消费方，还可以配一个 consumerConfig @Bean public ConsumerConfig consumerConfig() &#123; ConsumerConfig consumerConfig = new ConsumerConfig(); consumerConfig.setTimeout(3000); return consumerConfig; &#125;&#125; 指定 Dubbo 的扫描路径，就是自动扫描，提供方和消费方都是一样的配置： 12345@SpringBootApplication@DubboComponentScan(basePackages = "com.alibaba.dubbo.test.service.impl")public class ProviderTestApp &#123; // ...&#125; 再次强调，2.5.8 之后 &lt;dubbo:annotation&gt; 不再可用！服务提供方： 1234@Service(timeout = 5000)public class AnnotateServiceImpl implements AnnotateService &#123; // ...&#125; 相当于 XML 中的 &lt;dubbo:service /&gt; 用来暴露服务。服务消费方： 12345public class AnnotationConsumeService &#123; // 就是自动注入了 @Reference public AnnotateService annotateService;&#125; properties配置在比较简单的情况下，例如没有多注册中心，多协议等情况，或者想多个 Spring 容器想共享配置，Dubbo 将自动加载 classpath 根目录下的 dubbo.properties，可以通过 JVM 启动参数 -Ddubbo.properties.file=xxx.properties 改变缺省配置位置。将 XML 配置的标签名，加属性名，用点分隔，多个属性拆成多行： 比如：dubbo.application.name=foo等价于&lt;dubbo:application name=&quot;foo&quot; /&gt; 比如：dubbo.registry.address=10.20.153.10:9090等价于&lt;dubbo:registry address=&quot;10.20.153.10:9090&quot; /&gt; 如果 XML 有多行同名标签配置，可用 id 号区分，如果没有 id 号将对所有同名标签生效： 比如：dubbo.protocol.rmi.port=1234等价于&lt;dubbo:protocol id=&quot;rmi&quot; name=&quot;rmi&quot; port=&quot;1099&quot; /&gt; 比如：dubbo.registry.china.address=10.20.153.10:9090等价于&lt;dubbo:registry id=&quot;china&quot; address=&quot;10.20.153.10:9090&quot; /&gt; 下面是 dubbo.properties 的一个典型配置： 123dubbo.application.name=foodubbo.application.owner=bardubbo.registry.address=10.20.153.10:9090 关于优先级，JVM 启动参数 -D 最优先，其次是 XML 的配置，最后是 properties 文件。 启动时检查Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成 ，默认 check=&quot;true&quot;。 所以说，启动是有顺序的，默认下必须要服务方先启动。check 属性可以加在 dubbo:reference 、dubbo:consumer 、dubbo:registry 标签中，分别控制的是：某个服务的启动时检查（没有提供者时报错 ）、所有服务的启动时检查、注册中心启动时检查（注册订阅失败时报错）。优先级是 reference &gt; consumer ，还可以设置全局参数，例如：dubbo.reference.check=false 、 dubbo.consumer.check=false 等 集群容错在集群调用失败时，Dubbo 提供了多种容错方案，缺省为 failover 重试。 根据上面的架构图，理下各节点关系： 这里的 Invoker 是 Provider（服务提供方） 的一个可调用 Service 的抽象，Invoker 封装了 Provider 地址及 Service 接口信息。 Directory 代表多个 Invoker，可以把它看成 List&lt;Invoker&gt; ，但与 List 不同的是，它的值可能是动态变化的，比如注册中心推送变更。 Cluster（集群） 将 Directory 中的多个 Invoker 伪装成一个 Invoker，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个。 Router 负责从多个 Invoker 中按路由规则选出子集，比如读写分离，应用隔离等。 LoadBalance 负责从多个 Invoker 中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选。 容错模式Dubbo 提供的容错模式有： Failover Cluster默认的故障转移模式，就是失败自动切换，当出现失败，重试其它服务器 。通常用于读操作，但重试会带来更长延迟。可通过 retries 属性来指定重试的次数（不含第一次） Failfast Cluster快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe Cluster失败安全（指不会抛出异常），出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback Cluster失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking Cluster并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=&quot;2&quot; 来设置最大并行数。 Broadcast Cluster广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。 集群配置模式（以 XML 为例）： 1234&lt;!-- 服务提供方 --&gt;&lt;dubbo:service cluster="failsafe" /&gt;&lt;!-- 服务消费方 --&gt;&lt;dubbo:reference cluster="failsafe" /&gt; 负载均衡在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。可以自行扩展负载均衡策略。 负载均衡策略 Random LoadBalance随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive LoadBalance最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash LoadBalance一致性 Hash，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。缺省只对第一个参数 Hash （由 hash.arguments 参数控制）。缺省用 160 份虚拟节点（由 hash.nodes 参数控制）。 配置示例为基于 XML 的配置： 123456&lt;!-- 服务端服务级别 --&gt;&lt;dubbo:service interface="..." loadbalance="roundrobin" /&gt;&lt;!-- 客户端服务级别 --&gt;&lt;dubbo:reference interface="..." loadbalance="roundrobin" /&gt;&lt;!-- 方法级别调用配在 &lt;dubbo:method&gt; 中 --&gt; 直连提供者在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连，点对点直联方式，将以服务接口为单位，忽略注册中心的提供者列表，A 接口配置点对点，不影响 B 接口从注册中心获取列表。通过 XML 来配置直接的提供者，在 &lt;dubbo:reference&gt; 中配置 url 指向提供者，将绕过注册中心，多个地址用分号隔开：&lt;dubbo:reference id=&quot;xxxService&quot; interface=&quot;com.alibaba.xxx.XxxService&quot; url=&quot;dubbo://localhost:20890&quot; /&gt;除外，还可以通过 -D 参数来指定：java -Dcom.alibaba.xxx.XxxService=dubbo://localhost:20890配置的服务如果较多，可采用文件（properties）映射。 只订阅&amp;注册为方便开发测试，经常会在线下共用一个所有服务可用的注册中心，这时，如果一个正在开发中的服务提供者注册，可能会影响消费者不能正常运行。可以让服务提供者开发方，只订阅服务(开发的服务可能依赖其它服务)，而不注册正在开发的服务，通过直连测试正在开发的服务。配置方式： 123&lt;dubbo:registry address="10.20.153.10:9090" register="false" /&gt;&lt;!-- 或者 --&gt;&lt;dubbo:registry address="10.20.153.10:9090?register=false" /&gt; 对于只注册的情况，例如让服务提供者方只注册服务到某一注册中心，而消费方不从另外的注册中心订阅服务。 12345&lt;dubbo:registry id="hzRegistry" address="10.20.153.10:9090" /&gt;&lt;dubbo:registry id="qdRegistry" address="10.20.141.150:9090" subscribe="false" /&gt;&lt;!-- 或者 --&gt;&lt;dubbo:registry id="hzRegistry" address="10.20.153.10:9090" /&gt;&lt;dubbo:registry id="qdRegistry" address="10.20.141.150:9090?subscribe=false" /&gt; 多协议Dubbo 允许配置多协议，在不同服务上支持不同协议或者同一服务上同时支持多种协议。 不同服务不同协议不同服务在性能上适用不同协议进行传输，比如大数据用短连接协议，小数据大并发用长连接协议 123456789&lt;dubbo:application name="world" /&gt;&lt;dubbo:registry id="registry" address="10.20.141.150:9090" username="admin" password="hello1234" /&gt;&lt;!-- 多协议配置 --&gt;&lt;dubbo:protocol name="dubbo" port="20880" /&gt;&lt;dubbo:protocol name="rmi" port="1099" /&gt;&lt;!-- 使用dubbo协议暴露服务 --&gt;&lt;dubbo:service interface="com.alibaba.hello.api.HelloService" version="1.0.0" ref="helloService" protocol="dubbo" /&gt;&lt;!-- 使用rmi协议暴露服务 --&gt;&lt;dubbo:service interface="com.alibaba.hello.api.DemoService" version="1.0.0" ref="demoService" protocol="rmi" /&gt; 多协议暴露服务需要与 http 客户端互操作 1234567&lt;dubbo:application name="world" /&gt;&lt;dubbo:registry id="registry" address="10.20.141.150:9090" username="admin" password="hello1234" /&gt;&lt;!-- 多协议配置 --&gt;&lt;dubbo:protocol name="dubbo" port="20880" /&gt;&lt;dubbo:protocol name="hessian" port="8080" /&gt;&lt;!-- 使用多个协议暴露服务 --&gt;&lt;dubbo:service id="helloService" interface="com.alibaba.hello.api.HelloService" version="1.0.0" protocol="dubbo,hessian" /&gt; 当然，Dubbo 也支持多注册中心。 Dubbo协议默认协议，也是推荐的协议，这里就只说它，关于 dubbo:// 的基本介绍： Dubbo 缺省协议采用单一长连接和 NIO 异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。反之，Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。 特性： 连接个数：单连接 连接方式：长连接 传输协议：TCP 传输方式：NIO 异步传输 序列化：Hessian 二进制序列化 适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供者，尽量不要用 dubbo 协议传输大文件或超大字符串。 适用场景：常规远程服务方法调用 接口增加方法，对客户端无影响，如果该方法不是客户端需要的，客户端不需要重新部署。输入参数和结果集中增加属性，对客户端无影响，如果客户端并不需要新属性，不用重新部署。输入参数和结果集属性名变化，对客户端序列化无影响，但是如果客户端不重新部署，不管输入还是输出，属性名变化的属性值是获取不到的。总结：服务器端和客户端对领域对象并不需要完全一致，而是按照最大匹配原则。 服务分组&amp;多版本当一个接口有多种实现时，可以用 group 区分。XML 配置示例： 12345678&lt;!-- 服务 --&gt;&lt;dubbo:service group="feedback" interface="com.xxx.IndexService" /&gt;&lt;dubbo:service group="member" interface="com.xxx.IndexService" /&gt;&lt;!-- 引用 --&gt;&lt;dubbo:reference id="feedbackIndexService" group="feedback" interface="com.xxx.IndexService" /&gt;&lt;dubbo:reference id="memberIndexService" group="member" interface="com.xxx.IndexService" /&gt;&lt;!-- 任意组 --&gt;&lt;dubbo:reference id="barService" interface="com.foo.BarService" group="*" /&gt; 当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。 XML 配置示例： 12345678910&lt;!-- 服务提供者 --&gt;&lt;dubbo:service interface="com.foo.BarService" version="1.0.0" /&gt;&lt;dubbo:service interface="com.foo.BarService" version="2.0.0" /&gt;&lt;!-- 服务消费者 --&gt;&lt;dubbo:reference id="barService" interface="com.foo.BarService" version="1.0.0" /&gt;&lt;dubbo:reference id="barService" interface="com.foo.BarService" version="2.0.0" /&gt;&lt;!-- 如果不需要区分版本 --&gt;&lt;dubbo:reference id="barService" interface="com.foo.BarService" version="*" /&gt; 官方给的版本迁移建议： 在低压力时间段，先升级一半提供者为新版本 再将所有消费者升级为新版本 然后将剩下的一半提供者升级为新版本 结果缓存用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量 。 缓存类型 lru基于最近最少使用原则删除多余缓存，保持最热的数据被缓存。 threadlocal当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问。 jcache与 JSR107 集成，可以桥接各种缓存实现。 XML 配置示例： 12345&lt;dubbo:reference interface="com.foo.BarService" cache="lru" /&gt;&lt;!-- 或者 --&gt;&lt;dubbo:reference interface="com.foo.BarService"&gt; &lt;dubbo:method name="findBar" cache="lru" /&gt;&lt;/dubbo:reference&gt; 异步调用基于 NIO 的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小。首先，在 XML 中开启异步支持： 123456&lt;dubbo:reference id="fooService" interface="com.alibaba.foo.FooService"&gt; &lt;dubbo:method name="findFoo" async="true" /&gt;&lt;/dubbo:reference&gt;&lt;dubbo:reference id="barService" interface="com.alibaba.bar.BarService"&gt; &lt;dubbo:method name="findBar" async="true" /&gt;&lt;/dubbo:reference&gt; 代码中进行异步调用： 123456789101112131415161718// 此调用会立即返回nullfooService.findFoo(fooId);// 拿到调用的Future引用，当结果返回后，会被通知和设置到此FutureFuture&lt;Foo&gt; fooFuture = RpcContext.getContext().getFuture(); // 此调用会立即返回nullbarService.findBar(barId);// 拿到调用的Future引用，当结果返回后，会被通知和设置到此FutureFuture&lt;Bar&gt; barFuture = RpcContext.getContext().getFuture(); // 此时findFoo和findBar的请求同时在执行，客户端不需要启动多线程来支持并行，而是借助NIO的非阻塞完成 // 如果foo已返回，直接拿到返回值，否则线程wait住，等待foo返回后，线程会被notify唤醒Foo foo = fooFuture.get(); // 同理等待bar返回Bar bar = barFuture.get(); // 如果foo需要5秒返回，bar需要6秒返回，实际只需等6秒，即可获取到foo和bar，进行接下来的处理。 是否等待消息发出（配置在 dubbo:method 中即可）： sent=&quot;true&quot; 等待消息发出，消息发送失败将抛出异常。 sent=&quot;false&quot; 不等待消息发出，将消息放入 IO 队列，即刻返回。 如果你只是想异步，完全忽略返回值，可以配置 return=&quot;false&quot;，以减少 Future 对象的创建和管理成本其中，异步方式总是不等待返回。 并发&amp;连接控制通过在 dubbo:service 标签中设置 executes 属性来控制服务器端并发执行（或占用线程池线程数）不能超过的个数。通过设置 actives 属性（可在 dubbo:reference/service 中设置）来控制每客户端并发执行（或占用连接的请求数）不能超过的个数。如果 &lt;dubbo:service&gt; 和 &lt;dubbo:reference&gt; 都配了actives，&lt;dubbo:reference&gt; 优先。 Load Balance 均衡配置服务的客户端的 loadbalance 属性为 leastactive，此 Loadbalance 会调用并发数最小的 Provider（Consumer端并发数）。 可在 dubbo:reference/service 中设置。 连接控制分为服务端的连接控制和客户端的连接控制，XML 配置示例： 1234567&lt;!-- 限制服务器端接受的连接不能超过 10 个,两种方式 --&gt;&lt;dubbo:provider protocol="dubbo" accepts="10" /&gt;&lt;dubbo:protocol name="dubbo" accepts="10" /&gt;&lt;!-- 限制客户端服务使用连接不能超过 10 个，两种方式 --&gt;&lt;dubbo:reference interface="com.foo.BarService" connections="10" /&gt;&lt;dubbo:service interface="com.foo.BarService" connections="10" /&gt; 如果 &lt;dubbo:service&gt; 和 &lt;dubbo:reference&gt; 都配了 connections，&lt;dubbo:reference&gt; 优先. 其他列举下没用过，但是可能会用到的，所以这些并不全，最全的还要去官网的示例看： 分组聚合接口一样，但有多种实现，用 group 区分，现在消费方需从每种 group 中调用一次返回结果，合并结果返回。 回声测试回声测试用于检测服务是否可用，回声测试按照正常请求流程执行，能够测试整个调用是否通畅，可用于监控。所有服务自动实现 EchoService 接口，只需将任意服务引用强制转型为 EchoService，即可调用 $echo(&quot;OK&quot;) 方法看看是否返回 OK 上下文信息上下文中存放的是当前调用过程中所需的环境信息（获取提供方 IP、判断是否为消费端等）。所有配置信息都将转换为 URL 的参数；RpcContext 是一个 ThreadLocal 的临时状态记录器，当接收到 RPC 请求，或发起 RPC 请求时，RpcContext 的状态都会变化。 隐式参数通过 RpcContext 上的 setAttachment 和 getAttachment 在服务消费方和提供方之间进行参数的隐式传递。例如：RpcContext.getContext().set/getAttachment() 本地调用本地调用使用了 injvm 协议，是一个伪协议，它不开启端口，不发起远程调用，只在 JVM 内直接关联，但执行 Dubbo 的 Filter 链。 参数回调&amp;事件通知参数回调方式与调用本地 callback 或 listener 相同，只需要在 Spring 的配置文件中声明哪个参数是 callback 类型即可。Dubbo 将基于长连接生成反向代理，这样就可以从服务器端调用客户端逻辑。在调用之前、调用之后、出现异常时，会触发 oninvoke、onreturn、onthrow 三个事件，可以配置当事件发生时，通知哪个类的哪个方法。 延迟连接延迟连接用于减少长连接数。当有调用发起时，再创建长连接。 推荐用法在 Provider（提供方） 上尽量多配置 Consumer（消费方） 端属性： 作服务的提供者，比服务使用方更清楚服务性能参数，如调用的超时时间，合理的重试次数，等等 在 Provider 配置后，Consumer 不配置则会使用 Provider 的配置值，即 Provider 配置可以作为 Consumer 的缺省值。否则，Consumer 会使用 Consumer 端的全局设置，这对于 Provider 不可控的，并且往往是不合理的。覆盖规则：Consumer 端配置优于 Provider 配置，优于全局配置 这样可以让 Provider 实现者一开始就思考 Provider 服务特点、服务质量的问题。常见的配置有： timeout 方法调用超时 retries 失败重试次数，缺省是 2（会调用 3 次） loadbalance 负载均衡算法，缺省是随机 random。还可以有轮询 roundrobin、最不活跃优先 leastactive actives 消费者端，最大并发调用限制，即当 Consumer 对一个服务的并发调用到上限后，新调用会 Wait 直到超时 在方法上配置 dubbo:method 则并发限制针对方法，在接口上配置 dubbo:service，则并发限制针对服务 Provider 上可以配置的 Provider 端属性有： threads 服务线程池大小 executes 一个服务提供者并行执行请求上限，即当 Provider 对一个服务的并发调用到上限后，新调用会 Wait，这个时候 Consumer可能会超时。在方法上配置 dubbo:method 则并发限制针对方法，在接口上配置 dubbo:service，则并发限制针对服务。 XML 配置示例： 123456789101112131415&lt;dubbo:service interface="com.alibaba.hello.api.HelloService" version="1.0.0" ref="helloService" timeout="300" retry="2" loadbalance="random" actives="0"/&gt; &lt;dubbo:service interface="com.alibaba.hello.api.WorldService" version="1.0.0" ref="helloService" timeout="300" retry="2" loadbalance="random" actives="0" &gt; &lt;dubbo:method name="findAllPerson" timeout="10000" retries="9" loadbalance="leastactive" actives="5" /&gt;&lt;dubbo:service/&gt;&lt;!-- Provider 上配置合理的 Provider 端属性 --&gt;&lt;dubbo:protocol threads="200" /&gt; &lt;dubbo:service interface="com.alibaba.hello.api.HelloService" version="1.0.0" ref="helloService" executes="200" &gt; &lt;dubbo:method name="findAllPerson" executes="50" /&gt;&lt;/dubbo:service&gt; 配置管理信息：目前有负责人信息和组织信息用于区分站点。有问题时便于的找到服务的负责人，至少写两个人以便备份。负责人和组织的信息可以在注册中心的上看到，例如： 123456&lt;!-- 应用配置负责人、组织 --&gt;&lt;dubbo:application owner=”ding.lid,william.liangf” organization=”intl” /&gt;&lt;!-- service 配置负责人 --&gt;&lt;dubbo:service owner=”ding.lid,william.liangf” /&gt;&lt;!-- reference 配置负责人 --&gt;&lt;dubbo:reference owner=”ding.lid,william.liangf” /&gt; 配置 Dubbo 缓存文件（在提供者方设置列表缓存文件）: 1&lt;dubbo:registry file=”$&#123;user.home&#125;/output/dubbo.cache” /&gt; 文件的路径，应用可以根据需要调整，保证这个文件不会在发布过程中被清除。如果有多个应用进程注意不要使用同一个文件，避免内容被覆盖。这个文件会缓存注册中心的列表和服务提供者列表。 对于监控，推荐使用固定端口暴露服务，而不要使用随机端口，这样在注册中心推送有延迟的情况下，消费者通过缓存列表也能调用到原地址，保证调用成功。最后就是不要使用 dubbo.properties 文件配置，推荐使用对应 XML 配置]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式总结]]></title>
    <url>%2F2018%2F05%2F15%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[看完了 Head First 设计模式，收获还是蛮多的，这么多设计模式总算有了个了解，距离实用还有一定距离，毕竟是个经验活，不得不说设计模式中的思想真是太棒了！后面有几种模式没时间拿出来单独研究了，在这里就都战略性总结一下吧，啊哈哈~~希望我也能写出一手漂亮的代码！ 设计原则一览表： 设计原则名称 定 义 使用频率 单一职责原则 (Single Responsibility Principle, SRP) 一个类只负责一个功能领域中的相应职责 ★★★★☆ 开闭原则 (Open-Closed Principle, OCP) 软件实体应对扩展开放，而对修改关闭 ★★★★★ 里氏代换原则 (Liskov Substitution Principle, LSP) 所有引用基类对象的地方能够透明地使用其子类的对象 ★★★★★ 依赖倒转原则 (Dependence Inversion Principle, DIP) 抽象不应该依赖于细节，细节应该依赖于抽象 ★★★★★ 接口隔离原则 (Interface Segregation Principle, ISP) 使用多个专门的接口，而不使用单一的总接口 ★★☆☆☆ 合成复用原则 (Composite Reuse Principle, CRP) 尽量使用对象组合，而不是继承来达到复用的目的 ★★★★☆ 迪米特法则 (Law of Demeter, LoD) 一个软件实体应当尽可能少地与其他实体发生相互作用 ★★★☆☆ 基本的设计原则上面的表中都是专业的说法，但是貌似并不怎么好理解，反正我是看的比较懵逼，然后就用普通的语言来进行描述下，首先是下面总结的几条，应该是大纲级别的了 封装变化找出应用中可能需要变化的部分，把它们独立出来；不要和那些不需要变化的代码混在一起 针对接口编程，而不是针对实现编程 多用组合，少用继承 为交互对象之间的松耦合设计而努力 这样设计类才能更好的利用 OO 的特性，也是基础：抽象、封装、继承、多态；良好的 OO 设计必须具备：可复用、可扩充、可维护三个特性 对上面不太好理解的地方补充：松耦合：当两个对象之间松耦合，它们依然可以交互，但是不太清楚彼此的细节；也就是说，如果改变其中一方并不会影响到另一方，因为两者是松耦合的，所以只有它们之间的接口仍被遵守，那么我们就可以自由的改变它们（接口的重要性）松耦合的设计之所以能建立起富有弹性的 OO 系统，能够应对变化，是因为对象之间的互相依赖降到了最低。 开闭原则从表中也可以看出，这个使用的非常频繁，开闭原则是面向对象的可复用设计的第一块基石，它是最重要的面向对象设计原则；它规定：类应该对扩展开放，对修改关闭。即软件实体应尽量在不修改原有代码的情况下进行扩展。我们的目标是允许类容易扩展，在不修改现有代码的情况下，就可搭配新的行为；装饰模式就是遵守的这个原则的体现（硬要说，观察者模式也是遵循的）当然，并不是让每个地方都遵循开闭原则（这回增加代码的复杂度），我们要把注意力集中在最有可能改变的地方，然后应用开闭原则 依赖倒置原则规定的是：要依赖抽象，而不是依赖具体的类；emmm，和“针对接口编程，而不是针对实现编程”貌似是差不多的它说明：不能让高层组件依赖低层组件，而且，不管高层或者低层组件，两者都应该依赖于抽象！ 所谓高层组件，是由其他低层组件定义其行为的类。就比如人类是高层组件，男人就是低层组件，它的部分行为是由男人定义的（差不多就这个意思，不要太纠结） 那么，究竟是哪里“倒置”了呢，低层组件会依赖高层的抽象，高层组件也依赖相同的抽象（假定男人拥有一个抽象，那么相对于具体实现这个抽象是“高层”的，高层组件也依赖这个抽象）我们一般的思维是从顶端开始，然后往下到具体的类，倒置你的想法就是别从顶端开始，首先想的是甭管什么样的男人都需要一个共同的抽象类，然后人类也会依赖这个抽象类，这样想其实就已经倒置了！大概…..然后是几个指导方针： 变量不可以持有具体类的引用 不要让类派生自具体类 不要覆盖基类中已经实现的方法 但毕竟是指导作用，不可避免的在某些条件下要违反，只是尽量的遵守罢了；我们实例化字符串的时候都是 new 啊，违反了方针啊，但完全可以，因为字符串不可能改变，所以说要灵活工厂模式就是这个原则的代表吧 面向对象的五大基本原则单一职责原则（SRP）开放封闭原则（OCP）里氏替换原则（LSP）依赖倒置原则（DIP）接口隔离原则（ISP） OO原则下面列出了基本的原则： 封装变化 多用组合，少用继承 针对接口编程，不针对实现编程 为交互对象之间的松耦合设计而努力 类应该对修改关闭，对扩展开放 依赖抽象，不依赖具体类 只和“朋友”交谈（减少对象之间的交互） 别找（调用）我，我会找（调用）你 类应该只有一个改变的理由 设计模式一览这里只是列出了常用的一些模式，并不是全部，每一种模式只是做了简单的解释，具体的实践需要看以前的文章。 策略模式 定义算法族，分别封装起来，让它们之间可以互相替换；此模式让算法的变化独立于使用算法的客户。 比如，提供一些 setter 方法设置相应的“策略”，详细解释：飞机 观察者模式 在对象之间定义一对多依赖，这样一来，当一个对象改变状态，依赖它的对象都会收到通知，并且自动更新。 当有多个观察者时，不要依赖他们的通知次序，因为是不确定的。详细解释：飞机 装饰者模式 动态的将责任附加到对象上；想要扩展功能，装饰者提供有别于继承的另一种选择。 符合开闭原则，在动态代理中应用广泛。装饰者一般对组件的客户是透明的，装饰者会导致设计中出现许多小对象，过度使用会使系统变的复杂。通常，我们会在调用真正的原始对象方法之前或者之后做一些动作。详细解释：飞机 工厂模式 工厂方法模式：定义了一个创建对象的接口，但子类决定要实例化的类是哪一个，工厂方法让类把实例化推迟到子类。抽象工厂模式：提供一个接口，用于创建相关或依赖对象的家族，而不需要明确指定具体类。 我们常用的是简单工厂模式，而工厂方法模式相对复杂一些，另外还有抽象工厂模式；他们都是属于工厂模式。其中使用到了依赖倒置原则。详细解释：飞机 单例模式 确保一个类只有一个实例，并提供全局访问点。 最广泛的模式之一了吧，考察它的也相当多，写法也比较简单。不过需要注意多线程并发的问题，懒汉式和饿汉式，使用双重判断的弊端、原子性和一致性，指令重排等，内容还是比较多的。注意使用多个类加载器也会导致生成多实例。 命令模式 将“请求”封装成对象，以便使用不同的请求、队列或者日志来参数化其他对象。命令模式也支持可撤销的操作。 让请求调用者和请求接受者解耦，解耦的两者是通过命令对象进行沟通的，封装了其动作。其中可能会使用“空对象模式”，它可以实现队列请求、日志请求等需求（实现日志系统和事务系统）。宏命令是一种简单的延伸。详细解释：飞机 适配器&amp;外观模式 将一个类的接口，转换成客户期望的另一个接口。适配器让原本接口不兼容的类可以合作无间。 比如 JDK 中的枚举和迭代器？其中其实还包含有另一个模式：外观模式，它让接口更简单（改变接口的原因），也将客户从组件的子系统解耦。适配器的意图：“改变”接口符合客户的期望。外观模式的意图：提供子系统的一个简化接口。 提供了一个统一的接口，用来访问子系统中的一群接口。|外观定义了一个高层接口，让子系统更容易使用。 当需要简化并统一一个很大的接口或者一群复杂的接口时，使用外观。适配器：将一个对象包装起来以改变其接口。装饰者：将一个对象包装起来增加新的行为和责任。外观：将一群对象“包装”起来简化其接口。详细解释：飞机 模板方法模式 在一个方法中，定义一个算法骨架，而将一些步骤延迟到子类中。模板方法使子类可以在不改变算法结构的情况下，重新定义算法中的某些步骤。 其中可以使用钩子，钩子控制根据某些条件是否执行某部分算法。还使用了好莱坞原则：别调用我们，我们会调用你。模板方法可以是一种代码复用的技巧，可以定义具体方法、抽象方法、钩子（可以选择要不要覆盖）。为了防止子类改变模板方法中的算法，可以定义为 final，并且可以说工厂方法是模板方法的一种特殊版本。详细解释：飞机 迭代器&amp;组合模式 迭代器：提供一种方法顺序访问一个聚合对象中的各个元素，而又不暴露其内部的表示。组合：允许你将对象组合成树形结构来表现“整体/部分”层次结构。组合能让客户以一致的方式处理个别对象以及对象组合。 他们两个往往能拼在一起使用， 使用组合我们能把相同的操作应用在组合和个别对象上（叶子节点），他们通常有（或者提取出）共同的接口，也就是说，在大多数情况下，我们可以忽略对象组合和个别对象之间的差别（只不过有的可能是空实现）。 状态模式策略模式（主动）和状态模式是双胞胎，在出生时才分开。 允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它的类。 通常，状态模式用类代表状态；Context 会将行为委托给当前的状态对象，状态类可以被多个 Context 实例共享。同样，使用状态模式通常会导致设计中的类数量大大增加。 代理模式 为另一个对象提供一个替身或占位符以控制对这个对象的访问。被代理的对象可以是远程的对象（远程代理）、创建开销大的对象（虚拟代理）或需要安全控制的对象（保护代理）。 代理模式要做的主要就是：控制和管理访问，在 AOP 中使用广泛吧。。。Java 中的 RMI 就是一个典型的例子，远程代理是一般代理模式的一种实现。虚拟代理：只有当我们真正需要一个对象的时候才创建它，对象创建后代理就会将请求直接委托给对象（显示图片前的“加载中”）。为了让客户使用代理而不是真正的对象，一般是创建一个工厂，由工厂返回代理对象。因为实际的代理类是运行时创建的，我们称这个 Java 技术为动态代理。衍生类还有很多，比如防火墙代理、缓存代理、智能引用代理、同步代理等。代理在结构上类似装饰者，但是目的不同；装饰者模式是为对象加上行为；而代理是控制访问。Java 中内置代理支持，代理同样会造成你设计中类的数目增加。 复合模式 符合模式结合两个或以上的模式，组成一个解决方案，解决一再发生的一般性问题。 MVC 是典型的复合模式，其实控制器使用了策略模式（控制器是视图的策略）、模型使用了观察者模式、视图使用了组合模式。Web 的开发人员对 MVC 进行适配，使它符合 B/S 模型，我们称这样的适配为 Model 2。 其他模式 模式是在某情景（Context）下，针对某问题的某种解决方案。情景：就是应用某个模式的情况。这应该是会不断出现的情况。问题：你想在某情景下达到的目标，但也可以是某情景下的约束。解决方案：就是你所追求的一个通用的设计。 当然你可以改变模式。像设计原则一样，模式不是法律或准则，它只是指导方针，你可以改变模式来符合你的需要。 架构模式 应用模式三层架构、C/S 系统以及 Web 服务中 桥接模式不只改变你的实现，也改变你的抽象；会增加复杂度 生成器模式封装一个产品的构造过程，并允许按步骤构造。隐藏内部表现，产品的实现可以被替换，被用来创建组合结构。 责任链模式当你想要让一个以上的对象有机会能够处理某个请求的时候，就使用责任链模式。如果没有任何对象处理它，那它就可能不会被处理。 蝇量模式如果想要某个类的一个实例能用来提供许多“虚拟实例”，就使用此模式。 解释器模式 中介者模式使用中介者模式来集中相关对象之间复杂的沟通和控制方式。设计不当，本身会过于复杂 备忘录模式当你需要让对象返回之前的状态时（比如撤销操作），用于存储状态。存储和恢复的过程比较耗时，Java 中使用序列化 原型模式当创建给定类的实例过程很昂贵或者很复杂时，就使用原型模式 访问者模式当你想要为一个对象的组合增加新的能力，并且封装不重要时 与设计模式相处 保持简单你的目标应该是简单，而不是如何才能应用上模式；正确的说法是：为了让你的设计简单且有弹性，有时候使用模式是最好的方法。 考虑模式带来的后果 知道何时使用这更多的是一种经验，要考虑后果，模式往往是在重构中加入的。找出你设计中会改变的区域，通常这是需要模式的迹象。 利用模式进行重构比如：你的代码中充满了 if 语句，那么可能需要状态模式；或者意味着工厂模式将这些具体的依赖消除掉。 拿掉不需要的模式当你的系统变得非常复杂，并且不需要预留任何弹性的时候，就不要使用模式。 现在不需要，就别做你不确定这一块以后会不会变化就别“过度优化” 有些模式可能并不适合当前的情况，可以对其改编使其适合。不应急切于使用模式，而是致力于最能解决问题的简单方案。设计模式也会在你的设计中加入更多的层，这不但增加复杂性，而且效率下降。并且可以适当了解下反模式（看起来是个好模式，真正采用后就会带来麻烦）]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shiro学习笔记]]></title>
    <url>%2F2018%2F02%2F08%2FShiro%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Apache Shiro（读作“sheeroh”，即日语“城”）是一个开源安全轻量级框架，提供身份验证、授权、密码学和会话管理。Shiro 框架直观、易用，同时也能提供健壮的安全性。Apache Shiro 是 Java 的一个安全框架。目前，使用 Apache Shiro 的人越来越多，因为它相当简单，对比 Spring Security，可能没有 Spring Security 做的功能强大，但是在实际工作时可能并不需要那么复杂的东西，所以使用小而简单的 Shiro 就足够了。对于它俩到底哪个好，这个不必纠结，能更简单的解决项目问题就好了。Shiro 可以非常容易的开发出足够好的应用，其不仅可以用在 JavaSE 环境，也可以用在 JavaEE 环境。Shiro 可以帮助我们完成：认证、授权、加密、会话管理、与 Web 集成、缓存等。 引子在学习之前，先了解几个名词，对于权限管理来说，最重要的就是（用户）认证和（用户）授权。认证是为了确保你是个合法用户，常见的形式有账号+密码的方式，或者指纹、数字证书等。授权就是用来控制（认证后）用户访问资源的，确保对应的用户只能访问指定的资源。一般情况，当用户或者程序访问资源时，系统先判断该资源是否允许匿名访问，如果不允许就会检查该用户是否通过认证，没有就要求进行认证，也就是一般最常见的输入用户名和密码登陆。认证通过后就会进入权限控制，就是检查当前的用户是否拥有权限操作该资源，如果没有就直接拒绝访问。 所以在设计的时候就有了这几个词：用户、权限、资源；为了方便一般会加入一个角色达到管理一堆权限的目的。数据库设计中，一般就需要六张表：用户、权限、角色、资源、用户角色关系、角色权限关系。用户与角色、角色与权限之间是多对多关系；权限与资源是多对一关系。因为权限都是针对资源来说的，资源在系统中都是固定的，所以一个资源（比如用户列表）对应多个权限（查看用户列表、修改用户列表）。一般为了方便，会把资源和权限合并为一张表（资源名称、权限名称、资源地址）。 权限控制关于权限的控制，主要可分为两类： 基于角色的访问控制代码中是以角色为判断条件来控制的，当用户（角色）的权限需求变更时，只能修改对应的代码。比如类似：if(user.hasRole(&quot;管理员&quot;)){....} 基于资源的访问控制代码中是以权限为判断条件来控制的，当用户（角色）的权限需求变更时，只需要修改对应角色中的权限即可，不需要改变代码。比如类似：if(user.hasPermission(&quot;查看用户列表&quot;)){......} 可以看出，第一种的扩展性很差，不利于系统维护，因为角色是针对人的，而人是活的（资源是死的）；第二种只需要修改对应的角色权限列表即可，就是修改数据库的内容而已，完全可以通过 web 端应用做到，所以用的比较多。 权限管理的解决方案一般情况也是分为两种： 粗颗粒权限管理对资源类型的权限管理，比如菜单、用户信息等。具体的例子有：管理员可以访问用户信息等页面，部门管理员可查看用户信息。 细颗粒权限管理对资源实例的权限管理，就是资源类型的具体化，比如 XX 菜单、XX 用户信息。具体的例子有：部门经理只能查看本部门的员工信息，用户只能查看自己的菜单。 然后下面就该谈实现了，对于粗颗粒来说是比较简单的，因为代码可以进行抽取，放在系统架构级别上统一处理，比如 SpringMVC 的拦截器就可以做到授权，基本上只需要判断下是否有权限就可以了。但是对于细颗粒来说抽取就比较复杂了，在数据级别上是没有共性可言的，可以说是业务逻辑的一部分了（不单单是判断是否有权限那么简单了），在业务层去处理会比较简单，如果抽取到系统架构层面就非常麻烦，并且扩展性也很差，所以，一般情况下细颗粒的控制都是在业务层去实现。 下面来看具体的做法，一般情况下对于粗颗粒可以使用一些优秀的权限管理框架来做，比如 Shiro ，能够提高开发效率；如果不想用可以自己实现，方法一般用拦截器或者过滤器进行 url 的拦截（基于 url 拦截方式）。如果自己实现的话，需要两个拦截器，一个负责用户认证的拦截（前面），一个负责用户权限（授权）的拦截。可匿名访问的地址和公共地址一般是可配置的，也就是写在 prop 文件中，在写拦截器或者过滤器的时候读取这个文件来检查。PS：自己实现的时候管理资源的 URL 是最头疼的，每一个页面里都有很多链接，都需要配置，确实麻烦；所以就有人想出来使用标识符来统一管理（数据库中增加一个标志字段）。 初识Shiro最开始当然是先看看它能做什么，可以用一幅图来描述： Authentication：身份认证 / 登录，验证用户是不是拥有相应的身份； Authorization：授权，即权限验证，验证某个已认证的用户是否拥有某个权限；即判断用户是否能做事情常见的如：验证某个用户是否拥有某个角色。或者细粒度的验证某个用户对某个资源是否具有某个权限； Session Manager：会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；会话可以是普通 JavaSE 环境的，也可以是如 Web 环境的； Cryptography：加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储； Web Support：Web 支持，可以非常容易的集成到 Web 环境； Caching：缓存，比如用户登录后，其用户信息、拥有的角色 / 权限不必每次去查，这样可以提高效率； Concurrency：shiro 支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去； Testing：提供测试支持； Run As：允许一个用户假装为另一个用户（如果他们允许）的身份进行访问； Remember Me：记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了。 记住一点，Shiro 不会去维护用户、维护权限；这些需要我们自己去设计 / 提供；然后通过相应的接口注入给 Shiro 即可。 Shiro架构同样，还是根据图来学习，这是官方提供的架构图： Subject：主体可以看到主体可以是任何可以与应用交互的 “用户”；也就是说可以是人的操作也可以是程序的操作。 SecurityManager：安全管理器相当于 SpringMVC 中的 DispatcherServlet 或者 Struts2 中的 FilterDispatcher；是 Shiro 的心脏；所有具体的交互都通过 SecurityManager 进行控制；它管理着所有 Subject、且负责进行认证和授权、及会话、缓存的管理。 Authenticator：认证器负责主体认证的，这是一个扩展点，如果用户觉得 Shiro 默认的不好，可以自定义实现；其需要认证策略（Authentication Strategy），即什么情况下算用户认证通过了； Authrizer：授权器或者访问控制器，用来决定主体是否有权限进行相应的操作；即控制着用户能访问应用中的哪些功能； Realm：域、领域可以有 1 个或多个 Realm，可以认为是安全实体数据源，即用于获取安全实体（认证、授权相关数据）；可以是 JDBC 实现，也可以是 LDAP 实现，或者内存实现等等；由用户提供；在 realm 中存储授权和认证逻辑。注意：Shiro 不知道你的用户 / 权限存储在哪及以何种格式存储；所以我们一般在应用中都需要实现自己的 Realm； SessionManager：会话管理器主要用来管理 Session 的生命周期，Web 应用一般是用 Web 容器（比如 Tomcat）来管理。Shiro 并不仅仅可以用在 Web 环境，也可以用在如普通的 JavaSE 环境；所有呢，Shiro 就抽象了一个自己的 Session 来管理主体与应用之间交互的数据；这样的话，比如我们在 Web 环境用，想把两台服务器的会话数据放到一个地方，这个时候就可以实现自己的分布式会话（如把数据放到 Memcached 服务器）； SessionDAO通过 SessionDAO 管理 Session 数据，针对个性化的 Session 数据存储使用 SessionDAO。DAO 大家都用过，数据访问对象，用于会话的 CRUD，比如我们想把 Session 保存到数据库，那么可以实现自己的 SessionDAO，通过如 JDBC 写到数据库；比如想把 Session 放到 Memcached 中，可以实现自己的 Memcached SessionDAO；另外 SessionDAO 中可以使用 Cache 进行缓存，以提高性能； CacheManager：缓存管理器来管理如用户、角色、权限等的缓存的；因为这些数据基本上很少去改变，放到缓存中后可以提高访问的性能。Shiro 只提供了缓存逻辑，还需要具体的缓存实现，比如和 ehcache 整合。 Cryptography：密码模块Shiro 提高了一些常见的加密组件用于如密码加密 / 解密、散列的。 认证入门程序下面的入门代码是基于 SE 的，Realm 使用的是 ini 配置文件，就是说用户的认证信息都在 ini 配置文件里了： 123456789101112131415161718192021222324252627282930import org.apache.shiro.mgt;@Testpublic void testHelloworld() &#123; //1、获取 SecurityManager 工厂，此处使用 ini 配置文件初始化 SecurityManager Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory("classpath:shiro.ini"); //2、得到 SecurityManager 实例 SecurityManager securityManager = factory.getInstance(); //3、把 securityManager 绑定给 SecurityUtils（设置到当前的运行环境） SecurityUtils.setSecurityManager(securityManager); //4、得到 Subject 及创建用户名/密码身份验证 Token（即用户身份/凭证） Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken("zhang", "123"); try &#123; //5、登录，即身份验证 subject.login(token); &#125; catch (AuthenticationException e) &#123; // TODO &#125; // isAuthenticated 判断是否通过验证 Assert.assertEquals(true, subject.isAuthenticated()); //断言用户已经登录 //6、退出 subject.logout();&#125; 依赖什么的就不说了，太简单了，尤其是用了 Maven 后，然后还有 ini 的配置文件： 1234[users]zhang=123wang=123[roles] 用 ini 而不用 properties 的一个原因就是 ini 中可以对数据进行分组。下面来简单捋一下：SecurityManager 负责真正的身份验证逻辑；它会委托给 Authenticator 进行身份验证；Authenticator 才是真正的身份验证者，Authenticator 可能会委托给相应的 AuthenticationStrategy 进行多 Realm 身份验证，默认 ModularRealmAuthenticator 会调用 AuthenticationStrategy 进行多 Realm 身份验证；Authenticator 会把相应的 token 传入 Realm，从 Realm 获取身份验证信息。这是稍微详细一点的执行流程（序号不是很对应）： 首先通过 IniSecurityManagerFactory 并指定一个 ini 配置文件来创建一个 SecurityManager 工厂； 接着获取 SecurityManager 并绑定到 SecurityUtils，这是一个全局设置，设置一次即可； 通过 SecurityUtils 得到 Subject，其会自动绑定到当前线程；如果在 web 环境在请求结束时需要解除绑定；然后获取身份验证的 Token，如用户名 / 密码； 调用 subject.login() 方法进行登录，其会自动委托给 SecurityManager.login() 方法进行登录； SecurityManager 最终由 ModularRealmAuthenticator 进行认证（本例），ModularRealmAuthenticator 会调用 IniRealm 去配置文件查找用户信息。如果查到用户信息就给 ModularRealmAuthenticator 返回用户信息。（本例的账号、密码）如果查不到用户信息就给 ModularRealmAuthenticator 返回 null。 ModularRealmAuthenticator 接收到 IniRealm 返回的 Authenticator 信息，如果是 null 就抛出 UnknownAccountException。其他的可能抛出的 AuthenticationException 或其子类常见的如：DisabledAccountException（禁用的帐号）LockedAccountException（锁定的帐号）UnknownAccountException（错误的帐号）ExcessiveAttemptsException（登录失败次数过多）IncorrectCredentialsException （错误的凭证）ExpiredCredentialsException（过期的凭证）等 （拓展）SecurityManager 接着会委托给 Authorizer（ModularRealmAuthorizer）进行授权，也就是执行 realm 中的授权方法进行查询权限。 （拓展）权限信息返回给 ModularRealmAuthorizer 后会通过 PermissionResolver 把字符串转换成相应的 Permission 实例，然后进行对比，如果没有权限就抛出异常。 最后可以调用 subject.logout() 退出，其会自动委托给 SecurityManager.logout() 方法退出。 对于页面的错误消息展示，最好使用如 “用户名 / 密码错误” 而不是 “用户名错误”/“密码错误”，防止一些恶意用户非法扫描帐号库. 关于授权授权的流程和认证差不多，只不过用的是 hasRole（判断是否具有某个角色） 而已，就不多说了。授权的验证一般有三种形式，编程式（不推荐）、注解式、JSP/GSP 标签： 12345678910111213141516171819// 编程式，测试时可以用Subject subject = SecurityUtils.getSubject();if(subject.hasRole(“admin”)) &#123; //有权限&#125; else &#123; //无权限&#125;// 注解式@RequiresRoles("admin")// @RequiresPermissions("user:add")public void hello() &#123; // 有权限&#125;// 标签用法&lt;shiro:hasRole name="admin"&gt;// &lt;!—- 有权限 -—&gt;&lt;/shiro:hasRole&gt; 响应的在 ini 配置文件中也需要做出配置，主要是配角色： 12345678[users]# 规则：用户名=密码,角色1，角色2zhang=123,role1,role2wang=123,role1# 角色[roles]role1=user:create,user:updaterole2=user:create,user:delete 角色的配置规则是：资源:操作:实例 ，上面的例子中就是对 user 这个资源的所有实例进行 create 操作，写两段就意味着是 user:create:*，所以说是可以使用通配符 * 的；多个规则用逗号分割。下面是两个测试用例，一般分为两种，基于角色的授权和基于资源的授权；参考一下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 基于资源的授权，根据资源标识符判断@Testpublic void testIsPermitted() &#123; login("classpath:shiro-permission.ini", "zhang", "123"); //判断拥有权限：user:create Assert.assertTrue(subject().isPermitted("user:create")); //判断拥有权限：user:update and user:delete Assert.assertTrue(subject().isPermittedAll("user:update", "user:delete")); //判断没有权限：user:view Assert.assertFalse(subject().isPermitted("user:view"));&#125;@Test(expected = UnauthorizedException.class)public void testCheckPermission () &#123; login("classpath:shiro-permission.ini", "zhang", "123"); //断言拥有权限：user:create subject().checkPermission("user:create"); //断言拥有权限：user:delete and user:update subject().checkPermissions("user:delete", "user:update"); //断言拥有权限：user:view 失败抛出异常 subject().checkPermissions("user:view");&#125;// 基于角色的授权，根据角色名来判断@Testpublic void testHasRole() &#123; login("classpath:shiro-role.ini", "zhang", "123"); //判断拥有角色：role1 Assert.assertTrue(subject().hasRole("role1")); //判断拥有角色：role1 and role2 Assert.assertTrue(subject().hasAllRoles(Arrays.asList("role1", "role2"))); //判断拥有角色：role1 and role2 and !role3 boolean[] result = subject().hasRoles(Arrays.asList("role1", "role2", "role3")); Assert.assertEquals(true, result[0]); Assert.assertEquals(true, result[1]); Assert.assertEquals(false, result[2]);&#125;@Test(expected = UnauthorizedException.class)public void testCheckRole() &#123; login("classpath:shiro-role.ini", "zhang", "123"); //断言拥有角色：role1 subject().checkRole("role1"); //断言拥有角色：role1 and role3 失败抛出异常 subject().checkRoles("role1", "role3");&#125; isXXX 方法会返回布尔值，checkXXX 失败会抛出异常；篇幅限制，更多的内容去参考里的 wiki 看吧，因为项目中未使用，等用到了再进行补充。 自定义Realm 这是 Realm 的继承体系，自定义 Realm 可以直接继承自 Realm 这个顶级接口，也可以选择它的孩子;一般，选择 AuthorizingRealm（授权）即可；其继承了 AuthenticatingRealm（即身份验证），而且也间接继承了 CachingRealm（带有缓存实现），只需要实现验证和授权这两个方法逻辑就可以了。顶级 Realm 接口定义的方法： 123456//返回一个唯一的Realm名字String getName ();//判断此Realm是否支持此Tokenboolean supports (AuthenticationToken token);//根据Token获取认证信息AuthenticationInfo getAuthenticationInfo (AuthenticationToken token) throws AuthenticationException; 自定义 Realm 使用继承 AuthorizingRealm 的方式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class MyRealm implements AuthorizingRealm &#123; // 设置 realm 的名称，可省略 @Override public void setName(String name) &#123; return super.setName("MyRealm"); &#125; // 用于认证 @Override protected AuthenticationInfo doGetAuthenticationInfo (AuthenticationToken token) throws AuthenticationException &#123; // 从 token 得到用户名 String username = (String) token.getPrincipal(); // String password = new String((char[])token.getCredentials()); //得到密码 // 根据用户名查找密码，比如从数据库中；假如已经查到 String pwd = "123456"; // 如果查不到信息，用户不存在；假设这种情况 pwd 为 null if(pwd == null)&#123; return null; &#125; // 如果身份认证验证成功，返回一个 AuthenticationInfo 实现； return new SimpleAuthenticationInfo(username, pwd, this.getName()); &#125; // 用于授权 @Override protected AuthorizationInfo doGetAuthorizationInfo (PrincipalCollection principals) &#123; // 从 principals 获取身份信息 // 强转为认证的时候填充到 SimpleAuthenticationInfo 中的第一个参数类型 String username = (String) principals.getPrimaryPrincipal(); // 根据身份信息查询权限信息，比如从数据库中 List&lt;String&gt; permissions = new ArrayList&lt;&gt;(); permissions.add("user:create"); permissions.add("user:update"); // 构造 SimpleAuthorizationInfo 返回授权信息 SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo(); // authorizationInfo.setRoles(userService.findRoles(username)); authorizationInfo.setStringPermissions(permissions); return authorizationInfo; &#125;&#125; 如果查到的密码和 token 里的密码不符调用方就会抛出 IncorrectCredentialsException；如果返回 null 就会抛出 UnknownAccountException。最后别忘了配置下 ini 文件，把自定义的 Realm 写进去，要不然 Shiro 也不识别： 123456[main]# 声明一个 realmmyRealm=com.bfchengnuo.shiro.realm.MyRealm# 指定 securityManager 的 realms 实现# 也就是把自定义的 realm 设置到 securityManager securityManager.realms=$myRealm 其他的代码和入门程序一样，就是把配置文件改改就可以了。 编码和加密Shiro 还提供了一些关于常用的编码、加密。散列工具，使用也非常的简单，下面来看看简单的使用 编码&amp;解码Shiro 提供了 base64 和 16 进制字符串编码 / 解码的 API 支持，方便一些编码解码操作。Shiro 内部的一些数据的存储 / 表示都使用了 base64 和 16 进制字符串。 1234567891011// base64 操作String str = "hello";String base64Encoded = Base64.encodeToString(str.getBytes());String str2 = Base64.decodeToString(base64Encoded);Assert.assertEquals(str, str2);// 16 进制字符串编码 / 解码操作String str = "hello";String base64Encoded = Hex.encodeToString(str.getBytes());String str2 = new String(Hex.decode(base64Encoded.getBytes()));Assert.assertEquals(str, str2); 还有一个可能经常用到的类 CodecSupport，提供了 toBytes(str,”utf-8”) / toString(bytes,”utf-8”) 用于在 byte 数组 /String 之间转换。 散列散列算法一般用于生成数据的摘要信息，是一种不可逆的算法，一般适合存储密码之类的数据，常见的散列算法如 MD5、SHA 等。一般进行散列时最好提供一个 salt（盐）防止暴力破解，盐有时是随机的（需要记录）。 12345678910111213String str = "hello";String salt = "123";// 第一个参数：原文，明文// 第二个参数：盐值// 第三个参数：散列的次数，比如 2 就相当于：md5(md5(str))String md5 = new Md5Hash(str, salt, 1).toString();// 使用 SHA-256String sha1 = new Sha256Hash(str, salt).toString();// 使用 SimpleHash 方式// 内部使用 MessageDigestString simpleHash = new SimpleHash("SHA-1", str, salt, 1).toString(); SimpleHash 可以指定散列算法，其内部使用了 Java 的 MessageDigest 实现。为了方便使用，Shiro 提供了 HashService，默认提供了 DefaultHashService 实现。使用例子见参考。 加密&amp;解密Shiro 还提供对称式加密 / 解密算法的支持，如 AES、Blowfish 等；当前还没有提供对非对称加密 / 解密算法支持，未来版本可能提供。下面是一个使用 AES 的例子： 1234567891011121314AesCipherService aesCipherService = new AesCipherService();aesCipherService.setKeySize(128); //设置key长度// 生成 keyKey key = aesCipherService.generateNewKey();String text = "hello";// 加密String encrptText = aesCipherService.encrypt(text.getBytes(), key.getEncoded()).toHex();// 解密String text2 = new String(aesCipherService.decrypt(Hex.decode(encrptText), key.getEncoded()).getBytes());Assert.assertEquals(text, text2); Shiro 还提供了 PasswordService 及 CredentialsMatcher 用于提供加密密码及验证密码服务。Shiro 默认提供了 PasswordService 实现 DefaultPasswordService；CredentialsMatcher 实现 PasswordMatcher 及 HashedCredentialsMatcher（更强大）。 使用具体在 Shiro 中使用（也就是在 Realm 中）一般是先定义好 ini 文件，设置好使用什么，比如散列 md5，然后定义好散列几次 1234567891011121314[main]# 定义凭证匹配器credentialsMatcher=org.apache.shiro.authc.credential.HashedCredentialsMatcher# 散列算法credentialsMatcher.hashAlgorithmName=md5# 散列次数credentialsMatcher.hashIterations=2#credentialsMatcher.storedCredentialsHexEncoded=truemyRealm=com.bfchengnuo.shiro.realm.myRealm# 将凭证匹配器设置到 RealmmyRealm.credentialsMatcher=$credentialsMatchersecurityManager.realms=$myRealm 然后在自定义的 Realm 里返回身份信息的时候稍微改造一下： 1234567SimpleAuthenticationInfo ai = new SimpleAuthenticationInfo(username, password, getName());ai.setCredentialsSalt(ByteSource.Util.bytes(username+salt)); //盐可以是用户名+随机数// 或者SimpleAuthenticationInfo ai2 = new SimpleAuthenticationInfo(username, password, ByteSource.Util.bytes(salt), getName()); 其他的和上面自定义 Realm 的代码一致，不需要更改。这里的 pwd 和 salt 是从数据库获取的（假设存储介质是数据库），程序会拿着 token 中的密码和 salt 进行 md5 处理，然后和 pwd 进行对比。PS：这里针对是的散列的验证，用于认证；上面说的是生成，可放在注册或者修改密码逻辑中。 与Spring整合单独使用 Shiro 或者 SE 中用的情况其实不多，最多的还是 web 项目中用，自然就少不了要和 Spring 进行整合，首先准备工作当然是导入相应的依赖： 123456789101112131415161718192021222324252627282930313233&lt;!-- shiro核心jar --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- shiro对web的支持 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-web&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- shiro与spring整合jar --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- ehcache 核心依赖 --&gt;&lt;dependency&gt; &lt;artifactId&gt;ehcache-core&lt;/artifactId&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- ehcache 与 Shiro 的整合包 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-ehcache&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt;&lt;/dependency&gt; 然后就需要在 web.xml 中配置过滤器了，毕竟权限的管理主要还是靠过滤器： 12345678910111213141516171819&lt;!-- shiro过滤器，DelegatingFilterProxy通过代理模式将spring容器中的bean和filter关联起来 --&gt;&lt;filter&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt; &lt;!-- 设置true由servlet容器控制filter的生命周期 --&gt; &lt;init-param&gt; &lt;param-name&gt;targetFilterLifecycle&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- 设置spring容器filter的bean id，如果不设置则找与filter-name一致的bean--&gt; &lt;init-param&gt; &lt;param-name&gt;targetBeanName&lt;/param-name&gt; &lt;param-value&gt;shiroFilter&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 这里的关键是配 DelegatingFilterProxy，达到让 Spring 管理的 Bean 具有 Filter 的能力。下面就是正式开始配置 Shiro 的配置了，可以建一个 spring-shiro.xml 文件，注意：一定要配置在 Spring 这个父容器，如果配置在 SpringMVC 子容器里是没用的，前面也提到过关于这父子容器的关系。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106&lt;!-- Shiro 整合包里的 Web 过滤器，id 对应 web.xml 中指定的 targetBeanName --&gt;&lt;bean id="shiroFilter" class="org.apache.shiro.spring.web.ShiroFilterFactoryBean"&gt; &lt;property name="securityManager" ref="securityManager" /&gt; &lt;!-- loginUrl认证提交地址，如果没有认证将会请求此地址进行认证 --&gt; &lt;!-- 请求此地址将由下面配置的formAuthenticationFilter进行表单认证 --&gt; &lt;property name="loginUrl" value="/login.action" /&gt; &lt;!-- 认证成功统一跳转到 first.action，建议不配置，shiro 认证成功自动到上一个请求路径 --&gt; &lt;property name="successUrl" value="/first.action"/&gt; &lt;!-- 通过 unauthorizedUrl 指定没有权限操作时跳转页面--&gt; &lt;property name="unauthorizedUrl" value="/refuse.jsp" /&gt; &lt;!-- 自定义 filter 配置 --&gt; &lt;property name="filters"&gt; &lt;map&gt; &lt;!-- 将自定义 的FormAuthenticationFilter注入shiroFilter中--&gt; &lt;entry key="authc" value-ref="formAuthenticationFilter" /&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- 过虑器链定义，从上向下顺序执行，一般将/**放在最下边 --&gt; &lt;property name="filterChainDefinitions"&gt; &lt;value&gt; &lt;!-- 对静态资源设置匿名访问 --&gt; /images/** = anon /js/** = anon /styles/** = anon &lt;!-- 验证码，可匿名访问 --&gt; /validatecode.jsp = anon &lt;!-- 请求 logout.action地址，shiro去清除session，此地址可以是不存在的--&gt; /logout.action = logout &lt;!--商品查询需要商品查询权限 ，取消url拦截配置，使用注解授权方式 --&gt; &lt;!-- /items/queryItems.action = perms[item:query] /items/editItems.action = perms[item:edit] --&gt; &lt;!-- 配置记住我或认证通过可以访问的地址 --&gt; /index.jsp = user /first.action = user /welcome.jsp = user &lt;!-- /** = authc 所有url都必须认证通过才可以访问--&gt; /** = authc &lt;!-- /** = anon所有url都可以匿名访问 --&gt; &lt;/value&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- securityManager安全管理器 --&gt;&lt;bean id="securityManager" class="org.apache.shiro.web.mgt.DefaultWebSecurityManager"&gt; &lt;property name="realm" ref="customRealm" /&gt; &lt;!-- 注入缓存管理器 --&gt; &lt;property name="cacheManager" ref="cacheManager"/&gt; &lt;!-- 注入session管理器 --&gt; &lt;property name="sessionManager" ref="sessionManager" /&gt; &lt;!-- 记住我 --&gt; &lt;property name="rememberMeManager" ref="rememberMeManager"/&gt; &lt;/bean&gt;&lt;!-- 自定义 realm --&gt;&lt;bean id="customRealm" class="com.bfchengnuo.shiro.realm.CustomRealm"&gt; &lt;!-- 将凭证匹配器设置到realm中，realm按照凭证匹配器的要求进行散列 --&gt; &lt;property name="credentialsMatcher" ref="credentialsMatcher"/&gt;&lt;/bean&gt;&lt;!-- 凭证匹配器 --&gt;&lt;bean id="credentialsMatcher" class="org.apache.shiro.authc.credential.HashedCredentialsMatcher"&gt; &lt;property name="hashAlgorithmName" value="md5" /&gt; &lt;property name="hashIterations" value="1" /&gt;&lt;/bean&gt;&lt;!-- 缓存管理器 --&gt;&lt;bean id="cacheManager" class="org.apache.shiro.cache.ehcache.EhCacheManager"&gt; &lt;property name="cacheManagerConfigFile" value="classpath:shiro-ehcache.xml"/&gt;&lt;/bean&gt;&lt;!-- 会话管理器 --&gt;&lt;bean id="sessionManager" class="org.apache.shiro.web.session.mgt.DefaultWebSessionManager"&gt; &lt;!-- session的失效时长，单位毫秒 --&gt; &lt;property name="globalSessionTimeout" value="600000"/&gt; &lt;!-- 删除失效的session --&gt; &lt;property name="deleteInvalidSessions" value="true"/&gt;&lt;/bean&gt;&lt;!-- 自定义form认证过虑器 --&gt;&lt;!-- 基于Form表单的身份验证过滤器，不配置将也会注册此过虑器，表单中的用户账号、密码及loginurl将采用默认值，建议配置 --&gt;&lt;bean id="formAuthenticationFilter" class="com.bfchengnuo.shiro.CustomFormAuthenticationFilter "&gt; &lt;!-- 表单中账号的input名称 --&gt; &lt;property name="usernameParam" value="username" /&gt; &lt;!-- 表单中密码的input名称 --&gt; &lt;property name="passwordParam" value="password" /&gt; &lt;!-- 记住我input的名称 --&gt; &lt;property name="rememberMeParam" value="rememberMe"/&gt;&lt;/bean&gt;&lt;!-- rememberMeManager管理器，写cookie，取出cookie生成用户信息 --&gt;&lt;bean id="rememberMeManager" class="org.apache.shiro.web.mgt.CookieRememberMeManager"&gt; &lt;property name="cookie" ref="rememberMeCookie" /&gt;&lt;/bean&gt;&lt;!-- 记住我cookie --&gt;&lt;bean id="rememberMeCookie" class="org.apache.shiro.web.servlet.SimpleCookie"&gt; &lt;!-- rememberMe是cookie的名字 --&gt; &lt;constructor-arg value="rememberMe" /&gt; &lt;!-- 记住我cookie生效时间30天 --&gt; &lt;property name="maxAge" value="2592000" /&gt;&lt;/bean&gt; 内容比较多，需要注意的确实也不少，基本都是按最初没有使用 Spring 需要的那些对象来的，只要把那些对象搞定就 OK 了，比如 securityManager ，牵扯出了一系列的 Bean …..使用了 authc 当用户没有认证时会跳转到指定页面，提交表单后凭证会传送给 FormAuthenticationFilter 进行验证（实际上最终会传给 Realm 进行查找凭证），如果没有找到或者凭证不正确会向 request 域填充异常信息（默认 key 为 shiroLoginFailure）。然后可以在 controller 中从 request 取出这个异常信息判断是什么原因导致，值是异常的全路径，可以使用 xxx.class.getName().equals() 比较。认证成功后 Controller 不需要处理，直接还是返回 login 页面即可，Shiro 会进行处理，默认是跳转到上一个页面。在使用注解授权方式时，三层中都可以，但是推荐在 controller 中使用，比较直观，毕竟是入口；因为注解使用的是 AOP 代理的方式，所以还需要在 SpringMVC 的配置文件中开启 AOP 的支持： 123456&lt;!-- 开启aop，对类代理 --&gt;&lt;aop:config proxy-target-class="true"&gt;&lt;/aop:config&gt;&lt;!-- 开启shiro注解支持 --&gt;&lt;bean class="org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor"&gt; &lt;property name="securityManager" ref="securityManager" /&gt;&lt;/bean&gt; 配在 mvc 配置文件中的一个原因就是因为注解是加在 controller 上的，controller 的扫描是配在这的，所以放一起吧。注解一般用 RequiresPermissions，而不用基于角色的。举几个例子：@RequiresPermissions(value=&quot;XXX&quot;) or @RequiresPermissions(&quot;XXX&quot;) or @RequiresPermissions (value={“user:a”, “user:b”}, logical= Logical.OR) 拥有 user:a 或 user:a 权限。 缓存shiro 中提供了认证信息（上面已经配过了，更高级的如果需要用到 Redis 之类的保存 Session，那就研究下 SessionDAO）和授权信息的缓存.注意: shiro 默认关闭认证信息缓存, 但是对于授权信息的缓存默认是开启的.由于 Shiro 只提供了缓存的处理逻辑，并没有实现具体的缓存逻辑（其实也有提供简单的实现），这里使用 ehcache 作为缓存的实现了，前面已经导入了相关的依赖，Spring 配置文件中也配好了，还缺一个 eh 缓存的配置文件 shiro-ehcache.xml： 12345678910111213141516&lt;ehcache xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="../config/ehcache.xsd"&gt; &lt;!--diskStore：缓存数据持久化的目录 地址 --&gt; &lt;diskStore path="F:\develop\ehcache" /&gt; &lt;defaultCache maxElementsInMemory="1000" maxElementsOnDisk="10000000" eternal="false" overflowToDisk="false" diskPersistent="false" timeToIdleSeconds="120" timeToLiveSeconds="120" diskExpiryThreadIntervalSeconds="120" memoryStoreEvictionPolicy="LRU"&gt; &lt;/defaultCache&gt;&lt;/ehcache&gt; 参数就不解释了，以前也用过，在新版本中，无论用户正常退出还是非正常退出缓存都会自动清空。但是，当管理员修改了用户的权限，但是该用户还没有退出，在默认情况下，修改的权限无法立即生效。需要手动进行编程实现：在权限修改后调用 realm 的 clearCache 方法清除缓存。 12345//清除缓存public void clearCached() &#123; PrincipalCollection principals = SecurityUtils.getSubject().getPrincipals(); super.clearCache(principals);&#125; 将上面的方法放在自定义的 Realm 中，在修改权限的 Service 中调用即可，但是我觉得这样只会清除当前用户的缓存，还有相关的一些代码贴在 github，等日后要仔细研究下，TODO。 clearCache 其同时调用 clearCachedAuthenticationInfo 和 clearCachedAuthorizationInfo，意为清空 AuthenticationInfo 和 AuthorizationInfo。UserRealm 还提供了 clearAllCachedAuthorizationInfo、clearAllCachedAuthenticationInfo、clearAllCache，用于清空整个缓存。 principals：身份，即主体的标识属性，可以是任何东西，如用户名、邮箱等，唯一即可。一个主体可以有多个principals，但只有一个Primary principals，一般是用户名/密码/手机号 默认拦截器Shiro 内置了很多默认的拦截器，比如身份验证、授权等相关的。默认拦截器可以参考 org.apache.shiro.web.filter.mgt.DefaultFilter 中的枚举拦截器： 身份验证相关的包名太长，所以省略前面相同的 org.apache.shiro.web.filter.authc ；说明栏中的括号里的内容是默认值。 默认拦截器名 拦截器类 说明 authc FormAuthenticationFilter 基于表单的拦截器；如 /**=authc，如果没有登录会跳到相应的登录页面登录；主要属性：usernameParam：表单提交的用户名参数名（ username）；passwordParam：表单提交的密码参数名（password）； rememberMeParam：表单提交的密码参数名（rememberMe）；loginUrl：登录页面地址（/login.jsp）；successUrl：登录成功后的默认重定向地址；failureKeyAttribute：登录失败后错误信息存储 key（shiroLoginFailure）； authcBasic BasicHttpAuthenticationFilter Basic HTTP 身份验证拦截器主要属性： applicationName：弹出登录框显示的信息（application）； logout LogoutFilter 退出拦截器主要属性：redirectUrl：退出成功后重定向的地址（/）; 示例 ：/logout=logout user UserFilter 用户拦截器，用户已经身份验证或记住我登录的都可；示例 ：/**=user anon AnonymousFilter 匿名拦截器，即不需要登录即可访问；一般用于静态资源过滤；示例 /static/**=anon 另外还提供了一个 org.apache.shiro.web.filter.authz.HostFilter，即主机拦截器，比如其提供了属性：authorizedIps：已授权的 ip 地址deniedIps：表示拒绝的 ip 地址；不过目前还没有完全实现，不可用。 这些默认的拦截器会自动注册，可以直接在 ini 配置文件中通过 拦截器名.属性 设置其属性. 授权相关的包名太长，所以省略前面相同的 org.apache.shiro.web.filter.authz ；说明栏中的括号里的内容是默认值。 默认拦截器名 拦截器类 说明 roles RolesAuthorizationFilter 角色授权拦截器，验证用户是否拥有所有角色；主要属性：loginUrl：登录页面地址（/login.jsp）；unauthorizedUrl：未授权后重定向的地址；示例 ：/admin/**=roles[admin] perms PermissionsAuthorizationFilter 权限授权拦截器，验证用户是否拥有所有权限；属性和 roles 一样；示例 ：/user/**=perms[&quot;user:create&quot;] port PortFilter 端口拦截器，主要属性：port（80）：可以通过的端口；示例 ：/test= port[80]，如果用户访问该页面是非 80，将自动将请求端口改为 80 并重定向到该 80 端口，其他路径 / 参数等都一样 rest HttpMethodPermissionFilter rest 风格拦截器，自动根据请求方法构建权限字符串（GET=read, POST=create,PUT=update,DELETE=delete,HEAD=read,TRACE=read,OPTIONS=read, MKCOL=create）示例 ：/users=rest[user]，会自动拼出“user:read,user:create,user:update,user:delete” 权限字符串进行权限匹配（所有都得匹配，isPermittedAll ） ssl SslFilter SSL 拦截器，只有请求协议是 https 才能通过；否则自动跳转会 https 端口（443）；其他和 port 拦截器一样； 此外，还有一个 noSessionCreation（org.apache.shiro.web.filter.session.NoSessionCreationFilter）不创建会话拦截器，调用 subject.getSession(false) 不会有什么问题，但是如果 subject.getSession(true) 将抛出 DisabledSessionException 异常。 其他的 JSP 标签之类的看 wiki 吧，实在是太长了。 其他需要验证验证码的就需要自定义 FormAuthenticationFilter 了，因为它是负责表单验证的，写一个类继承 FormAuthenticationFilter ，然后重写它的 onAccessDenied 方法，先从 Session 获取验证码和输入的比对，如果错误直接返回 true 终止执行，最好往 shiroLoginFailure 里加一下异常，便于 controller 的判断，通过则调用 super 的方法进行表单验证。记得在配置文件里配一下自定义的验证器，上面其实已经配过了。 使用 Shiro 的记住我功能时需要把相关的 bean 设置为可序列化的，然后再在配置文件中配置 CookieRememberMeManager。然后就可以使用 User 过滤器来指定那些 URL 是可以认证或者通过记住我就可以访问的。也就是说 authc 拦截器即使使用了 记住我 也不会放行，user 可以。 参考http://wiki.jikexueyuan.com/project/shiro/overview.htmlhttps://www.ibm.com/developerworks/cn/java/j-lo-shiro/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>安全</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记]]></title>
    <url>%2F2018%2F01%2F29%2FMongoDB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[MongoDB 是由 C++ 语言编写的，是一个基于分布式文件存储的开源数据库系统。在高负载的情况下，添加更多的节点，可以保证服务器性能。MongoDB 旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。MongoDB 将数据存储为一个文档，数据结构由键值 (key =&gt; value ) 对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。作为 NoSQL 里的著名项目，它比其他 NoSQL 数据库的优势之一是它强大的、基于文档的查询语言，由于查询非常容易转换(将 SQL 语句转换成 MongoDB 查询函数调用)，这使得从关系数据库到 MongoDB 的过渡变得简单，并且官方提供了完善的驱动支持，它的目的就是替代传统的 SQL 数据库。 特性 无数据结构限制mongo 没有”表”的概念,也不用设计表。它使用”集合”存储 多个”键值对”,取代表的功能；想像一下 JSON 这种数据类型。 支持完全索引 支持复制和故障恢复 快速、高扩展性(分片扩展) mongo有数据库的概念,但可以不经创建,直接使用。 当然它是不保证实时一致性，并且不支持事务的，绝大多数的 NoSQL 好像都是不支持的，带来的好处就是快！更好的性能！mongo 没有mysql中”记录”的概念,mongo使用”文档”存储任意数量的”键值对”信息；mongo 无需手动设置”主键”,系统会自动为每一个”文档”自动添加”_id”键值对,保证数据的唯一性. mysql mongodb 表(table) 集合(collection) 记录(row) 文档(document) 主键(primary key) 手动设置 _id 自动生成 然后再来说说以前写过的 Redis，同样是 NoSQL ，区别在那？具体的就不展开了，说主要的：Redis 是内存型数据库，常用于做缓存；MongoDB 是持久化存储的也就是存硬盘的，易用性不错，也更加的灵活，毕竟目标是取代 SQL 数据库的，性能也不俗，这或许是 NoSQL 的一大特点。Redis 最大的优点就是快，快，还 TMD 是快！ MongoDB 为什么比 MySQL 快？写操作 MongoDB 比传统数据库快的根本原因是 Mongo 使用的内存映射技术 ：写入数据时候只要在内存里完成就可以返回给应用程序，这样并发量自然就很高。而保存到硬体的操作则在后台异步完成。读操作MongoDB快的原因是：1）MongoDB 的设计要求你常用的数据（working set) 可以在内存里装下。这样大部分操作只需要读内存，自然很快。2）文档性模式设计一般会是的你所需要的数据都相对集中在一起（内存或硬盘），大家知道硬盘读写耗时最多是随机读写所产生的磁头定位时间，数据集中在一起则减少了关系性数据库需要从各个地方去把数据找过来（然后Join）所耗费的随机读时间。再有就是分布式集群的水平扩展所带来的压力分担。 安装我是在 Linux 上做测试，所以就简单说说 linux 下的安装，直接从官网下载二进制包（参考官网文档使用 yum 也可以），使用 tar -zxvf 解压，移到 /usr/local 即可。接下来你可以配置下环境变量(比如编辑当前用户目录下的 .bash_profile 文件的方式)，然后使用 mongod 命令启动 mongodb 的服务。执行 ./bin/mongod 服务默认会在前台执行，如果数据目录不是 /data/db 那么可以手动指定目录，启动命令为：./bin/mongod --dbpath=/usr/mongo_data MongoDB 数据库服务的默认端口是 27017 .无论指定目录还是使用默认目录都需要手动进行创建。 其实还可以进行下简单的配置:进入到 bin 目录下，编辑 mongodb.conf 文件，内容如下： 12345678910dbpath = /data/db #数据文件存放目录logpath = /data/logs/mongodb.log #日志文件存放目录port = 27017 #端口fork = true #以守护程序的方式启用，即在后台运行# 通过访问http://IP:28017/可以查看到mongodb启动的一些信息，同时也对mongodb运行# 的统计情况进行监控。在使用mongodb过程中，我们可以使用参数将该功能禁用掉。# 修改配置文件 mongodb.conf，增加参数选项：nohttpinterface = true 即可。# 3.6+ 版本中已经被删除nohttpinterface = true 执行./mongod -f mongodb.conf命令表示启动 MongoDB. 相关配置说明：–dbpath 数据库路径(数据文件)–logpath 日志文件路径–master 指为主机器–slave 指定为从机器–source 指定主机器的IP地址–pologSize 指定日志文件大小不超过 64M. 因为 resync 是非常操作量大且耗时，最好通过设置一个足够大的oplogSize来避免resync(默认的 oplog大小是空闲磁盘大小的5%)。–logappend 日志文件末尾添定加–port 启用端口号–fork 在后台运行–only 指定只复制哪一个数据库–slavedelay 指从复制检测的时间间隔–auth 是否需要验证权限登录(用户名和密码)–smallfiles 选项会减少数据文件的初始大小，并将最大大小限制为512 mb。 smallfiles也会将每个日志文件的大小从1gb减少到128mb 测试执行mongo命令表示表示进入到 MongDB 的控制台，进入到控制台之后，我们输入db.version()命令，如果能显示出当前 MongoDB 的版本号，说明安装成功了。默认情况下，连接地址是 127.0.0.1:27017，连接的数据库是 test 数据库，我们也可以手动指定连接地址和连接的数据库：mongo 127.0.0.1:27017/admin如果是 3.0+ 的版本，可能会提示下面这样的错误： WARNING: /sys/kernel/mm/transparent_hugepage/enabled is ‘always’.We suggest setting it to ‘never’WARNING: /sys/kernel/mm/transparent_hugepage/defrag is ‘always’.We suggest setting it to ‘never’ 解决方案：执行下面的两句命令： 12echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag 弊端是重启后会失效，所以可以加入到开机启动里面，编辑 /etc/rc.local 追加下面的代码： 123456if test -f /sys/kernel/mm/transparent_hugepage/enabled; then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiif test -f /sys/kernel/mm/transparent_hugepage/defrag; then echo never &gt; /sys/kernel/mm/transparent_hugepage/defragfi 这样就一劳永逸了，那么这个警告的原因是什么？原因就是 HDFS 会因为这个性能严重受影响。设置以后就是允许 hugepage 可以动态分配，而不是系统启动时预先分配，看上去对内存消耗很大的服务都不喜欢它。感觉这是一个 lazy loading 的设计思想。 关闭服务在客户端里使用 db.shutdownServer() 命令可以关闭到 MongoDB 服务，但是这个命令的执行要在 admin 数据库下，所以先切换到 admin (use admin) 基本操作查看所有数据库：show dbs删除当前数据库：db.dropDatabase()PS：默认是不需要手动创建数据库的，mongodb 会自动根据需要来创建（使用 use 直接进入即可，当不存在数据的时候使用 dbs 查询不会显示）。查看当前使用的数据库：db.getName()查看当前数据库中的表（集合）：show tables or show collections 插入在 MongoDB 中，数据以集合的形式存储。如果需要，您可以分割文档。下面创建一个文档并把它存储到一个名为 “colors” 的新集合（数据库）中（Json 格式）：db.colors.save({name:&quot;red&quot;,value:&quot;FF0000&quot;});通过查询数据库来验证文档已保存( findOne 可以返回一条数据)：db.colors.find();默认会查出所有的记录。MongoDB 中的文档以 BSON（二进制 JSON）形式存储。因为 Mongodb 支持 Js 语法，所以可以使用 for 来批量插入：for(i=1;i&lt;=10;i++)db.demo.insert({index:i});还可以进行计数操作：db.demo.find().count();使用 skip、limit、sort 操作(1：升序；-1：降序)：db.demo.find().skip(2).limit(3).sort({index:1})上面插入数据使用了两种方式，一种是 save，另一种是 insert，它们的区别是：insert：当主键 “_id” 在集合中存在时，不做任何处理。save：当主键 “_id” 在集合中存在时，进行更新。 更新更新数据同样使用的是 update 关键字：db.demo.update({index:1},{index:100})如果只需要更新部分字段，那么就需要使用 set 操作符：db.colors.update({name:&quot;red&quot;},{$set:{value:&quot;red&quot;}})当记录不存在时，插入一条数据：db.demo.update({index:1},{index:100},true)有时候查询会查到多条数据，默认只会更新第一条，如果需要批量更新那么可以使用：db.colors.update({name:&quot;red&quot;},{$set:{value:&quot;red&quot;}},false,true)和第二条类似，批量更新只支持 set 方式，第三个参数就是不存在时是否创建，第四个就是是否更新全部数据（默认false） 删除与更新不同，删除默认会删除所有查到的数据，关键字为 remove ，参数不允许为空：db.demo.remove({index:100})删除表（集合）使用的是 drop 关键字：db.demo.drop() 查找find 已经用过了，很简单，默认查出所有的数据，或者你需要一条用 findOne，配合 skip 和 limit 是非常有用的。如果是查找单条数据可以 find({x:1}) ，就是说如果你记得其中的属性的话，配合索引速度会更快。或者根据某个属性是否存在来查询：find({m:{$exists:false}}) ；强制使用索引查询：find({m:{$exists:false}}).hint(&quot;name&quot;) 。范围查询：find({age:{$gte:12, $lte:23}})更多强大的查找功能参考拓展里，如果我用到或许会来进行补充。 索引索引的作用就是为了加快查询速度，这个都是一样的，3.0+ 的版本创建索引使用 createIndex，查看索引使用 getIndexes。db.demo.createIndex({index:1})和排序类似，1 代表正向，-1 代表反向。PS：索引可以重复创建，不会报错，第二次会直接返回成功。上面通过 createIndex 给 index 创建了索引（单键索引），如果我们插入的数据是：{index:[1,2,3,4]} 这样的形式，那么系统就会自动创建一个多键索引，并不需要显示的创建。 MongoDB automatically creates a multikey index if any indexed field is an array; you do not need to explicitly specify the multikey type. 如果要根据多个字段来创建索引那就是所谓的复合索引了：db.demo.createIndex({index:1,name:1})删除索引可以通过 dropIndex(name) 的形式，name 指的是索引名，可以通过 getIndexes() 获得。 过期索引这个就非常有用了，比如可以存用户的登陆信息、日志等，因为它的特性是：超过设定的时间后索引被删除，同时相应的数据也会被清除。创建也非常的简单，只需要加一个时间参数：db.demo.createIndex({time:1},{expireAfterSeconds:10})这样数据 10 秒后就会被删除。db.demo.insert({time:new Date()})以这个例子来说，time 字段必须是 ISODate 或者 ISODate 数组（按照最小的时间进行删除），不能使用时间戳，否则不能被自动删除。过期索引不能是复合索引，因为无法根据两个日期来进行删除。删除的时间是不精确的，删除过程是由后台程序每 60s 跑一次，而且删除也需要一些时间，所以存在误差。 全文索引看名字应该就知道是什么意思了，主要用在搜索上，根据某些关键字就能搜出相应的数据，创建全文索引的方式有下面几种： 12345678# key-字段名，value-固定字符串textdb.articles.createIndex(&#123;key:"text"&#125;)# 在多个字段上创建全文索引db.articles.createIndex(&#123;key1:"text",key2:"text"&#125;)# 给所有字段创建全文索引db.articles.createIndex(&#123;"$**":"text"&#125;) 然后就是使用了，根据建立的全文索引来查询： 123456789101112131415# 查询包含 coffee 的内容的文档db.article.find(&#123;$text:&#123;$search:"coffee"&#125;&#125;)#（或查询）查询包含 aa 或 bb 或 cc 的内容的文档db.article.find(&#123;$text:&#123;$search:"aa bb cc"&#125;&#125;)# -为排除包含有 cc 内容的文档db.article.find(&#123;$text:&#123;$search:"aa bb -cc"&#125;&#125;)#（与查询）加查询内容前后用""包含，查询既包含 aa 又包含 bb cc 的内容的文档db.article.find(&#123;$text:&#123;$search:'"aa" "bb" "cc"'&#125;&#125;)# 使用 $meta 操作符来查看相似度，并根据相似度来排序db.imooc_2.find(&#123;$text:&#123;$search:"aa bb"&#125;&#125;,&#123;score:&#123;$meta:"textScore"&#125;&#125;)db.imooc_2.find(&#123;$text:&#123;$search:"aa bb"&#125;&#125;,&#123;score:&#123;$meta:"textScore"&#125;&#125;).sort(&#123;score:&#123;$meta:"textScore"&#125;&#125;); 在 MongoDB 中每个数据集合只能创建一个全文索引, 所以使用全文索引进行查询时不会起冲突.在 3.2+ 的版本中支持了对中文的全文搜索。英文搜索中是按单词来匹配，也就是说内容有空格进行区分，如果是混在一起的字符串，那效果也非常的差。注意事项： 每次查询，只能指定一个 $text 查询 $text 查询不能出现在 $nor 查询中 查询中如果包含了 $text, hint 不再起作用 全文索引会导致 mongodb 写入性能下降，因为所有字符串都要拆分，存储到不同地方。还有就是虽然支持了中文，但是效果并不好，并且只有在企业版中才可以使用 rlp 之类，并且整句的话关键词搜索还是不好，解决方案现在有使用 Elastic Search 同步 mongodb，或者使用分词工具把分词存为单独的一个 tags。 其他创建索引的时候系统会默认生成一个名字，为了可读性，我们可以手动定义生成索引的名字，就像这样：db.articles.createIndex({x:1},{name:&quot;test&quot;})就是说可以传入第二个参数，这里是 name 作为例子，其他的还有 unique（是否唯一）、sparse （稀疏性，默认 false）。开启稀疏后当记录不存在索引字段时就不会创建索引，减少了磁盘的占用，但是带来的问题是强制使用索引查询（hint）时会出错（不存在）。 然后就是地理位置索引，目前感觉用不到，简单提提，根据平面（X/Y 坐标）或者球面来定位，比如查找距离某个点 一定距离的点，包含在某区域内的点。 评判索引构建情况（是否合理）的几种方式： mongostat 工具自带的查看运行状态的工具，使用方法：mongostat -h 127.0.0.1:27017 有用户名密码的话再加 -u x -p x具体的状态说明，自行搜索（因为我没用到过，关注 qr/qw/idx miss） profile 集合查看状态：db.getProfilingStatus() ，一个是级别，另一个 slowms 是慢查询的阀值。查看/设置等级：db.get/setProfilingLevel() 同时 set 也算是开启 profile ；等级有三种，加一个关闭状态：0：不开启；1：记录慢命令，默认为大于100ms；2：记录所有命令；3、查询 profiling 记录。 日志配置文件里可以加入 berbose 来控制级别，数值在 1-5 个 V 之间。 explain 分析使用：db.demo.find({x:100}).explain()用来查看设置索引后是否起作用。 一些补充： 1234567891011121314151617181920212223242526db.system.profile.find()&#123; "op" : "query",--操作类型 "ns" : "imooc.system.profile", --查询的命名空间，;databasename.collectionname' "query" : &#123; "query" : &#123; &#125;, --查询条件 "orderby" : &#123; "$natural" : -1 &#125; &#125;, --约束条件 "ntoreturn" : 1, --返回数据条目 "ntoskip" : 0, --跳过的条目 "nscanned" : 1, --扫描的数目含索引 "nscannedObjects" : 1, --扫描的数据数目 "keyUpdates" : 0, -- "numYield" : 0, --其他情况 "lockStats" : &#123; --锁状态 "timeLockedMicros" : &#123; --锁占用时间（毫秒） "r" : NumberLong(82), --读锁 "w" : NumberLong(0) --写锁 &#125;, "timeAcquiringMicros" : &#123; "r" : NumberLong(2), "w" : NumberLong(2) &#125; &#125;, "nreturned" : 1, "responseLength" : 651, --返回长度 "millis" : 0, --查询时间&#125; 线上环境不推荐开启 profile ，上线前的观察可以使用。 用户管理设置用户名密码是最常用的保护措施，虽然安全性不是很高，但是方便；开启权限认证有两种方法：auth（配置文件里加 auth = true 即可） 或者 keyfile。然后还需要创建用户： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&gt; use admin&gt; db.createUser(... &#123;... user: "dba",... pwd: "dba",... roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ]... &#125;... )&gt; db.createUser(&#123; user:"root", pwd:"123456", roles:["root"] &#125;)&gt; db.createUser(&#123; user:"mps", pwd:"12345..", roles:[&#123; role: "dbOwner", db: "webflux" &#125;] &#125;)# 角色说明Built-In Roles（内置角色）：1. 数据库用户角色：read、readWrite;2. 数据库管理角色：dbAdmin、dbOwner、userAdmin；3. 集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager；4. 备份恢复角色：backup、restore；5. 所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase6. 超级用户角色：root // 这里还有几个角色间接或直接提供了系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase）7. 内部角色：__systemRead：允许用户读取指定数据库readWrite：允许用户读写指定数据库dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profileuserAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。root：只在admin数据库中可用。超级账号，超级权限db.system.users.find(); 帐号是跟着库走的，所以在指定库里授权，必须也在指定库里验证，账号信息存放在 admin 数据库中。开始时没进行登陆可以使用 db.auth(&#39;name&#39;,&#39;pwd&#39;) 来进行授权，返回 1 表示成功。就说到这吧，虽然是最简单的一些操作。 拓展更多内容参见：https://www.cnblogs.com/TankMa/archive/2011/06/08/2074947.htmlhttps://www.ibm.com/developerworks/cn/opensource/os-mongodb4/]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8新特性学习（二）]]></title>
    <url>%2F2018%2F01%2F19%2FJava8%E6%96%B0%E7%89%B9%E6%80%A7%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Java 8 为 Java 语言、编译器、类库、开发工具与 JVM（ Java 虚拟机）带来了大量新特性。上一篇了解了最重要的 Stream 和 Lambda 表达式（或者说闭包，虽然不是很恰当），或者可以理解为 Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中），或者把代码看成数据。这篇就补全剩下的部分，默认方法啦、方法引用（双冒号运算符）、新的类库，当然这些也是不全的，我认为经常用的就这些了，全部的新特性可以见参考的链接。 接口的默认方法与静态方法Java 8 用默认方法与静态方法这两个新概念来扩展接口的声明。默认方法与抽象方法不同之处在于抽象方法必须要求实现，但是默认方法则没有这个要求。相反，如果接口定义了默认方法，那么必须提供一个所谓的默认实现，这样所有的接口实现者将会默认继承它（如果有必要的话，可以覆盖这个默认实现）。 123456789101112131415161718private interface Defaulable &#123; // Interfaces now allow default methods, the implementer may or // may not implement (override) them. default String notRequired() &#123; return "Default implementation"; &#125; &#125;// 不需要实现默认方法 private static class DefaultableImpl implements Defaulable &#123;&#125;// 可以覆盖默认方法private static class OverridableImpl implements Defaulable &#123; @Override public String notRequired() &#123; return "Overridden implementation"; &#125;&#125; Java 8带来的另一个有趣的特性是接口可以声明（并且可以提供实现）静态方法。 1234567891011121314private interface DefaulableFactory &#123; // Interfaces now allow static methods static Defaulable create( Supplier&lt; Defaulable &gt; supplier ) &#123; return supplier.get(); &#125;&#125;public static void main( String[] args ) &#123; Defaulable defaulable = DefaulableFactory.create( DefaultableImpl::new ); System.out.println( defaulable.notRequired() ); defaulable = DefaulableFactory.create( OverridableImpl::new ); System.out.println( defaulable.notRequired() );&#125; 在JVM中，默认方法的实现是非常高效的，并且通过字节码指令为方法调用提供了支持。默认方法允许继续使用现有的 Java 接口，而同时能够保障正常的编译过程。这方面好的例子是大量的方法被添加到 java.util.Collection 接口中去：stream()，parallelStream()，forEach()，removeIf()，……尽管默认方法非常强大，但是在使用默认方法时我们需要小心注意一个地方：在声明一个默认方法前，请仔细思考是不是真的有必要使用默认方法，因为默认方法会带给程序歧义，并且在复杂的继承体系中容易产生编译错误。 为什么要有默认方法在 java 8 之前，接口与其实现类之间的 耦合度 太高了（tightly coupled），当需要为一个接口添加方法时，所有的实现类都必须随之修改。默认方法解决了这个问题，它可以为接口添加新的方法，而不会破坏已有的接口的实现。这在 lambda 表达式作为 java 8 语言的重要特性而出现之际，为升级旧接口且保持向后兼容（backward compatibility）提供了途径。这个 forEach 方法是 jdk 1.8 新增的接口默认方法，正是因为有了默认方法的引入，才不会因为 Iterable 接口中添加了 forEach 方法就需要修改所有 Iterable 接口的实现类。 关于继承和其它方法一样，接口默认方法也可以被继承。 123456789101112131415161718192021222324252627282930313233343536interface InterfaceA &#123; default void foo() &#123; System.out.println("InterfaceA foo"); &#125;&#125;interface InterfaceB extends InterfaceA &#123;&#125;interface InterfaceC extends InterfaceA &#123; @Override default void foo() &#123; System.out.println("InterfaceC foo"); &#125;&#125;// 覆写默认方法并将它重新声明为抽象方法interface InterfaceD extends InterfaceA &#123; @Override void foo();&#125;public class Test &#123; public static void main(String[] args) &#123; new InterfaceB() &#123;&#125;.foo(); // 打印：“InterfaceA foo” new InterfaceC() &#123;&#125;.foo(); // 打印：“InterfaceC foo” new InterfaceD() &#123; @Override public void foo() &#123; System.out.println("InterfaceD foo"); &#125; &#125;.foo(); // 打印：“InterfaceD foo” // 或者使用 lambda 表达式 ((InterfaceD) () -&gt; System.out.println("InterfaceD foo")).foo(); &#125;&#125; 接口默认方法的继承分三种情况（分别对应上面的 InterfaceB 接口、InterfaceC 接口和 InterfaceD 接口）： 不覆写默认方法，直接从父接口中获取方法的默认实现。 覆写默认方法，这跟类与类之间的覆写规则相类似。 覆写默认方法并将它重新声明为抽象方法，这样新接口的子类必须再次覆写并实现这个抽象方法。 然后来考虑下多继承的问题，是的，默认方法在接口里，接口可以继承，接口可以多实现，那么自然就带来了默认方法多继承的问题；但是 Java 使用的是单继承、多实现的机制，为的是避免多继承带来的调用歧义的问题。当接口的子类同时拥有具有相同签名的方法时，就需要考虑一种解决冲突的方案。 123456789101112131415161718192021222324252627282930313233343536interface InterfaceA &#123; default void foo() &#123; System.out.println("InterfaceA foo"); &#125;&#125;interface InterfaceB &#123; default void bar() &#123; System.out.println("InterfaceB bar"); &#125;&#125;interface InterfaceC &#123; default void foo() &#123; System.out.println("InterfaceC foo"); &#125; default void bar() &#123; System.out.println("InterfaceC bar"); &#125;&#125;// 不存在冲突class ClassA implements InterfaceA, InterfaceB &#123;&#125;// 错误，存在冲突//class ClassB implements InterfaceB, InterfaceC &#123;&#125;class ClassB implements InterfaceB, InterfaceC &#123; @Override public void bar() &#123; InterfaceB.super.bar(); // 调用 InterfaceB 的 bar 方法 InterfaceC.super.bar(); // 调用 InterfaceC 的 bar 方法 System.out.println("ClassB bar"); // 做其他的事 &#125;&#125; 在 ClassB 类中，它实现的 InterfaceB 接口和 InterfaceC 接口中都存在相同签名的 foo 方法，需要手动解决冲突。覆写存在歧义的方法，并可以使用 InterfaceName.super.methodName(); 的方式手动调用需要的接口默认方法。 下面来看特殊情况：接口继承行为发生冲突时的解决规则。比如，出现了下面的这种情况： 1234567891011121314151617181920212223interface InterfaceA &#123; default void foo() &#123; System.out.println("InterfaceA foo"); &#125;&#125;interface InterfaceB extends InterfaceA &#123; @Override default void foo() &#123; System.out.println("InterfaceB foo"); &#125;&#125;// 正确class ClassA implements InterfaceA, InterfaceB &#123;&#125;class ClassB implements InterfaceA, InterfaceB &#123; @Override public void foo() &#123; // InterfaceA.super.foo(); // 错误 InterfaceB.super.foo(); &#125;&#125; 因为 InterfaceB 接口继承了 InterfaceA 接口，那么 InterfaceB 接口一定包含了所有 InterfaceA 接口中的字段方法，因此一个同时实现了 InterfaceA 接口和 InterfaceB 接口的类与一个只实现了 InterfaceB 接口的类完全等价。这很好理解，就相当于 class SimpleDateFormat extends DateFormat 与 class SimpleDateFormat extends DateFormat, Object 等价（如果允许多继承）。而覆写意味着对父类方法的屏蔽，这也是 Override 的设计意图之一。因此在实现了 InterfaceB 接口的类中无法访问已被覆写的 InterfaceA 接口中的 foo 方法。这是当接口继承行为发生冲突时的规则之一，即 被其它类型所覆盖的方法会被忽略。如果想要调用 InterfaceA 接口中的 foo 方法，只能通过自定义一个新的接口同样继承 InterfaceA 接口并显示地覆写 foo 方法，在方法中使用 InterfaceA.super.foo(); 调用 InterfaceA 接口的 foo 方法，最后让实现类同时实现 InterfaceB 接口和自定义的新接口，代码如下： 123456789101112131415161718192021222324252627interface InterfaceA &#123; default void foo() &#123; System.out.println("InterfaceA foo"); &#125;&#125;interface InterfaceB extends InterfaceA &#123; @Override default void foo() &#123; System.out.println("InterfaceB foo"); &#125;&#125;interface InterfaceC extends InterfaceA &#123; @Override default void foo() &#123; InterfaceA.super.foo(); &#125;&#125;class ClassA implements InterfaceB, InterfaceC &#123; @Override public void foo() &#123; InterfaceB.super.foo(); InterfaceC.super.foo(); &#125;&#125; 注意！ 虽然 InterfaceC 接口的 foo 方法只是调用了一下父接口的默认实现方法，但是这个覆写 不能省略，否则 InterfaceC 接口中继承自 InterfaceA 接口的隐式的 foo 方法同样会被认为是被 InterfaceB 接口覆写了而被屏蔽，会导致调用 InterfaceC.super.foo() 时出错。通过这个例子，应该注意到在使用一个默认方法前，一定要考虑它是否真的需要。因为 默认方法会带给程序歧义，并且在复杂的继承体系中容易产生编译错误。滥用默认方法可能给代码带来意想不到、莫名其妙的错误。 接口与抽象类当接口继承行为发生冲突时的另一个规则是，类的方法声明优先于接口默认方法，无论该方法是具体的还是抽象的。 1234567891011121314151617181920212223242526272829303132interface InterfaceA &#123; default void foo() &#123; System.out.println("InterfaceA foo"); &#125; default void bar() &#123; System.out.println("InterfaceA bar"); &#125;&#125;abstract class AbstractClassA &#123; public abstract void foo(); public void bar() &#123; System.out.println("AbstractClassA bar"); &#125;&#125;class ClassA extends AbstractClassA implements InterfaceA &#123; @Override public void foo() &#123; InterfaceA.super.foo(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; ClassA classA = new ClassA(); classA.foo(); // 打印：“InterfaceA foo” classA.bar(); // 打印：“AbstractClassA bar” &#125;&#125; ClassA 类中并不需要手动覆写 bar 方法，因为优先考虑到 ClassA 类继承了的 AbstractClassA 抽象类中存在对 bar 方法的实现，同样的因为 AbstractClassA 抽象类中的 foo 方法是抽象的，所以在 ClassA 类中必须实现 foo 方法。虽然 Java 8 的接口的默认方法就像抽象类，能提供方法的实现，但是他们俩仍然是 不可相互代替的： 接口可以被类多实现（被其他接口多继承），抽象类只能被单继承。 接口中没有 this 指针，没有构造函数，不能拥有实例字段（实例变量）或实例方法，无法保存 状态（state），抽象方法中可以。 抽象类不能在 java 8 的 lambda 表达式中使用。 从设计理念上，接口反映的是 “like-a” 关系，抽象类反映的是 “is-a” 关系。 顺便复习了下接口和抽象类的知识点~~ 其他补充下其他的知识点： default 关键字只能在接口中使用（以及用在 switch 语句的 default 分支），不能用在抽象类中。 接口默认方法不能覆写 Object 类的 equals、hashCode 和 toString 方法。 接口中的静态方法必须是 public 的，public 修饰符可以省略，static 修饰符不能省略。 即使使用了 java 8 的环境，一些 IDE 仍然可能在一些代码的实时编译提示时出现异常的提示（例如无法发现 java 8 的语法错误），因此不要过度依赖 IDE。 方法引用其实就是上篇所说的双冒号操作，不知道还有没有印象，即 目标引用::方法 ，下面就来看看具体的几种用法。方法引用提供了非常有用的语法，可以直接引用已有 Java 类或对象的方法或构造器。与 lambda 联合使用（一般是不能独立使用的），方法引用可以使语言的构造更紧凑简洁，减少冗余代码。下面来看看 Java 支持的这四种不同的方法引用： 1234567891011121314151617181920212223242526272829303132public static class Car &#123; public static Car create( final Supplier&lt; Car &gt; supplier ) &#123; return supplier.get(); &#125; public static void collide( final Car car ) &#123; System.out.println( "Collided " + car.toString() ); &#125; public void follow( final Car another ) &#123; System.out.println( "Following the " + another.toString() ); &#125; public void repair() &#123; System.out.println( "Repaired " + this.toString() ); &#125;&#125;// 第一种Car car = Car.create( Car::new );final List&lt; Car &gt; cars = Arrays.asList( car );// 第二种cars.forEach( Car::collide );// 第三种cars.forEach( Car::repair );// 第四种final Car police = Car.create( Car::new );cars.forEach( police::follow ); 这四类可以定义为： 类名::new 类名::静态方法名 类名::实例方法名这种方法引用有些特殊之处：当使用这种方式时，一定是 lambda 表达式所接收的第一个参数来调用实例方法，如果lambda表达式接收多个参数，其余的参数作为方法的参数传递进去。参考：http://sfau.lt/b5ZD16 对象::实例方法名 下面就来解释下上面例子里的四种方式，说的都是在本例的情况下。第一种方法引用是构造器引用，它的语法是 Class::new，或者更一般的 Class&lt; T &gt;::new。new 不就是调用构造函数嘛~请注意构造器没有参数。第二种方法引用是静态方法引用，它的语法是 Class::static_method ，请注意这个方法接受一个 Car 类型的参数。第三种方法引用是特定类的任意对象的方法引用，它的语法是 Class::method。请注意，这个方法没有参数，并且是非静态。最后，第四种方法引用是特定对象的方法引用，它的语法是 instance::method。请注意，这个方法接受一个 Car 类型的参数 类库新特性Java 8 通过增加大量新类，扩展已有类的功能的方式来改善对并发编程、函数式编程、日期/时间相关操作以及其他更多方面的支持。 Optional到目前为止，臭名昭著的空指针异常是导致 Java 应用程序失败的最常见原因。以前，为了解决空指针异常，Google 公司著名的 Guava 项目引入了 Optional 类，Guava 通过使用检查空值的方式来防止代码污染，它鼓励程序员写更干净的代码。受到 Google Guava 的启发，Optional 类已经成为 Java 8 类库的一部分。Optional 实际上是个容器：它可以保存类型 T 的值，或者仅仅保存 null。Optional 提供很多有用的方法，这样我们就不用显式进行空值检测。在 Javadoc 中的描述翻译过来就是：这是一个可以为 null 的容器对象。如果值存在则 isPresent() 方法会返回 true，调用 get() 方法会返回该对象。下面就来看看它的几个方法（在前面说 stream 的时候大量使用了 Optional ）： of为非 null 的值创建一个 Optional。of 方法通过工厂方法创建 Optional 类。需要注意的是，创建对象时传入的参数不能为 null。如果传入参数为 null，则抛出 NPE。 ofNullable为指定的值创建一个 Optional，如果指定的值为 null，则返回一个空的 Optional。 empty此方法用于创建一个没有值的 Optional 对象；如果对 emptyOpt 变量调用 isPresent() 方法会返回 false，调用 get() 方法抛出 NullPointerException 异常。 isPresent如果值存在返回 true，否则返回 false。 ifPresent如果 Optional 实例有值则为其调用 consumer（比如 lambda 表达式），否则不做处理 get如果 Optional 有值则将其返回，否则抛出 NoSuchElementException。 orElse如果有值则将其返回，否则返回指定的其它值(默认值)。empty.orElse(&quot;There is no value present!&quot;); orElseGetorElseGet 与 orElse 方法类似，区别在于得到的默认值。orElse 方法将传入的字符串作为默认值，orElseGet 方法可以接受 Supplier 接口的实现用来生成默认值。empty.orElseGet(() -&gt; &quot;Default Value&quot;); orElseThrow如果有值则将其返回，否则抛出 supplier 接口创建的异常。在 orElseThrow 中我们可以传入一个 lambda 表达式或方法，如果值不存在来抛出异常。empty.orElseThrow(ValueAbsentException::new); map如果有值，则对其执行调用 mapping 函数得到返回值。如果返回值不为 null，则创建包含 mapping 返回值的 Optional 作为 map 方法返回值，否则返回空 Optional。Optional&lt;String&gt; upperName = name.map((value) -&gt; value.toUpperCase()); flatMap如果有值，为其执行 mapping 函数返回 Optional 类型返回值，否则返回空 Optional。flatMap 与 map（Funtion）方法类似，区别在于 flatMap 中的 mapper 返回值必须是 Optional。调用结束时，flatMap 不会对结果用 Optional 封装。upperName = name.flatMap((value) -&gt; Optional.of(value.toUpperCase())); filter如果有值并且满足断言条件返回包含该值的 Optional，否则返回空 Optional。对于 filter 函数我们应该传入实现了 Predicate 接口的 lambda 表达式。Optional&lt;String&gt; longName = name.filter((value) -&gt; value.length() &gt; 6); 要理解 ifPresent 方法，首先需要了解 Consumer 类。简答地说，Consumer 类包含一个抽象方法。该抽象方法对传入的值进行处理，但没有返回值。Java8 支持不用接口直接通过 lambda 表达式传入参数。 在 Java 9 中，对 Optional 还进行了增强，多加了几个方法，感兴趣的可以去：http://sfau.lt/b5KDt8最后通过一个例子来综合的展示下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class OptionalDemo &#123; public static void main(String[] args) &#123; //创建Optional实例，也可以通过方法返回值得到。 Optional&lt;String&gt; name = Optional.of("Sanaulla"); //创建没有值的Optional实例，例如值为'null' Optional empty = Optional.ofNullable(null); //isPresent方法用来检查Optional实例是否有值。 if (name.isPresent()) &#123; //调用get()返回Optional值。 System.out.println(name.get()); &#125; try &#123; //在Optional实例上调用get()抛出NoSuchElementException。 System.out.println(empty.get()); &#125; catch (NoSuchElementException ex) &#123; System.out.println(ex.getMessage()); &#125; //ifPresent方法接受lambda表达式参数。 //如果Optional值不为空，lambda表达式会处理并在其上执行操作。 name.ifPresent((value) -&gt; &#123; System.out.println("The length of the value is: " + value.length()); &#125;); //如果有值orElse方法会返回Optional实例，否则返回传入的错误信息。 System.out.println(empty.orElse("There is no value present!")); System.out.println(name.orElse("There is some value!")); //orElseGet与orElse类似，区别在于传入的默认值。 //orElseGet接受lambda表达式生成默认值。 System.out.println(empty.orElseGet(() -&gt; "Default Value")); System.out.println(name.orElseGet(() -&gt; "Default Value")); try &#123; //orElseThrow与orElse方法类似，区别在于返回值。 //orElseThrow抛出由传入的lambda表达式/方法生成异常。 empty.orElseThrow(ValueAbsentException::new); &#125; catch (Throwable ex) &#123; System.out.println(ex.getMessage()); &#125; //map方法通过传入的lambda表达式修改Optonal实例默认值。 //lambda表达式返回值会包装为Optional实例。 Optional&lt;String&gt; upperName = name.map((value) -&gt; value.toUpperCase()); System.out.println(upperName.orElse("No value found")); //flatMap与map（Funtion）非常相似，区别在于lambda表达式的返回值。 //map方法的lambda表达式返回值可以是任何类型，但是返回值会包装成Optional实例。 //但是flatMap方法的lambda返回值总是Optional类型。 upperName = name.flatMap((value) -&gt; Optional.of(value.toUpperCase())); System.out.println(upperName.orElse("No value found")); //filter方法检查Optiona值是否满足给定条件。 //如果满足返回Optional实例值，否则返回空Optional。 Optional&lt;String&gt; longName = name.filter((value) -&gt; value.length() &gt; 6); System.out.println(longName.orElse("The name is less than 6 characters")); //另一个示例，Optional值不满足给定条件。 Optional&lt;String&gt; anotherName = Optional.of("Sana"); Optional&lt;String&gt; shortName = anotherName.filter((value) -&gt; value.length() &gt; 6); System.out.println(shortName.orElse("The name is less than 6 characters")); &#125;&#125; Java 8 提倡函数式编程，新增的许多 API 都可以用函数式编程表示，Optional类也是其中之一。这里有几条关于Optional使用的建议： 尽量避免在程序中直接调用Optional对象的get()和isPresent()方法(活用 orElse 系列)； 避免使用Optional类型声明实体类的属性； 第一条建议中直接调用get()方法是很危险的做法，如果Optional的值为空，那么毫无疑问会抛出 NPE 异常，而为了调用get()方法而使用isPresent()方法作为空值检查，这种做法与传统的用 if 语句块做空值检查没有任何区别。第二条建议避免使用 Optional 作为实体类的属性，它在设计的时候就没有考虑过用来作为类的属性，如果你查看 Optional 的源代码，你会发现它没有实现 java.io.Serializable 接口，这在某些情况下是很重要的（比如你的项目中使用了某些序列化框架），使用了 Optional 作为实体类的属性，意味着他们不能被序列化。 12345678910111213141516171819User user = ... if (user != null) &#123; String userName = user.getUserName(); if (userName != null) &#123; return userName.toUpperCase(); &#125; else &#123; return null; &#125; &#125; else &#123; return null; &#125;// 简化 if-elseUser user = ... Optional&lt;User&gt; userOpt = Optional.ofNullable(user);return user.map(User::getUserName) .map(String::toUpperCase) .orElse(null); 当你很确定一个对象不可能为 null 的时候，应该使用 of() 方法，否则，尽可能使用 ofNullable() 方法 新的时间和日期APIJava 8 另一个新增的重要特性就是引入了新的时间和日期 API，它们被包含在 java.time 包中。借助新的时间和日期 API 可以以更简洁的方法处理时间和日期。在 Java 8 之前，所有关于时间和日期的 API 都存在各种使用方面的缺陷，主要有： Java 的 java.util.Date和java.util.Calendar类易用性差，不支持时区，并且是可变的，也就意味着他们都不是线程安全的； 用于格式化日期的类DateFormat被放在java.text包中，它是一个抽象类，所以我们需要实例化一个 SimpleDateFormat 对象来处理日期格式化，并且 DateFormat 也是非线程安全，这意味着如果你在多线程程序中调用同一个 DateFormat 对象，会得到意想不到的结果。 对日期的计算方式繁琐，而且容易出错，因为月份是从0开始的，这意味着从Calendar中获取的月份需要加一才能表示当前月份。 由于以上这些问题，出现了一些三方的日期处理框架，例如 Joda-Time，data4j 等开源项目。但是，Java 需要一套标准的用于处理时间和日期的框架，于是 Java 8 中引入了新的日期 API。新的日期 API 是 JSR-310 规范的实现，Joda-Time 框架的作者正是 JSR-310 的规范的倡导者，所以能从 Java 8 的日期 API 中看到很多 Joda-Time 的特性。常用的几个类就是 LocalDate, LocalTime, LocalDateTime, Instant, Period, Duration 等.下面通过几个示例代码来快速学会使用新版的日期时间 API： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107// 获取当前日期LocalDate today = LocalDate.now();System.out.println("Today's Local date : " + today); // 2014-01-14// 获取年月日LocalDate today = LocalDate.now();int year = today.getYear();int month = today.getMonthValue();int day = today.getDayOfMonth();System.out.printf("Year : %d Month : %d day : %d t %n", year, month, day);int length = today.lengthOfMonth(); // 月份的天数boolean leapYear = today.isLeapYear(); // 是否为闰年// 处理特定日期（只需要传入年月日）LocalDate dateOfBirth = LocalDate.of(2010, 01, 14);System.out.println("Your Date of birth is : " + dateOfBirth); // 2010-01-14// 判断日期是否相等LocalDate date1 = LocalDate.of(2014, 01, 14);if(date1.equals(today))&#123; System.out.printf("Today %s and date1 %s are same date %n", today, date1);&#125;// 检查像生日这种周期性事件,类似的还有 YearMonthLocalDate dateOfBirth = LocalDate.of(2010, 01, 14);MonthDay birthday = MonthDay.of(dateOfBirth.getMonth(), dateOfBirth.getDayOfMonth());MonthDay currentMonthDay = MonthDay.from(today); if(currentMonthDay.equals(birthday))&#123; System.out.println("Many Many happy returns of the day !!");&#125;else&#123; System.out.println("Sorry, today is not your birthday");&#125;// 获取当前时间（这次不是日期是时间）LocalTime time = LocalTime.now();System.out.println("local time now : " + time);// 获取日期和时间，还可以进行拼接，或者 toLocalDate 拆分LocalDateTime ldt1 = LocalDateTime.of(2017, Month.JANUARY, 4, 17, 23, 52);LocalDate localDate = LocalDate.of(2017, Month.JANUARY, 4);LocalTime localTime = LocalTime.of(17, 23, 52);LocalDateTime ldt2 = localDate.atTime(localTime);// 时间的操作（因为是不可变对象，所以操作后的是新实例）LocalTime time = LocalTime.now();LocalTime newTime = time.plusHours(2); // adding two hoursSystem.out.println("Time after 2 hours : " + newTime);// 日期的操作，通过 withMonth 等方法可修改指定日期LocalDate nextWeek = today.plus(1, ChronoUnit.WEEKS); // 可处理天，周，月System.out.println("Today is : " + today);System.out.println("Date after 1 week : " + nextWeek);LocalDate previousYear = today.minus(1, ChronoUnit.YEARS); // 处理年System.out.println("Date before 1 year : " + previousYear);LocalDate nextYear = today.plus(1, YEARS);System.out.println("Date after 1 year : " + nextYear);// 是否早于或者晚于一个日期LocalDate tomorrow = LocalDate.of(2014, 1, 15);if(tommorow.isAfter(today))&#123; System.out.println("Tomorrow comes after today");&#125;LocalDate yesterday = today.minus(1, DAYS);if(yesterday.isBefore(today))&#123; System.out.println("Yesterday is day before today");&#125;// 处理时区ZoneId america = ZoneId.of("America/New_York");LocalDateTime localtDateAndTime = LocalDateTime.now();ZonedDateTime dateAndTimeInNewYork = ZonedDateTime.of(localtDateAndTime, america );System.out.println("Current date and time in a particular timezone : " + dateAndTimeInNewYork);// 计算两个日期之间的天数和月数LocalDate java8Release = LocalDate.of(2014, Month.MARCH, 14);Period periodToNextJavaRelease = Period.between(today, java8Release);System.out.println("Months left between today and Java 8 release : " + periodToNextJavaRelease.getMonths() );// 获取时间戳Instant timestamp = Instant.now();Instant instant = Instant.ofEpochSecond(120, 100000); // 第一个参数秒，第二个纳秒System.out.println("What is value of this instant " + timestamp);// 自定义格式化工具，第一个为字符串转日期，第二个日期转字符串String goodFriday = "Apr 18 2014";try &#123; DateTimeFormatter formatter = DateTimeFormatter.ofPattern("MMM dd yyyy"); LocalDate holiday = LocalDate.parse(goodFriday, formatter); System.out.printf("Successfully parsed String %s, date is %s%n", goodFriday, holiday);&#125; catch (DateTimeParseException ex) &#123; System.out.printf("%s is not parsable!%n", goodFriday); ex.printStackTrace();&#125;LocalDateTime arrivalDate = LocalDateTime.now();try &#123; DateTimeFormatter format = DateTimeFormatter.ofPattern("MMM dd yyyy hh:mm a"); String landing = arrivalDate.format(format); System.out.printf("Arriving at : %s %n", landing);&#125; catch (DateTimeException ex) &#123; System.out.printf("%s can't be formatted!%n", arrivalDate); ex.printStackTrace();&#125; 上面所用的 API 大部分都是不可变的，也就是说是线程安全的，可放心食用！Instant 用于表示一个时间戳，它与我们常使用的 System.currentTimeMillis() 有些类似，不过 Instant 可以精确到纳秒（Nano-Second），System.currentTimeMillis() 方法只精确到毫秒（Milli-Second）。类似的还有 Duration、Period 它们通过 between 方法来确定一段时间。 Base64的API在 JDK1.6 之前，JDK 核心类一直没有 Base64 的实现类，有人建议用 Sun/Oracle JDK 里面的 sun.misc.BASE64Encoder 和 sun.misc.BASE64Decoder，使用它们的优点就是不需要依赖第三方类库，缺点就是可能在未来版本会被删除（用 maven 编译会发出警告），而且性能不佳，性能测试见最后的参考链接。JDK1.6 中添加了另一个 Base64 的实现，javax.xml.bind.DatatypeConverter 两个静态方法 parseBase64Binary 和 printBase64Binary，隐藏在 javax.xml.bind 包下面，不被很多开发者知道。在 Java 8 在 java.util 包下面实现了 BASE64 编解码 API，而且性能不俗，API 也简单易懂，下面展示下这个类的使用例子。 123456789101112131415161718192021/* Basic编码：是标准的BASE64编码，用于处理常规的需求 */// 编码String asB64 = Base64.getEncoder().encodeToString("some string".getBytes("utf-8"));System.out.println(asB64); // 输出为: c29tZSBzdHJpbmc=// 解码byte[] asBytes = Base64.getDecoder().decode("c29tZSBzdHJpbmc=");System.out.println(new String(asBytes, "utf-8")); // 输出为: some string/* URL编码：使用下划线替换URL里面的反斜线“/” */String urlEncoded = Base64.getUrlEncoder().encodeToString("subjects?abcd".getBytes("utf-8"));System.out.println("Using URL Alphabet: " + urlEncoded);// 输出为: Using URL Alphabet: c3ViamVjdHM_YWJjZA==/* MIME编码：使用基本的字母数字产生BASE64输出，而且对MIME格式友好：每一行输出不超过76个字符，而且每行以“\r\n”符结束。 */StringBuilder sb = new StringBuilder();for (int t = 0; t &lt; 10; ++t) &#123; sb.append(UUID.randomUUID().toString());&#125;byte[] toEncode = sb.toString().getBytes("utf-8");String mimeEncoded = Base64.getMimeEncoder().encodeToString(toEncode);System.out.println(mimeEncoded); 之前我们常用的第三方工具有：Apache Commons Codec library 里面的 org.apache.commons.codec.binary.Base64 ；Google Guava 库里面的 com.google.common.io.BaseEncoding.base64() 这个静态方法；net.iharder.Base64 ，这个 jar 包就一个类；号称 Base64 编码速度最快的 MigBase64，而且是 10 年前的实现.关于他们之间的性能测试去参考里面的最后一个链接查看，总之，用 Java 8 自带的就足足够了！ JVM新特性PermGen 空间被移除了，取而代之的是 Metaspace（JEP 122）。JVM 选项 -XX:PermSize 与 -XX:MaxPermSize 分别被 -XX:MetaSpaceSize 与 -XX:MaxMetaspaceSize 所代替。 参考http://ebnbin.com/2015/12/20/java-8-default-methods/http://www.importnew.com/11908.htmlhttp://www.importnew.com/6675.htmlhttp://www.importnew.com/15637.htmlhttps://lw900925.github.io/java/java8-newtime-api.htmlhttp://www.importnew.com/14961.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8新特性学习]]></title>
    <url>%2F2018%2F01%2F17%2FJava8%E6%96%B0%E7%89%B9%E6%80%A7%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[很惭愧，关于 Java 8 的特性早就开坑想学，然而….现在都到 Java 9 了依然没填完，这份 MD 文档在桌面也放了半年多了（ :XD 捂脸），一直因为各种原因放在哪里吃灰，今天看了看有必要填坑了！Java 8 新加入的一些特性是很有用的！Spring Framework 5+ 都是基于 Java 8 了，再不学要跟不上了！这篇就说了说 Lambda 表达式和 Stream，发现已经很长了，其他的下次再聊，下篇应该不会太久，大概…. Lambda表达式简单说，Lambda 表达式是用来简化匿名类的一种写法，使用它有个条件就是，匿名类实现的接口中只能有一个方法，也就是只有一个需要实现的方法，并且它的写法有很多种，各种个样的简化。其实 Lambda 表达式的本质只是一个”语法糖“，由编译器推断并帮你转换包装为常规的代码，因此你可以使用更少的代码来实现同样的功能。Lambda 表达式赋予了 Java 程序员相较于其他函数式编程语言缺失的特性，结合虚拟扩展方法之类的特性，Lambda 表达式能写出一些极好的代码。Lambda 表达式的加入，使得 Java 拥有了函数式编程的能力。建议不要乱用，因为这就和某些很高级的黑客写的代码一样，简洁，难懂，难以调试，维护人员想骂娘。Java SE 8 添加了 2 个对集合数据进行批量操作的包: java.util.function 包以及 java.util.stream 包。流 (stream) 就如同迭代器 (iterator),但附加了许多额外的功能。 总的来说，lambda 表达式和 stream 是自 Java 语言添加泛型(Generics)和注解(annotation)以来最大的变化。简单的例子： 1234567891011121314// 1. 不需要参数,返回值为 5 () -&gt; 5 // 2. 接收一个参数(数字类型),返回其2倍的值 x -&gt; 2 * x // 3. 接受2个参数(数字),并返回他们的差值 (x, y) -&gt; x – y // 4. 接收2个int型整数,返回他们的和 (int x, int y) -&gt; x + y // 5. 接受一个 string 对象,并在控制台打印,不返回任何值(看起来像是返回void) (String s) -&gt; System.out.print(s) 上面的这些是最最基本的，下面继续来看看还有那些骚操作吧 基本的Lambda例子比如我们要遍历一个 List： 1234567891011121314151617String[] atp = &#123;"Rafael Nadal", "Novak Djokovic", "Stanislas Wawrinka", "David Ferrer","Roger Federer", "Andy Murray","Tomas Berdych", "Juan Martin Del Potro"&#125;;List&lt;String&gt; players = Arrays.asList(atp);// 以前的循环方式for (String player : players) &#123; System.out.print(player + "; ");&#125; // 使用 lambda 表达式以及函数操作(functional operation)players.forEach((player) -&gt; System.out.print(player + "; "));// 在 Java 8 中使用双冒号操作符(double colon operator)players.forEach(System.out::println); 常用的还有使用 lambdas 来实现 Runnable接口，以及实现自定义排序： 123456789101112131415161718192021222324252627282930313233// 1.1使用匿名内部类new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("Hello world !"); &#125; &#125;).start();// 1.2使用 lambda expressionnew Thread(() -&gt; System.out.println("Hello world !")).start();// 2.1使用匿名内部类Runnable race1 = new Runnable() &#123; @Override public void run() &#123; System.out.println("Hello world !"); &#125; &#125;; // 2.2使用 lambda expressionRunnable race2 = () -&gt; System.out.println("Hello world !");// 直接调用 run 方法(没开新线程哦!)race1.run();race2.run();// 3.1 使用 lambda expression 排序 playersComparator&lt;String&gt; sortByName = (String s1, String s2) -&gt; (s1.compareTo(s2));Arrays.sort(players, sortByName); // 3.2 也可以采用如下形式:Arrays.sort(players, (String s1, String s2) -&gt; (s1.compareTo(s2))); 这样一看，还是挺爽的。 使用 :: 运算符作为 Lambda 调用特定方法的缩写，并且拥有更好的可读性。 使用Lambdas和StreamsStream 是对集合的包装,通常和 lambda 一起使用。 使用 lambdas 可以支持许多操作，如 map, filter, limit, sorted, count, min, max, sum, collect 等等。同样，Stream 使用懒运算，他们并不会真正地读取所有数据，遇到像 getFirst() 这样的方法就会结束链式语法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061List&lt;Person&gt; javaProgrammers = new ArrayList&lt;Person&gt;() &#123; &#123; add(new Person("Elsdon", "Jaycob", "Java programmer", "male", 43, 2000)); add(new Person("Tamsen", "Brittany", "Java programmer", "female", 23, 1500)); add(new Person("Floyd", "Donny", "Java programmer", "male", 33, 1800)); add(new Person("Sindy", "Jonie", "Java programmer", "female", 32, 1600)); add(new Person("Vere", "Hervey", "Java programmer", "male", 22, 1200)); add(new Person("Maude", "Jaimie", "Java programmer", "female", 27, 1900)); add(new Person("Shawn", "Randall", "Java programmer", "male", 30, 2300)); add(new Person("Jayden", "Corrina", "Java programmer", "female", 35, 1700)); add(new Person("Palmer", "Dene", "Java programmer", "male", 33, 2000)); add(new Person("Addison", "Pam", "Java programmer", "female", 34, 1300)); &#125;&#125;;List&lt;Person&gt; phpProgrammers = new ArrayList&lt;Person&gt;() &#123; &#123; add(new Person("Jarrod", "Pace", "PHP programmer", "male", 34, 1550)); add(new Person("Clarette", "Cicely", "PHP programmer", "female", 23, 1200)); add(new Person("Victor", "Channing", "PHP programmer", "male", 32, 1600)); add(new Person("Tori", "Sheryl", "PHP programmer", "female", 21, 1000)); add(new Person("Osborne", "Shad", "PHP programmer", "male", 32, 1100)); add(new Person("Rosalind", "Layla", "PHP programmer", "female", 25, 1300)); add(new Person("Fraser", "Hewie", "PHP programmer", "male", 36, 1100)); add(new Person("Quinn", "Tamara", "PHP programmer", "female", 21, 1000)); add(new Person("Alvin", "Lance", "PHP programmer", "male", 38, 1600)); add(new Person("Evonne", "Shari", "PHP programmer", "female", 40, 1800)); &#125;&#125;;System.out.println("所有程序员的姓名:"); javaProgrammers.forEach((p) -&gt; System.out.printf("%s %s; ", p.getFirstName(), p.getLastName())); phpProgrammers.forEach((p) -&gt; System.out.printf("%s %s; ", p.getFirstName(), p.getLastName())); System.out.println("给程序员加薪 5% :"); Consumer&lt;Person&gt; giveRaise = e -&gt; e.setSalary(e.getSalary() / 100 * 5 + e.getSalary()); javaProgrammers.forEach(giveRaise); phpProgrammers.forEach(giveRaise); System.out.println("下面是月薪超过 $1,400 的PHP程序员:");phpProgrammers.stream() .filter((p) -&gt; (p.getSalary() &gt; 1400)) .forEach((p) -&gt; System.out.printf("%s %s; ", p.getFirstName(), p.getLastName())); // 定义 filters Predicate&lt;Person&gt; ageFilter = (p) -&gt; (p.getAge() &gt; 25); Predicate&lt;Person&gt; salaryFilter = (p) -&gt; (p.getSalary() &gt; 1400); Predicate&lt;Person&gt; genderFilter = (p) -&gt; ("female".equals(p.getGender())); System.out.println("下面是年龄大于 24岁且月薪在$1,400以上的女PHP程序员:"); phpProgrammers.stream() .filter(ageFilter) .filter(salaryFilter) .filter(genderFilter) .forEach((p) -&gt; System.out.printf("%s %s; ", p.getFirstName(), p.getLastName())); // 重用filters System.out.println("年龄大于 24岁的女性 Java programmers:"); javaProgrammers.stream() .filter(ageFilter) .filter(genderFilter) .forEach((p) -&gt; System.out.printf("%s %s; ", p.getFirstName(), p.getLastName())); 上面展示了一些对于集合的“新操作”，这都是平常用的最多的，使用了语法糖后，真是更爽了。 Lambda表达式的结构让我们了解一下 Lambda 表达式的结构。 一个 Lambda 表达式可以有零个或多个参数 参数的类型既可以明确声明，也可以根据上下文来推断。例如：(int a)与(a)效果相同 所有参数需包含在圆括号内，参数之间用逗号相隔。例如：(a, b) 或 (int a, int b) 或 (String a, int b, float c) 空圆括号代表参数集为空。例如：() -&gt; 42 当只有一个参数，且其类型可推导时，圆括号（）可省略。例如：a -&gt; return a*a Lambda 表达式的主体可包含零条或多条语句 如果 Lambda 表达式的主体只有一条语句，花括号可省略。匿名函数的返回类型与该主体表达式一致 如果 Lambda 表达式的主体包含一条以上语句，则表达式必须包含在花括号中（形成代码块）。匿名函数的返回类型与代码块的返回类型一致，若没有返回则为空 函数式接口在 Java 中，Marker（标记）类型的接口是一种没有方法或属性声明的接口，简单地说，marker 接口是空接口。相似地，函数式接口是只包含一个抽象方法声明的接口。java.lang.Runnable 就是一种函数式接口，在 Runnable 接口中只声明了一个方法 void run() ；每个 Lambda 表达式都能隐式地赋值给函数式接口，当不指明函数式接口时，编译器会自动解释这种转化。 12345Runnable r = () -&gt; System.out.println("hello world");new Thread( () -&gt; System.out.println("hello world")).start(); @FunctionalInterface 是 Java 8 新加入的一种接口，用于指明该接口类型声明是根据 Java 语言规范定义的函数式接口。Java 8 还声明了一些 Lambda 表达式可以使用的函数式接口，当你标注的接口不是有效的函数式接口时，可以使用 @FunctionalInterface 解决编译层面的错误。 1234@FunctionalInterfacepublic interface WorkerInterface &#123; public void doSomeWork();&#125; 根据定义，函数式接口只能有一个抽象方法，如果你尝试添加第二个抽象方法，将抛出编译时错误.需要记住的一件事是：默认方法（default）与静态方法并不影响函数式接口的契约，可以任意使用。 双冒号操作符双冒号（::）操作符是 Java 中的方法引用。 当们使用一个方法的引用时，目标引用放在 :: 之前，目标引用提供的方法名称放在 :: 之后，即 目标引用::方法。比如：Person::getAge; 1234// 获取 getAge 方法的 Function 对象Function&lt;Person, Integer&gt; getAge = Person::getAge;// 传参数调用 getAge 方法Integer age = getAge.apply(p); 目标引用的参数类型是 Function&lt;T,R&gt;，T 表示传入类型，R 表示返回类型。比如，表达式 person -&gt; person.getAge();，传入参数是 person，返回值是 person.getAge()，那么方法引用 Person::getAge 就对应着 Function&lt;Person,Integer&gt; 类型。“::” 称之为定界符，当我们使用它的时候，只是用来引用要使用的方法，而不是调用方法，所以不能在方法后面加 ()。你不能直接调用方法引用，只是用来替代 Lambda 表达式，所以，哪里使用 Lambda 表达式了，哪里就可以使用方法引用了。 Lambda与匿名类使用匿名类与 Lambda 表达式的一大区别在于关键词的使用。对于匿名类，关键词 this 解读为匿名类，而对于 Lambda 表达式，关键词 this 解读为写入 Lambda 的类。 1234567891011121314151617181920212223public class Example &#123; private String firstName = "Tom"; public void example() &#123; Function&lt;String, String&gt; addSurname = surname -&gt; &#123; // equivalent to this.firstName return firstName + " " + surname; // or even, &#125;; &#125;&#125;// Java 8 之前必须显示调用public class Example &#123; private String firstName = "Jerry"; public void anotherExample() &#123; Function&lt;String, String&gt; addSurname = new Function&lt;String, String&gt;() &#123; @Override public String apply(String surname) &#123; return Example.this.firstName + " " + surname; &#125; &#125;; &#125;&#125; Lambda 表达式与匿名类的另一不同在于两者的编译方法。Java 编译器编译 Lambda 表达式并将他们转化为类里面的私有函数，它使用 Java 7 中新加的 invokedynamic 指令动态绑定该方法.还有一点 Lambda 表达式的代码块不允许调用接口中定义的默认方法，但是 Lambda 表达式创建的对象与匿名内部类生成的对象一样，都可以直接调用从接口中集成的默认方法。Lambda 表达式不需要每次都要被实例化，对于 Java 来说，带来巨大的好处。不像实例化匿名类，对内存的影响可以降到最小。 Java8中的forEachforEach 方法是 JAVA 8 中在集合父接口 java.lang.Iterable 中新增的一个 default 实现方法. 123456default void forEach(Consumer&lt;? super T&gt; action) &#123; Objects.requireNonNull(action); for (T t : this) &#123; action.accept(t); &#125;&#125; 可以看出，forEach 方法接受一个在 JAVA 8 中新增的 java.util.function.Consumer 的消费行为或者称之为动作 (Consumer action )类型；然后将集合中的每个元素作为消费行为的 accept 方法的参数执行。那么自然我们就可以自定义消费行为动作 Consumer，只需要实现 Consumer 接口的 accept 方法。 Java8中的StreamStream 作为 Java 8 的一大亮点，它与 java.io 包里的 InputStream 和 OutputStream 是完全不同的概念。它也不同于 StAX 对 XML 解析的 Stream，也不是 Amazon Kinesis 对大数据实时处理的 Stream。Java 8 中的 Stream 是对集合（Collection）对象功能的增强，它专注于对集合对象进行各种非常便利、高效的聚合操作（aggregate operation），或者大批量数据操作 (bulk data operation)。Stream API 借助于同样新出现的 Lambda 表达式，极大的提高编程效率和程序可读性。同时它提供串行和并行两种模式进行汇聚操作，并发模式能够充分利用多核处理器的优势，使用 fork/join 并行方式来拆分任务和加速处理过程。通常编写并行代码很难而且容易出错, 但使用 Stream API 无需编写一行多线程的代码，就可以很方便地写出高性能的并发程序。所以说，Java 8 中首次出现的 java.util.stream 是一个函数式语言+多核时代综合影响的产物。 聚合操作但在当今这个数据大爆炸的时代，在数据来源多样化、数据海量化的今天，很多时候不得不脱离 RDBMS，或者以底层返回的数据为基础进行更上层的数据统计。而 Java 的集合 API 中，仅仅有极少量的辅助型方法，更多的时候是程序员需要用 Iterator 来遍历集合，完成相关的聚合应用逻辑。这是一种远不够高效、笨拙的方法。下面比较下在 Java 7 与 Java 8 中，如果要发现 type 为 grocery 的所有交易，然后返回以交易值降序排序好的交易 ID 集合的写法： 1234567891011121314151617181920212223// Java 7:List&lt;Transaction&gt; groceryTransactions = new Arraylist&lt;&gt;();for(Transaction t: transactions)&#123; if(t.getType() == Transaction.GROCERY)&#123; groceryTransactions.add(t); &#125;&#125;Collections.sort(groceryTransactions, new Comparator()&#123; public int compare(Transaction t1, Transaction t2)&#123; return t2.getValue().compareTo(t1.getValue()); &#125;&#125;);List&lt;Integer&gt; transactionIds = new ArrayList&lt;&gt;();for(Transaction t: groceryTransactions)&#123; transactionsIds.add(t.getId());&#125;// Java 8List&lt;Integer&gt; transactionsIds = transactions.parallelStream() .filter(t -&gt; t.getType() == Transaction.GROCERY) .sorted(comparing(Transaction::getValue).reversed()) .map(Transaction::getId) .collect(toList()); Java 8 使用 Stream，代码更加简洁易读；而且使用并发模式，程序执行速度更快. 什么是流Stream 不是集合元素，它不是数据结构并不保存数据，它是有关算法和计算的，它更像一个高级版本的 Iterator。原始版本的 Iterator，用户只能显式地一个一个遍历元素并对其执行某些操作；高级版本的 Stream，用户只要给出需要对其包含的元素执行什么操作，比如 “过滤掉长度大于 10 的字符串”、“获取每个字符串的首字母”等，Stream 会隐式地在内部进行遍历，做出相应的数据转换。Stream 就如同一个迭代器（Iterator），单向，不可往复，数据只能遍历一次，遍历过一次后即用尽了，就好比流水从面前流过，一去不复返。而和迭代器又不同的是，Stream 可以并行化操作，迭代器只能命令式地、串行化操作。顾名思义，当使用串行方式去遍历时，每个 item 读完后再读下一个 item。而使用并行去遍历时，数据会被分成多个段，其中每一个都在不同的线程中处理，然后将结果一起输出。Stream 的并行操作依赖于 Java7 中引入的 Fork/Join 框架（JSR166y）来拆分任务和加速处理过程。Stream 的另外一大特点是，数据源本身可以是无限的。 流的构成当我们使用一个流的时候，通常包括三个基本步骤：获取一个数据源（source）→ 数据转换 → 执行操作获取想要的结果；每次转换原有 Stream 对象不改变，返回一个新的 Stream 对象（可以有多次转换），这就允许对其操作可以像链条一样排列，变成一个管道。有多种方式生成 Stream Source：从 Collection 和数组： Collection.stream() Collection.parallelStream() Arrays.stream(T array) or Stream.of() 从 BufferedReader： java.io.BufferedReader.lines() 静态工厂： java.util.stream.IntStream.range() java.nio.file.Files.walk() 自己构建： java.util.Spliterator 其它： Random.ints() BitSet.stream() Pattern.splitAsStream(java.lang.CharSequence) JarFile.stream() 一般我们用的比较多的就是第一种吧。 流的操作流的操作类型分为两种： Intermediate：一个流可以后面跟随零个或多个 intermediate 操作。其目的主要是打开流，做出某种程度的数据映射/过滤，然后返回一个新的流，交给下一个操作使用。这类操作都是惰性化的（lazy），就是说，仅仅调用到这类方法，并没有真正开始流的遍历。 Terminal：一个流只能有一个 terminal 操作，当这个操作执行后，流就被使用“光”了，无法再被操作。所以这必定是流的最后一个操作。Terminal 操作的执行，才会真正开始流的遍历，并且会生成一个结果，或者一个 side effect。 因为转换操作都是 lazy 的，所以多个转换操作只会在 Terminal 操作的时候融合起来，一次循环完成。我们可以这样简单的理解，Stream 里有个操作函数（把定义的处理方法就叫操作函数吧）的集合，每次转换操作就是把转换函数放入这个集合中，在 Terminal 操作的时候循环 Stream 对应的集合，然后对每个元素执行所有的函数。还有一种操作被称为 short-circuiting。用以指： 对于一个 intermediate 操作，如果它接受的是一个无限大（infinite/unbounded）的 Stream，但返回一个有限的新 Stream。 对于一个 terminal 操作，如果它接受的是一个无限大的 Stream，但能在有限的时间计算出结果。 当操作一个无限大的 Stream，而又希望在有限时间内完成操作，则在管道内拥有一个 short-circuiting 操作是必要非充分条件。下面就来看一个简单的例子： 1234int sum = widgets.stream() .filter(w -&gt; w.getColor() == RED) .mapToInt(w -&gt; w.getWeight()) .sum(); stream() 获取当前小物件的 source，filter 和 mapToInt 为 intermediate 操作，进行数据筛选和转换，最后一个 sum() 为 terminal 操作，对符合条件的全部小物件作重量求和。具体的操作可进行简单的分类： Intermediate：map (mapToInt, flatMap 等)、 filter、 distinct、 sorted、 peek、 limit、 skip、 parallel、 sequential、 unordered Terminal：forEach、 forEachOrdered、 toArray、 reduce、 collect、 min、 max、 count、 anyMatch、 allMatch、 noneMatch、 findFirst、 findAny、 iterator Short-circuiting：anyMatch、 allMatch、 noneMatch、 findFirst、 findAny、 limit 具体的例子在下一节展示。 流的使用简单说，对 Stream 的使用就是实现一个 filter-map-reduce 过程，产生一个最终结果，或者导致一个副作用（side effect）。需要注意的是，对于基本数值型，目前有三种对应的包装类型 Stream：IntStream、LongStream、DoubleStream。当然我们也可以用 Stream&lt;Integer&gt;、Stream&lt;Long&gt;、Stream&lt;Double&gt;，但是 boxing 和 unboxing 会很耗时，所以特别为这三种基本数值型提供了对应的 Stream。Java 8 中还没有提供其它数值型 Stream，因为这将导致扩增的内容较多。而常规的数值型聚合运算可以通过上面三种 Stream 进行。流的构造和转换的例子： 123456789101112131415// 数值流的构造IntStream.of(new int[]&#123;1, 2, 3&#125;).forEach(System.out::println);IntStream.range(1, 3).forEach(System.out::println);IntStream.rangeClosed(1, 3).forEach(System.out::println);// 流的转换// 1. ArrayString[] strArray1 = stream.toArray(String[]::new);// 2. CollectionList&lt;String&gt; list1 = stream.collect(Collectors.toList());List&lt;String&gt; list2 = stream.collect(Collectors.toCollection(ArrayList::new));Set set1 = stream.collect(Collectors.toSet());Stack stack1 = stream.collect(Collectors.toCollection(Stack::new));// 3. StringString str = stream.collect(Collectors.joining()).toString(); 一个 Stream 只可以使用一次，上面的代码为了简洁而重复使用了数次。然后就来看一些比较经典的栗子吧： map/flatMapmap 操作应该是用的比较多的一种了，它的作用就是把 input Stream 的每一个元素，映射成 output Stream 的另外一个元素；也就是说是个 1:1 映射，每个输入元素，都按照规则转换成为另外一个元素。还有一些场景，是一对多映射关系的，这时需要 flatMap。 12345678910111213141516171819// 转换大写List&lt;String&gt; output = wordList.stream() .map(String::toUpperCase) .collect(Collectors.toList());// 平方根List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4);List&lt;Integer&gt; squareNums = nums.stream() .map(n -&gt; n * n) .collect(Collectors.toList());// 一对多Stream&lt;List&lt;Integer&gt;&gt; inputStream = Stream.of( Arrays.asList(1), Arrays.asList(2, 3), Arrays.asList(4, 5, 6));Stream&lt;Integer&gt; outputStream = inputStream .flatMap((childList) -&gt; childList.stream()); flatMap 把 input Stream 中的层级结构扁平化，就是将最底层元素抽出来放到一起，最终 output 的新 Stream 里面已经没有 List 了，都是直接的数字. filterfilter 对原始 Stream 进行某项测试，通过测试的元素被留下来生成一个新 Stream。 12345678910// 留下偶数Integer[] sixNums = &#123;1, 2, 3, 4, 5, 6&#125;;Integer[] evens = Stream.of(sixNums).filter(n -&gt; n%2 == 0).toArray(Integer[]::new);// 挑选单词List&lt;String&gt; output = reader.lines() .flatMap(line -&gt; Stream.of(line.split(REGEXP))) .filter(word -&gt; word.length() &gt; 0) .collect(Collectors.toList()); 这段代码首先把每行的单词用 flatMap 整理到新的 Stream，然后保留长度不为 0 的，就是整篇文章中的全部单词了。 forEach上面已经用烂了，不多说，关键是它的一些特点：forEach 是为 Lambda 而设计的，保持了最紧凑的风格。而且 Lambda 表达式本身是可以重用的，非常方便。当需要为多核系统优化时，可以 parallelStream().forEach()，只是此时原有元素的次序没法保证，并行的情况下将改变串行时操作的行为，此时 forEach 本身的实现不需要调整，而 Java8 以前的 for 循环 code 可能需要加入额外的多线程逻辑。但一般认为，forEach 和常规 for 循环的差异不涉及到性能，它们仅仅是函数式风格与传统 Java 风格的差别。 另外一点需要注意，forEach 是 terminal 操作，因此它执行后，Stream 的元素就被“消费”掉了，你无法对一个 Stream 进行两次 terminal 运算；具有相似功能的 intermediate 操作 peek 可以解决这个问题： 123456Stream.of("one", "two", "three", "four") .filter(e -&gt; e.length() &gt; 3) .peek(e -&gt; System.out.println("Filtered value: " + e)) .map(String::toUpperCase) .peek(e -&gt; System.out.println("Mapped value: " + e)) .collect(Collectors.toList()); peek 对每个元素执行操作并返回一个新的 Stream；forEach 不能修改自己包含的本地变量值，也不能用 break/return 之类的关键字提前结束循环。 reduce这个方法的主要作用是把 Stream 元素组合起来。它提供一个起始值（种子），然后依照运算规则（BinaryOperator），和前面 Stream 的第一个、第二个、第 n 个元素组合。从这个意义上说，字符串拼接、数值的 sum、min、max、average 都是特殊的 reduce。 12345678910111213141516// 字符串连接，concat = "ABCD"String concat = Stream.of("A", "B", "C", "D").reduce("", String::concat);// 求最小值，minValue = -3.0double minValue = Stream.of(-1.5, 1.0, -3.0, -2.0).reduce(Double.MAX_VALUE, Double::min);// 求和，sumValue = 10, 有起始值int sumValue = Stream.of(1, 2, 3, 4).reduce(0, Integer::sum);// 求和，sumValue = 10, 无起始值sumValue = Stream.of(1, 2, 3, 4).reduce(Integer::sum).get();// 过滤，字符串连接，concat = "ace"concat = Stream.of("a", "B", "c", "D", "e", "F") .filter(x -&gt; x.compareTo("Z") &gt; 0) .reduce("", String::concat); 上面代码例如第一个示例的 reduce()，第一个参数（空白字符）即为起始值，第二个参数（String::concat）为BinaryOperator。这类有起始值的 reduce() 都返回具体的对象。而对于第四个示例没有起始值的 reduce()，由于可能没有足够的元素，返回的是 Optional，请留意这个区别。 Optional 这也是一个模仿 Scala 语言中的概念，作为一个容器，它可能含有某值，或者不包含。使用它的目的是尽可能避免 NullPointerException。使用 Optional 代码的可读性更好，而且它提供的是编译时检查，能极大的降低 NPE 这种 Runtime Exception 对程序的影响，或者迫使程序员更早的在编码阶段处理空值问题，而不是留到运行时再发现和调试。Stream 中的 findAny、max/min、reduce 等方法等返回 Optional 值。还有例如 IntStream.average() 返回 OptionalDouble 等等。后面会详细说这一特性。 limit/skiplimit 返回 Stream 的前面 n 个元素；skip 则是扔掉前 n 个元素（它是由一个叫 subStream 的方法改名而来）。 12345678910111213141516public void testLimitAndSkip() &#123; List&lt;Person&gt; persons = new ArrayList(); for (int i = 1; i &lt;= 20; i++) &#123; Person person = new Person(i, "name" + i); persons.add(person); &#125; List&lt;String&gt; personList2 = persons.stream() .map(Person::getName).limit(10).skip(3).collect(Collectors.toList()); // limit 和 skip 对 sorted 后的运行次数无影响 List&lt;Person&gt; personList3 = persons.stream() .sorted((p1, p2) -&gt; p1.getName().compareTo(p2.getName())) .limit(2) .collect(Collectors.toList());&#125; limit(10) 就是取前十条，skip(3) 就是跳过前三条咯，果然是更加灵活了。有一种情况是 limit/skip 无法达到 short-circuiting 目的的，就是把它们放在 Stream 的排序操作后，原因跟 sorted 这个 intermediate 操作有关：此时系统并不知道 Stream 排序后的次序如何，所以 sorted 中的操作看上去就像完全没有被 limit 或者 skip 一样。在后面的一个例子中，即虽然最后的返回元素数量是 2，但整个管道中的 sorted 表达式执行次数没有像前面例子相应减少。最后有一点需要注意的是，对一个 parallel 的 Steam 管道来说，如果其元素是有序的，那么 limit 操作的成本会比较大，因为它的返回对象必须是前 n 个也有一样次序的元素。取而代之的策略是取消元素间的次序，或者不要用 parallel Stream（parallel 用于多核并发操作）。 MatchStream 有三个 match 方法，从语义上说： allMatch：Stream 中全部元素符合传入的 predicate，返回 true anyMatch：Stream 中只要有一个元素符合传入的 predicate，返回 true noneMatch：Stream 中没有一个元素符合传入的 predicate，返回 true 它们都不是要遍历全部元素才能返回结果。例如 allMatch 只要一个元素不满足条件，就 skip 剩下的所有元素，返回 false。 1234567891011121314List&lt;Person&gt; persons = new ArrayList();persons.add(new Person(1, "name" + 1, 10));persons.add(new Person(2, "name" + 2, 21));persons.add(new Person(3, "name" + 3, 34));persons.add(new Person(4, "name" + 4, 6));persons.add(new Person(5, "name" + 5, 55));boolean isAllAdult = persons.stream() .allMatch(p -&gt; p.getAge() &gt; 18);System.out.println("All are adult? " + isAllAdult);boolean isThereAnyChild = persons.stream() .anyMatch(p -&gt; p.getAge() &lt; 12);System.out.println("Any child? " + isThereAnyChild); 确实挺方便的，如果你记得的话…. 其他其他的像 sorted/min/max/distinct 这些方法就不多说了，想看的可以去 IBM 的原文找下，在参考里；至于它们的优势，比如排序，它比数组的排序更强之处在于你可以首先对 Stream 进行各类 map、filter、limit、skip 甚至 distinct 来减少元素数量后，再排序，这能帮助程序明显缩短执行时间。 总结&amp;归纳 不是数据结构 它没有内部存储，它只是用操作管道从 source（数据结构、数组、generator function、IO channel）抓取数据。 它也绝不修改自己所封装的底层数据结构的数据。例如 Stream 的 filter 操作会产生一个不包含被过滤元素的新 Stream，而不是从 source 删除那些元素。 所有 Stream 的操作必须以 lambda 表达式为参数 不支持索引访问 你可以请求第一个元素，但无法请求第二个，第三个，或最后一个。不过请参阅下一项。 很容易生成数组或者 List 惰性化 很多 Stream 操作是向后延迟的，一直到它弄清楚了最后需要多少数据才会开始。 Intermediate 操作永远是惰性化的。 并行能力 当一个 Stream 是并行化的，就不需要再写多线程代码，所有对它的操作会自动并行进行的。 可以是无限的集合有固定大小，Stream 则不必。limit(n) 和 findFirst() 这类的 short-circuiting 操作可以对无限的 Stream 进行运算并很快完成。 参考https://www.ibm.com/developerworks/cn/java/j-lo-java8streamapi/https://segmentfault.com/a/1190000009186509http://www.cnblogs.com/IcanFixIt/p/6744973.htmlhttp://blog.csdn.net/renfufei/article/details/24600507深入探讨Lambda]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker化你的应用]]></title>
    <url>%2F2018%2F01%2F06%2FDocker%E5%8C%96%E4%BD%A0%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Docker 是一个使用 Go 语言开发的开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的机器上。Docker 的发展速度和火爆程度着实令人惊叹，一发不可收拾，形成了席卷整个IT界的新浪潮。记得在公众号科普过 Docker 的一些基本概念，简单可以理解为集装箱，可以把你的程序、环境、配置等等全部装进去，这样在其他机器上达到开箱即用，也就是解决了环境不一致的问题。还有就是每一个 Docker 都是相对隔离的，避免了资源使用上的一些问题。Docker 的思想：标准化、集装箱、隔离，核心有镜像、仓库、容器等概念Docker 是容器化技术的一个代表，这项技术并不是很新，在内核中很早之前就已经存在了，不过确实是因为 Docker 才火起来的，在云计算的领域，Docker 可谓是更加的火热，让程序猿和运维的关系更好了Docker 你可以粗糙的理解为轻量级的虚拟机 走进Docker下面来说说核心的三个词：镜像、仓库、容器；仓库就相当于码头，镜像就是集装箱，容器就是运行程序的地方，所以一般的使用步骤就是从仓库（码头）拉取镜像到本地，然后用命令把镜像运行起来。相关的命令：Build（构建镜像）、ship（运输镜像）、run。Docker 仓库的地址：hub.docker.com，如果速度慢，可以尝试国内的 c.163yun.com ，或者 daocloud 也不错。如果我们的镜像比较私密，不想让别人知道，那么可以自己搭一个镜像仓库，就像 Maven 仓库哪样PS：安装 Docker 推荐是在 ubuntu 上，因为本身就是在 ubuntu 上进行开发的，所以支持应该是最好的。 基本使用下面介绍几个常用的命令： 从仓库拉取镜像：docker pull name 查看本机的镜像：docker images 运行镜像：docker run name如果需要在后台运行可以使用 -d 参数还可以加 -e 指定用到的环境变量（用 MySQL 的时候可能会用到），指定多个环境变量可使用多次 -e；一般情况，都是加个自定义的名字，这样：docker run --name myName -d mysql:latest或者运行基础镜像：docker run --name mps -dit ubuntu 查看本机正在运行的容器：docker ps另外，它可以加个 -a 参数来查看所有 进入容器：docker exec -it [id] bashid 由 run 命令（后台启动）返回，或者使用 ps 来查看，并且不需要输入完整，前几位就可以，与 git 比较类似。然后就进入了这个容器，并且是以 bash 这个 shell 的方式，在容器中和 Linux 的操作基本一致，如同一个小型的 Linux 系统，并且是根据需求配置好的。使用 exit 命令即可退出容器 停止容器：docker stop id ，主要是对于那些后台启动的容器来说。这里的停止并不是删除，区分下镜像和容器的关系。所以，也可以使用 docker start id 来启动容器。 重启容器：docker restart id 删除镜像：docker rmi id注意和 rm 区分，一个是删除镜像，一个是删除容器。 清除运行过的镜像记录（缓存）：docker rm id ，可以使用 docker ps -a 来查看记录 如果对命令参数不熟悉，可以查看帮助，如：docker run --help ；可以看出除了 pull 和 run 大部分命令都是依赖于镜像的 id 的；一个镜像可以启动（run）多个容器，一个容器可以理解为一个虚拟环境。 如果我们需要将本机的某个文件放到容器里，有个快捷的命令：docker cp xxx.html [id]://usr/share/nginx/htmlid 自然指的就是相应的容器了，不过你得熟悉你容器的文件分布情况才行，这种改动操作是临时的，当容器停止后改动不会被保存如果需要永久保存，需要执行 commit 操作：docker commit -m &quot;test&quot; [id] name这样就相当于是保存修改了，和 Git 是不是很像？它实际会根据改动生成一个新的镜像，所以要在最后指定新镜像的名字（版本也可以） 使用 docker tag xxx newName 可以实现镜像的复制….想要 push 自己的镜像除了必要的账号，需要先进行登陆 docker login ，然后 docker push name 就可以了 Docker中的网络Docker 会虚拟出一个运行环境，这个环境当然包括网络、文件等，也是通过 namespace 来进行区分，对于网络，虚拟有三种方式： Bridge也就是我们所说的桥接，会虚拟出独立的一套网络配置（网卡），有独立的 ip、端口、iptab 规则等这也是启动 Docker 的默认模式 Host使用物理机的网卡，和主机共用一套网络配置 None不配置网络，也就是容器内的程序不会与外界发生通讯 虚拟的方式是不是和配置虚拟机差不多呢~在配置 Bridge 模式时，通常我们会配置端口映射，简单说就是当我们访问主机的某一个端口时，实际访问的是容器里的某一个端口，这个过程通过 Docker 提供的一个网桥实现（处理请求转发）映射的配置可以在启动时就指定：docker run -d -p 8080:80 name-p 后的第一个是本机的端口，第二个是对应的容器里的端口，上面的命令就实现了把本机 8080 的请求转发到容器的 80 端口上。不放心的话可以使用 netstat -na|grep 8080 看看是不是处于监听的状态了。 另外一种就是使用 -P 的方式，是大写的 P，这种不需要再指定对应的端口，它会在本机开一些随机端口然后映射到容器里对应应用的端口上。 制作镜像重头戏来了，如何把自己开发的程序打包成一个 Docker 应用呢，就像仓库里的那些一样。用到的是 Dockerfile 和 build 命令，Dockerfile 可以理解为描述了打包流程，然后使用 build 会根据这个流程进行打包。 编写Dockerfile是的，Dockerfile 只是一个文本文件，内容才是最重要的，可以直接使用 vim 来编写，名字就叫 Dockerfile，主要包括： 1234567891011121314# 1.设置基础镜像（在某个镜像的基础上）from tomcat# alpine 是针对 Docker 做的一个极小的 linux 环境，也常用做基础镜像# from alpine:latest# 2.作者信息,也可以不写MAINTAINER Kerronex bfchengnuo@gmail.com# 官方现在推荐使用 LABEL maintainer="yourname &lt;xxx@xxx.com&gt;"# 3.将 war 包放进容器里COPY xxx.war /usr/local/tomcat/webapps# 拓展其他CMD echo "Hello World!" 上面是个简单的发布 Java 程序的环境，tomcat 的目录在那是从官网 tomcat 镜像说明页面找到的。然后下面执行 docker build -t appName:latest . 来进行打包就可以了；-t 的作用是可以指定名字和版本，最后一个 . 表示当前目录。然后使用 docker images 命令就可以找到我们自己发布的本地镜像了，运行和其他的一样 镜像分层Docker 是分层存储的，在 Dockerfile 中一行就是一层，每一层都有它唯一的 ID。这些层在 image (镜像)状态都是只读的（RO），当 image 运行为一个容器时，会产生一个容器层 (container layer)，它是可读可写的（RW）。分层的一个好处就是会减轻不少存储压力，多个 image 难免会有许多的相同的命令，分层后就可以实现共用。 存储再来看看 Volume 吧，它提供独立于容器之外的持久化存储。它就可以解决我们在容器内修改不会保存的问题，并且它可以提供容器与容器之间的共享数据 映射容器里的目录到本机在运行时执行命令：docker run -d --name nginx -v /usr/share/nginx/html nginx命令里的路径是容器里的地址，还给它起了个名字，运行后就会将配置的目录映射到本机（Host）的一个目录下，想要确定这个目录可以使用下面的命令查看容器信息：docker inspect nginx后面跟的是我们起的那个名字哦，不是镜像名，然后重点看 Mounts 下的 Source 和 Destination 是不是正确，就是把容器内的 Destination 地址映射到了本机的 Source 目录下。在本机的 Source 下修改会同步到容器里的 Destination 目录。 映射主机里的目录到容器与上面正好相反，把本机的一个目录映射到容器里，命令：docker run -d -v $PWD/html:/usr/share/nginx/html nginx这条命令就是把当前目录下的 html 文件夹映射到容器里 nginx 的目录下，这种方式用的比较多，非常的方便 建立只存数据的容器这种情况或者说需求也是比较常见的，使用的命令是：docker create -v $PWD/data:/var/mydata --name data_container ubuntu这样我们就创建了一个仅仅用来存储数据的新容器 data_container ，它会把当前目录下的 data 文件夹挂载到与之关联的容器的 /var/mydata 目录下，然后我们运行一个新的容器来测试下：docker run --volumes-from data_container ubuntuvolumes-from 的作用就是从另一个容器挂载，这样就实现了当前容器挂载仅有数据容器的目的，主要想体现的是这种仅有数据的容器可以被多个容器挂载使用，用来做数据的共享。 多容器APP玩这个之前需要一个软件就是 docker-compose，在 Mac/Windows 下是自带的。然后需要配置 docker-compose.yml 文件，文件名是固定的，使用的是 yaml 语法，也就是用缩进来表示层次关系，非常流行的一种配置文件格式。 Dockerfile 可以让用户管理一个单独的应用容器；而 Compose 则允许用户在一个模板（YAML 格式）中定义一组相关联的应用容器（被称为一个 project，即项目），例如一个 Web 服务容器再加上后端的数据库服务容器等 最后写好 docker-compose 的配置文件后，就可以使用 docker-compose up -d 来运行了，停止就是 stop，rm 是删除；当重新修改配置文件后需要用 docker-compose build 重新来构建。PS：在 docker-compose 配置的名字可以在其他的容器里直接用（比如 nginx 的配置文件里），不需要再配置解析。关于 docker-compose 的使用这里不会多说，具体操作还是蛮复杂的，计划是等用到后再来补充，先暂且知道有这么个多容器的概念，可参考慕课网视频：https://www.imooc.com/video/15735 其他有时我们需要合并两个镜像，但是这个功能并没有被提供，在 Dockerfile 中也不能使用多次 from，那么有什么好方法呢，可以使用命令先把镜像逆向出来，然后手动合并，镜像 –&gt; Dockerfile 的命令：docker history --no-trunc=true image &gt; image1-dockerfile但是如果命令中有 ADD、COPY 之类的命令就会有一些问题，因为这些文件并不在你的机器上，需要自行处理了。 附:Dockerfile示例一般是创建一个空目录，然后在这个空目录写 Dockerfile 文件，名字就叫 Dockerfile，这样打包的时候可以放心的用 . ： 123456789101112131415from ubuntuLABEL maintainer="Kerronex &lt;bfchengnuo@gmail.com&gt;"# 执行命令# 替换镜像地址，加速下载，把 archive.ubuntu.com 换成 mirrros.ustc.edu.cnRUN sed -i 's/archive.ubuntu.com/mirrros.ustc.edu.cn/g' /etc/apt/sources.listRUN apt-get updateRUN apt-get install -y nginxCOPY index.html /var/www/html# 设置容器的入口，会将数组展开来运行# 命令的意思是将 nginx 作为前台程序执行，而不是守护进程ENTRYPOINT ["/usr/sbin/nginx","-g","daemon off;"]# 暴露端口EXPOSE 80 然后可以使用 curl http://localhost:80 来测试一下。文件中没有用的其他关键字：ADD 也是添加文件，它比 COPY 更加强大，可以添加远程文件；CMD 也是执行命令的意思，可以用于指定容器的入口，也可以使用 ENTRYPOINT 命令 命令 用途 WORKDIR 指定路径 MAINTAINER 维护者，现在推荐使用 LABEL maintainer ENV 设定环境变量 ENTRYPOINT 容器入口 USER 指定用户 VOLUME mount point（容器所挂载的卷） 当没有指定 ENTRYPOINT 的时候，就用 CMD 来启动容器，如果指定了 ENTRYPOINT 那么 CMD 指定的字串就变成了 argus （参数） 关于DockerCompose使用一个 Dockerfile 模板文件可以定义一个单独的应用容器，如果需要定义多个容器就需要服务编排。服务编排有很多种技术方案，Docker 官方产品是 Docker Compose 。Dockerfile 可以让用户管理一个单独的应用容器；而 Compose 则允许用户在一个模板（YAML 格式）中定义一组相关联的应用容器（被称为一个 project，即项目），例如一个 Web 服务容器再加上后端的数据库服务容器等。 通过 Docker-Compose 用户可以很容易地用一个配置文件定义一个多容器的应用，然后使用一条指令安装这个应用的所有依赖，完成构建。Docker-Compose 解决了容器与容器之间如何管理编排的问题。 Compose 中有两个重要的概念： 服务 (service) ：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project) ：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 一个项目可以由多个服务（容器）关联而成，Compose 面向项目进行管理，通过子命令对项目中的一组容器进行便捷地生命周期管理。Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 安装：https://docs.docker.com/compose/install/ 但是现在基本都是 K8S 的天下了，并且 K8S 支持将其转换为自己的配置文件。 拓展https://yeasy.gitbooks.io/docker_practice/content/image/build.html]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java定时任务调度工具]]></title>
    <url>%2F2018%2F01%2F03%2FJava%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[什么是定时任务调度呢？ 基于给定的时间点，给定的时间间隔或者给定的执行次数自动执行的任务 Java 中最常见的两款定时任务调度工具就是 Timer 和 Quartz，一般来说 Timer 能解决 60% 的需求，解决不了的就交给大哥 Quartz 了，Timer 是 JDK 自带的，不需要其他依赖，而 Quartz 是开源软件。需要注意的是：能用 Timer 实现的就用 Timer，因为大哥的出场费是很贵的…. Timer简单定义：有且只有一个后台线程对多个业务线程进行定时定频率的调度。构件关系：Timer （包含有一个队列和一个后台线程）定时调用 TimerTask 我记得在上篇文章中我是写过关于 Timer 的，具体内容可以点击这里回看，这里就简单复习或者补充下： 123456789101112131415161718192021222324252627public class Main &#123; public static void main(String[] args) &#123; Timer timer = new Timer(); timer.schedule(new MyTask(), 0, 1000); // timer.scheduleAtFixedRate(new MyTask(), 0, 1000); // 取消所有任务 timer.cancel(); // 返回取消任务的个数 int s = timer.purge(); &#125; public class MyTask extends TimerTask &#123; @Override public void run() &#123; System.out.println("task run!!"); // 获取最近一次任务执行的时间 System.out.println(new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(scheduledExecutionTime())); // 取消任务 // cancel(); &#125; &#125;&#125; 这里还是要再次说明：使用 Timer 执行周期性任务时，出现异常后会自动退出（全部任务）,因为它是基于单线程的。所以应该尽量使用 ScheduledExecutorService （支持周期任务的线程池）的方式来创建。 12345678ScheduledExecutorService scheduledExecutorService = newScheduledThreadPool(3);scheduledExecutorService.scheduleAtFixedRate(new MyTask, 0, 1, TimeUnit.SECONDS);// 结束线程池正在执行的任务，不再接受新任务，等待当前任务完成scheduledExecutorService.shutdown();// 线程池里的任务是否全部完成scheduledExecutorService.isTerminated(); 通过上面，知道 Timer 主要的方法有两个 schedule 和 scheduleAtFixedRate；然后来说说他们最大的区别： 首次执行时间早于当前时间也就是说规定 12:00 执行 task，但是 12:05 的时候才执行到 schedule / scheduleAtFixedRate；对于这种情况，schedule 会以当前时间为准，然后间隔指定时间重复执行；对于 scheduleAtFixedRate 它会尽可能的多执行几次以赶上落下的任务，比如说规定没 2 分钟执行一次，那么它会在执行 scheduleAtFixedRate 后连续执行两次 task 来弥补缺失的“工作量” 执行耗时超过了间隔时间也就是说规定每隔 3 秒执行一次，但是 task 3 秒还没执行完的情况；对于 scheduleAtFixedRate，当 task 超时后，第二次会很快被执行，它的间隔计算方式是程序开始执行的时间；对于 schedule ，它的间隔计算方式是程序执行完后计时的，也就是说规定每隔 3 秒执行一次，task 耗时 5 秒，task 执行完后再等 3 秒才会执行第二次，从运行开始算的话也就是差了 8 秒 他们两个方法的主要区别就集中在这两种情况上了，缺点也可以看出来了，因为是单线程所以在处理并发时效果会非常的不理想，基本不会做到并发执行；还有就是抛出异常所有的任务都会终止了。所以在对时效性要求较高的多任务并发作业和对复杂任务的调度（可能抛出异常）Timer 是不适合的 Quartz然后就到了介绍大哥的时候了，它的强大就不用说了，你想实现的它基本都能搞定，听说也是作为 Spring 默认的调度框架，并且它的分布式和集群能力也不错。Quartz 的核心有三个：调度器、任务、触发器；可以对应为 Scheduler、Job、Trigger。简单的流程就是：调度器根据触发器来执行任务 12345678910111213141516171819202122232425262728293031323334353637383940414243public class HelloScheduler &#123; public static void main(String[] args) throws SchedulerException &#123; // 1.创建 JobDetail 实例，和 job 类进行绑定 JobDetail jobDetail = JobBuilder.newJob(HelloJob.class) .withIdentity("myJob", "group1") // 传入自定义参数 .usingJobData("msg","Loli") .build(); // 2.创建一个 trigger 实例，来控制执行的规则 Trigger trigger = TriggerBuilder.newTrigger() // 这里的组和上面的完全不一样，虽然名字一样，但不是在一个类 .withIdentity("myTrigger", "group1") // 传入自定义参数 .usingJobData("age", 12) // 立即执行 .startNow() // 每隔两秒执行一次，直到永远，使用 SimpleSchedule // .withSchedule(SimpleScheduleBuilder.simpleSchedule() // .withIntervalInSeconds(2) // .repeatForever()) // 使用 CronTrigger 和 Cron 表达式 .withSchedule(CronScheduleBuilder.cronSchedule("* * * * * ? *")) .build(); // 3.创建 Scheduler 实例 SchedulerFactory schedulerFactory = new StdSchedulerFactory(); Scheduler scheduler = schedulerFactory.getScheduler(); scheduler.start(); // 按照 trigger 指定的日期执行 jobDetail // 它返回最近一次要执行的时间 scheduler.scheduleJob(jobDetail, trigger); // 挂起，可以被重启 scheduler.standby(); // 终止，不能被重启，如果传入 true 表示会等待任务结束后才(标记为)终止，默认 false scheduler.shutdown(); &#125;&#125; 上面代码结合下面的解释看最好啦，这也仅仅只是一个简单的小例子 Job实现业务逻辑的任务接口，没错它是个接口，非常容易实现，只有一个 execute 方法，相当于 TimerTask 的 run 方法；在里面编写逻辑即可，当任务执行失败时会抛出 JobExecutionException 异常。 12345678910111213141516171819202122public class HelloJob implements Job &#123; public void execute(JobExecutionContext jobExecutionContext) throws JobExecutionException &#123; System.out.println(MessageFormat.format("当前时间：&#123;0&#125;", new Date())); // System.out.println("Hello World!"); // jobExecutionContext.getJobDetail().getKey().getName(); // jobExecutionContext.getJobDetail().getKey().getGroup(); // jobExecutionContext.getTrigger().getKey().getName(); // jobExecutionContext.getTrigger().getKey().getGroup(); // 获取自定义的参数 String msg = jobExecutionContext.getJobDetail().getJobDataMap().getString("msg"); Integer age = jobExecutionContext.getTrigger().getJobDataMap().getInt("age"); System.out.println(msg + "::" + age); // 合并后的大 Map,如果有相同的 key 以 Trigger 为准 jobExecutionContext.getMergedJobDataMap(); &#125;&#125; 这是一个简单的 Job，只是展示部分常用的，结合下面的说明阅读比较好~ 生命周期每次调度器（Scheduler）调用执行 job 的 execute 方法前会会创建一个新的 job 实例，当调用完成后，关联的 job 实例会被释放，释放的实例就会被 GC 回收 JobExecutionContext当调度器调用一个 Job ，就会将 JobExecutionContext 传递给 job 的 execute 方法；这样 Job 就可以通过 JobExecutionContext 来访问 Quartz 运行时候的环境以及 Job 本身的明细数据。从另一方面 JobExecutionContext 就是为了解决不同 Job 需要不同参数的问题。通过 JobExecutionContext 对象，可以获得 JobDetail 或者 Trigger 设置的自定义参数。 JobDataMap在进行任务调度时，JobDataMap 存储在 JobExecutionContext 中，非常方便获取。JobDataMap 可以用来装载任何可序列化的数据对象，当 job 实例执行时，这些参数对象会传递给它。JobDataMap 实现了 JDK 的 Map 接口，并添加了一些非常方便的方法用来存取基本数据类型。 关于获取，除了使用 JobExecutionContext ，还有另一种方式，那就是在 Job 的实现类里添加相应的成员变量，并且设置 setter 方法，默认的 JobFactory 实现类在初始化 Job 实例的时候会自动调用这些 setter 方法，把 JobDataMap 中相应的值放进去 JobDetailJobDetail 为 job 实例提供了许多设置属性，以及 JobDataMap 成员变量属性，它用来存储特定的 job 实例的状态信息，调度器需要借助 JobDetail 对象来添加 Job 实例。简单说就是 JobDetail 是用来绑定 Job 实例的，并携带一些 Job 实例没有携带的状态信息。下面看看它的一些重要属性：name 和 group ，这两个是必须的，group 的默认值是 DEFAULT；还有 jobClass 也是必须的，就是绑定的 Job 类；此外还有一个 JobDataMap ，可简单理解为是来传递数据的。查看可以使用：jd.getKey().getName()/getGroup(); TriggerQuartz 中的触发器用来告诉调度程序作业什么时候触发。即 Trigger 对象是用来触发执行 Job 的。触发器的通用属性： JobKey表示 Job 实例的标识，触发器被触发时，该指定的 Job 实例会执行 StartTime触发器的时间表首次被触发的时间，类型是 Date，startAt() EndTime指定触发器的不再被触发的时间，类型也是 Date，endAt()，它的优先级比设置的重复执行次数高 SimpleTrigger首先来看它的作用：在一个指定时间段内执行一次作业任务，或者是在指定的时间间隔内多次执行作业任务。我们通过 TriggerBuilder.newTrigger() 代码实际上就是生成了一个 SimpleTrigger CronTrigger它的作用：基于日历的作业调度器，它不像 SimpleTrigger 那样精确的控制，但是更加常用。说到 CronTrigger 就必须得说 Cron 表达式了，熟悉 Linux 的可能接触过，因为 Linux 有一个 crontab 命令来控制计划任务，也是 Cron 表达式。简单说，Cron 表达式是由 7 个子表达式组成的字符串，描述了时间表的详细信息，格式：[秒] [分] [小时] [日] [月] [周] [年]* ：表示“每”的意思，放在第一个位置就是每秒。? ：表示不确定，不指定值，就是不关心。, ：表示或的意思。- ：表示至的意思，就是范围啦。/ ：表示的也是每的意思，举个例子，如果出现在秒的位置 0/5 就是从 0 秒开始，每隔 5 秒钟。# ：表示第，例如在周的位置有 5#3 意思就是第三周的星期四 。L ：意思是 List，在周位置 3L 就表示最后一周的周二。需要注意的是，时分秒范围是从 0 开始，日月周就是 1 开始了，年可以省略 在西方，星期天是一个星期的第一天。所以星期天是 1 ，以此类推 掌握了表达式其他的就不是问题了： 月周可以使用英文单词的前三个字母，有在线生成 Cron 表达式的工具哦 Scheduler调度器 Scheduler 可以说是 Quartz 的发动机，它是通过工厂模式来创建的，常用的两个是 StdSchedulerFactory 和 DirectSchedulerFactory。常用的还是 StdSchedulerFactory 因为它允许使用声明式的配置，也就是可以配置到 XML/Properties 文件中，而 DirectSchedulerFactory 生成的只允许在代码中配置。StdSchedulerFactory 默认会加载工程目录下的 quartz.properties ，如果不存在就会去读取自带的配置文件（jar 中），在配置文件中可以配置调度器属性、线程池属性（如线程数）、作业存储位置、插件配置等，可参考 jar 包里的配置。 Quartz与Spring整合Quartz 与 Spring 能够进行完美的整合，用的也是比较多，下面就赶紧来学习一下，首先，必要的一些依赖： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.2.3&lt;/version&gt;&lt;/dependency&gt; 然后是在 Spring 的配置文件中配置必要的 Bean 了，没办法，总要写配置文件的，或者使用 SpringBoot ？在 Spring 中配置使用 Quartz 有两种方式：MethodInvokingJobDetailFactoryBean 和 JobDetailFactoryBean，它们主要是来确定要执行的任务的，也就是 Job MethodInvokingJobDetailFactoryBean使用这种方式，只需要写一个普通的 Bean 即可 123456&lt;!-- 相当于设置执行的 Job 为 myBean 的 printMessage 方法 --&gt;&lt;bean id="simpleJobDetail" class="org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean"&gt; &lt;property name="targetObject" ref="myBean" /&gt; &lt;property name="targetMethod" value="printMessage" /&gt;&lt;/bean&gt; 对应执行的 Bean： 12345678@Component("myBean")public class MyBean &#123; public void printMessage() &#123; Date date = new Date(); SimpleDateFormat sf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); System.out.println("MyBean Executes!" + sf.format(date)); &#125;&#125; 这是一种相对简单的方法，然后都猜得出简单的方式定制性不太高，如果逻辑太复杂可能就无能为力了 JobDetailFactoryBean当需要给作业传递数据，想要更加灵活的话就使用这种方式，配置文件： 1234567891011&lt;bean id="firstComplexJobDetail" class="org.springframework.scheduling.quartz.JobDetailFactoryBean"&gt; &lt;property name="jobClass" value="com.imooc.springquartz.quartz.FirstScheduledJob" /&gt; &lt;property name="jobDataMap"&gt; &lt;map&gt; &lt;entry key="anotherBean" value-ref="anotherBean" /&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name="Durability" value="true"/&gt; &lt;/bean&gt; 因为 firstComplexJobDetail 是通过代码的方式创建的并不是 Spring 容器注入的，启动时可能会报错，提示没有绑定触发器，这里使用的是 Spring 来配置的 trigger，它扫描不到，可以把 Durability 属性设置为 true 表示即使没有绑定触发器也会将其保存在任务容器中。通过 jobClass 来指定 Job 类，对这个类的要求就是要继承 QuartzJobBean，实现 executeInternal 方法，这个还是比较熟悉的，和最开始我们学习时写法类似： 123456789101112131415161718192021222324public class FirstScheduledJob extends QuartzJobBean&#123; private AnotherBean anotherBean; public void setAnotherBean(AnotherBean anotherBean)&#123; this.anotherBean = anotherBean; &#125; @Override protected void executeInternal(JobExecutionContext arg0) throws JobExecutionException &#123; Date date = new Date(); SimpleDateFormat sf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); System.out.println("FirstScheduledJob Executes!" + sf.format(date)); this.anotherBean.printAnotherMessage(); &#125;&#125;/* -----分割线------ */@Component("anotherBean")public class AnotherBean &#123; public void printAnotherMessage() &#123; System.out.println("AnotherMessage"); &#125;&#125; 这里加了点“难度”，设置自定参数时，我们可以设置对象，通过 value-ref 实现了自动注入，还是挺爽的。 配置Trigger和Scheduler有了任务 Job，Quartz 还差两大核心，下面就来搞一下，完善 Spring 的配置文件： 123456789101112131415161718192021222324252627&lt;!-- 距离当前时间1秒之后执行，之后每隔两秒钟执行一次 --&gt;&lt;bean id="mySimpleTrigger" class="org.springframework.scheduling.quartz.SimpleTriggerFactoryBean"&gt; &lt;property name="jobDetail" ref="simpleJobDetail"/&gt; &lt;property name="startDelay" value="1000"/&gt; &lt;property name="repeatInterval" value="2000"/&gt;&lt;/bean&gt;&lt;!-- 每隔5秒钟执行一次 --&gt;&lt;bean id="myCronTrigger" class="org.springframework.scheduling.quartz.CronTriggerFactoryBean"&gt; &lt;property name="jobDetail" ref="firstComplexJobDetail"/&gt; &lt;property name="cronExpression" value="0/5 * * ? * *"/&gt;&lt;/bean&gt;&lt;bean class="org.springframework.scheduling.quartz.SchedulerFactoryBean"&gt; &lt;property name="jobDetails"&gt; &lt;list&gt; &lt;ref bean="simpleJobDetail"/&gt; &lt;ref bean="firstComplexJobDetail"/&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="triggers"&gt; &lt;list&gt; &lt;ref bean="mySimpleTrigger"/&gt; &lt;ref bean="myCronTrigger"/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 定义 Trigger 的时候也完全活用了上面学习的两种方式，一个使用 SimpleTrigger ，一个使用 CronTrigger；这样当程序启动后就会执行配置的两个 Job 了，simpleJobDetail 使用方式一运行，firstComplexJobDetail 使用方式二运行，还是挺和谐的。上面的也仅仅都是 Quartz 的初级使用，高级的并发、异常处理、持久化等没有涉及，以后有机会再补充吧！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaSE二周目计划（二）]]></title>
    <url>%2F2017%2F12%2F21%2FJavaSE%E4%BA%8C%E5%91%A8%E7%9B%AE%E8%AE%A1%E5%88%92%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这次就不仅仅是复习了，大部分讲的是以前学习 JavaSE 的时候没有接触到的知识，并且很多情况下还是很有用的。这篇主要讲解 Java 中的队列和线程池（包括支持周期任务的线程池），这也算得上是 SE 中的精华部分吧，当然还有一些对于日期的操作补充，平时用的也挺多的，算是非常简单的作为开胃菜~~ 日期处理代码中经常接触到日期的操作，我们不喜欢它默认的格式，大多数情况是需要进行格式化的，下面就说下最简单的格式化方式： 12345678910111213141516SimpleDateFormat sf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");sf.format(new Date()); // 格式化当前日期// 将字符串格式化为 Date 类型new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").parse("2017-12-20 18:44:00");// 判断时间是否超过五分钟public static boolean isOverstepMinute(Date date)&#123; Calendar cal = Calendar.getInstance(); Calendar cal1 = Calendar.getInstance(); cal1.setTime(date); cal1.add(Calendar.MINUTE, +5); return cal1.compareTo(cal) &lt;= 0;&#125; 另外可以通过 date 对象获取到年月日等信息，但是很遗憾已经过时，所以就有了 Calendar， Calendar 对象用的也很多，可以看看 API；那么他什么用呢？我们现在已经能够格式化并创建一个日期对象了，但是我们如何才能设置和获取日期数据的特定部分呢，比如说小时，日，分钟? 我们又如何在日期的这些部分加上或者减去值呢? 答案是使用Calendar 类。 关于队列很遗憾我看的视频里并没有讲这个，但是这个却非常的终于，好在现在知道了。Java 中的队列 Queue 在 util 包下，它是个接口，它更倾向于是一种数据结构，也可以理解为集合吧，毕竟 Queue 是 Collection 的一个子接口，与 List、Set 同一级别。首先来认识下什么是队列： 队列是计算机中的一种数据结构，保存在其中的数据具有“先进先出（FIFO,First In First Out）”的特性。 简单易懂的介绍，它本来也不是什么难题；在 Java 中，队列分为 2 种形式，一种是单队列，一种是循环队列 ，循环队列就是为了解决数组无限延伸的情况，让它们闭合起来形成一个圈，这就不会出现角标越界问题了。通常，都是使用数组来实现队列，假定数组的长度为6，也就是队列的长度为6，如果不指定一般默认是 internet 的最大值。因为 LinkedList 实现了 Queue 接口，所以定义一个接口 new 时可以直接使用 LinkedList ，但它不是同步的！Java 中给了许多的队列实现，甚至有双端（读写）的、按优先级的，普通常用的就是阻塞和非阻塞的一些同步队列 关于阻塞队列阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。这样非阻塞也就明白了吧？阻塞队列提供了四种处理方法： 方法\处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e,time,unit) 移除方法 remove() poll() take() poll(time,unit) 检查方法 element() peek() 不可用 不可用 BlockingQueue接口阻塞队列，当队列为空是取数据阻塞，队列满，插入数据阻塞线程安全的(批量操作不是) 是否是有界队列需要看具体的实现常用的实现类有： ArrayBlockingQueue规定大小的 BlockingQueue,其构造函数必须带一个 int 参数来指明其大小.其所含的对象是以FIFO(先入先出)顺序排序的 LinkedBlockingQueue大小不定的 BlockingQueue，若其构造函数带一个规定大小的参数，生成的 BlockingQueue 有大小限制，若不带大小参数，所生成的 BlockingQueue 的大小由 Integer.MAX_VALUE 来决定.其所含的对象是以 FIFO (先入先出)顺序排序的是作为生产者消费者的首选 SynchronousQueue特殊的 BlockingQueue，对其的操作必须是放和取交替完成的 PriorityBlockingQueue类似于 LinkedBlockQueue，但其所含对象的排序不是 FIFO，而是依据对象的自然排序顺序或者是构造函数的 Comparator 决定的顺序 至于它是如何实现同步的，两个 ReentrantLock 读和写。 PriorityQueue类不是按照先进先出的顺序，是按照优先级（Comparator 定义或者默认顺序,数字、字典顺序）每次从队列中取出的是具有最高优先权的元素内部通过堆排序实现 transient Object[] queue; 每次新增删除的时候，调整堆 非阻塞队列非阻塞队列一般就直接实现自 Queue 了，特点就不说了，对比上面的阻塞队列就行了，下面说说常见的非阻塞队列：ConcurrentLinkedQueue虽然是非阻塞，但也是线程安全的，按照 FIFO 来进行排序，采用CAS操作，来保证元素的一致性 非阻塞算法通过使用低层次的并发原语，比如比较交换，取代了锁。原子变量类向用户提供了这些底层级原语，也能够当做“更佳的volatile变量”使用，同时提供了整数类和对象引用的原子化更新操作。关键字：CAS线程安全就是说多线程访问同一代码，不会产生不确定的结果 ConcurrentLinkedQueue 的 size() 是要遍历一遍集合的，所以尽量要避免用 size 而改用 isEmpty()，以免性能过慢。 队列的操作一般情况下，操作队列不推荐使用 add 和 remove ，因为如果队列为空它就会抛异常；常使用的是 offer 和 poll 来添加和取出元素，如果此队列为空，则返回 null，如果使用 peek 取出元素则不会移除此元素，对于阻塞的队列，可以使用 put 和 take 来插入和获取。带有 Deque 的一般是双端队列，不细说，我用的起码是非常少的关于遍历队列如果使用 foreach 的方式相当于仅仅是 peek，也就是不会移除元素，如果需要遍历队列并且是取出，那么可以搭配 where 来使用： 1234567891011public void run() &#123; // 遍历队列 Order order; while ((order = queue.poll()) != null)&#123; rechargeOrder(order); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 这个过程注意 size，如果一边放一边遍历的话是没有尽头的 线程池同样视频里是没有提到的，只是讲了多线程的一些使用和注意事项，对于线程池，提及的很少，也许是因为 JavaEE 中并不常用，都是交给 Web 应用服务器来维护。如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，这样频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁线程需要时间，和连接池是一个道理。Java 中的线程池，最核心的就是 ThreadPoolExecutor 了ThreadPoolExecutor 继承了 AbstractExecutorService 类，并提供了四个构造器，事实上，通过观察每个构造器的源码具体实现，发现前面三个构造器都是调用的第四个构造器进行的初始化工作。下面解释下一下构造器中各个参数的含义： corePoolSize：核心池的大小在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了 prestartAllCoreThreads() 或者 prestartCoreThread() 方法，从这2个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建 corePoolSize 个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到 corePoolSize 后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数这个参数也是一个非常重要的参数，它表示在线程池中最多能创建多少个线程；当核心池大小满了，等待队列也满了，就开始创建非核心线程，直到达到最大线程数。 keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于 corePoolSize 时，keepAliveTime 才会起作用，直到线程池中的线程数不大于 corePoolSize，即当线程池中的线程数大于 corePoolSize 时，如果一个线程空闲的时间达到 keepAliveTime，则会终止，直到线程池中的线程数不超过 corePoolSize。但是如果调用了 allowCoreThreadTimeOut(boolean) 方法，在线程池中的线程数不大于 corePoolSize 时， keepAliveTime 参数也会起作用，直到线程池中的线程数为 0； unit：参数 keepAliveTime 的时间单位有7种取值，比如天、时、分、秒、毫秒等 workQueue：一个阻塞队列，用来存储等待执行的任务这个参数的选择也很重要，会对线程池的运行过程产生重大影响，一般来说，这里的阻塞队列有以下几种选择：ArrayBlockingQueue;LinkedBlockingQueue;SynchronousQueue;ArrayBlockingQueue 和 PriorityBlockingQueue 使用较少，一般使用 LinkedBlockingQueue 和 Synchronous。线程池的排队策略与 BlockingQueue 有关。 threadFactory：线程工厂，主要用来创建线程； handler：表示当拒绝处理任务时的策略有以下四种取值：ThreadPoolExecutor.AbortPolicy ：丢弃任务并抛出 RejectedExecutionException 异常。ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 上面仅仅是对构造方法参数的一些介绍，相关的几个类或者接口就是 ThreadPoolExecutor、AbstractExecutorService、ExecutorService 和 Executor，名字越短越抽象，最后的 Executor 为顶级接口 定义的方法下面来了解下关于线程池中定义的几个方法： execute()方法实际上是 Executor 中声明的方法，在 ThreadPoolExecutor 进行了具体的实现，这个方法是 ThreadPoolExecutor 的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。 submit()方法在 ExecutorService 中声明的方法，在 AbstractExecutorService 就已经有了具体的实现，在 ThreadPoolExecutor 中并没有对其进行重写，这个方法也是用来向线程池提交任务的，但是它和 execute() 方法不同，它能够返回任务执行的结果，去看 submit() 方法的实现，会发现它实际上还是调用的 execute() 方法，只不过它利用了 Future 来获取任务执行结果 shutdown() 和 shutdownNow() 是用来关闭线程池的。 其他的方法还有 getQueue() 、getPoolSize() 、getActiveCount()、getCompletedTaskCount() 等获取与线程池相关属性的方法，详细介绍去看 API 吧 线程池的状态当创建线程池后，初始时，线程池处于 RUNNING 状态；如果调用了 shutdown() 方法，则线程池处于 SHUTDOWN 状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕；如果调用了 shutdownNow() 方法，则线程池处于 STOP 状态，此时线程池不能接受新的任务，并且会去尝试终止正在执行的任务；当线程池处于 SHUTDOWN 或 STOP 状态，并且所有工作线程已经销毁，任务缓存队列已经清空或执行结束后，线程池被设置为 TERMINATED 状态。 线程池的创建&amp;使用先来看一个简单使用的例子： 12345678910111213public class Test &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); for(int i=0;i&lt;15;i++)&#123; MyTask myTask = new MyTask(i); executor.execute(myTask); System.out.println("线程池中线程数目："+executor.getPoolSize()+"，队列中等待执行的任务数目："+ executor.getQueue().size()+"，已执行玩别的任务数目："+executor.getCompletedTaskCount()); &#125; executor.shutdown(); &#125;&#125; 在 java doc中，并不提倡我们直接使用 ThreadPoolExecutor，而是使用 Executors 类中提供的几个静态方法来创建线程池： Executors.newCachedThreadPool();创建一个缓冲池，缓冲池容量大小为 Integer.MAX_VALUE Executors.newSingleThreadExecutor();创建容量为 1 的缓冲池 Executors.newFixedThreadPool(int);创建固定容量大小的缓冲池 看一下他们三个的具体实现： 12345678910111213141516public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 从它们的具体实现来看，它们实际上也是调用了 ThreadPoolExecutor，只不过参数都已配置好了。newFixedThreadPool 创建的线程池 corePoolSize 和 maximumPoolSize 值是相等的，它使用的 LinkedBlockingQueue；newSingleThreadExecutor 将 corePoolSize 和 maximumPoolSize 都设置为 1，也使用的 LinkedBlockingQueue；newCachedThreadPool 将 corePoolSize 设置为 0，将 maximumPoolSize 设置为 Integer.MAX_VALUE，使用的 SynchronousQueue，也就是说来了任务就创建线程运行，当线程空闲超过60秒，就销毁线程。实际中，如果 Executors 提供的三个静态方法能满足要求，就尽量使用它提供的三个方法，因为自己去手动配置 ThreadPoolExecutor 的参数有点麻烦，要根据实际任务的类型和数量来进行配置。另外，如果 ThreadPoolExecutor 达不到要求，可以自己继承 ThreadPoolExecutor 类进行重写。 配置线程池一般需要根据任务的类型来配置线程池大小，当然也是仅供参考：如果是 CPU 密集型任务，就需要尽量压榨 CPU，参考值可以设为 NCPU+1如果是 IO 密集型任务，参考值可以设置为 2*NCPU当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。 定时任务一提到定时任务，首先想到的是使用 Timer，但是使用 Timer 执行周期性任务时，出现异常后自动退出（全部）,因为它是基于单线程的。所以应该尽量使用 ScheduledExecutorService （支持周期任务的线程池）的方式来创建。是的，这也是一个线程池，只不过它支持周期任务而已，看到这里对线程池应该也有所了解了，所以定时任务也就不难了 它继承的 ThreadPoolExecutor 那些就不说了，来看看它特有的几个方法： Schedule(Runnable command, long delay, TimeUnit unit)elay 指定的时间后，执行指定的 Runnable 任务，可以通过返回的 ScheduledFuture&lt;?&gt; 与该任务进行交互 schedule(Callable\ callable, long delay, TimeUnit unit)delay 指定的时间后，执行指定的 Callable&lt;V&gt; 任务，可以通过返回的 ScheduledFuture&lt;V&gt; 与该任务进行交互。 scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)initialDelay 指定的时间后，开始按周期 period 执行指定的 Runnable 任务。假设调用该方法后的时间点为 0，那么第一次执行任务的时间点为 initialDelay，第二次为 initialDelay + period，第三次为 initialDelay + period + period，以此类推。 scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit)initialDelay 指定的时间后，开始按指定的 delay 延期性的执行指定的 Runnable 任务。假设调用该方法后的时间点为 0，每次任务需要耗时 T(i)（i 为第几次执行任务），那么第一次执行任务的时间点为 initialDelay，第一次完成任务的时间点为 initialDelay + T(1)，则第二次执行任务的时间点为 initialDelay + T(1) + delay；第二次完成任务的时间点为 initialDelay + (T(1) + delay) + T(2)，所以第三次执行任务的时间点为 initialDelay + T(1) + delay + T(2) + delay，以此类推。 简单解释下 scheduleAtFixedRate 和 scheduleWithFixedDelay，前者会开始执行为起始点，如果任务耗时超过了间隔时间，那么在任务完成候第二次会很快（马上）执行，而后者会等待任务执行完后才开始计算周期间隔时间。创建线程池的方式也与上面差不多，都有对应的方法：Executors.newScheduledThreadPool(int corePoolSize)Executors.newSingleThreadScheduledExecutor() Apache 的 BasicThreadFactory 或许会更好….待进一步研究 补充ScheduledFuture 接口 继承自 Future 接口，所以 ScheduledFuture 和任务的交互方式与 Future 一致。所以通过ScheduledFuture，可以 判断定时任务是否已经完成，获得定时任务的返回值，或者取消任务等关于 Future 后面应该会再进行补充可以先看一下：这篇文章 单例模式这个很简单，没什么好说的，简单说就是构造函数的私有化，然后定义一个本类类型的静态变量，通过静态方法进行提供需要注意的是，静态变量的初始化时机，比较一致的观点是：如果你确定这个类肯定要用，那么可以在定义静态变量的时候就直接进行实例化，否则可以放在静态方法中进行实例化（这样会有线程安全问题）比如： 123456789101112private static Singleton is = new Singleton();public static Singleton getInstance()&#123; return is;&#125;// 或者是获取的时候实例化public static Singleton getInstance()&#123; if(is == null)&#123; is = new Singleton(); &#125; return is;&#125; 是的，单例模式需要注意的也就是这里了：线程安全问题如果你选择了在静态方法中进行实例化，并且使用了多线程技术，那么极有可能它并不是单例的；原因我想大概都知道，当然也有相应的解决方案，一般就从这三种中进行选择： 同步方法这是最简单的方式，如果不考虑性能的情况下是可以使用的，使用同步就意味着可能造成执行效率下降100倍public static synchronized getInstance(){} 急切实例化这个就是上面的第一种方式，在定义的时候就直接实例化在创建运行时负担不重的情况下可以采用 双重检查加锁在同步方法中我们发现，其实只需要第一次加锁就可以了，因为第一次创建出 is 后后面都是直接返回的所以，可以进行下面的优化（java5+）： 12345678910111213141516public class Test&#123; private volatile static Singleton is; private Test&#123;&#125; public static Singleton getInstance()&#123; if(is == null)&#123; synchronized(Test.class)&#123; if(is == null)&#123; is = new Singleton(); &#125; &#125; &#125; return is; &#125;&#125; 这样可以大大减少 get 方法的时间消耗，如果确实不考虑性能，使用这个就有点大材小用了。 这个方法表面上看起来很完美，你只需要付出一次同步块的开销，但它依然有问题。除非你声明 is 变量时使用了 volatile 关键字。没有 volatile 修饰符，可能出现 Java 中的另一个线程看到个初始化了一半的 is 的情况，但使用了 volatile 变量后，就能保证先行发生关系（happens-before relationship）参考下面的：无序写入 静态内部类这也是一种懒汉式的实现，相比双重锁检查，更简单，更高效吧 123456789101112public class SingletonIniti &#123; private SingletonIniti() &#123;&#125; private static class SingletonHolder &#123; private static final SingletonIniti INSTANCE = newSingletonIniti(); &#125; public static SingletonIniti getInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 加载一个类时，其内部类不会同时被加载。一个类被加载，当且仅当其某个静态成员（静态域、构造器、静态方法等）被调用时发生。并且外部类可以访问内部类的 private 方法。 单例模式的使用情景并不是太多，并且如果程序有多个类加载器，还是会造成有多个实例的情况，所以如果用到了多个类加载器记得指定使用同一个类加载器 volatile关键字关于这个，确实不太常见，很多人以为使用这个关键字，在进行多线程并发处理的时候就可以万事大吉。Java 语言是支持多线程的，为了解决线程并发的问题，在语言内部引入了 同步块 和 volatile 关键字机制。 对于 synchronized 我们都知道：通过 synchronized 关键字来实现，所有加上 synchronized 和块语句，在多线程访问的时候，同一时刻只能有一个线程能够用 synchronized 修饰的方法 或者 代码块。用 volatile 修饰的变量，线程在每次使用变量的时候，都会读取变量修改后的最的值。volatile 很容易被误用，用来进行原子性操作。 在 java 垃圾回收整理一文中，描述了jvm运行时刻内存的分配。其中有一个内存区域是 jvm 虚拟机栈，每一个线程运行时都有一个线程栈，线程栈保存了线程运行时候变量值信息。当线程访问某一个对象的值的时候，首先通过对象的引用找到对应在堆内存的变量的值，然后把堆内存变量的具体值 load 到线程本地内存中，建立一个变量副本，之后线程就不再和对象在堆内存变量值有任何关系，而是直接修改副本变量的值，在修改完之后的某一个时刻（线程退出之前），自动把线程变量副本的值回写到对象在堆中变量。这样在堆中的对象的值就产生变化了。 原文：http://www.cnblogs.com/aigongsi/archive/2012/04/01/2429166.html 从上面的解释也可以看出 volatile 并不能保证原子性，它的作用就是在每次使用的时候获取最新的值 无序写入双重检查锁定背后的理论是完美的。不幸地是，现实完全不同。双重检查锁定的问题是：并不能保证它会在单处理器或多处理器计算机上顺利运行。双重检查锁定失败的问题并不归咎于 JVM 中的实现 bug，而是归咎于 Java 平台内存模型。内存模型允许所谓的“无序写入”，这也是这些习语失败的一个主要原因。关键原因就是： instance = new Singleton(); 不是原子操作。然后从两个方面来看原因： 有序性：是因为 instance = new Singleton(); 不是原子操作。编译器存在指令重排，从而存在线程1 创建实例后（初始化未完成），线程2 判断对象不为空，但实际对象扔为空，造成错误。 可见性：是因为线程1 创建实例后还只存在自己线程的工作内存，未更新到主存。线程 2 判断对象为空，创建实例，从而存在多实例错误。 也就是，要想保证安全，必须保证这句代码的有序性和可见性。volatile 对 singleton 的创建过程的重要性：禁止指令重排序（有序性）。实例化一个对象其实可以分为三个步骤： 分配内存空间。 初始化对象。 将内存空间的地址赋值给对应的引用。 但是由于操作系统可以对指令进行重排序，所以上面的过程也可能会变成如下过程： 分配内存空间。 将内存空间的地址赋值给对应的引用。 初始化对象 如果是这个流程，多线程环境下就可能将一个未初始化的对象引用暴露出来，从而导致不可预料的结果。因此，为了防止这个过程的重排序，我们需要将变量设置为 volatile 类型的变量，volatile 的禁止重排序保证了操作的有序性。除了这种方案，还有人提出在“构造对象”和“连接引用与实例”之间加上一道内存屏障来保证有序性： 1234Singleton temp = new Singleton();//构造与赋值之间随意做点事情保证顺序temp.toString();instance=temp; 这想法确实 nice~ 关于可见性，第二次非 null 判断是在加锁以后（也就是说后面的线程在获取锁以后判断 instance 是否为 null 必然是在第一个线程引用赋值完成释放锁以后），则根据这一条，另一个线程一定能看到这个引用被赋值。所以即使没有 volatile，依旧能保证可见性。 https://www.zhihu.com/question/56606703 PS：据说因为 JVM 的实现不同，volatile 未必能保证绝对的安全，在 HotSpot 应该是没问题的。 参考&amp;拓展http://www.infoq.com/cn/articles/java-blocking-queuehttp://blog.csdn.net/xiaohulunb/article/details/38932923https://www.cnblogs.com/dolphin0520/p/3932921.htmlhttps://segmentfault.com/a/1190000008038848https://blog.csdn.net/chenchaofuck1/article/details/51702129拓展：http://www.jianshu.com/p/925dba9f5969深入理解Java线程池]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaSE二周目计划]]></title>
    <url>%2F2017%2F11%2F28%2FJavaSE%E4%BA%8C%E5%91%A8%E7%9B%AE%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[基础不能落下，习惯使用框架后基本功都忘得差不多，是时候复习一波了，跳着看的，重点放在多线程、IO、Socket 上；这一篇是个开头，也正好以前学 SE 的时候还没搭博客所以也没 md 笔记，这次就顺便补上；开头算是个补充，泛型、动态代理、悲观锁/乐观锁的小补充，后面是复习系列的多线程和 IO 关于泛型泛型还算是很简单的，稍微提一提，用的也很广泛，泛型可以用在方法上也可以用在类上，例如： 123456789101112131415161718192021222324252627// 用在类上，使用时最好是显式指定public class Box&lt;T&gt; &#123; // T stands for "Type" private T t; public void set(T t) &#123; this.t = t; &#125; public T get() &#123; return t; &#125;&#125;// 用在方法上public class Util &#123; public static &lt;K, V&gt; boolean compare(Pair&lt;K, V&gt; p1, Pair&lt;K, V&gt; p2) &#123; return p1.getKey().equals(p2.getKey()) &amp;&amp; p1.getValue().equals(p2.getValue()); &#125;&#125;public class Pair&lt;K, V&gt; &#123; private K key; private V value; public Pair(K key, V value) &#123; this.key = key; this.value = value; &#125; public void setKey(K key) &#123; this.key = key; &#125; public void setValue(V value) &#123; this.value = value; &#125; public K getKey() &#123; return key; &#125; public V getValue() &#123; return value; &#125;&#125; 在 JDK1.7+ 后会自动根据 type inference 进行推导，所以说在调用方法是是可以不写泛型的泛型的定义一般是在类名的后面，或者方法返回值之前，有了泛型，省去了很多的强转，在编译阶段就能发现错误然后说一下比较重要的，泛型的上限和下限： 上限（&lt;? extends E&gt;）说明传入的可以是 E 或者 E 的子类 下限（&lt;? super E&gt;）说明传入的值可以是 E 或者 E 的父类 实现原理在 C++ 中采用的方案是当实例化一个泛型会生成一个新类，例如模板类是 List ，然后你用 int ,double,string, Employee 分别去实例化， 那编译的时候，我们就会生成四个新类出来，例如List_int和List_double，List_string, List_Employee。 而在 Java 中采用的是擦除法，简单来说就是一个参数化的类型经过擦除后会去除参数， 例如 ArrayList&lt;T&gt; 会被擦除为 ArrayList ，既然这样类中使用的泛型怎么办，擦除了岂不是消失了？最终其实泛型都会被替换为 Object，然后还有一个问题，比如 Integer i = list1.get(0); 现在类型被擦除，返回值变成 Object 了， 怎么处理啊？很简单，只需要在编译的时候做点手脚来个强转就行了，因为可以保证类型一致，否则也不会通过编译检测：Integer i = (Integer)list1.get(0); 数据类型Java 中的两大类型：基本数据类型和引用数据类型，这个很熟悉都知道，在之前的 Java复习之内存 也说的比较清晰了，但是还有一些小的知识点漏掉了，现在就来补上都知道字符串、数组、自定对象之类的都是引用数据类型，会存储在堆内存中，默认会进行初始化，能够赋为 null，我想要说的是，基本数据类型也有可能放在堆中，也会进行初始化，那就是当它被定义为全局变量的时候，所以你能在定义全局变量的时候写：int a; 默认为 0，而在方法内定义时必须要这样写：int a = 0; 给它一个初始值，但是不能设置为 null。 动态代理&amp;ASM关于什么是动态代理、如何使用在前面的 Spring 文章和 Java 知识补充中已经说的很详细了，这里来补充一波没提到（写的好分散，忍忍吧，有时间再整理到一起）官方的动态代理基于接口实现（通过反射）CGLib（Code Generation Library）采用的是继承的方式实现，CGLIb 为了提高性能，还用了一种叫做FastClass 的方式来直接调用一个对象的方法，而不是通过反射。 ASM 虽然很低调，但它确实很厉害，Spring , hibernate 的核心都是基于它来实现的，更不要说 AOP 了，以及上面的 CGLib 也是基于 ASM，那么它到底是什么呢？ 它可以动态的修改已经编译过的 class , 还可以动态的生成新的 java class, 注意我说的动态这个词， 那可以是完全在运行时， 在内存中完成的， 这是一件非常厉害的本事。 并不是仅仅像 jsp 那样， 使用 JavaComplier 接口在运行时动态的编译一个 java 源代码 名字的由来：C语言中的 __asm__ 这个关键字， 可以允许你们在 C 语言中写点汇编， 他就把 ASM 这个关键字挪用了需要注意的是，想使用 ASM 需要非常透彻的理解 Java 虚拟机指令和 Java 虚拟机内部结构，我们常用的 Spring、CGLib 都是进行了高级的封装使用起来更加友好。有很多的语言是利用 ASM 来动态的生成字节码（解释性语言），跑在 JVM 的虚拟机上 官方提供的文档：http://download.forge.objectweb.org/asm/asm4-guide.pdf 悲观锁和乐观锁应该是和数据库相关了，但是多少有点联系，也找不到合适的地方就写在这里吧悲观锁：总认为数据会被别人修改，所以就总是给数据加锁，如果持有锁的时间过长，那么用户等待的时间也就越长乐观锁：本质其实是不加锁，但是会在字段的旁边加一个版本记录，读取时获取数值和版本，写入时先检查版本是否一致，如果不一致就重新获取重新计算；这样的话如果冲突很多数据库争用激烈会导致不断的进行重试，反而降低了性能 Java 中的 Sychronized 可以看作是悲观，CAS（Compare and Swap） 操作可以看作是乐观。java.util.concurrent 包中借助 CAS 实现了区别于 synchronouse 同步锁的一种乐观锁关于 CAS 其实还是蛮复杂的，不在深究，等以后如果用到了再看吧，这里给个网址和一点介绍吧 目前的处理器基本都支持 CAS，只不过不同的厂家的实现不一样罢了。CAS 有三个操作数：内存值 V、旧的预期值 A、要修改的值 B，当且仅当预期值 A 和内存值 V 相同时，将内存值修改为 B 并返回 true，否则什么都不做并返回 false。 http://ifeve.com/compare-and-swap/ 匿名内部类&amp;继承要使用匿名内部类需要有一个前提条件：继承某个类或者实现某个接口形式就是在需要某个类对象的时候直接 new 它的父类或者接口，然后去实现或者覆盖某些方法，并且一般需要实现的方法不会超过三个，否则阅读性极差，在回调机制中的用的较多，Android 上应该也有大量使用。特殊的，因为所有的类都继承 object，所以甚至可以这么写： 123new Object()&#123; public void show()&#123;System.out.println("haha");&#125;&#125;.show(); 但是不能定义一个 Obj 类型的变量去接收（这就相当于隐式的向上转型了），这样里面的方法就没法调了 现在我知道了 JVM 在创建子类的时候首先会创建一个父类对象，并且放在子类对象的堆空间里（就是说是子对象的一部分）然后在内存的方法区（分为静态方法区和非静态方法区），将 this 和 super 放在子对象拥有的空间里（和方法在一起），super 指向的就是父对象。父类的 private 方法不能称之为覆盖，因为子类压根不知道有这个方法，称为覆盖不是很合适，就当做是子类的一个普通方法就行了，还有就是构造方法不能覆盖（构造方法和本类名一样，怎么可能覆盖 XD）因为子类在创建时需要先创建父类，如果父类没有空的构造函数，需要在子类的构造函数中的第一行手动用 super 指定调用那个构造函数，也能看出一个规律： this() 和 super() 必须放在第一行 线程先来说说线程的几种状态（还可细分，那就需要 JVM 的知识了）：创建 、 运行 、 阻塞 、 冻结（睡眠、等待）、 消亡说明一下，阻塞状态是拥有执行资格但是没有执行权；冻结是没有执行资格，也自然没有执行权；运行状态是有执行资格也有执行权（当前正在运行）。 然后就是重要的线程安全问题了，同步函数（函数上加 synchronized）默认使用的锁是 this，但是当函数被静态修饰的时候使用的不再是 this（这时内存中没有本类对象只有本类的字节码对象），用的是 className.class 字节码对象（相当于是某个类的 class 对象，也就是字节码对象），当其他地方需要用这把锁的时候就需要传入 XX.class 了。在单例模式中的懒汉式，如果使用多线程访问就会存在安全性，但是如果将 get 方法加锁会变的低效，比较好的做法就是： 123456789101112// 单例对象必须是 volatile 修饰的，保证在多核处理器安全运行// 详细的原因可参考 二周目计划（二）public XX getXX()&#123; if(xxx == null)&#123; synchronized(XX.class)&#123; if(xxx == null)&#123; xxx = new XX(); &#125; &#125; &#125; return xxx;&#125; 因为 get 方法一般是静态的，所以我们使用本类的字节码对象作为锁，并且做双重判断来解决线程安全和同步函数低效的问题，一般还是用饿汉式吧，简单方便；一般来说如果你确定这个对象一定会用到那就肯定用饿汉式 死锁的出现一般是同步的嵌套，但是用的锁（顺序）不同的情况下，并且死锁并不一定会出现，万一和谐了呢….只要是多个线程在处理同一个资源一般就需要进行加锁，不管这几个线程是不是在一个类中，并且还要保证锁是相同的，这才是线程安全的前提。多个线程在操作共享数据的时候（并且是执行同一个方法）可以采用一个标志位来做等待唤醒机制，达到生产者生产一个，消费者消费一个，即使生产者和消费者有多个线程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Resource&#123; private String name; private int count = 1; private boolean flag = false; public synchronized void set(String name)&#123; while(flag) try&#123;this.wait();&#125;catch(InterruptedException e)&#123;&#125; this.name = name + count++; System.out.println(Thread.currentThread().getName()+"...生产者..."+this.name); flag = true; notifyAll(); &#125; public synchronized void out()&#123; while(!flag) try&#123;this.wait();&#125;catch(InterruptedException e)&#123;&#125; System.out.println(Thread.currentThread().getName()+"...消费者........"+this.name); flag = false; notifyAll(); &#125;&#125;// 生产者class Producer implements Runnable &#123; private Resource r; Producer(Resource r)&#123; this.r = r; &#125; public void run()&#123; while(true)&#123; r.set("烤鸭"); &#125; &#125;&#125;// 消费者class Consumer implements Runnable &#123; private Resource r; Consumer(Resource r)&#123; this.r = r; &#125; public void run()&#123; while(true)&#123; r.out(); &#125; &#125;&#125;class ProducerConsumerDemo &#123; public static void main(String[] args) &#123; Resource r = new Resource(); Producer pro = new Producer(r); Consumer con = new Consumer(r); Thread t0 = new Thread(pro); Thread t1 = new Thread(pro); Thread t2 = new Thread(con); Thread t3 = new Thread(con); t0.start(); t1.start(); t2.start(); t3.start(); &#125;&#125; 在 set 和 out 方法上加 synchronized 是为了保证不会出现生产一个消费两次，或者生产两个消费一次的情况。就是说多个线程要参与同一件事就需要在 wait 之前循环判断标记（where，不能用 if），防止某个线程唤醒后直接往下走（此时另一个线程已经重新修改了标记）。但是循环判断标记带来的另一个问题就是可能所有的线程都会一直处于 wait 状态（消费者唤醒的是消费者这种情况），就会出现和死锁差不多的情况，所以要使用 notifyAll。 补充：wait() 、 notify() 这两个方法定义在 Obj 中，只有锁才能调用这两个方法，因为任何对象都可以是锁，所以这两个方法就只能定义在 object 中了吧 使用Locks上面的是曾经的写法，在 JDK1.5+ 后出现了新的工具，所以说 1.5 真是里程碑式的升级，新的工具在 java.util.concurrent.locks 包下，最重要的就是 Lock 替代了 synchronized 的使用，Condition 替代了 Object 监视器（wait 、notufy）的使用. JDK1.5 以后将同步和锁封装成了对象，并将操作锁的隐式方式定义到了该对象中，将隐式动作变成了显示动作。Lock 接口： 替代了同步代码块或者同步函数。将同步的隐式锁操作变成现实锁操作。同时更为灵活。可以一个锁上加上多组监视器。lock(): 获取锁。unlock(): 释放锁，通常需要定义 finally 代码块中。Condition接口：出现替代了 Object 中的 wait 、 notify 、 notifyAll 方法。将这些监视器方法单独进行了封装，变成 Condition 监视器对象。可以与任意锁进行组合。await();signal();signalAll(); 使用新特性改造代码，关键是可以绑定多个监视器，就可以指定唤醒对方的某个线程而不是全部唤醒了： 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.concurrent.locks.*;class Resource&#123; private String name; private int count = 1; private boolean flag = false; Lock lock = new ReentrantLock(); // 获取锁 //通过已有的锁获取两组监视器，一组监视生产者，一组监视消费者。 Condition producer_con = lock.newCondition(); Condition consumer_con = lock.newCondition(); public void set(String name)&#123; lock.lock(); try&#123; while(flag) try&#123;producer_con.await();&#125;catch(InterruptedException e)&#123;&#125; this.name = name + count++; System.out.println(Thread.currentThread().getName()+"...生产者5.0..."+this.name); flag = true; // 唤醒对方 consumer_con.signal(); &#125;finally&#123; lock.unlock(); &#125; &#125; public void out()&#123; lock.lock(); try&#123; while(!flag) try&#123;cousumer_con.await();&#125;catch(InterruptedException e)&#123;&#125; System.out.println(Thread.currentThread().getName()+"...消费者.5.0......."+this.name); flag = false; // 唤醒对方 producer_con.signal(); &#125;finally&#123; lock.unlock(); &#125; &#125;&#125; 虽然可能感觉写法上麻烦了一些，但是这样确实是更清晰了 停止线程真正的让线程停下来只有一种方法，那就是 run 方法结束，我们写的 run 方法一般都是循环结构，所以只要控制住循环就能让线程停下来。特殊情况下，线程处于冻结状态（wait 或 sleep）然后没人唤醒的情况下怎么办，不能一直挂在这啊，Thread 类有个方法叫 interrupt 方法，作用就是强制唤醒冻结状态的线程，毕竟是强制，所以同时会抛出一个 InterruptedException 异常.可以认为，只要出现了这个异常就说名有人在调用 interrupt 方法，也就是有人想让它停下来，所以直接在 catch 里让自己停下来就行了，比如修改标志 守护线程简单介绍下什么是守护线程，守护线程在一般情况和用户线程没啥区别，都是在和主线程争夺 CPU ，关键是在结束的时候， 当主线程结束时，守护线程会自动结束，不管有没有执行完 ，有点像依赖关系。设置一个线程为守护线程的方式为（必须要在 start 方法调用之前执行）：t.setDaemon(true) 然后 start 就可以了，然后当程序的线程全是守护线程时，JVM 会自动退出。 其他介绍些其他的方法，线程方法还有个 join 方法，作用可以理解为抢夺 CPU 的执行权，例如在主线程执行到了 t1.join(); 那么主线程会把执行权让给 t1 ，自己进入冻结状态，当 t1 执行完后再恢复执行。设置优先级是：t.setPriority(Thread.MAX_PRIORITY); 优先级就 1-10，其中最明显的 1、5、10 都有常量定义，阅读性好些。当线程执行到 Thread.yield(); 时，会自动的交出执行权。 IO流JavaSE 中重要的一环，现在来复习一下，总体上可分为两类，字节流和字符流字节流的两个基类：InputStream 和 OutputStream字符流的两个基类：Reader 和 Writer它们都是抽象的，然后一个一个来看。 然后除了上面的两大类，还有个桥梁，转换流，如果只有字节流的话就不需要转换了，所以转换流的体系是在字符流中，主要是：InputStreamReader 和 OutputStreamWriter；使用非常的简单，一般直接是扔进字符流的缓冲区包装类就行了，Bufr/Bufw 就都会用了。最佳的测试方法就是用标准输入输出流了，System.in / System.out 它们都属于字节流，为了便于操作一般都将其转换为字符流，这样就用上上面的两个类了。转换流还有一个好用的功能就是可以指定码表，这也许才是本体 2333 字符流就以它的一个简单的孩子来看：FileWriter 对象，当它被 new 时会关联一个文件，此时文件会自动创建，如果存在会被覆盖，可通过其他的重载形式传入一个 Boolean（true） 来使用追加模式。为了提高效率，一般都是用缓冲区，Java 提供了两个类来支持缓冲：BufferedWriter、BufferedReader命名也是很有意思的，一般前面是功能描述，后面是所属的类别，比如上面的就是缓冲+字符流，大体就能知道是做什么的了。bufr 最爽的就是能一次读一行了（返回结果中不包含换行符 ，原理是用数组做缓存，还是一个个读，都存到数组中，遇到回车标记返回整个数组的数据），用的很频繁。调用缓冲区的 close 方法就不需要调用 FileWriter 的 close ，本质上它们是一样的，真正操作文件的还是 fw。缓冲功能的设计其实就是使用了装饰模式，对已有对象的功能进行增强。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public void WriterFile() &#123; String data = "xxxxxxxxxxxxx我是数据"; FileWriter fw = null; try &#123; // true 代表以追加的形式写入，也可以使用一个参数的构造方法（不使用追加） // 如果文件不存在此方法会自动创建文件 fw = new FileWriter("test.txt",true); fw.write(data); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (fw != null) try &#123; fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;public void ReadFile() &#123; FileReader fr = null; try &#123; fr = new FileReader("test.txt"); // 单个读取 int ch = 0; while ((ch = fr.read()) != -1)&#123; // 自动查码表 System.out.print((char)ch); &#125; // 使用缓冲区 char[] buf = new char[1024]; int len = 0; while ((len = fr.read(buf)) != -1)&#123; System.out.print(new String(buf,0,len)); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; if (fr != null) &#123; try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;// 使用JAVA提供的缓冲区public void WriterFileBufr() &#123; FileWriter fw = null; try &#123; fw = new FileWriter("test.txt"); BufferedWriter bufw = new BufferedWriter(fw); for (int i = 0; i &lt; 4; i++) &#123; bufw.write("test + " + i); // 根据系统插入相应的换行 bufw.newLine(); bufw.flush(); // 使用缓冲区，记得刷新 &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; if (fw != null) fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;public void ReadFileBufr() &#123; FileReader fr = null; try &#123; fr = new FileReader("tesst.txt"); BufferedReader bufr = new BufferedReader(fr); String len; while ((len = bufr.readLine()) != null) &#123; // 并不会读取换行符，所以需要手动换行 System.out.println(len); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; if (fr != null) &#123; try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 上面的仅供参考，最基本的 IO 读写文件的写法吧，使用字符流最后记得调用 flush 刷进去，因为它内部必定是有一小块缓存的，保证至少读完一个字符才写（不同的编码所占的空间不同，底层还是使用字节流来读取），并不是都一个写一个，大体就是这么个意思。 字节流看过字符流后字节流也基本一致，API 高度相似，最大的区别就是字节流的缓冲数组是 byte 数组，字符流是 char 数组，原因也很好理解单纯使用字节流写入并不需要刷新，字符流中一个中文字符是两个字节（编码不同而不同），所以他必须得先存起来（一个非常小的缓存吧），所以最后需要刷新，字节流没这个问题，可以直接写，毕竟字符流的底层也是用的字节流。下面主要说说字节流缓冲区的使用，其他的和上面基本一致： 12345678910111213141516171819202122232425262728293031public static void copy() &#123; BufferedInputStream bufis = null; BufferedOutputStream bufos = null; try &#123; bufis = new BufferedInputStream(new FileInputStream("test.txt")); bufos = new BufferedOutputStream(new FileOutputStream("test.txt")); int len; // 先把文件的部分内容读到缓冲区（字节数组），再从缓冲区一个一个的读 while ((len = bufis.read()) != -1) &#123; bufos.write(len); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; if (bufis != null) &#123; try &#123; bufis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (bufos != null) &#123; try &#123; bufos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 至于缓冲区的实现原理，大概的代码可以参考：我的Github 其他class 前面没有加任何的访问修饰符，通常称为“默认访问模式”（default ），在该模式下，这个类只能被同一个包中的类访问或引用，这一访问特性又称包访问性。一个 .java 文件可以有多个类，但是只能有一个 public 类, 而且如果有 public 类的话，这个文件的名字要和这个类的名字一样，编译的时候会分开生成多个 class 文件 关于 IO，Java 中有一个 Properties 对象，利用它可以获取系统的“环境变量”，我想表达的就是可以看平台的默认文件编码集设置的是什么，一般中文系统默认是 GBK，所以要想输出其他编码格式的文件不能用 FileWriter，用转换流进行转换才行： 123Properties prop = System.getProperties();prop.list(System.out);// new PrintStream("a.txt"); 在处理异常的时候的那个对象也和 IO 有点关系，可以看看它的 API，封装的还是挺方便的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fiddler使用介绍]]></title>
    <url>%2F2017%2F11%2F18%2FFiddler%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[关于 Fiddler 的使用在之前我是看过的，但是今天偶然发现已经差不多全部忘光了，所以呀，还是记录下比较好，意外的发现它的附加的一个功能非常好用，就是排除 UWP 应用在“安全沙箱”运行，让 UWP 应用能够使用系统的代理。关于它的简单介绍： Fiddler（中文名称：小提琴）是一个HTTP的调试代理，以代理服务器的方式，监听系统的Http网络数据流动，Fiddler可以也可以让你检查所有的HTTP通讯，设置断点，以及Fiddle所有的“进出”的数据（我一般用来抓包）,Fiddler还包含一个简单却功能强大的基于JScript .NET事件脚本子系统，它可以支持众多的HTTP调试任务。 Fiddler 是以代理WEB服务器的形式工作的,浏览器与服务器之间通过建立TCP连接以HTTP协议进行通信，浏览器默认通过自己发送HTTP请求到服务器;它使用代理地址:127.0.0.1, 端口:8888. 当Fiddler开启会自动设置代理， 退出的时候它会自动注销代理，这样就不会影响别的程序。不过如果Fiddler非正常退出，这时候因为Fiddler没有自动注销，会造成网页无法访问。解决的办法可以重新启动下Fiddler 官方提供了详细的文档和视频教程，如果英语好，可以去参考 界面Fiddler的主界面分为 工具面板、会话面板、监控面板、状态面板；应该都看得见在那，都在主页面上下面主要来看看比较复杂的工具面板，从左往右包含：说明注释、重新请求、删除会话、继续执行、流模式/缓冲模式、解码、保留会话、监控指定进程、寻找、保存会话、切图、计时、打开浏览器、清除IE缓存、编码/解码工具、弹出控制监控面板、MSDN、帮助 PS：在新版本增加了对 Win8+ 的 UWP 应用代理支持，就是第一个 WinConfig 在里面可以设置 UWP 应用的白名单，也就是绕过“安全沙箱”可以让其使用代理 模式上面提到了某个按钮是转换模式的，分为两种： 缓冲模式（Buffering Mode）Fiddler直到 HTTP 响应完成时才将数据返回给应用程序。可以控制响应，修改响应数据。但是时序图有时候会出现异常 流模式（Streaming Mode）Fiddler 会即时将 HTTP 响应的数据返回给应用程序。更接近真实浏览器的性能。时序图更准确,但是不能控制响应。 采用哪种模式看需求吧，默认应该是流模式，更加符合实际情况 监控面板这个面板很重要，看的最多的应该就这个了，里面也包含了很多的选项卡，下面就来说说各个选项卡是做什么的 审查（inspectors ）指的是 inspectors 选项卡，不要在意前面的翻译类似于 Chrome 的 F12 里的 NetWork，不过应该是更强大，可以对某个请求的头信息、内容、响应进行查看和操作，反正是很强大关键是能修改啊，配合断点简直爽 统计（statistics）是的，就是统计一些请求或者其他啥的数据，还有图标显示，主要包含有： 请求总数、请求包大小、响应包大小。 请求起始时间、响应结束时间、握手时间、等待时间、路由时间、TCP/IP、传输时间。 HTTP状态码统计。 返回的各种类型数据的大小统计以及饼图展现。 当然并不只是这些，数据的描述都是英文，像我这种英语渣渣只能复制到 Google 去翻译下看看什么意思….. 时间轴（Timeline）每个网络请求都会经历域名解析、建立连接、发送请求、接受数据等阶段。把多个请求以时间作为 X 轴，用图表的形式展现出来，就形成了瀑布图。在Fiddler中，只要在左侧选中一些请求，右侧选择 Timeline 标签，就可以看到这些请求的瀑布图，不同的请求形式会有不同的颜色 绿色的请求表示这是一个“有条件的请求”。HTTP 协议定义了 5 个条件请求头部，最常见的两个是“If-Modified-Since”和“If-None-Match”。服务器根据这两个头部来验证本地缓存是否过期，如果过期则正常返回资源的最新版本；否则仅返回 304 Not Modified，浏览器继续使用本地缓存。包含条件请求头部的请求用绿色显示，否则用黑色。 有阴影线的请求它是缓冲模式下的请求，实心的是流模式下的请求。Fiddler 提供了缓冲（Buffering）和流（Streaming）两种抓包模式：缓冲模式下，Fiddler 会在响应完成时才将数据返回给应用程序（通常是浏览器），这种模式下可以控制响应，方便地修改响应内容；流模式下，Fiddler 会实时返回响应数据给浏览器，但没办法控制响应。一般使用流模式，瀑布图会更真实一些。这两种模式可以通过 Fiddler 的工具栏选择。特别的，通过 Fiddler 的“AutoResponder”功能返回的响应，只能是缓冲模式。 请求条的不同颜色对应着不同类型的响应，根据响应头的 MIME Type 来归类。如浅绿色表示图片类型的响应；深绿色是 JavaScript；紫色是 CSS；其它都是蓝色。 请求中的黑色竖线表示的是浏览器收到服务端响应的第一个字节这一时刻。这个时间受 DNS 解析、建立连接、发送请求、等待服务端响应等步骤的影响。 请求条后面的图标表示响应的某些特征如软盘图标表示这个响应正文从本地获得，也就是说服务端返回了 304；闪电表示这是 Fiddler 的“AutoResponder”的响应；向下的箭头表示响应是 302，需要重定向；红色感叹号说明这个请求有错误发生（状态码是 4XX 或 5XX）。特别的，如果请求条后面有一个红色的X，说明服务端响应完这个请求之后，断开了连接。出现这种情况一般有两种可能：HTTP/1.0 的响应中没有 Connection: Keep-Alive；或者是 HTTP/1.1 的响应中包含了 Connection: close。使用持久连接可以省去建立连接的开销，也可以减小 TCP 慢启动和其它拥塞控制机制带来的影响，总之是好处多多。 请求前面的红色圆圈表示这个连接是新建的，绿色表示是复用的。上面的圆圈表示的是浏览器到 Fiddler 的连接，下面的圆圈是 Fiddler 到服务端的连接。 时间轴也是分析的时候很重要的一个面板 模拟响应（AutoResponder）这是 Fiddler 比较重要且比较强大的功能之一。可用于拦截某一请求，并重定向到本地的资源，或者使用Fiddler的内置响应。可用于调试服务器端代码而无需修改服务器端的代码和配置，因为拦截和重定向后，实际上访问的是本地的文件或者得到的是Fiddler的内置响应。因此，如果要调试服务器的某个脚本文件，可以将该脚本拦截到本地，在本地修改完脚本之后，再修改服务器端的内容，这可以保证，尽量在真实的环境下去调试，从而最大限度的减少bug发生的可能性。一般设置步骤为： 选中想要拦截的请求，比如 /api/foo 开启模拟（Enable rules）再选择 “Add Rule” 在最下面的下拉框选择 “Create New Response…” 然后 “Save” 匹配支持正则，支持常用的重定向方式，这个功能真是一大杀器 模拟请求（Composer）老版本的 fiddler 中叫 request-builder. 顾名思义，可以构建相应的请求，有两种常用的方式构建请求: Parsed 输入请求的 url 之后 executed 即可，也可以修改相应的头信息如添加常用的 accept, host, referrer, cookie，cache-control 等头部后再 execute.这个功能的常见应用是：“刷票”（不是火车票！！），如刷新页面的访问量（那啥，不要做为好） Raw。使用 HTTP 头部信息构建 http 请求。与上类似。不多叙述 其他常用的也就是这几个，还有一个就是 Filter，基本你能想到的基本都支持了，不多说脚本功能应该也很 NB ，但我是没怎么用过，或者可以去找网友写好的脚本使用 状态面板这个在软件界面的左下角，还是蛮重要的，控制台Fiddler的左下角有一个命令行工具叫做 QuickExec,允许你直接输入命令（就是那个黑框框）。常见得命令有： 命令 解释 help 打开官方的使用页面介绍，所有的命令都会列出来 cls 清屏 (Ctrl+x 也可以清屏) select 选择会话的命令 ?.png 用来选择png后缀的图片 bpu 截获request bpafter 截获response 然后最左边的图标控制拦截的开启和关闭，右边是请求的来源，还有比较重要的就是设置断点了，在请求数的左边有一个空白的位置，它可以点击，有三种状态，每点击一次就切换一种状态： 空白：不设置断点。 箭头向上：表示断点请求。此时客户端的请求是无法直接到达目标服务器的，需要手动控制。 箭头向下：表示断点响应。此时目标服务器的响应是无法直接到达客户端的，需要手动控制。 断点操作一般就是这几步： 设置断点请求，手机端访问接口 点击对应的会话 查看请求报文信息 修改请求内容 完成断点，放行，把该请求发送给目标服务器。 对应这幅图： 修改的时候留意下超时时间就行了上面的操作其实都可以通过命令行来完成 其他常用常用的功能还有抓 HTTPS 的包、移动端抓包等，等想到更多了再来补充 抓取 HTTPSFiddler不仅能监听HTTP请求而且默认情况下也能捕获到HTTPS请求，Tool -&gt; Options... -&gt; HTTPS 下面进行设置，勾选上 “Decrypt HTTPS traffic”还可以选择忽略证书错误（建议勾上）和忽略指定的 HOST 地址 移动端抓包Fiddler 不但能截获各种浏览器发出的HTTP请求, 也可以截获各种智能手机发出的HTTP/HTTPS请求。前提条件是：安装Fiddler的机器，跟设备在同一个网络里， 否则不能把 HTTP 发送到安装 Fiddler 的机器上来。 具体的设置步骤为（以 iPhone 为例）： 打开 Fiddler,选择 Tools-&gt; Options。（配置完后记得要重启Fiddler）. 选中Connections 选项卡下的 “Allow remote computers to connect”. 是允许别的机器把 HTTP/HTTPS 请求发送到 Fiddler 上来 查看 Fiddler 所在机器的 IP（可通过 ipconfig 命令） 安装 Fiddler 证书这一步是为了让 Fiddler 能捕获HTTPS请求。 如果你只需要截获 HTTP 请求， 可以忽略这一步首先要知道 Fiddler 所在的机器的IP地址，假如我安装了 Fiddler 的机器的IP地址是:192.168.1.104；打开IPhone 的Safari, 访问 http://192.168.1.104:8888， 点 “FiddlerRoot certificate” 然后安装证书 打开iPhone, 找到你的网络连接， 打开HTTP代理， 输入 Fiddler 所在机器的 IP 地址以及 Fiddler 的端口号8888 现在 Fiddler 就可以获取到移动设备的数据请求了 关于WiresharkWireshark 是另外一种抓包工具，这种工具比 fiddler 更强大，消息量更多，开源，支持更多的协议。这里说下，在测试中，发现用 fiddler 抓包，有些包是没有抓到的，比如在验证反作弊信息的时候，反作弊pingback 信息的消息用 fiddler 就没抓到，用 wireshark 就抓到了。 Wireshark 使用 WinPCAP 作为接口，直接与网卡进行数据报文交换，注重的是网络封包分析使用wireshark的人必须了解网络协议，否则就看不懂wireshark了。为了安全考虑，wireshark只能查看封包，而不能修改封包的内容，或者发送封包Fiddler 更注重的是 HTTP 协议调试方面 Wireshark 配置起来比 fiddler 麻烦一些，学习门槛相对来说比较高，有机会再来学一学。其他的抓包工具还有 tcpdump 可以理解为没有界面的 Wireshark 在 Linux 上用的比较多Wireshark 能获取HTTP，也能获取HTTPS，但是不能解密HTTPS，所以wireshark看不懂HTTPS中的内容，总结：如果是处理 HTTP,HTTPS 还是用Fiddler, 其他协议比如TCP,UDP 就用wireshark. 其他还记得被广泛用于游戏外挂的 WPE 三件套（Winsock Packet Editor、Easy2Game、CCProxy），就是因为可以修改、发送封包，配合代理简直就…..好孩子就不要研究了，也不简单。可参考：Wiki 上的一些介绍 参考http://www.jianshu.com/p/99b6b4cd273chttps://segmentfault.com/a/1190000004240812]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>Fiddler</tag>
        <tag>抓包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java消息中间件]]></title>
    <url>%2F2017%2F11%2F16%2FJava%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[学习这个的契机是在做练手项目的时候用到了兔子（RabbitMQ），它遵循 AMQP 协议，属于消息中间件实现的一种，既然这样就来看看什么是消息中间件。正好看到慕课网有相关的课程就顺便学习下吧，为什么使用消息中间件？，比较常用的是 RabbitMQ、RocketMQ、ActiveMQ、Kafka。解耦、异步、横向扩展、安全可靠、顺序保证，这些还不够么，回想 Rabbit 官网的那几幅图吧如果忘记了就再看看吧：飞机 关于JMSJava消息服务（Java Message Service，JMS）应用程序接口是一个 Java 平台中关于面向消息中间件（MOM）的 API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java 消息服务是一个与具体平台无关的 API，绝大多数 MOM 提供商都对 JMS 提供支持。 JSM 的组成有： JMS提供者连接面向消息中间件的，JMS接口的一个实现。提供者可以是Java平台的JMS实现，也可以是非Java平台的面向消息中间件的适配器。 JMS客户生产或消费消息的基于Java的应用程序或对象。 JMS生产者创建并发送消息的JMS客户。 JMS消费者接收消息的JMS客户。 JMS消息包括可以在JMS客户之间传递的数据的对象 JMS队列一个容纳那些被发送的等待阅读的消息的区域。队列暗示，这些消息将按照顺序发送。一旦一个消息被阅读，该消息将被从队列中移走。 JMS主题 一种支持发送消息给多个订阅者的机制。 Java 消息服务应用程序结构支持两种模型： 点对点或队列模型包含生产者和消费者，队列中的消息只能被一个消费者消费，消费者可以随时消费消息每一个连接都依次平均分担消息队列中的消息（即使一个应用建立了两个连接） 发布/订阅模型包括发布者和订阅者，主题中的消息会被所有的订阅者消费，消费者不能消费在订阅之前的消息每一个连接都会收到主题中完整的消息 关于架构等详细信息可参考 wiki ：https://zh.wikipedia.org/wiki/Java%E6%B6%88%E6%81%AF%E6%9C%8D%E5%8A%A1 中间件&amp;AMQP中间件（英语：Middleware），是提供系统软件和应用软件之间连接的软件，以便于软件各部件之间的沟通，特别是应用软件对于系统软件的集中的逻辑，在现代信息技术应用框架如Web服务、面向服务的体系结构等中应用比较广泛。如数据库、Apache的Tomcat，IBM公司的WebSphere,BEA公司的WebLogic应用服务器，东方通公司的Tong系列中间件，以及Kingdee公司的等都属于中间件。或者简单说就是：非底层操作系统软件，非业务应用软件，不是直接给最终用户使用的，不能直接给用户带来价值的软件统称为中间件 高级消息队列协议（AMQP）是一个异步消息传递所使用的应用层协议规范。作为线路层协议，而不是 API（例如 JMS），AMQP 客户端能够无视消息的来源任意发送和接受信息。现在，已经有相当一部分不同平台的服务器和客户端可以投入使用。 注意：JMS 是规范（针对 Java），AMQP 是协议 然后来张图来比较，JMS 和 AMQP： 然后是市面上常见的一些 MQ 方案： ActiveMQ使用导入依赖就不用多说了，下面的 Java 代码中的使用（以主题订阅模式为例），先来消息发布者： 123456789101112131415161718192021222324252627282930private static final String URL = "tcp://127.0.0.1:61616";private static final String TOPIC_NAME = "topic-test";public static void main(String[] args) throws JMSException &#123; // 1.创建 ConnectionFactory ConnectionFactory connectionFactory = new ActiveMQConnectionFactory(URL); // 2.创建连接 Connection connection = connectionFactory.createConnection(); // 3.启动连接 connection.start(); // 4.创建会话 // 第一个参数是是否在事务中，第二个是自动提交 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 5.创建一个目标(只需要修改这里) Destination destination = session.createTopic(TOPIC_NAME); // 6.创建一个生产者 MessageProducer producer = session.createProducer(destination); // 7.创建消息/发送消息 TextMessage message = session.createTextMessage("testMessage"); producer.send(message); // 8.关闭连接 connection.close();&#125; 然后是消息订阅者，在此模式下订阅之前发的消息是没法接收到的： 1234567891011121314151617181920212223242526272829303132333435363738private static final String URL = "tcp://127.0.0.1:61616";private static final String TOPIC_NAME = "topic-test";public static void main(String[] args) throws JMSException &#123; // 1.创建 ConnectionFactory ConnectionFactory connectionFactory = new ActiveMQConnectionFactory(URL); // 2.创建连接 Connection connection = connectionFactory.createConnection(); // 3.启动连接 connection.start(); // 4.创建会话 // 第一个参数是是否在事务中，第二个是自动提交 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 5.创建一个目标 Destination destination = session.createTopic(TOPIC_NAME); // 6.创建一个消费者 MessageConsumer consumer = session.createConsumer(destination); // 7.创建监听器（异步监听） consumer.setMessageListener(new MessageListener() &#123; public void onMessage(Message message) &#123; TextMessage msg = (TextMessage) message; try &#123; System.out.println(msg.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); // 9.关闭连接(监听器异步，应该在程序退出时关闭) // connection.close();&#125; 并且接收消息的过程是异步的，所以不要马上 close；完整的代码见 GitHub 关于集群集群总的来说就是为了实现高可用和负载均衡，以 ActiveMQ 为例，集群不只是在服务器端配置，客户端也需要支持；在ActiveMQ 中提供了失效转移的支持，URL 类似于：failover:(tcp://127.0.0.1:61617,tcp://192.168.1.11:61617)?randomize=true在服务器方面，想要实现负载均衡就要保证服务器之间的消息同步，采用的是 “网络连接器”的方式，用于服务器的透传消息，分为静态连接器和动态连接器（用网址来代替写死的 IP 地址）。对于高可用服务器的方案一般有两种： 共享存储集群采用集群共享一份持久化数据（使用 NAS 或者 JDBC），服务器获取排它锁来独占资源的方式，当此服务器宕机时会释放（配合客户的的失效转移机制）备用服务器会获取锁成为新的 Master 基于复制的 LeveIDB Store至少需要三台来保证稳定性，还用到了 ZooKeeper（ZK 本身也需要三台来保证自己的稳定性）也就是 Master 由 ZK 来选举，然后通过 ZK 来实现各服务器之间的消息同步 他们能达到高可用，但是实现不了负载均衡，想要同时实现就需要进行一些改造。还有一些其他的问题解决方案：实现每个系统消费各自的消息可以使用 ActiveMQ 提供的虚拟主题功能；解决消息发送的一致性问题可以使用 JMS 中的 XA 系列接口；解决幂等性的问题，方案和上面一样，使用本地事务或者内存日志 JMS 中的 XA 协议常用于分布式事务，因为效率较低所以不太使用，或者还可以使用本地事务、内存日志解决（都要配合消息补偿机制）解决这些问题一般分段考虑比较好。 幂等性就是指处理一次和多次的消息最终的效果是一样的。HTTP方法的幂等性是指一次和多次请求某一个资源应该具有同样的副作用 为了解决代码过于复杂和复用，可以使用“基于消息机制的事件总线”，简单说 EDA （事件驱动架构）就是：有事你叫我，没事别烦我，这样一般就需要先在事件总线上注册，事件总线一般还需要包含消息提供者（各种 MQ 的实现）。可以尝试面向服务的架构 Spring集成主要涉及的有：ConnectionFactory：用于管理连接的连接工厂；Spring 提供了两个具体的类，SingleConnectionFactory 和 CachingConnectionFactory ，后者是带有缓存功能的。JmsTemplate：用于发送和接收消息的模板类，它是线程安全的MessageListerner：消息监听器使用时记得引入 Spring-jms 依赖，具体的相关依赖有： 123456789101112131415161718&lt;!-- Java JMS 原生API --&gt;&lt;dependency&gt; &lt;groupId&gt;javax.jms&lt;/groupId&gt; &lt;artifactId&gt;javax.jms-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- spring-jms API --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- active-mq核心包 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-core&lt;/artifactId&gt; &lt;version&gt;5.7.0&lt;/version&gt;&lt;/dependency&gt; 接下来就是在 Spring 的配置文件中进行配置 Bean 了： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!-- 配置连接ActiveMQ的ConnectionFactory --&gt;&lt;bean id="amqConnectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory" p:brokerURL="tcp://localhost:61616"/&gt;&lt;!--为了提高效率，配置一个连接池--&gt;&lt;bean id="cachedConnectionFactory" class="org.springframework.jms.connection.CachingConnectionFactory" p:targetConnectionFactory-ref="amqConnectionFactory" p:sessionCacheSize="10"/&gt;&lt;!--配置队列--&gt;&lt;bean id="destination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;constructor-arg value="$&#123;queue.name&#125;"/&gt;&lt;/bean&gt;&lt;!--配置主题--&gt;&lt;bean id="destinationTopic" class="org.apache.activemq.command.ActiveMQTopic"&gt; &lt;constructor-arg value="$&#123;topic.name&#125;"/&gt;&lt;/bean&gt;&lt;!-- **************配置消息生产者************* --&gt;&lt;!--点对点模型--&gt;&lt;bean id="queueJmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;constructor-arg ref="connectionFactory"/&gt; &lt;!--消息持久--&gt; &lt;property name="deliveryPersistent" value="true"/&gt; &lt;property name="defaultDestination" ref="destination"/&gt; &lt;property name="pubSubDomain" value="false"/&gt;&lt;/bean&gt;&lt;!--发布/订阅模型--&gt;&lt;bean id="topicJmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;constructor-arg ref="connectionFactory"/&gt; &lt;!--消息持久--&gt; &lt;property name="deliveryPersistent" value="true"/&gt; &lt;!--目的地--&gt; &lt;property name="defaultDestination" ref="destinationTopic"/&gt; &lt;!--订阅模型 --&gt; &lt;property name="pubSubDomain" value="true"/&gt;&lt;/bean&gt;&lt;!-- **************配置消息消费者************* --&gt;&lt;!-- 配置消息队列监听者（Queue） --&gt;&lt;bean id="queueMessageListener" class="com.bfchengnuo.Filter.QueueMessageListener" /&gt;&lt;!-- 显示注入消息监听容器（Queue），配置连接工厂，监听器是上面定义的监听器 --&gt;&lt;bean id="queueListenerContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="cachedConnectionFactory" /&gt; &lt;property name="destination" ref="destination" /&gt; &lt;property name="messageListener" ref="queueMessageListener" /&gt;&lt;/bean&gt; 消费者和生产者的配置最好是分开来放，可以抽取相同的配置到独立的文件再利用 include 导入Java 代码的使用，也分为两个角色，写在一起了： 1234567891011121314151617181920212223// 接收消息，记得配置到 IOC 容器中public class MyQueueMessageListener implements MessageListener &#123; @Override public void onMessage(Message message) &#123; try &#123; TextMessage textMessage = (TextMessage) message; System.out.println("MyQueueMessageListener收到消息：" + textMessage.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125;// 发送消息 Destination 已注入public void sendMessage(final String msg)&#123; // String destination = jmsTemplate.getDefaultDestinationName(); System.out.println(Thread.currentThread().getName()+" 向队列"+destination+"发送消息---&gt;"+msg); jmsTemplate.send(destination, new MessageCreator() &#123; public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage(msg); &#125; &#125;);&#125; 实际使用时记得把它们给分开，更改模式只需要在 Destination 注入的时候选择合适的模式即可，其他的地方不需要修改 更多内容：https://my.oschina.net/thinwonton/blog/889805http://www.cnblogs.com/jaycekon/p/ActiveMq.html SB集成SpringBoot 必定是主流，所以与 SB 的集成必定要了解，并且会更加的简单。首先增加依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-activemq&lt;/artifactId&gt;&lt;/dependency&gt; 然后在主配置文件中进行一些配置，这里只写几个最基本的： 12345678910111213141516spring.activemq.broker-url=tcp://localhost:61616spring.activemq.user=adminspring.activemq.password=admin# 消息模式 true:广播(Topic)，false:队列(Queue),默认时false#spring.jms.pub-sub-domain=true# 其他可选配置#是否启用内存模式(也就是不安装MQ,也可以使用MQ功能)spring.activemq.in-memory=false#ActiveMQ连接池是否启用spring.activemq.pool.enabled=true#ActiveMQ连接池最大连接数spring.activemq.pool.max-connections=5#ActiveMQ连接池连接空闲时间，默认为30秒spring.activemq.pool.idle-timeout=30000 之后就可以使用 SB 提供的 JmsMessagingTemplate 进行简单的 MQ 相关操作了，不放心的话可以手动开启 @EnableJms。下面是一些简单的测试用例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344@RunWith(SpringRunner.class)@SpringBootTestpublic class ActivemqApplicationTests &#123; @Autowired private JmsMessagingTemplate jmsMessagingTemplate; /** * 消息发送/生产 */ @Test public void testQueueMsg()&#123; // 创建名称为zyQueue的队列 // 广播模式使用 ActiveMQTopic 对象，此时所有监听者都能获得每一个生产的对象 Queue queue = new ActiveMQQueue("zyQueue"); // 向队列发送消息 jmsMessagingTemplate.convertAndSend(queue,"这是一个队列消息！"); &#125;&#125;/** * 消息消费方 */@Componentpublic class Consumer &#123; private static DateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss,sss"); /** * destination 目标地址即队列 */ @JmsListener(destination = "zyQueue") public void receiveMessage(String text)&#123; System.out.println("接收队列消息时间："+ df.format(new Date()) +", 接收到消息内容:"+text); &#125; /** * 多个消费者，默认会平均消息消费（轮训） */ @JmsListener(destination = "zyQueue") public void receiveMessage(String text)&#123; System.out.println("q2-接收队列消息时间："+ df.format(new Date()) +", 接收到消息内容:"+text); &#125;&#125; 那么，如果我们想要同时支持两种模式呢，单从配置已经没法改了，这时候需要我们自定义工厂： 12345678910111213141516171819202122232425262728293031323334353637383940@SpringBootConfigurationpublic class ActiveMqConfig &#123; @Bean("queueListenerFactory") public JmsListenerContainerFactory&lt;?&gt; queueListenerFactory(ConnectionFactory connectionFactory)&#123; DefaultJmsListenerContainerFactory factory = new DefaultJmsListenerContainerFactory(); factory.setConnectionFactory(connectionFactory); //设置消息模型为队列 factory.setPubSubDomain(false); return factory; &#125; @Bean("topicListenerFactory") public JmsListenerContainerFactory topicListenerFactory(ConnectionFactory connectionFactory)&#123; DefaultJmsListenerContainerFactory factory = new DefaultJmsListenerContainerFactory(); factory.setConnectionFactory(connectionFactory); //设置消息模型为队列 factory.setPubSubDomain(true); return factory; &#125; /** * 之后我们使用的时候指定对应的 containerFactory 即可 */ @JmsListener(destination = "zyQueue", containerFactory = "queueListenerFactory") public void receiveMessage(String text)&#123; System.out.println("接收队列消息时间："+ df.format(new Date()) +", 接收到消息内容:"+text); &#125; /** * 设置存入mq的数据格式为json */ @Bean public MessageConverter jacksonJmsMessageConverter() &#123; MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter(); converter.setTargetType(MessageType.TEXT); converter.setTypeIdPropertyName("_type"); return converter; &#125;&#125; 上面的配置列举了一些常见的配置，不涉及集群，如果需要 SC 集成，可以参考这篇文章关于 JmsTemplate 的性能问题参考：使用Spring/Spring Boot集成JMS的陷阱 RabbitMQ基于 AMQP 所以功能是绝对够用，这里就只说与 SB 的集成，也是主流了。步骤也都是类似的，加依赖，写配置： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 主要的配置： 12345678# rabbitmq配置spring: rabbitmq: host: 127.0.0.1 port: 5672 username: guest password: guest virtualHost: / 下面就是简单的一些代码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189@Configurationpublic class RabbitConfig &#123; /** * 普通队列模式 * 定义demoQueue队列 */ @Bean public Queue demoString() &#123; return new Queue("demoQueue"); &#125; //=================== fanout广播模式 ==================== @Bean public Queue fanoutA() &#123; return new Queue("fanout.a"); &#125; @Bean public Queue fanoutB() &#123; return new Queue("fanout.b"); &#125; @Bean public Queue fanoutC() &#123; return new Queue("fanout.c"); &#125; /** * 定义个fanout交换器 */ @Bean FanoutExchange fanoutExchange() &#123; // 定义一个名为 fanoutExchange 的 fanout 交换器 return new FanoutExchange("fanoutExchange"); &#125; /** * 将定义的fanoutA队列与fanoutExchange交换机绑定 */ @Bean public Binding bindingExchangeWithA() &#123; return BindingBuilder.bind(fanoutA()).to(fanoutExchange()); &#125; /** * 将定义的fanoutB队列与fanoutExchange交换机绑定 */ @Bean public Binding bindingExchangeWithB() &#123; return BindingBuilder.bind(fanoutB()).to(fanoutExchange()); &#125; /** * 将定义的fanoutC队列与fanoutExchange交换机绑定 */ @Bean public Binding bindingExchangeWithC() &#123; return BindingBuilder.bind(fanoutC()).to(fanoutExchange()); &#125; //=================== topic主题模式 ==================== @Bean public Queue topiocA() &#123; return new Queue("topic.a"); &#125; @Bean public Queue topicB() &#123; return new Queue("topic.b"); &#125; @Bean public Queue topicC() &#123; return new Queue("topic.c"); &#125; /** * 定义个topic交换器 */ @Bean TopicExchange topicExchange() &#123; // 定义一个名为fanoutExchange的fanout交换器 return new TopicExchange("topicExchange"); // 另一种定义 // durable(true) 表面重启之后交换机还在 // return ExchangeBuilder.topicExchange(EXCHANGE_TOPICS_INFORM).durable(true).build(); &#125; /** * 将定义的topicA队列与topicExchange交换机绑定 */ @Bean public Binding bindingTopicExchangeWithA() &#123; // return BindingBuilder.bind(queue).to(exchange).with(ROUTINGKEY_EMAIL).noargs(); return BindingBuilder.bind(topiocA()).to(topicExchange()).with("topic.msg"); &#125; /** * 将定义的topicB队列与topicExchange交换机绑定 */ @Bean public Binding bindingTopicExchangeWithB() &#123; return BindingBuilder.bind(topicB()).to(topicExchange()).with("topic.#"); &#125; /** * 将定义的topicC队列与topicExchange交换机绑定 */ @Bean public Binding bindingTopicExchangeWithC() &#123; return BindingBuilder.bind(topicC()).to(topicExchange()).with("topic.*.z"); &#125;&#125;/** * 消息生产者 */@Componentpublic class RabbitProducer &#123; @Autowired private AmqpTemplate rabbitTemplate; public void sendDemoQueue() &#123; Date date = new Date(); String dateString = new SimpleDateFormat("YYYY-mm-DD hh:MM:ss").format(date); System.out.println("[demoQueue] send msg: " + dateString); // 第一个参数为刚刚定义的队列名称 this.rabbitTemplate.convertAndSend("demoQueue", dateString); // 注意 第一个参数是我们交换机的名称 ，第二个参数是routerKey 我们不用管空着就可以，第三个是你要发送的消息 this.rabbitTemplate.convertAndSend("fanoutExchange", "", dateString); // 这条信息将会被 topic.a topic.b接收 this.rabbitTemplate.convertAndSend("topicExchange", "topic.msg", dateString); // 这条信息将会被topic.b接收 this.rabbitTemplate.convertAndSend("topicExchange", "topic.good.msg", dateString); // 这条信息将会被topic.b、topic.c接收 this.rabbitTemplate.convertAndSend("topicExchange", "topic.m.z", dateString); &#125;&#125;@Component@RabbitListener(queues = "demoQueue")public class DemoQueueConsumer &#123; /** * 消息消费 * @RabbitHandler 代表此方法为接受到消息后的处理方法 */ @RabbitHandler public void recieved(String msg) &#123; System.out.println("[demoQueue] recieved message: " + msg); &#125;&#125;@Component@RabbitListener(queues = "fanout.a")public class FanoutAConsumer &#123; /** * 消息消费 * @RabbitHandler 代表此方法为接受到消息后的处理方法 */ @RabbitHandler public void recieved(String msg) &#123; System.out.println("[fanout.a] recieved message: " + msg); &#125;&#125;@Component@RabbitListener(queues = "topic.b")public class TopicBConsumer &#123; /** * 消息消费 * @RabbitHandler 代表此方法为接受到消息后的处理方法 */ @RabbitHandler public void recieved(String msg) &#123; System.out.println("[topic.b] recieved message:" + msg); &#125;&#125;/** * 另一种消费模式 */@Componentpublic class ReceiveHandler &#123; @RabbitListener(queues = &#123;RabbitmqConfig.QUEUE_INFORM_EMAIL&#125;) public void receiveEmail(String msg, Message message, Channel channel)&#123; System.out.println(msg); &#125; @RabbitListener(queues = &#123;RabbitmqConfig.QUEUE_INFORM_SMS&#125;) public void receiveSms(String msg, Message message, Channel channel)&#123; System.out.println(msg); &#125;&#125; 以上，都是最基本的使用，不涉及集群相关，RocketMQ 也是类似，使用的是 RocketMQTemplate。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JMS</tag>
        <tag>ActiveMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle学习笔记]]></title>
    <url>%2F2017%2F11%2F12%2FOracle%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[关于 Oracle 这个数据库就不需要做什么解释了，名声那么大，怎么还不死都懂得，就是死贵死贵的，还是要学一学，因为企业用的还是挺多的。然后来看一下关于它的一些基本介绍，默认端口为 1521 一般固定不用改，数据库的数据文件是 DBF 结尾。版本有三个，结尾分别为 i（internet）、g（grid）、c（clound）；对应早期、近期、现在；早期的就不说了，一般没人用了，现在基本用的都是 g 系列，c 系列太贵了是融合了云计算服务器，一般用 g 就够了，g 表示的网格可以简单理解为是一个集群，将多台 Oracle 服务器当做一台主机协调使用 关于SQL准备知识，SQL92/99 标准的四大分类： DML（数据操纵语言）：select,insert,update,delete DDL（数据定义语言）：create table,alter table，drop table，truncate table DCL（数据控制语言）：grant 权限 to scott，revoke 权限 from scott TCL（事务控制语言）：commit，rollback，rollback to savepoint SQL92/99 标准，可以说是访问任何关系型数据库的标准，基本所有的数据库都或多或少的支持;有些地方会把 DML 再拆分，将查询独立出来形成 DQL（数据查询语言） 准备工作安装就不说了，一搜一大把，卸载也有，Oracle 的卸载挺麻烦的，官方给的卸载工具是基本没人用，因为本身无法删除；一般我们把它称作是 Oracle 数据库服务器，听到这个名字其实指的就是平常说的数据库，但是下面我为了省劲直接说 Oracle 了然后就是 Oracle 其实包括两大部分，实例和数据库，并且必须通过实例才能操作数据库（SQL 语句是通过实例执行的），数据库可以理解为类，看得见的，比如前面的 DBF 文件，实例理解为对象，是看不见的。一个数据库就是一个目录，可在安装目录下的 oradata 文件夹下查看，Oracle 创建、删除数据库是非常复杂的，需要专门的工具，经过大约 12 步才能搞定。连接 Oracle 可以使用自带的 sqlplus (命令行，需要配环境变量)和 sqldeveloper，或者使用第三方的 PLSQL 安装完成后会创建一个超级管理员，用户名：sys，角色：dba；Oracle 可以使用账号密码登录也可以使用用户和角色登录；常用的普通用户还有 scott 和 hr，默认都是被锁的，注：从 12c 版本开始 scott 就没了（这其实是最早进入 Oracle 的员工的名字） 1234567891011-- 斜线前后：username/pwd，或者用角色登录：&gt; sqlplus / as sysdba-- 解锁alter user scott account unlock;-- 设置密码alter user scott identified by tiger;&gt; sqlplus scott/tiger-- or sqlplus 上面就是一些安装后基础的设置了 基本操作查看当前用户：show user;查询当前用户下所有的对象(tab 表每个用户都有)：select * from tab;执行最近一次命令（专有）：/查询表结构：desc tName;条件查询（其中一种，(不)包括12与18）：select * from users where age (not)between 12 and 18;包含关系：select * from users where age (not)in (8,12); 查询不重复的内容关键字一样是 distinct ，记住了。Oracle 中有一张叫 dual 的表，称之为哑表或者伪表，只会返回一行数据（比如查询日期：select sysdate from dual ，返回的格式反正很恶心）。单引号一般用于日期和字符串（字符串），双引号一般用于给列设置别名（常量），字符串与字段可以使用 || 来拼接。可以适用 spool d:/aa.sql 命令开启“日志记录”，最后执行 spool off; 这样期间执行的 sql 和结果都会保存到这个文件；与之对应的是读取 sql 文件执行命令：@ d:/a.sql; 。适用条件查询不等于某个数可以使用 a != 30 也可以使用 a &lt;&gt; 30 ；然后 Oracle 中的 where 好像是从右往左进行判断，MySQL 的企业版好像也是（关于效率，现在的版本会自动优化，先去识别筛选最有利的条件）。占位符和 MySQL 是一样的，下划线表示一个，百分号表示零或者多个，需要查询下划线的用转义：like &#39;abc\_&#39; escape &#39;\&#39; ;判断字段是否为 null 只能使用 is (not) null ，它不能参与精确比较，null 可以是任何类型，并且在排序的时候把 null 看做为最大的值；not 最好不要常用，能用其他代替的就不用，会影响效率。 格式化日期显示可以设置下环境变量：nls_date_format=YYYY-MM-DD HH24:MI:SS 常用函数并不保证在其他的数据库也能适用…..函数分为两类，单行函数（一进一出）和多行函数（多进一出），也就是传入参数个数的问题；用到了去查 API 也可以 NVL用法：NVL(a,b)，如果 a 为 null 就返回 b，如果 a 不为 null 就返回 a NVL2用法：NVL2(a,b,c) ，如果 a 不为 null 则返回 b ，否则取 c NULLIF用法：NULLIF(a, b) ，在类型一致的情况下，如果 a 和 b 相同则返回 null ，否则返回 a case…end 表达式SQL 99 标准里的所以 Mysql 也是支持的，用法： 1234567891011select name, age, case age when 12 then age+1 else age+2 end "测试"from users;case 字段 when 条件1 then 表达式1 else 表达式2end 这个…..我倒是真没用过，这个主要是用于显示，并不会进行写入 decode这个函数就是优化了上面的 case 表达式，是 Oracle 专用 123select name, age, decode(age,12,age+1,age+2)from users; 也就是说，decode 的第一个参数是字段最后一个是 else 的表达式，中间可以拼多个 when 单行函数常用的有：lower / upper / initcap ；分别对应为转小写、转大写、首字母大写；select upper(&#39;aaaabv&#39;) from dual;截取字符串使用 substr 函数，两个参数为：从第几个开始取（从 1 开始），取几个；计算长度用 length 或 lengthb 分别为字符和字节数，字节就和编码有关了；查找某个字符首次出现的位置 instr(‘hello’, ‘h’) ，返回的坐标是从 1 开始，找不到返回 0 。select trim(&#39;o&#39; from &#39;ooabcodoo&#39;) from dual; ：去掉两边的指定字符。select replace(&#39;abcd&#39;,&#39;a&#39;,&#39;t&#39;) from dual; ：把 abcd 中的 a 替换为 t。对数字的处理常用的有 round / trunc / mod 分别对应四舍五入、截取、取余(相当于n1/n2)，需要传入两个参数，第二个是精度，前两个可以用在日期类型上，并且日期是可以直接进行简单的四则运算的（sysdate + 1 就是加一天）。对日期处理的函数有 add_months / next_day / last_day(最后一天) ，日期的运算是可以负数，就是前一天或者上一个月了 多行函数最常见的多行函数就是 count(*) 了吧（这是SQL查行数的标准，内部会优化），因为会扫描所有字段，这也是不推荐用的原因，还不如用 count(1)，或者用主键，还可以使用类似 count(distinct dep) ；max / min / sum / avg 这些就更不用说了，常用的，不要忘了使用分组： 12345select deptno "部门编号" ,trunc(avg(sal), 0) "部门平均工资"from empgroup by deptnohaving trunc(avg(sal), 0) &gt; 2000order by 2 desc; 也是复习下 SQL，虽然和多行函数关系不是很大，记得 order 是放在最后的，因为 where 是先执行而 having 是后执行，所以能用 where 就用 where，避免查询不必要的数据；如果使用 order by ，那么 select 中的字段必须出现在 order by 中，但是反过来在 order by 中出现的不一定要在 select 中 类型转换隐式的转换自然就不需要说了，关键是显式的转换 日期转字符串示例：select to_char(sysdate, &#39;yyyy&quot;年&quot;mm dd day hh24:mi:ss&#39;) from dual; 不区分大小写 数值转字符串示例（比如货币）：select to_char(1234, &#39;L9,999&#39;) from dual; ，9 表示的是数字 字符串转日期和上面差不多，就是倒过来使用：select to_char(&#39;2017-11-06&#39;, &#39;yyyy-mm-dd&#39;) from dual; 多表查询最简单的笛卡尔集表（列数之和，行数之积，其中很多数据都是不合理的）：select * from emp,dept;其实这种方式在 SQL 标准中叫做交叉连接（CROSS JOIN） ，标准的语法是：SELECT * FROM emp CROSS JOIN dept;。 还有就是子查询，那个和 MySQL 基本一致就不多说了，可以用在 select 后也可以用在 where 后，非常的灵活；关于连接： 内连接查询只能查询出符合条件的记录（使用 where 对笛卡尔表进行过滤） 外链接查询既能查询出符合条件的记录，也能根据一方强行将另一方查询出来特别的提一下全（外）连接：FULL OUTER JOIN…ON; –&gt; 把两张表中没有的数据都显示 然后就来看看左外连接/右外连接吧，下面的这个方法是 Oracle 专用，总结一下就是：那边的数据少就在那边写 (+) 但是只能写在最后，从命名上，如果左边的数据多就叫左外连接….按照这个规则，那么可以总结为： (+)=：放在了等号的左边，表示的是右连接； =(+)：放在了等号的右边，表示的是左连接； 其实并不需要刻意的区分左和右，根据查询的结果如果发现有些需要的数据没有显示出来，就使用此符号更改连接方向就行了。 1234select dept.deptno "部门号" , dept.dname "部门名", count(emp.empno) "人数"from dept,empwhere emp.deptno(+) = dept.deptnogroup by dept.deptno, dept.dname; 然后就是比较难的自连接，只要设置好别名，一切都不是问题。涉及到子查询的，又分为两类： 单行子查询子查询只会返回一个结果，父查询使用 = 、&lt;&gt;、&gt;=、&lt;= 来连接 多行子查询子查询会返回多个结果，父查询使用 in、any、all 这些符号来比较 平时尽可能的使用多表查询（最终是查的一张笛卡尔表），效率相对子查询高一些，但是总的来说 多表查询的性能肯定不高 更多参考：https://www.cnblogs.com/mchina/archive/2012/09/07/2651568.html 优化首先，确定下他们之间的优先级：from 优先级最高；然后是 where 的优先级次之，group by 的优先级仅次于 where；having 紧随 group by 之后；order by 写在最后的，优先级是最低的；select 的优先级仅仅是高于 order by。对于进行连接查询的，首先进行连接然后再对结果进行 where 过滤，所以在连接之前尽量使用 ON 后的条件进行筛选。 having 的效率极低，能不用就别用，前面尽量用 where 筛选好。 因为 in 的效率低，所以用 exists 代替；in 的效率低是因为需要对后面的内容进行一一匹配，当然 exists 并不是完全能替代 in，只是尽量少使用 in（not in）：select * from school s where exists (select * from student st where st.sid = s.id); 什么时候使用多表连接？什么时候子查询？ 如果需要查询的数据在多个表中，一定要使用多表连接 不需要表 A 中的列，但是又有该表 A 的条件，可以用子查询 子查询中如果使用了 in、some any、all 这几个关键字，效率比较低，可以考虑转换成多表关联的方式 稍微总结一下就是： 建议不用 * 来代替所有列名 用 truncate 代替 delete 在确保语句完整性的情况下多用 commit 语句（用在 begin..end 中） 尽量减少表的查询次数（少用子查询） 用 not exists 代替 not in 表连接时，把能过滤多数据的连接放在右边，执行方式是先 ON 后 where，从右往左执行（新版会自动识别，并不需要刻意排序） 合理使用索引 sql 语句尽量用大写的，oracle 总是先解析 sql 语句，把小写的转换成大写的在执行 连接多个表时尽量使用表的别名，减少解析时间 优化 group by，将不需要的记录在 group by 之前过滤掉 SQL 的效率查询，也就是慢查询： 1234567SELECT EXECUTIONS,DISK_READS,BUFFER_GETS, ROUND((BUFFER_GETS-DISK_READS)/BUFFER_GETS,2) HIT_RADIO, ROUND(DISK_READS/EXECUTIONS,2) Reads_per_run,SQL_TEXT FROM V$SQLAREA WHERE EXECUTIONS &gt; 0 AND BUFFER_GETS &gt; 0 AND (BUFFER_GETS-DISK_READS)/BUFFER_GETS &lt; 0.8 ORDER BY 4 DESC; Mysql 中也有类似的命令，通过这个来排除那些执行慢的 SQL。 集合查询关键字：并集：union（只取相交部分的一个）、union all (相交部分的全部，即使重复)交集：intersect差集：minus（在第一个并且不在第二个中的）这些关键字是用在两条查询语句中的，也就是连接两条 select 语句的。需要注意的有： 集合操作时，必须保证集合的列数是相等的 集合操作时，必须保证集合的列类型是对应相等的 多个集合操作时，结果的列名由前一个集合决定 A UNION B UNION C = C UNION B UNION A 从选择来说，优先级：多表查询 –&gt; 子查询 –&gt; 集合查询 分页在 Oracle 中没有 limit 关键字，分页还是蛮复杂的；说到这里就得说 Oracle 中的关键字（隐藏列） rownum，它可以当做一个字段写在 select 语句中，与表同存在，并且是一直递增（在一次的查询中，从 1 开始），是 number 类型，能参与运算；需要注意的是 &gt; 、 &gt;= 、 = 都是无值的（0 除外，&gt;=1 也可以），因为 rownum 是随着记录变化的，后面的可能是无限，无法确定的就返回空，并且 &lt;&gt; 和 &lt; 是一样的。也就是说它只能参与小于或者小于等于的运算。使用子查询进行分页(查询 2-8 的记录)： 123select xx.*from (select rownum id ,emp.* from emp where rownum &lt;= 8) xxwhere id &gt;= 2; 注意：子查询中加别名不能使用双引号了。 分页查询中，不要直接使用 order by 进行排序，这样会打乱 rownum 的顺序，如果确实需要按照某个字段来排序，那么只能使用嵌套子查询，比如：select rownum,a.* from (select rownum rn,name from (select * from test order by name) where rownum&lt;=30) a where rn&gt;=21; PS：还有一个隐藏列是 rowid ，它映射每一行数据物理地址的唯一标识，通常适用于删除完全重复的数据。例如：delete from lyric where rowid not in (select min(rowid) from lyric group by content); 批量操作基本和 MySQL 保持一致吧，或许是 SQL99 的标准吧；参考其他表结构来创建表，但是不要数据： 12create table emp asselect * from back_emp where 1 &lt;&gt; 1; 其他的有批量从其他表插入数据的，当然要保证列的类型能对应才行，都写在下面吧： 123456insert into emp select * from back_emp;-- 选择添加insert into emp(id,name) select id,name from back_emp; 基本都是一个套路，差不多的 关于事务事务的开始不需要手动执行某条语句（MySQL 是 start transaction），它默认是从第一条 DML 操作（增删改查）作为事务的开始。提交就是直接输入 commit ，回滚就是 rollback 都一样的了。关于事务的提交，除了使用上面的显式 commit，还可以是 DDL/DCL/exit（sqlplus 工具） 语句，也就是说当你执行一条 DDL 语句默认会先进行事务提交。同样也是有隐式回滚的，当关闭窗口、死机、停电等情况时就会进行自动的回滚。 下面就是回滚点的知识了，让我们能回滚到一个事务中的指定位置，看个栗子就知道了： 1234567891011select * from emp;delete from emp where id = 1;savepoint a; -- 设置回滚点delete from emp where id = 2;rollback to savepoint a;commit;rollback;commit; 回滚点多了也不好，起码会占用空间啊，MySQL 也是一样的；隔离级别我就不说了，前面说过，Oracle 只支持两个 表操作介绍常用的表修改操作，比如修改表名、增加字段啥的： 123456789101112131415161718-- 如何修改已经创建的表名alter table student rename to teacher;-- 建表以后 增加字段alter table student add email varchar2(20);-- 建表以后 修改字段名alter table student rename column email to yx;-- 建表以后 修改字段数据类型alter table student modify name varchar2(10);-- 建表以后 删除字段alter table student drop column yx;-- 建表以后 删除表drop table student purge;truncate table student; 关系比较中 &lt;&gt; 是标准语法，可以移植到其他任何平台，!= 是非标准语法，可移植性差。 约束相关最简单的，约束可以分为这几类：非空：not null唯一：unique检查：check外键约束：foreign key （references）主键约束：primary key举个例子： 12345678910111213141516create table class( id number(3) primary key, name char(6) not null unique, qq varchar2(20) check(length(qq)&gt;=8), tid number(3) references teacher(id));-- 如何添加约束：alter table test add constraint aa primary key(tid);-- 如何查看已经存在的约束：select constraint_name,constraint_type from user_constraintswhere table_name = upper('test');--如何删除约束：alter table test drop constraint SYS_C0011126; 创建外键的时候，FOREIGN KEY 关键字是可以省略的，直接写 REFERENCES 。 序列sequence 一个单独的数据对象，是一个能够生成有序的整数列值的对象；oracle 通过调用序列的形式来实现主键自增。关于序列的一些常规操作： 12345678910111213141516171819202122232425262728create sequence seq_testincrement by 1/-1 --增长，一次增1 正数+1 负数-1start with 1 --从1开始 升序默认就是+1 降序-1minvalue 1 --最小值 -10的26次幂maxvalue 100 --最大值 10的27次幂cycle --循环 默认 nocyclenocache; --不缓存 默认生成20个序列号-- 查询当前用户下有多少序列select * from user_sequences;-- 如何获取下一个列的值insert into test values(seq_test.nextval,'g');-- 如何获取当前序列的值select seq_test.currval from test;-- 如何删除序列drop sequence seq_test;-- 如何修改序列alter sequence seq_test increment by 50;-- 消除延迟段创建特性alter session set deferred_segment_creation=false;create table name(id number) segment creation immediate; 关于最后一个 消除延迟段创建特性 ，这是因为在 11g+ 的版本中有了一个新特性，它会导致序列直接从 2 开始，尚不知道有什么用处，解决方案可以在建表的时候就指定立即使用序列。 视图视图 view 可以简单理解为是一张假表，用查询的结果动态生成一张表。视图是编译后将查询语言保存到数据库中，下次调用视图，可以不用在编译，直接执行。创建一个简单的视图：create view 视图名 as select * from student;为什么要使用视图？ 节省编译时间，提高查询效率 屏蔽原表中的字段：避免没有权限的用户查询其他的字段（看到不该看的） 视图中能够根据原表的状态动态刷新 简单的视图是可以更新（插入等）原表中的数据 复杂的视图无法更新（插入等），因为涉及到多个表 需要注意的一点是创建视图后也许并不会马上进行编译，或者后来表结构发生了变化视图并不会进行更新，如果想立即生效可以尝试进行手动更新：alter view 视图名 compile; 索引数据库会在具有唯一性的列上自动添加唯一性索引，索引包含：普通索引（normal）、唯一索引（unique）、位图索引（bitmap）、函数索引。正常操作： 123456789101112131415161718192021222324-- 创建索引create index 索引名 on 表名(字段);-- 查询索引select index_name,table_name,uniqueness,statusfromuser_indexeswhere table_name = 'STUDENT';-- 修改索引alter index index_name rename to new_index_name;-- 删除索引drop index 索引名;-- 唯一性索引create unique index 索引名 on 表名(列名);-- 位图索引（分类），这种索引适合用在数据量比较大，基数比较小的列（比如：男/女）create bitmap index 索引名 on 表名(列名);-- 函数索引：在一个列上经过函数计算后的结果上创建索引create index 索引名 on 表名(函数(列名)); 创建索引的优缺点： 能够更快的帮助我们进行提高查询效率 增删改表中的数据，数据库就需要耗费资源去维护索引，降低增删改的效率 数据量如果很少，没必要用索引 数据量比较大，不需要经常增删改操作而且查询比较多，适合使用索引 存储过程&amp;触发器什么是存储过程，简单说就是在服务器端，能够被一个或多个应用程序调用的一段 sql 语句集（已被预编译）。存储过程的创建等操作和 MySQL 中也大同小异，看些例子： 123456789101112131415161718192021222324252627282930313233343536373839-- 创建/覆盖存储过程create or replace procedure过程名（参数名 in 参数类型，参数名 out 参数类型）as变量名 变量类型 := 值;beginsql语句集;end;create or replace procedurepro_hi(mykey in number,value out varchar)asbeginif mykey = 1then value := '你好';else if mykey = 2then value := '再见';end if;end if;end;/-- 调用存储过程declare 变量 类型:=初始值;begin过程名（参数，变量）;end;set serveroutput on;declareval varchar2(20):='';beginpro_hi(1,val);dbms_output.put_line(val);pro_hi(2,val);dbms_output.put_line(val);end; 其中 := 的意思是赋值，存储过程可以没有参数，如果没有参数则过程名之后不能出现括号。 触发器是在数据库中，在执行对数据有异动的动作时，先行拦截并处理的一种数据库对象，它大部份会设在数据表中，作为强制运行特定动作的程序，因此又称为数据操纵语言触发器 或者可以理解为执行某个操作时自动触发执行存储过程？比如用来日志的记录和主键的自动增长。触发器可具体分为行级触发器和表级触发器，具体的简单使用： 12345678910111213141516171819202122232425262728293031323334353637-- 创建触发器：create trigger 触发器名before/after insert/update/delete on 表名for each rowbeginsql语句集;end;-- 创建一个行级触发器，当执行插入操作时，自动查询序列的下一个值-- 将查询到的值赋给表中的 id 列create or replace trigger tri_insert_goodbefore insert on goodfor each rowbeginselect seq_ids.nextvalinto:new.idfrom dual;end;/-- 增加数据insert into good (good_name) values ('loli');--创建表级触发器，用于记录日志create trigger test_deptbefore delete or insert or update on deptdeclare var_tag varchar2(20); --声明一个变量beginif inserting thenvar_tag :='insert';elsif updating thenvar_tag :='update';elsif deleting thenvar_tag :='delete';end if;insert into test_log values(var_tag,sysdate);end;/ 使用 elsif 的形式末尾只需要一个 end if 就可以了，如果使用的是 else if 的形式，那么有多少个 if 就要对应多少个 end if。 其他默认情况下，删除表会放进回收站，通过命令 show recyclebin; 可以查看回收站。还原表：flashback table name to before drop; 或者 flashback table name to before drop rename to newName;彻底删除表：drop table users purge;清空回收站：purge recyclebin; 数据类型中的 varchar2(num) 中的 num 指的是字节数。关于约束，Oracle 有专用的 check 约束如果使用 sqlplus 工具进行连接，还可以使用占位符，它会提示你输入相关信息，占位符用 &amp; 符号加描述就可以了，用的应该也不是很多 关于清空表数据可以用传统的 delete 语句，它是从上往下删，速度比较慢，属于 DML 语句，可以进行回滚；还可以使用表截断：truncate table ，速度快属于 DDL，不可回滚，不支持 where。还有就是 drop 语句了，这个干的很彻底，属于 DDL，如果删错了，那就跑路吧。drop table student purge;删除空数据的用 where xx is null ，因为不确定不能用 = 号 查看自己的权限：select * from user_sys_privs; 表空间oracle 中的用户都有属于自己的默认的空间，在一段内存空间中大部分存储的是表，所以称为表空间。表空间可简单的分为：系统用户的表空间和普通用户的表空间。那么，为什么要创建用户的表空间呢？项目中很可能与其他项目使用同一个数据库，多个用户在使用同一个数据库的时候有可能访问同一个数据库文件，就会造成资源争用问题，给不同的用户指定不同的表空间，就可以让他们使用不同的数据文件，解决争用问题。数据最终存储在数据块中，从小往大的顺序是：数据块=&gt;盘区=&gt;段=&gt;数据文件=&gt;表空间。相关操作： 12345678910111213141516171819202122232425262728293031/* 第1步：创建临时表空间 */create temporarytablespace user_temp tempfile 'D:\app\user_temp.dbf' size 50m autoextend on next 50m maxsize 20480m extent management local; /* 第2步：创建数据表空间 */create tablespace user_data logging datafile 'D:\app\user_data.dbf' size 50m autoextend on next 50m maxsize 20480m extent management local; /* 第3步：创建用户并指定表空间 */create user username identified by password default tablespace user_data temporary tablespace user_temp; /* 第4步：给用户授予权限 */grant connect,resource,dba to username;-- 创建用户后修改alter user et1803 default tablespace 表空间名;-- 删除表空间drop tablespace 表空间名 including ontents and datailes; 删除表空间后，原先指向该表空间的用户仍然默认的空间位置，需要通过 alter user 命令将用户的表空间指向一个有效的表空间。 truncate 、delete与drop区别 https://www.cnblogs.com/8765h/archive/2011/11/25/2374167.html 相同点： truncate 和不带 where 子句的 delete、以及 drop 都会删除表内的数据。 drop、truncate 都是 DDL 语句(数据定义语言)，执行后会自动提交。 不同点： truncate 和 delete 只删除数据不删除表的结构(定义)，而 drop 语句将删除表的结构被依赖的约束(constrain)、触发器(trigger)、索引(index)；依赖于该表的存储过程/函数将保留,但是变为 invalid 状态。 delete 语句是数据库操作语言(dml)，这个操作会放到 rollback segement 中，事务提交之后才生效；如果有相应的 trigger，执行的时候将被触发。truncate、drop 是数据库定义语言(ddl)，操作立即生效，原数据不放到 rollback segment 中，不能回滚，操作不触发 trigger。 delete 语句不影响表所占用的 extent，高水线(high watermark)保持原位置不动；drop 语句将表所占用的空间全部释放。truncate 语句缺省情况下见空间释放到 minextents个 extent，除非使用reuse storage；truncate 会将高水线复位(回到最开始)。 速度，一般来说: drop&gt; truncate &gt; delete 安全性：小心使用 drop 和 truncate，尤其没有备份的时候.否则哭都来不及 delete 是 DML 语句,不会自动提交。drop/truncate 都是 DDL 语句,执行后会自动提交。 TRUNCATE TABLE 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 TRUNCATE TABLE 删除表中的所有行，但表结构及其列、约束、索引等保持不变。 TRUNCATE TABLE 不能用于参与了索引视图的表。 附:Select 执行顺序 http://www.cnblogs.com/likeju/p/5039128.html 全文的链接在上面，这里摘出其中的一点：注意多表连接的连接条件的选择与表示。多表连接的连接条件对索引的选择有着重要的意义，所以我们在写连接条件条件的时候需要特别注意。 多表连接的时候，连接条件必须写全，宁可重复，不要缺漏。 连接条件尽量使用聚集索引 注意 ON、WHERE 和 HAVING 部分条件的区别：ON 是最先执行， WHERE 次之，HAVING 最后；因为 ON 是先把不符合条件的记录过滤后才进行统计，它就可以减少中间运算要处理的数据，按理说应该速度是最快的，WHERE 也应该比 HAVING 快点的，因为它过滤数据后才进行 SUM 考虑联接优先顺序： INNER JOIN LEFT JOIN (注：RIGHT JOIN 用 LEFT JOIN 替代) CROSS JOIN 其它注意和了解的地方有： 在 IN 后面值的列表中，将出现最频繁的值放在最前面，出现得最少的放在最后面，减少判断的次数 注意UNION和UNION ALL的区别。–允许重复数据用UNION ALL好 注意使用DISTINCT，在没有必要时不要用 TRUNCATE TABLE 与 DELETE 区别 减少访问数据库的次数 查询语句的执行顺序：1、FROM 子句：执行顺序为从后往前、从右到左。数据量较少的表尽量放在后面。2、WHERE子句：执行顺序为自下而上、从右到左。将能过滤掉最大数量记录的条件写在WHERE 子句的最右。3、GROUP BY：执行顺序从左往右分组，最好在GROUP BY前使用WHERE将不需要的记录在GROUP BY之前过滤掉。4、HAVING 子句：消耗资源。尽量避免使用，HAVING 会在检索出所有记录之后才对结果集进行过滤，需要排序等操作。5、SELECT子句：少用号，尽量取字段名称。ORACLE 在解析的过程中, 通过查询数据字典将号依次转换成所有的列名, 消耗时间。6、ORDER BY子句：执行顺序为从左到右排序，消耗资源。 附:函数整理补充单行函数： ceil()：返回大于等于x的最小整数 floor()：返回小于等于x的最大整数 round()：四舍五入round(a1,a2) 保留指定位小数位的四舍五入 trunc()：直接截断trunc(a1,a2)：保留指定位数的小数 sign()：求符号位 正数：1 负数：-1 零返回0 power(a,b)：求a的b次方 sqrt()：求正平方根 转换函数： to_number(C)：将一个字符类型的数字变成数值类型 to_char()：将数字转换为字符串常用在货币单位，格式化字符串日期转换：to_char(日期，&#39;yyyy-MM-dd HH:mm:ss&#39;) OR to_char(systimestamp,&#39;yyyy-mm-dd hh24:mi:ss:ff3&#39;) to_date(c1,c2)c1:字符类型的日期c2:格式，例如： to_date(‘2018-03’,’yyyy-mm’)日期可以加减（整数）运算 ，单位是：天，但是日期没法相加 months_between(c1,c2) 计算两个日期之间的月份，就是 c1-c2 last_day(c1) 计算给定日期的所在月份的最后一天 next_day(c1,c2) ，距离周几最近的日期c1:日期c2:周中的某天 字符函数： lower():转换成小写 upper():转换成大写 initcap():首字母大写 length():求长度 substr(c1,c2,c3) 截取字符串c1:被截取的字符串c2:从哪个位置开始截取c3:截取长度 默认截取到最后 instr(c1,c2,c3,c4) 索引字符串c1:被查询的字符串c2:希望找到的字符c3:从哪个位置开始找 默认是1c4:第几次出现 concat(c1,c2) 拼接字符串 lpad(c1,c2,c3) 左侧补全c1:希望补全的字符串c2:补全到多少位c3:以哪个字符来补全 rpad(c1,c2,c3) 右侧补全 trim(c1) 默认c1的两侧去除空格trim(c1 from c2) 把c2的两侧移除指定的c1 ltrim(c1,c2) 左侧去除c1:被去除的字符串c2:去除的字符串 默认是空格 rtrim(c1,c2) 右侧去除 replace(c1,c2,c3) 完全替换c1:原字符串c2:被替换的字符串c3:替换的字符串 通用函数： nvl()：空值处理nvl(字段，替换显示的内容) nvl2():空值处理二代nvl2(字段，不是空显示什么，是空显示什么) wm_concat(column) 字段合并select u_id, wmsys.wm_concat(goods || &#39;(&#39; || num || &#39;斤)&#39; ) goods_sum from name group by u_id用法详细展示：http://www.cnblogs.com/yangxia-test/p/4272493.html 用于 to_char(numeric) 的模板 ： 模板 描述 9 带有指定位数的值 0 前导零的值 . （句点） 小数点 , （逗号） 分组（千）分隔符 PR 尖括号内负值 S 带负号的负值（使用本地化） L 货币符号（使用本地化） D 小数点（使用本地化） G 分组分隔符（使用本地化） MI 在指明的位置的负号（如果数字 &lt; 0） PL 在指明的位置的正号（如果数字 &gt; 0） SG 在指明的位置的正/负号 RN 罗马数字（输入在 1 和 3999 之间） TH or th 转换成序数 例子： 输入 输出 to_char(now(),’Day, HH12:MI:SS’) &#39;Tuesday , 05:39:18&#39; to_char(now(),’FMDay, HH12:MI:SS’) &#39;Tuesday, 05:39:18&#39; to_char(-0.1,’99.99’) &#39; -.10&#39; to_char(-0.1,’FM9.99’) &#39;-.1&#39; to_char(0.1,’0.9’) &#39; 0.1&#39; to_char(12,’9990999.9’) &#39; 0012.0&#39; to_char(12,’FM9990999.9’) &#39;0012&#39; to_char(485,’999’) &#39; 485&#39; to_char(-485,’999’) &#39;-485&#39; to_char(485,’9 9 9’) &#39; 4 8 5&#39; to_char(1485,’9,999’) &#39; 1,485&#39; to_char(1485,’9G999’) &#39; 1 485&#39; to_char(148.5,’999.999’) &#39; 148.500&#39; to_char(148.5,’999D999’) &#39; 148,500&#39; to_char(3148.5,’9G999D999’) &#39; 3 148,500&#39; to_char(-485,’999S’) &#39;485-&#39; to_char(-485,’999MI’) &#39;485-&#39; to_char(485,’999MI’) &#39;485&#39; to_char(485,’PL999’) &#39;+485&#39; to_char(485,’SG999’) &#39;+485&#39; to_char(-485,’SG999’) &#39;-485&#39; to_char(-485,’9SG99’) &#39;4-85&#39; to_char(-485,’999PR’) &#39;&lt;485&gt;&#39; to_char(485,’L999’) &#39;DM 485 to_char(485,’RN’) &#39; CDLXXXV&#39; to_char(485,’FMRN’) &#39;CDLXXXV&#39;]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis入门]]></title>
    <url>%2F2017%2F10%2F28%2FRedis%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Redis 是一个使用 ANSI C 编写的开源、支持网络、基于内存、可选持久性的键值对存储数据库。从 2015 年 6 月开始，Redis 的开发由 Redis Labs 赞助，而2013年5月至2015年6月期间，其开发由 Pivotal 赞助。在2013年5月之前，其开发由 VMware 赞助。根据月度排行网站 DB-Engines.com 的数据显示，Redis 是最流行的键值对存储数据库 数据模型Redis 的外围由一个键、值映射的字典构成。与其他非关系型数据库主要不同在于：Redis 中值的类型不仅限于字符串，还支持如下抽象数据类型： 字符串列表 无序不重复的字符串集合 有序不重复的字符串集合 键、值都为字符串的哈希表 值的类型决定了值本身支持的操作。Redis 支持不同无序、有序的列表，无序、有序的集合间的交集、并集等高级服务器端原子操作 更多的基本信息可以查看 wiki：https://zh.wikipedia.org/wiki/Redis 关于缓存&amp;NoSQL我们所听说的 Redis 都基本是用于缓存，缓存也是非常必要的，除了 Redis 还有一种缓存技术是使用 Memcached ，它出现比 Redis 早，功能比较单一。但从性能方面来说，也就是从缓存的命中率来说 Memcached 的性能更好，但是和 Redis 的差距并不大，但是 Redis 所提供的功能就更强大了。重要区别： Memcached 是多线程的 Redis 是单线程的 关于 NoSQL 就不多说了，比较出名的 NoSQL 产品还有 MongoDB ，它是基于 JSON 来存储的，这样就能做到不同的记录字段的长度不同 NoSQL 指的是 Not only SQL，也就是不仅仅是 SQL，读作 N、 O、 SQL，不要读 no SQL。它一般用于高并发的读写，这样的情况传统的关系数据库搞不定，并且有高扩展（横向）、高可用的特性。具体到 Redis 它虽然查询速度快但是结构性不强，并没有什么两全其美的东西。应用：缓存、任务队列、排行榜、数据过期处理、分布式集群的 Session 分离 版本安装官方其实并没有提供 Windows 版的（厉害），微软基于官方发布的版本进行编译创建了 Windows 的分支，所以一般情况 Windows 版都是比较落后的。win 下双击运行即可，无需安装，cli 是客户端，默认会进行连接，使用的是 6379 端口，为什么用这个是有一段故事的，想知道就自己 Google 下吧。另外可以直接安装到服务，这样就不用每次还得打开那个黑窗口了，安装方法在下载包的文档里有现成的，删除服务命令是：sc delete XXName启动 Redis 后会创建一个 dat 文件，大小和本机的内存一样，但是一般测试没必要搞这么大，在配置文件的 maxmemory 可以修改，例如：maxmemory 200mb渣渣的我只能先玩 win 版的了….在我下载的 3.2 版本中需要手动进入命令行指定配置文件才能运行：redis-server.exe &quot;redis.windows.conf&quot;退出命令：redis-cli.exe shutdown Linux下载源码后首先编译 make，为什么编译安装都造，性能好，需要有相应的 gcc 库才可以哦然后就可以进行安装了：make PREFIX=/usr/local/redis install下一步拷贝 conf 配置文件到安装目录就可以了，在配置文件中最好设置为 daemonize yes这样启动就是后台启动了，启动命令：redis-server ./redis.conf可以使用 ps -ef | grep -i redis 来查看是否启动成功。关闭推荐使用：redis-cli shutdown ，强制关闭：kill -9 pid 基本命令除了使用 cli 自动连接，也可以手动进行连接：redis-cli -h 127.0.0.1 -p 6379然后可以用 ping 来测试下是否正常，正常情况它会回你一个 pong Redis 默认有 16 个数据库，以数字命名，从 0 开始，并且是不支持修改的，数量可以在配置文件中设置（database）使用 SELECT 命令来切换数据库，例如：SELECT 0数据库直接并不是完全隔离的，也就是说当执行 FLUSHALL 命令时，会清空所有数据库的数据，如果只想清空当前数据库的数据，要执行 FLUSHDB并且，官方建议不设置数据库的密码，安全应该由服务器来保证（并且也不支持设置单个数据库的密码） 下面就开始看看使用频率很高的命令 : keys前面说过 Redis 是以键值对存储的，可以想象为一个大 Map，这个命令也就是查询键了！用法如：KEYS * 查询所有;也可以使用通配符，有四种，? 、 * 、 [] 以及转义符号，至于什么意思，学过正则的都知道哈; exists判断 key 是否存在，存在返回 1 不存在返回 0 。 del可以删除一个或者多个 key，返回的是删除键的个数，键删除了相应的值自然也删除了，删除多个以空格分开. type获得键值的数据类型，返回的值可能是 string、hash、list（列表）、set（集合）、zset（有序集合） rename就是 key 的重命名了，特别的如果重命名新名称已存在会直接覆盖，所以 Redis 中还有大量的以 nx 结尾的命令，nx 结尾的命令都会进行一些判断，例如：renamenx a b 当 b 这个 key 已经存在时，此操作就不会生效，返回的是 0. set/get/setex设置、获取 key；setex 是 set 和 expire 的简写，可以顺便设置生存时间，例如：setex a 100 a ；类似的还有 psetex 只不过时间单位成了毫秒。 getrange获取值的指定范围的内容，例如 getrange key 0 2 就是获取第 0 个到第 2 个，是个闭合区间，包含 0 和 2； getset特点就是先 get 再 set，相当于在 set 的时候把旧的值拿出来了。 expire设置 key 的生存时间，默认单位为秒，到期后会自动销毁 ttl查看 key 剩余的生存时间，默认单位秒，如果返回 -1 表示无限制，返回 -2 表示不存在。 randomkey获取一个随机的 key flash/flashall清空当前（全部）数据库 info查看 Redis 的一些运行信息 help帮助命令，就不多说了，教给你命令怎么用，有种用法是：help 空格 [tab] 字符串数据类型字符串数据类型是 Redis 最基本的数据类型了，它能存储任何形式的字符串，包括二进制数据；允许存储的数据容量最大 512 MB存取字符串用的就是 set/get 命令了，还有一个 MGET/MSET 这个命令可以批量读取/设置值（MSET k1 v1 k2 v2）；特别的这个指令也支持 msetnx ，这有点像事务了，如果其中一个 key 已存在，那么整个批量操作都会失败。INCR 是递增命令，并且会返回递增后的值，默认每次递增的是 1，如需特殊指定就是 INCRBY name 2 这样就会每次递增二，如果不存在就会先初始化为 0 然后再递增；相应的 DECR 就是递减了，比如： DECR id 就是递减 id 这个 key，默认也是每次一，同样也可以指定递减多少，用法和上面一样；APPEND 是往尾部追加内容，用在这里就是追加字符串内容，比如：APPEND name 233 ，返回是的追加后的字符串的长度。STRLEN 获取字符串的长度，没啥可说的。还有一个是 getset 先获取值再设置值 Hash其结构可以比作 Java 里的 Map&lt;String, String&gt;常用的几个命令有：获取/存储值 123456hset key name1 value1hget key name1# 批量存储/获取hmset key name1 value1 name2 val2hmget key name1 name2 其他的一些指令就一起说了吧： 1234567891011121314151617181920# 获取全面的属性和值hgetall key# 删除多个值，删除 key 用 del 哦hdel key name1 name2# 递增递减都差不多，举一个栗子，前提是 age 这个值（val）要有，并且是数字类型hincrby key age 5# 判断属性是否存在hexists key name# 属性的个数hlen key# 获取所有的属性名hkeys key# 获取所有的值hvals key 常用的一般就是这些吧 List同样它也有两种形式，数组的和链表的；特点和数据结构中的也一致 12345678910111213141516171819202122232425262728293031323334# 两端添加，一个是左边一个是右边lpush key a b c # c 就在最左边rpush key a b c# 两端弹出rpop keylpop key# 查看listlrange key start end例如: lrange key 0 -1 # 左边数，第一个到最后一个# 查看长度llen key# 插入到头部，如果 key 不存在就不插入，不会自动创建lpushx key arpushx key a# 删除，count 的负号表示方向lrem key count vallrem key 2 3 # 删除2个3lrem key -2 1 # 从后面（负号）删除2个1lrem key 0 2 # 删除所有的2# 修改值lset key 3 v # 在第四个位置修改为 v；从 0 开始数# 插入linsert key before b ll # 在 b 的前面插入 lllinsert key after b ll# 其他rpoplpush key1 key2 # 把 key1 的右边弹出，添加到 key2 的左边 push 返回的是长度，pop 返回的是弹出的元素，rpoplpush 这种命令非常适合用于 MQ Set相当于无序的 List，并且是唯一的常用的命令有： 1234567891011121314151617181920212223242526272829# 添加删除sadd key a b csrem key c# 查看值smembers key # 是否存在？存在返回 1，不存在返回 0sismember key a# 相差比较，和顺序有关，可以理解为 key1 - key2sdiff key1 key2# 求交集、并集sinter k1 k2sunion k1 k2# 查看数量scard key# 获取一个随机值srandmember key# 移除一个随机元素，并返回（比如可做订单号缓存池）spop key# 其他sdiffstore k1 k2 k3 # k2 和 k3 的交集存到 k1sunionstore k1 k2 k3 # k2 和 k3 的并集存到 k1 在做并集交集的处理时非常有优势，因为服务器端的聚合效率更高。内部还是使用哈希表来实现，时间复杂度是 O(1) Sorted-set它存的都是字符串的内容，并且有一个分数与之关联，可用来排序，分数是可重复的，值不可以 123456789101112131415161718192021222324252627282930# 增加、删除数据（返回插入元素的个数，已有的值分数会被覆盖）zadd key 10 n1 20 n2zrem key n1 n2# 获取分数zscore key name# 获取成员的数量zcard key# 范围查找zrange key 0 -1zrange key 0 -1 withscores # 显示分数zrevrange key 0 -1 # 从大到小进行排序，默认是从小到大# 按照范围删除（插入顺序）zremrangebyrank key 0 4# 按照分数的范围删除zremrangebyscore 80 100# 按照分数排序zrangebyscore key 0 100zrangebyscore key 0 100 withscores limit 0 2 # 显示部分数据# 操作分数zincrby key 3 name # 给 name 的分数 +3# 计算区间，80-90 之间有几个zcount key 80 90 可以用来做热点话题和游戏排名之类的；同上，使用哈希表实现，通过分数来保证顺序（默认从小到大），时间复杂度也是 O(1) 生存时间前面说过，Redis 基本是用来做缓存的，并且它是基于内存的，所以当然有必要设置生存时间了；设置生存时间（PEXPIRE 可以设置毫秒）：EXPIRE Key seconds然后可以使用 TTL 来查询，返回的是剩余的生存时间，单位是秒；如果是没有限制返回的是 -1；数据已删除是 -2清除生存时间(重新设置值也会清除生存时间)：PERSIST key Java客户端然后下面就说说在 Java 中的使用，有很多的 Java 客户端支持 Redis，当然这都不是官方的，其中用的比较多的是 jedis；它的使用也非常的简单，导入相应的依赖： 1234567&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 然后最简单是使用： 123Jedis jedis = new Jedis("localhost");jedis.set("foo", "bar");String value = jedis.get("foo"); 它有一个好处是方法和命令的名字是一致的，这是用起来最爽的，后面还会涉及到连接池和集群，具体可以参考 GitHub 上的示例代码如果使用的是集群（下面会讲）就使用下面的代码操作： 1234Set&lt;HostAndPort&gt; jedisClusterNodes = new HashSet&lt;HostAndPort&gt;();jedisClusterNodes.add(new HostAndPort("127.0.0.1",6379));JedisCluster jc = new JedisCluster(jedisClusterNodes);jc.set("foo","bar"); 这里的 jc 不需要手动关闭，内部已经自动关闭了 持久化Redis 提供了两种持久化的方式，一种是 RDB （默认开启），另一种是 AOF，它们既可以单独使用也可以混合使用。RDB 方式是通过快照完成的，当符合一定条件时Redis会自动将内存中的所有数据进行快照并且存储到硬盘上。进行快照的条件在配置文件中指定，有2个参数构成：时间和改动的键的个数，当在指定时间内被更改的键的个数大于指定数值时就会进行快照。默认在配置文件中已经有一些配置了，就是 save 开头的；除了自动也可以手动保存，使用 SAVE 或者 BGSAVE 命令，区别就是一个在主进程（会阻塞）一个会 fork 一个子线程进行（需要的物理内存是 Redis 设置的内存的一倍） Redis 的 AOF 持久化策略是将发送到Redis服务端的每一条命令都记录下来，并且保存到硬盘中的AOF文件，AOF文件的位置和RDB文件的位置相同，都是通过dir参数设置，默认的文件名是appendonly.aof，可以通过appendfilename参数修改。可以使用 BGREWRITEAOF 命令来重写 AOF 文件 参数介绍auto-aof-rewrite-percentage 100当前的AOF文件大小超过上一次重写时的AOF文件大小的百分之多少时会再次进行重写，如果之前没有重写过，则以启动时的AOF文件大小为依据。auto-aof-rewrite-min-size 64mb限制了允许重写的最小AOF文件大小，通常在AOF文件很小的时候即使其中有些冗余的命令也是可以忽略的。 文件写入默认情况下会先写入到系统的缓存中，系统每30秒同步一次，才是真正的写入到硬盘，如果在这30秒服务器宕机那数据也会丢失的，Redis可以通过配置来修改同步策略：appendfsync always 每次都同步 （最安全但是最慢）appendfsync everysec 每秒同步 （默认的同步策略）appendfsync no 不主动同步，由操作系统来决定 （最快但是不安全） 主从复制（读写分离）和数据库的读写分离是类似的，主要是解决读取压力过大的问题，以及….避免宕机相比来说，比 MySQL 数据库要简单的多，如果是在一台机器做测试，除了修改端口 pidfile 也要改，不能相同 设置主从： redis.conf 配置文件中设置 slaveof 客户端内执行：slaveof &lt;masterip&gt; &lt;masterport&gt;重启后将会失效 然后可以使用 NFO replication 查看关系，默认的从库只能读取不能写入，这样更合理些，除了设置一主多从还可以设置主从从的架构，就是 A 的从是 B ，B 的从是 C，这样就减轻了主库的压力同步的原理： 当从库和主库建立MS关系后，会向主数据库发送SYNC命令； 主库接收到SYNC命令后会开始在后台保存快照（RDB持久化过程），并将期间接收到的写命令缓存起来； 当快照完成后，主Redis会将快照文件和所有缓存的写命令发送给从Redis； 从Redis接收到后，会载入快照文件并且执行收到的缓存的命令； 之后，主Redis每当接收到写命令时就会将命令发送从Redis，从而保证数据的一致； 如果硬盘的 IO 不好会拖累性能，从 2.8.18 的版本开始，Redis 支持无盘复制，就是直接通过网络传输，不过这个功能目前好像不是很稳定，开启无磁盘复制：repl-diskless-sync yes 哨兵当从库宕机后，只需要重启就可以了，会自动进行同步（增量同步，主库会记录上一次同步的偏移量）当主库宕机就比较麻烦了，解决方案是： 在从数据库中执行 SLAVEOF NO ONE 命令，断开主从关系并且提升为主库继续服务 将主库重新启动后，执行 SLAVEOF 命令，将其设置为其他库的从库，这时数据就能更新回来 然后手动敲不能保证正确（避免人肉运维），所以 Redis 就提供了哨兵的功能，就是对所有的 Redis 进行监控，可以设置多个哨兵，它们也会互相监控，看看对方是不是挂了，哨兵肯定是独立的线程具体的配置就不贴了，太多了，总之就是检查到主库宕机后会自动执行上面的方案 关于集群现在一般有两种，一种是官方的，一种是分片式的，当然是官方的好了，但是由于在 3.0+ 的版本官方才支持，所以在以前都是玩分片式的集群 分片式集群原理其实就是计算 key 的哈希值来进行存储（到相应的 Redis 数据库），这样就会有一个问题：无法动态的增加、减少服务节点，因为毕竟节点的数量涉及到哈希的计算，其实在读取的时候也会涉及到哈希的计算，要不然它怎么知道去那一台找 官方提供的集群需要 3.0 + 的版本哦，并且需要前面的主从复制知识如果是在一台机器上测试，只需要拷贝不同的配置文件，然后启动的时候到相应的目录指定即可设置集群主要是在配置文件开启： 1234# 开启集群cluster-enabled yes# 指定集群的配置文件cluster-config-file &quot;nodes-xxxx.conf&quot; 然后就是使用一个官方提供的 Ruby 脚本，运行下就好了。分布式的原理为通过插槽的分配确定存储位置，特点有： 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽. 节点的fail是通过集群中超过半数的节点检测失效时才生效 客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可 redis-cluster把所有的物理节点映射到[0-16383] slot（插槽）上, cluster 负责维护 node &lt;–&gt;slot &lt;–&gt; value 一般情况插槽会平均分配到各个 Redis，存储数据的时候根据 key 来计算插槽值（当 key 有闭合的大括号时，大括号中的数据为有效值），然后做相应的存储，这样就需要在使用客户端的时候加一个 -c 的命令，设置为自动跟踪重定向，也就是当插槽值不在当前数据库时自动切换，所以直连一个就可以了 当一半以上的服务器 PING 不通某一个服务器（当一个服务器 PING 不通就将其标记为疑似下线），这个服务器就会被标记为下线，同时插槽出现空档，整个集群被标记为不可用。解决方案可以和前面的主从联系起来，将每个节点设置为主从架构，这样就能保证高可用和负载均衡。集群中的节点只能使用 0 号数据库，切换数据库（SELECT）会报错 拓展下面都是在 Java 中经常使用的一些代码了，使用的客户端为 jedis 与Spring整合在 Spring 的配置文件中定义下面几个 bean，就不需要进行实例化了，之间注入使用即可： 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd" &gt; &lt;!-- 连接池配置 --&gt; &lt;bean id="jedisPoolConfig" class="redis.clients.jedis.JedisPoolConfig"&gt; &lt;property name="maxTotal" value="$&#123;redis.MaxTotal&#125;" /&gt; &lt;/bean&gt; &lt;bean id="shardedJedisPool" class="redis.clients.jedis.ShardedJedisPool"&gt; &lt;constructor-arg index="0" ref="jedisPoolConfig" /&gt; &lt;constructor-arg index="1" &gt; &lt;list&gt; &lt;bean class="redis.clients.jedis.JedisShardInfo"&gt; &lt;constructor-arg index="0" value="$&#123;redis.node1.host&#125;" /&gt; &lt;constructor-arg index="1" value="$&#123;redis.node1.port&#125; "/&gt; &lt;/bean&gt; &lt;/list&gt; &lt;/constructor-arg&gt; &lt;/bean&gt;&lt;/beans&gt;&lt;!-- redis.MaxTotal=50 redis.node1.host=127.0.0.1 redis.node1.port=6379--&gt; 然后创建一个 Service 来统一操作 Redis： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475@Servicepublic class RedisService &#123; @Autowired(required=false) // 设置为不是必须的，这样即使没有配相关的 bean 也不会抛异常 private ShardedJedisPool shardedJedisPool; // 对重复的代码进行封装 private &lt;T&gt; T execute(Function&lt;T, ShardedJedis&gt; fun) &#123; ShardedJedis shardedJedis = null; try &#123; // 从连接池中获取到jedis分片对象 shardedJedis = shardedJedisPool.getResource(); // 从redis中获取数据后进行回调，这时输入已经确定 return fun.callback(shardedJedis); &#125; finally &#123; if (null != shardedJedis) &#123; // 关闭，检测连接是否有效，有效则放回到连接池中，无效则重置状态 shardedJedis.close(); &#125; &#125; &#125; // 保存字符串到数据库（set操作） public String set(String key, String value) &#123; // 这里就确定了返回值 return this.execute(new Function&lt;String, ShardedJedis&gt;() &#123; @Override public String callback(ShardedJedis e) &#123; return e.set(key, value); &#125; &#125;); &#125; // 保存字符串到数据库（set操作），并且设置生存时间 public String set(String key, String value, int seconds) &#123; return this.execute(new Function&lt;String, ShardedJedis&gt;() &#123; @Override public String callback(ShardedJedis e) &#123; String result = e.set(key, value); e.expire(key, seconds); return result; &#125; &#125;); &#125; // 从数据库获取数据 public String get(String key) &#123; return this.execute(new Function&lt;String, ShardedJedis&gt;() &#123; @Override public String callback(ShardedJedis e) &#123; return e.get(key); &#125; &#125;); &#125; // 删除一条（缓存）数据 public Long del(String key) &#123; return this.execute(new Function&lt;Long, ShardedJedis&gt;() &#123; @Override public Long callback(ShardedJedis e) &#123; return e.del(key); &#125; &#125;); &#125; // 设置生存时间, 单位秒 public Long expire(String key, int seconds) &#123; return this.execute(new Function&lt;Long, ShardedJedis&gt;() &#123; @Override public Long callback(ShardedJedis e) &#123; return e.expire(key, seconds); &#125; &#125;); &#125;&#125; 其中为了封装重复的代码达到复用的目的，使用了 js 的回调思想（什么设计模式来….），涉及的接口很简单： 1234public interface Function&lt;T, E&gt; &#123; // 简单理解为 T 为输出；E 为输入 public T callback(E e);&#125; 其实还是很好理解的。嗯，到这里就差不多了吧，应该很全了，如果不全等以后用到了再来补充吧]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7搭建PPTP服务]]></title>
    <url>%2F2017%2F10%2F27%2FCentOS7%E6%90%AD%E5%BB%BAPPTP%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[至于为什么要搭建，这个就不好说明了，都懂得，总之可以说是突然来了兴趣，每天晚上抽半小时研究，三天才弄好啊！主要是掉进了一个坑里没出来……其实搭建还是很简单的，我也就记录一下简单的搭建方法，避免日后再进坑使用的是腾讯云的云主机，毕竟是练手嘛….CentOS7.2 的系统 或者你可以使用一键安装脚本，比如：https://github.com/userrory/centos7-pptp 检查是否支持终端输入：modprobe ppp-compress-18 &amp;&amp; echo yes返回 yes 表示支持 pptp 若你使用 XEN 架构的 VPS，下面的步骤不用执行 ，腾讯云是 XEN ….检测PPP是否开启：cat /dev/ppp开启成功的标志：cat: /dev/ppp: No such file or directory 或者 cat: /dev/ppp: No such device or address，可以继续 安装组件运行下面的命令安装 iptables、ppp、pptpd ：yum install ppp iptables pptpd没什么好说的，如果没有那就尝试换个源或者用 rpm 包。 补充官方如果没有提供可以先安装 epel 源： 12yum install epel-releaseyum install pptpd 可用性自行测试 在 CentOS 6 X64 上可以用： 12rpm -i http://poptop.sourceforge.net/yum/stable/rhel6/pptp-release-current.noarch.rpmyum -y install pptpd 没测试过，不知道能不能用 配置组件编辑pptpd.conf编辑 pptpd.conf 这个文件：vim /etc/pptpd.conf找到下面的两行，并且去掉注释： 12localip 192.168.0.1remoteip 192.168.0.234-238,192.168.0.245 第一行的意思是VPN连接成功后，VPN server（就是你启动 pptpd 服务）的地址，就是说通过这个地址来找到 VPN server。第二行是分配给客户端的 ip 范围。remoteip最好不用和VPN client本身所在的局域网的ip冲突，默认即可 编辑options.pptpd终端输入：vim /etc/ppp/options.pptpd其他的一般保持默认即可，主要是设置下 DNS： 12ms-dns 8.8.8.8ms-dns 8.8.4.4 同样是去掉注释，然后修改，一般设置个 DNS 就可以了，或者根据需要设置其他，给个栗子： 12345678910111213141516name pptpd #自行设定的VPN服务器的名字，可以任意#refuse-pap #拒绝pap身份验证#refuse-chap #拒绝chap身份验证#refuse-mschap #拒绝mschap身份验证require-mschap-v2 #为了最高的安全性，我们使用mschap-v2身份验证方法require-mppe-128 #使用128位MPPE加密ms-dns 8.8.8.8 #设置DNSms-dns 8.8.4.4proxyarp #启用ARP代理，如果分配给客户端的IP与内网卡同一个子网#debug #关闭debuglocknobsdcompnovjnovjccomp#nologfd #不输入运行信息到stderrlogfile /var/log/pptpd.log #存放pptpd服务运行的的日志 其实大部分默认都给配置好了 设置账号密码终端输入： vim /etc/ppp/chap-secrets格式上面有提示，就是：用户名 pptpd 密码 ** 表示的是随机分配 ip 修改内核输入：vim /etc/sysctl.conf如果没有这个文件就自己创建一个吧，在文件的最后写入一行：net.ipv4.ip_forward=1这条命令的意思是使内核支持转发，还需要下面一条命令来使修改生效：sysctl -p 添加转发规则XEN架构：iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -o eth0 -j MASQUERADEOpenVZ架构：iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -j SNAT --to-source VPS公网IP 腾讯云是 XEN 架构，阿里云是 OpenVZ 但是添加转发规则后重启就会失效，Centos 6 系统可以使用 service iptables save 保存配置，但 Centos 7 不支持，我们需要将配置写入 rc.local 文件中，开机自动设置: 12chmod +x /etc/rc.d/rc.localvim /etc/rc.d/rc.local 编辑 rc.local 文件在最后写入上面的转发规则就可以了 补充可以使用 iptables -X -t nat 清除转发规则(iptables -F)；iptables -t nat -L 可以查看 NAT 表是否已经生效 12345678910111213141516#带行号查看当前所有规则 iptables -L -n --line-numbers#清除所有规则iptables -F#删除指定行号（以下命令中的“5”为指定行号）规则iptables -D 5#保存当前配置;相当于旧版/etc/init.d/iptables saveservice iptables save#重启iptables;相当于旧版本/etc/init.d/iptables restartservice iptables restart#注册iptables服务;相当于旧版 chkconfig iptables onsystemctl enable iptables.service#开启服务systemctl start iptables.service#查看状态systemctl status iptables.service 这应该很全了，ipdables 配置文件位于 /etc/sysconfig/iptables 启动服务关于启动服务的命令，网上有几种，用 /etc/init.d/pptpd start 、用 service pptpd start ….我测试是都不行，在 centOS 7+ 中，他会提示让你使用 systemctl ，所以启动应该是：systemctl start pptpd下面的命令应该会让 pptpd 开机启动(未测试)：systemctl enabled pptpd 连不上VPN检查是否开放了 1723 端口，一般是因为这个原因吧，其他错误见参考的连接 查看VPS的架构首先安装一个软件：yum install virt-what装好后使用命令 virt-what 即可查看 linux 下 VPS 的架构了 打不开网页这真是个坑，我搞了好长时间，原来是 MTU 的原因，值太小了！可以在服务器端使用：ifconfig ppp0 mtu 1472 设置，但是这样不持久。 阿里云、腾讯云设置 MTU :vim /etc/ppp/ip-up在 exit 0 前写入 : ifconfig $1 mtu 1500这样就解决无法访问网页的问题了！ 主要就是 MTU，具体的原因参考：http://blog.sandy1890.com/2016/10/23/pptpd_iptables/关于这个深深感到知识不够，看不明白，等以后在来补充吧 MAC 系统需要设置下网卡：wifi—&gt;高级—-&gt;硬件—-&gt;默认是1500，修改为1472 参考http://www.jianshu.com/p/12f7b66d1cabhttp://www.wanghailin.cn/centos-7-pptp/http://blog.sandy1890.com/2016/10/23/pptpd_iptables/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot初尝试]]></title>
    <url>%2F2017%2F10%2F15%2FSpringBoot%E5%88%9D%E5%B0%9D%E8%AF%95%2F</url>
    <content type="text"><![CDATA[Spring Boot 是由 Pivotal 团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Boot 致力于在蓬勃发展的快速应用开发领域（rapid application development）成为领导者。简单说就是为了实现免 XML 配置的开发体验，让开发者不在需要编写 XML 配置文件。 进一步了解都知道 Spring 框架功能很强大，项目中大多会使用吧，它带来方便的同时，也有一个麻烦的事，就算创建是一个很简单的项目，我们也要配置很多东西，写一堆的配置文件。因此就有了 Spring Boot 框架，它的作用很简单，就是帮我们自动配置。Spring Boot 框架的核心就是自动配置，只要存在相应的 jar 包，Spring 就帮我们自动配置。如果默认配置不能满足需求，我们还可以替换掉自动配置类，使用我们自己的配置。另外，Spring Boot 还集成了嵌入式的 Web 服务器（所以可以脱离 Tomcat 来运行），系统监控等很多有用的功，让我们快速构建企业及应用程序。 从名字也可以看出，它就是一个启动 spring 项目的一个工具而已，从最根本上来讲，Spring Boot 就是一些库的集合，它能够被任意项目的构建系统所使用。简便起见，该框架也提供了命令行界面，它可以用来运行和测试 Boot 应用。 可以说 Spring 的整个生态系统都使用到了 Groovy 语言，感兴趣的可以了解下 Spring Boot 不是一门新技术。从本质上来说，Spring Boot 就是 Spring，它做了一些对 Spring Bean 的默认配置。 为什么会出现以前在写 spring 项目的时候，要配置各种 xml 文件，还记得曾经被 ssh 框架支配的恐惧。随着spring3，spring4的相继推出，约定大于配置逐渐成为了开发者的共识，大家也渐渐的从写 xml 转为写各种注解，在spring4的项目里，你甚至可以一行xml都不写。虽然spring4已经可以做到无xml，但写一个大项目需要茫茫多的包，maven 配置要写几百行，也是一件很可怕的事。现在，快速开发一个网站的平台层出不穷，nodejs，php 等虎视眈眈，并且脚本语言渐渐流行了起来（Node JS，Ruby，Groovy，Scala等），spring 的开发模式越来越显得笨重。在这种环境下，spring boot 伴随着 spring4 一起出现了。 可以做什么spring boot 并不是一个全新的框架，它不是 spring 解决方案的一个替代品，而是 spring 的一个封装。所以，你以前可以用 spring 做的事情，现在用 spring boot 都可以做。现在流行微服务与分布式系统，springboot 就是一个非常好的微服务开发框架，你可以使用它快速的搭建起一个系统。同时，你也可以使用 spring cloud（Spring Cloud是一个基于Spring Boot实现的云应用开发工具）来搭建一个分布式的网站。 自动配置：针对很多Spring应用程序常见的应用功能，Spring Boot 能自动提供相关配置 起步依赖：告诉Spring Boot需要什么功能，它就能引入需要的库。 命令行界面：这是Spring Boot的可选特性，借此你只需写代码就能完成完整的应用程序，无需传统项目构建。 Actuator：让你能够深入运行中的Spring Boot应用程序，一套究竟。 小总结Spring Boot 是这几年 微服务 概念流行后，Spring 开发的一套快速开发 Spring 应用的框架（它非常年轻）。它本身并不提供 Spring 框架的核心特性以及扩展功能，只是用于快速、敏捷地开发新一代基于 Spring 框架的应用程序（开箱即用，快速启动）。也就是说，它并不是用来替代 Spring 的解决方案，而是和 Spring 框架紧密结合用于提升 Spring 开发者体验的工具。同时它集成了大量常用的第三方库配置（例如Jackson, JDBC, Mongo, Redis, Mail等等），Spring Boot 应用中这些第三方库几乎可以零配置的开箱即用（out-of-the-box），大部分的 Spring Boot 应用都只需要非常少量的配置代码，开发者能够更加专注于业务逻辑。 从概念上说，Spring Boot 需要使用到 Spring 框架的各个部分，并且对它们进行了大量的默认约定配置 PS：当你用过 SpringMVC 后再使用 Spring Boot 才能体会到是多么的幸福啊~~ spring boot 相当于腾讯的 Wegame，里面装了自家的各种游戏如 spring data 、spring mvc…… 你通过 Wegame 可以轻松 快速 安装、使用(引入、整合) Wegame 旗下的游戏 使用Java配置一般来说 Spring 中，IOC 推荐使用注解；AOP 推荐使用配置文件来配置；到了 spring4.x+ 或者 springBoot 时代，官方则推荐使用 Java 配置的方式，可完全替代 XML。 使用 Java 配置最重要的就是两个注解： @Configuration 和 @Bean 注解带有 @Configuration 的注解类表示这个类可以使用 Spring IoC 容器作为 bean 定义的来源。@Bean 注解告诉 Spring，一个带有 @Bean 的注解方法将返回一个对象，该对象应该被注册为在 Spring 应用程序上下文中的 bean简单说就是：@Configuration 注解相当于我们之前 Spring 的 XML 配置文件；@Bean 就相当于我们在 XML 中配置的 bean 标签。 123456789@Configuration// 默认会扫描配置类所在的包@ComponentScan(basePackages=&#123;"com.bfchengnuo.service", "com.bfchengnuo.dao"&#125;)public class HelloWorldConfig &#123; @Bean public HelloWorld helloWorld()&#123; return new HelloWorld(); &#125;&#125; 这样就完全不用写配置文件了！上面的配置等同于： 123&lt;beans&gt; &lt;bean id="helloWorld" class="com.tutorialspoint.HelloWorld" /&gt;&lt;/beans&gt; 注意 id 的名字哦，然后获取 IOC 容器就不是用 ClassPathXmlApplicationContext 而是 AnnotationConfigApplicationContext ： 1234567public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(HelloWorldConfig.class); HelloWorld helloWorld = ctx.getBean(HelloWorld.class); helloWorld.setMessage("Hello World!"); helloWorld.getMessage();&#125; 如果想要在创建对象的时候注入一些参数，那就更简单了，直接用相应的构造函数 new 出来就行了，或者 set 设置，总之你返回的那个对象有相应的属性值就够了，引用其他 bean 也会了吧，这样一来一般是不会写错名字的。 另外还可以使用 @import 注解允许从另一个配置类中加载 @Bean 定义。 12345678910111213141516@Configurationpublic class ConfigA &#123; @Bean public A a() &#123; return new A(); &#125;&#125;/*******************分割线******************************/@Configuration@Import(ConfigA.class)public class ConfigB &#123; @Bean public B b() &#123; return new B(); &#125;&#125; 这样在 AnnotationConfigApplicationContext 只需要传入一个 ConfigB.class 就够了，A 和 B 都可以获取到，其实和 import 标签差不多。另外，其中也是可以使用 @Autowired 进行注入的，如果涉及到跨类（相当于跨配置文件）的引用，也可以用这个来解决，不要忘了扫描包就好了。还可以使用 @ImportResource 注解来导入配置文件，达到混用的目的；然后还可以使用 @PropertySource 注解来引入外部配置文件，比如最常用的：@PropertySource(value= {&quot;classpath:jdbc.properties&quot;}) ，然后在 Java 代码中的属性上使用 @Value(&quot;${jdbc.url}&quot;) 来注入即可 因为使用 @Configuration 注解的类本身也是一个Bean，因为 @Configuration 被 @Component 注解了，因此 @Configuration 注解可以指定 value 属性值，如 “ctxConfig” 就是该Bean的名字，如使用 ctx.getBean(&quot;ctxConfig&quot;) 将返回该 Bean。 @Bean 注解的属性： name：指定 Bean 的名字，可有多个，第一个作为 Id，其他作为别名，默认不指定的时候使用方法名作为 ID； autowire：自动装配，默认 no 表示不自动装配该 Bean，另外还有 Autowire.BY_NAME 表示根据名字自动装配，Autowire.BY_TYPE 表示根据类型自动装配； initMethod和destroyMethod：指定 Bean 的初始化和销毁方法。 使用 @Bean 注解的方法不能是 private、final 或 static 的 在 Spring Boot项目中推荐使用 @SpringBootConfiguration 替代 @Configuration 入门官方建议我们使用 Maven 或者 Gradle 来构建项目，然后这里就先用 Maven 了。虽然 SpringBoot 可以零配置，但是还是建议写一个全局的配置文件，位置是 Resources 下，或者类路径下的 config 也行，但是不建议，名字也是固定写法，application.properties 或者 application.yml ，yml 格式的更简单吧，起码在 IDEA 中是有提示的。还有一个好消息是 SpringBoot 中默认的编码统一设置为了 UTF-8 ，这就舒爽多了啊，下面就是一个简单的入门栗子，深入研究以后再说 pom文件我是用 IDEA 建的工程，所以大部分都自动生成了，主要是 parent 是必须的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.bfchengnuo&lt;/groupId&gt; &lt;artifactId&gt;firstspringboot&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;FirstSpringBoot&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; spring-boot-starter-web 相当于是 web 的快速构建包（自动配置），需要其他技术的导入相应的依赖即可，plugin 可有可无，是为了能使用 mvn 命令启动工程，spring-boot-starter-data-jpa 相当于是对支持 jpa 的 ORM 框架的支持，我用 hibernate 所以可以理解为对 hibernate 的支持 定义Controller看过 SpringMVC 的都知道，这里的 Controller 和哪里的是一样的，使用起来也很熟悉，其实指的就是 springmvc 中的 handler： 123456789101112131415161718192021@RestControllerpublic class HelloController &#123; // 注入自定义属性 @Value("$&#123;girl.name&#125;") private String name; @Resource private Girl girl; @RequestMapping(value = "hello",method = RequestMethod.GET) public String test() &#123; return "Hello World! " + name + "; age:" + girl.getAge(); &#125; // 组合注解，method=get @GetMapping("hello2") public String test2() &#123; return "Hello World! " + name ; &#125;&#125; @RestController 注解在 Spring4.x+ 的版本中加入，相当于 @ResponseBody 和 @Controller 的集合体；再来看看配置的 application.yml 文件（properties 文件也是可以的），其实不配也是完全可以的，这里是为了演示获取配置文件中的自定义属性： 12345678server: port: 8080 context-path: /# 自定义属性girl: name: 测试 age: 12 可以配一些 boot 相关的配置，有很多，放在 Github 了，然后还可以配自定义属性（Girl），甚至注入到对象，但是配置属性注入到对象并不推荐使用，还是单个值比较好： 123456789101112131415161718192021222324@Component// 注入自定义的配置（对象）@ConfigurationProperties(prefix = "girl")// @PropertySource("classpath:application.yml")public class Girl &#123; private String name; private Integer age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125;&#125; 上面就是利用配置文件的值来自动注入对象，但是不推荐使用，主要用到的是 @ConfigurationProperties 注解，prefix 用来确定是用什么前缀的配置注入 在新版的 SpringBoot 中，@ConfigurationProperties 去掉了 localhost 属性，在没指定路劲的情况下，默认在 resource 下的 applications.properties (yml) 中查找，如果找到则返回值，如果没有找到则返回默认值null/0/false… 如果需要指定路径，配合 @PropertySource 和 @Component 注解 启动入口下面配一个 SpringBoot 的启动入口就可以直接启动了，不需要配 Tomcat、web.xml 之类的，只需要定义一个主函数： 123456789@SpringBootApplication// @ComponentScan(basePackages="com.bfchengnuo")public class FirstSpringBootApplication &#123; public static void main(String[] args) &#123; // 启动 SpringBoot 所必须的入口 SpringApplication.run(FirstSpringBootApplication.class, args); &#125;&#125; @SpringBootApplication 注解是必须的，主要目的是开启自动配置，这就完成了，如果不是为了测试配置文件，可以更简单，只需要几行代码就可以完成 关于配置文件可以使用分散配置，比如一份用于开发环境，一份用于生产环境：application-dev.yml ，application-prod.yml ；然后还需要一份主配置文件 application.yml ： 1234567891011121314151617# 配置 spring 的“环境”，加载那个配置文件spring: profiles: active: dev datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/ssm?useUnicode=true&amp;characterEncoding=UTF-8 username: root password: 123 jpa: hibernate: ddl-auto: update show-sql: trueserver: tomcat: uri-encoding: utf-8 主要是前三行，后面顺便配了数据源相关的东西，因为下面要说，配置在主配置文件中的内容全部环境都可以使用；配置环境，除了使用多文件的方式也可以写在同一份配置文件中，不同的环境之间使用 --- 进行分割就可以了。 数据库操作这里使用的是 spring 提供的 spring-boot-starter-data-jpa 组件，然后使用的 ORM 框架是 hibernate，先确认以及导入了相关的依赖！先是实体的定义，使用的全部是 JPA 注解的方式： 123456789101112131415161718192021222324252627282930313233@Entitypublic class Girl &#123; @Id @GeneratedValue private Integer id; private String name; @Min(value = 8,message = "你太小了！") private Integer age; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125;&#125; 非常熟悉了，使用了一个 @Min 注解，这是用来校验的，后面会看到，使用也非常的简单，按照以前的套路还需要写 Dao 层，现在只需要定义一个接口，继承 JpaRepository 就可以了，我把自己定义的这个接口放在了 Repository 包（和命名的最后保持一致比较好吧）： 1234public interface GirlRepository extends JpaRepository&lt;Girl,Integer&gt; &#123; // 扩展接口，通过指定的条件来查询 List&lt;Girl&gt; findByAge (Integer age);&#125; 泛型第一个就是对应的实体类，第二个是主键的类型，最好指定下；其实接口不用写任何东西就已经支持基本的 CRUD 操作了，如果需要自定义查询条件之类的，那么就需要再扩展，比如上面扩展的根据年龄字段查询，命名就是这么个格式，不要瞎写下面就是在 Controller 中的使用情况了： 123456789101112131415161718192021222324252627282930313233343536373839404142@RestController@RequestMapping("girl")public class GirlController &#123; @Resource private GirlRepository girlRepository; @GetMapping("getList") public List&lt;Girl&gt; getList() &#123; return girlRepository.findAll(); &#125; @GetMapping("get/&#123;id&#125;") public Girl getGirl(@PathVariable("id") Integer id) &#123; return girlRepository.findOne(id); &#125; @PostMapping("add") public Girl add(@RequestParam("name") String name , @RequestParam("age") Integer age) &#123; Girl girl = new Girl(); girl.setAge(age); girl.setName(name); System.out.println(name); return girlRepository.save(girl); &#125; @PostMapping("update") public Girl add(@Valid Girl girl, BindingResult bindingResult) &#123; if (bindingResult.hasErrors()) &#123; System.out.println(bindingResult.getFieldError().getDefaultMessage()); return null; &#125; return girlRepository.save(girl); &#125; @GetMapping("getAge/&#123;age&#125;") public List&lt;Girl&gt; getAge(@PathVariable("age") Integer age) &#123; return girlRepository.findByAge(age); &#125;&#125; GirlRepository 直接注入就可以了，其他的东西都是 springmvc 里的，应该比较属性了，主要就是 @Valid 注解用来校验的，结果会放到 BindingResult 中 使用AOPSpring 的精髓啊，怎么能不用一用呢，关于 Spring 中的 AOP 前面已经写的很清楚了，那就直接用了(如有必要，记得导入相应的类)： 1234567891011121314151617181920212223242526272829@Aspect@Componentpublic class HttpAspect &#123; public final static Logger LOGGER = LoggerFactory.getLogger(HttpAspect.class); @Pointcut("execution(public * com.bfchengnuo.firstspringboot.controller.GirlController.*(..))") public void log() &#123;&#125; @Before("log()") public void doBefore(JoinPoint joinPoint) &#123; ServletRequestAttributes requestAttributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = requestAttributes.getRequest(); LOGGER.info("url---&gt;&#123;&#125;", request.getRequestURL()); LOGGER.info("method---&gt;&#123;&#125;", request.getMethod()); LOGGER.info("ip---&gt;&#123;&#125;", request.getRemoteAddr()); LOGGER.info("class_method---&gt;&#123;&#125;", joinPoint.getSignature().getDeclaringTypeName() + "." + joinPoint.getSignature().getName()); LOGGER.info("args---&gt;&#123;&#125;", joinPoint.getArgs()); &#125; @AfterReturning(returning = "o", pointcut = "log()") public void doAfterReturning(Object o) &#123; // 获取方法执行完后的返回值 if (o != null) &#123; LOGGER.info("response:&#123;&#125;", o); &#125; &#125;&#125; 不多说，非常简单的切面定义，使用了 slf 的 log4j 日志工具，确实是比较爽的 关于异常异常用好了也能提高效率，这里定义了一个自定义异常，但并没怎么用，具体如何使用要看需求了，简单说说，毕竟这篇就是简单的初尝试而已： 12345678910111213141516public class GirlException extends RuntimeException &#123; private Integer code; public GirlException(String message, Integer code) &#123; super(message); this.code = code; &#125; public Integer getCode() &#123; return code; &#125; public void setCode(Integer code) &#123; this.code = code; &#125;&#125; 需要注意的就是要继承 RuntimeException ，否则 SpringBoot 不会进行回滚事务的，注意：Exception 的构造方法需要一个 message 哦。既然有异常，那就有相应的处理（捕获）异常的方法： 12345678910@ControllerAdvicepublic class ExceptionHandle &#123; // 要捕获那个异常 @ExceptionHandler(value = Exception.class) @ResponseBody // 转成 json....额 public String handle(Exception e) &#123; return e.getMessage(); &#125;&#125; 加了 @ControllerAdvice 注解它就会自动捕获规定的异常了，配合 @ExceptionHandler 注解使用，然后就随你怎么干了 关于测试Spring 中应该是提供了专门的测试注解，这个我还没研究，这里说两种简单的测试，比如，这是 IDEA 自动生成的测试代码，可以借鉴一下： 123456789@RunWith(SpringRunner.class) // 低层用的还是 Junit，表明是在测试环境下跑@SpringBootTest // 表明会启动整个工程public class FirstSpringBootApplicationTests &#123; @Test public void contextLoads() &#123; // 使用断言 // Assert.assertEquals(); &#125;&#125; 这是最普通的测试了，SpringBootTest 很重要，只有加入了这个测试的时候才运行整个工程。然后就是 MVC 的测试方法了，我们要测 URL 是不是能用啊： 12345678910111213141516171819@RunWith(SpringRunner.class)@SpringBootTest@AutoConfigureMockMvc // 针对 MVC 的测试，为测试 URL 访问public class GirlControllerTest &#123; @Resource private MockMvc mvc; @Test public void getAge() throws Exception &#123; mvc.perform(MockMvcRequestBuilders.get("/getAge/12")) .andExpect(MockMvcResultMatchers.status().isOk()); // 对返回的内容进行判断断言 // mvc.perform(MockMvcRequestBuilders.get("/getAge/12")) // .andExpect(MockMvcResultMatchers.content().string("xxxx")); &#125;&#125; 也差不多，非常的简单，Spring 都提供给我们 MocMvc 这个类，一切就好说了。 执行 mvn clean package 命令打包的时候会自动进行测试，并显示结果(在命令行)；由于一些原因也可以跳过测试直接打包：mvn clean package -Dmaven.test.skip=true 几种启动方式启动 SpringBoot 项目常用的就这三种:第一就是 IDE 直接启动（通过 @SpringBootApplication 配合 SpringApplication.run）;第二种是使用 java 命令来启动：java -jar xxx.jar -spring.profiles.active=dev 后面可以加一些参数，为了得到这个 jar 所以最好先 mvn install 命令来安装下;第三种是用 mvn 命令来运行：mvn spring-boot:run ，但需要在 pom 文件中导入了相应的插件才行： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 对于调试来说，这几种都算是非常方便的，因为内嵌了 tomcat 这种服务器，所以即使单独运行 jar 也是可以的，真正在部署的时候还是用 war 包比较稳！ 关于2.0现在发布了 SpringBoot2.X 版本，虽然还不是正式版，变动有点大，下面来稍微总结下，在 Github 的代码仓库里有一个用 2.0 的例子，可参考。然后，sb2.x 是基于 spring framwork 5.0 的，所以要 jdk8+，这样才能使用某些特性。全新特性：web flux 包含： 声明式的函数编程 Java8 的 lambda 响应式编程，Reactive Streams 异步编程，Servlet 3.1 或 Asyc NIO 其他的还没看到，先 TODO…. 参考&amp;拓展http://blog.csdn.net/qq_31655965/article/details/71258191https://www.tianmaying.com/qa/205Spring Boot 是什么，不是什么]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSM框架整合简述]]></title>
    <url>%2F2017%2F10%2F11%2FSSM%E6%A1%86%E6%9E%B6%E6%95%B4%E5%90%88%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[现在最流行的应该就是 SSM 了，SSH 一去不复返，SSM 指的是：SpringMVC + Spring + MyBatis弃用 Struts2 的原因在于漏洞比较多吧（毕竟人用的也多），危害还比较大，官方实力甩锅 至于弃用 Hibernate 倒不是它不好，因为太大了，对于小项目用不着这么重量级的东西，MyBatis 这种轻量级的框架是很适合的，并且性能也好不得不说，相比 SSH 来说，SSM 整合确实更加简单，错觉？ Spring和MyBatis整合万年不变定律，先导包，除了 Spring 和 MyBatis 的基本包，别忘了还有一个 MyBatis-spring 的“插件包”，应该也是需要连接池的吧，除了 C3P0 还可以选择 bonecp，据说它的性能比 C3P0 还高，如果用它就别忘了导入 bonecp-spring 的依赖，我的栗子用的 Maven 构建的，pom 文件一览： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.bfchengnuo&lt;/groupId&gt; &lt;artifactId&gt;FirstSSM&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;!-- spring版本号 --&gt; &lt;spring.version&gt;5.0.0.RELEASE&lt;/spring.version&gt; &lt;!-- mybatis版本号 --&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt; &lt;!-- log4j日志文件管理包版本 --&gt; &lt;slf4j.version&gt;1.7.25&lt;/slf4j.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!--测试、日志依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- spring 相关依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.2.4&lt;/version&gt; &lt;/dependency&gt; &lt;!--MyBatis 相关依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;4.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.jsqlparser&lt;/groupId&gt; &lt;artifactId&gt;jsqlparser&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--数据库驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;6.0.6&lt;/version&gt; &lt;/dependency&gt; &lt;!--整合相关--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接池--&gt; &lt;dependency&gt; &lt;groupId&gt;com.jolbox&lt;/groupId&gt; &lt;artifactId&gt;bonecp-spring&lt;/artifactId&gt; &lt;version&gt;0.8.0.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Jackson Json处理工具包 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Apache工具组件 --&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.6&lt;/version&gt; &lt;/dependency&gt; &lt;!--JSP 相关--&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;FirstSSM&lt;/finalName&gt; &lt;plugins&gt; &lt;!--设置编译版本--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.7.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;showWarnings&gt;true&lt;/showWarnings&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 这样大部分依赖就处理完了，接下来就是配置 Spring、MyBatis 的环境，就是写写配置文件，这里我拆了一下，分成 jdbc.properties 、log4j.properties 、applicationContext.xml 、applicationContext-mybatis.xml 等几个文件：日志： 12345log4j.rootLogger=DEBUG,A1log4j.logger.org.mybatis = DEBUGlog4j.appender.A1=org.apache.log4j.ConsoleAppenderlog4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=%-d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c]-[%p] %m%n applicationContext： 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd"&gt; &lt;context:property-placeholder location="classpath:jdbc.properties"/&gt; &lt;!--配置扫描器--&gt; &lt;context:component-scan base-package="com.bfchengnuo" /&gt; &lt;!--设置数据源--&gt; &lt;bean id="dataSource" class="com.jolbox.bonecp.BoneCPDataSource" destroy-method="close"&gt; &lt;!-- 数据库驱动 --&gt; &lt;property name="driverClass" value="$&#123;jdbc.driver&#125;"/&gt; &lt;!-- 相应驱动的jdbcUrl --&gt; &lt;property name="jdbcUrl" value="$&#123;jdbc.url&#125;"/&gt; &lt;!-- 数据库的用户名 --&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;!-- 数据库的密码 --&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;!-- 检查数据库连接池中空闲连接的间隔时间，单位是分，默认值：240，如果要取消则设置为0 --&gt; &lt;property name="idleConnectionTestPeriodInMinutes" value="60"/&gt; &lt;!-- 连接池中未使用的链接最大存活时间，单位是分，默认值：60，如果要永远存活设置为0 --&gt; &lt;property name="idleMaxAgeInMinutes" value="30"/&gt; &lt;!-- 每个分区最大的连接数 --&gt; &lt;property name="maxConnectionsPerPartition" value="150"/&gt; &lt;!-- 每个分区最小的连接数 --&gt; &lt;property name="minConnectionsPerPartition" value="5"/&gt; &lt;/bean&gt;&lt;/beans&gt; applicationContext-mybatis （数据源的定义我想抽在这里也许也比较好）： 123456789101112131415161718192021222324252627282930&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;!-- 结合Spring和Mybatis --&gt; &lt;bean id = "sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!--注入数据源--&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;!--指定MyBatis的配置文件--&gt; &lt;property name="configLocation" value="classpath:mybatis/mybatis-config.xml" /&gt; &lt;!-- 自动扫描mapping.xml文件 --&gt; &lt;property name = "mapperLocations" value="classpath:mybatis/mappers/UserMapper.xml" /&gt; &lt;!--设置别名扫描--&gt; &lt;property name="typeAliasesPackage" value="com.bfchengnuo.pojo" /&gt; &lt;/bean&gt; &lt;!-- 扫描包，Spring 会自动查找其下的类（mapper），将其实例化 --&gt; &lt;bean class = "org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;property name = "basePackage" value="com.bfchengnuo.mapper" /&gt; &lt;!-- 多数据源的时候使用下面指定 --&gt; &lt;!--&lt;property name = "sqlSessionFactoryBeanName" value = "sqlSessionFactory" /&gt;--&gt; &lt;/bean&gt;&lt;/beans&gt; 基本实现了自动化，根据定义的接口自动实例化 Mapper ，通过包扫描的方式来自动扫描定义的接口，这样不需要使用 getMapper 方法了，也不需要自己在 beans 中一个个的定义 Mapper 了。其中引用了 MyBatis 的配置文件，这个文件的大部分内容都可以在 spring 中配置了，少数几个还不行，比如 Settings、插件等还是需要配置在 MyBatis 的配置文件中才行： 12345678910&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;settings&gt; &lt;!--开启驼峰映射--&gt; &lt;setting name="mapUnderscoreToCamelCase" value="true"/&gt; &lt;/settings&gt;&lt;/configuration&gt; 这样配置文件就算是写完了，接下来就是写几个 mapper 映射文件和 mapper 对应的接口去测试一下了，详情参考 Github 与SpringMVC整合这个就更简单了，SpringMVC 本来就是 spring 的子项目，可以说是无缝整合，不需要其他任何的插件包，直接导入 spring-web 包就可以用了，记得写写配置文件就行：firstssm-servlet.xml ： 12345678910111213141516171819202122232425&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.1.xsd"&gt; &lt;!--定义注解驱动--&gt; &lt;mvc:annotation-driven /&gt; &lt;!--controller 扫描包--&gt; &lt;context:component-scan base-package="com.bfchengnuo.controller" /&gt; &lt;!--定义视图解析器--&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;!-- 最后有 / 啊！ --&gt; &lt;property name="prefix" value="/WEB-INF/views/" /&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;/bean&gt; &lt;!--避免静态资源被拦截--&gt; &lt;mvc:default-servlet-handler /&gt;&lt;/beans&gt; 然后把关键的 web.xml 文件搞一搞就完事了！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd" version="3.0"&gt; &lt;display-name&gt;FirstSSM&lt;/display-name&gt; &lt;!--手动指定 spring 文件的位置--&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/applicationContext*.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!--Spring的ApplicationContext 载入 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 编码过滤器，以UTF8编码，处理POST方式提交的乱码 --&gt; &lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!--Spring MVC 配置--&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 自定义SpringMVC配置文件路径 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/firstssm-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- 随容器自动启动完成初始化 --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;!-- 拦截所有的请求 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt;&lt;/web-app&gt; 因为配置的是拦截所有的请求，所以记得在 springmvc 的配置文件中放行静态资源，要不然 css、js 都是会 404 的 测试然后就可以写个简单的栗子看看行不行了，来看定义的 controller ： 1234567891011121314151617181920@RequestMapping("user")@Controllerpublic class UserController &#123; @Resource private UserService userService; // 临时测试 @RequestMapping("users") public String toUsers() &#123; return "users"; &#125; @RequestMapping("getList") public ModelAndView getUserList() &#123; ModelAndView mv = new ModelAndView("users"); List&lt;User&gt; users = userService.queryUserList(); mv.addObject("userList",users); return mv; &#125;&#125; 有了自动包扫描就是方便，只需要配置一个 @Controller 注解就可；然后是 service，很薄： 123456789@Servicepublic class UserService &#123; @Resource private UserMapper userMapper; public List&lt;User&gt; queryUserList() &#123; return userMapper.queryUserList(); &#125;&#125; 省下的 mapper 接口和相应的映射文件就不贴了，完整代码：Github 关于外部配置文件有时候需要从 properties 文件中加载配置，最原始的方案是这样的： 123456789101112&lt;bean class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;!-- 允许JVM参数覆盖 --&gt; &lt;property name="systemPropertiesModeName" value="SYSTEM_PROPERTIES_MODE_OVERRIDE" /&gt; &lt;!-- 忽略没有找到的资源文件 --&gt; &lt;property name="ignoreResourceNotFound" value="true" /&gt; &lt;!-- 配置资源文件 --&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;classpath:jdbc.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 到后来是这样的（Spring 也推荐这样用）： 1&lt;context:property-placeholder location="classpath:spring/jdbc.properties" /&gt; 以上两种方式，在 bean 定义中依然可以通过“${}”这种方式来取值。&lt;context:property-placeholder/&gt; 这个基于命名空间的配置，其实内部就是创建一个 PropertyPlaceholderConfigurerBean 而已，并且它有一个“有趣”的现象：使用分散配置的方式，在两个模块中都引入了这个标签，单独运行某个模块没问题，当一起运行时就会报 Could not resolve placeholder... 的错误。 原因： Spring 容器采用反射扫描的发现机制，在探测到 Spring 容器中有一个 PropertyPlaceholderConfigurer 的 Bean 就会停止对剩余PropertyPlaceholderConfigurer 的扫描（Spring 3.1 已经使用 PropertySourcesPlaceholderConfigurer 替代PropertyPlaceholderConfigurer 了）。 换句话说，即 Spring 容器仅允许最多定义一个 PropertyPlaceholderConfigurer (或 &lt;context:property-placeholder/&gt; )，其余的会被 Spring 忽略掉 至于如何解决，我找到了两种，第一种就是不使用分散配置了，集合读取： 1234567891011&lt;?xml version="1.0" encoding="UTF-8" ?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.2.xsd"&gt; &lt;context:property-placeholder location="classpath*:conf/conf*.properties"/&gt; &lt;import resource="a.xml"/&gt; &lt;import resource="b.xml"/&gt; &lt;/beans&gt; 这种方式应该算是比较优雅的了，如果有特殊需求，不能使用这种方式，可以用第二种来“强制”支持多个pp标签： 123&lt;context:property-placeholder order="1" location="classpath*:conf/conf_a.properties" ignore-unresolvable="true"/&gt;&lt;!-- 这两个配置可能不在一个文件中 --&gt;&lt;context:property-placeholder order="2" location="classpath*:conf/conf_b.properties" ignore-unresolvable="false"/&gt; 首先来解释下这两个属性： ignore-unresolvable单独使用来看是“是否忽视不存在的配置项”，不仅如此，经过测验，有一个隐含意思：是否还要扫描其他配置项：如果扫描到的为 false，则会忽视后续的 property-placeholder默认值为 false order会反应出顺序，值越小优先级越高即越早执行 也就是说，当配置多个 property-placeholder 的时候，要配置 order，并且最后一个的 ignore-unresolvable 要保证为 false，其他的为 true 关于事务上面的基本配置中应该是没有加入事务管理的，除去纯 AOP 的配置，常用的还有“声明式”和“编程式”，他们两个是可以共同使用的。 123456789&lt;!-- 配置事务管理器 --&gt;&lt;bean id="tx" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;!-- 标识事务唯一，配合注解的 value 在多事务管理器的场景下使用 --&gt; &lt;!-- &lt;qualifier value=""/&gt; --&gt;&lt;/bean&gt;&lt;!-- 默认使用JDK的接口代理，如果没有接口就不起作用 proxy-target-class=false --&gt;&lt;tx:annotation-driven transaction-manager="tx" proxy-target-class="true" /&gt; 这是“声明式”事务的简单配置，这样可以在要启用事务的方法上直接使用 @Transactional 注解来开启事务，然后通过 propagation 属性可以配置其传播特性等，只需要注意下嵌套方法调用的情况就好了。还有就是事务一般是加在 Service 层，加在 Controller 层会失效，起码和 SpringMVC 时是这样，这里和父子容器有点关系，还有就是在扫描时注意子容器的范围，这个在 Github 已经做了笔记。然后下面就来介绍“编程式”事务： 123456&lt;!-- transactionManager 配置和上面一样，省略 --&gt;&lt;!-- 编程式事务 --&gt;&lt;bean id="tt" class="org.springframework.transaction.support.TransactionTemplate"&gt; &lt;property name="transactionManager" ref="tx" /&gt;&lt;/bean&gt; 其实就是配置了个 TransactionTemplate，然后在代码中手动调用 TransactionTemplate 来管理事务，这样能解决多线程下 @Transactional 事务失效的问题。 12345678910111213@Autowiredprivate TransactionTemplate template;public void save3() &#123; template.execute((status) -&gt; &#123; // TransactionStatus - status 参数：事务开启、回滚等状态 User user = new User(); user.setId(UUID.randomUUID().toString()); user.setUsername("Nxuan"); // int x = 1/0; return userMapper.add(user); &#125;);&#125; 最后，如果不想写 @Transactional 注解，可以使用纯 AOP 的配置方式，简单贴下： 123456789101112131415161718192021&lt;!-- 定义事务管理器 --&gt;&lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource" /&gt;&lt;/bean&gt;&lt;!--定义事务策略 --&gt;&lt;tx:advice id="txAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;!-- 所有以 query 开头的都是只读的 --&gt; &lt;tx:method name="query*" read-only="true" /&gt; &lt;!-- 其他方法默认 --&gt; &lt;tx:method name="*" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 定义一个切面 --&gt;&lt;aop:config&gt; &lt;aop:pointcut id="myPointcut" expression="execution(* com.bfchengnuo.manage.service.*.*(..))" /&gt; &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="myPointcut" /&gt;&lt;/aop:config&gt; 不过这种总体来说用的不算多，但是某些情景下是非常实用的。 参考http://www.iteye.com/topic/1131688]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
        <tag>Spring</tag>
        <tag>SSM</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC学习笔记（二）]]></title>
    <url>%2F2017%2F10%2F06%2FSpringMVC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[接着上一篇来说，这篇包括 RequestMapping 的进一步探索、与 Struts2 的主要区别、为什么用 JSTL、设置 JSON 格式相关、文件上传以及（自定义）拦截器的相关内容。相比之下这篇的内容就少了，果然 SpringMVC 是轻量级的原因么，比 Struts 的篇幅要少得多，但依然做到高效开发，有一些源码级别的内容还没深入看，以后慢慢再继续补充。 深入RequestMapping都知道它是一个处理请求地址的注解，来研究下当一个方法被 @RequestMapping 标记后（处理器方法），它所支持的方法参数和返回类型。 支持的方法参数 HttpServlet 对象，主要包括 HttpServletRequest 、HttpServletResponse 和 HttpSession 对象。但是有一点需要注意的是在使用 HttpSession 对象的时候，如果此时 HttpSession 对象还没有建立起来的话就会有问题。 Spring 自己的 WebRequest 对象。使用该对象可以访问到存放在 HttpServletRequest 和 HttpSession 中的属性值。 InputStream 、OutputStream 、Reader 和 Writer 。InputStream 和 Reader 是针对 HttpServletRequest 而言的，可以从里面取数据；OutputStream 和 Writer 是针对 HttpServletResponse 而言的，可以往里面写数据。 使用 @PathVariable 、@RequestParam 、@CookieValue 和 @RequestHeader 标记的参数。 使用 @ModelAttribute 标记的参数。 java.util.Map 、Spring 封装的 Model 和 ModelMap 。这些都可以用来封装模型数据，用来给视图做展示。 实体类。可以用来接收上传的参数。 Spring 封装的 MultipartFile 。用来接收上传文件的。 Spring 封装的 Errors 和 BindingResult 对象。这两个对象参数必须紧接在需要验证的实体对象参数之后，它里面包含了实体对象的验证结果。 仔细看看确实不算少，但大部分都已经看过了。 支持的返回类型 一个包含模型和视图的 ModelAndView 对象。 一个模型对象，这主要包括 Spring 封装好的 Model 和 ModelMap ，以及 java.util.Map ，当没有视图返回的时候视图名称将由 RequestToViewNameTranslator 来决定。 一个 View 对象。这个时候如果在渲染视图的过程中模型的话就可以给处理器方法定义一个模型参数，然后在方法体里面往模型中添加值。 一个 String 字符串。这往往代表的是一个视图名称。这个时候如果在渲染视图的过程中需要模型的话就可以给处理器方法一个模型参数，然后在方法体里面往模型中添加值就可以了。 返回值是 void 。这种情况一般是我们直接把返回结果写到 HttpServletResponse 中了，如果没有写的话，那么Spring 将会利用 RequestToViewNameTranslator 来返回一个对应的视图名称。如果视图中需要模型的话，处理方法与返回字符串的情况相同。 如果处理器方法被注解 @ResponseBody 标记的话，那么处理器方法的任何返回类型都会通过 HttpMessageConverters 转换之后写到 HttpServletResponse 中，而不会像上面的那些情况一样当做视图或者模型来处理。 除以上几种情况之外的其他任何返回类型都会被当做模型中的一个属性来处理；而返回的视图还是由 RequestToViewNameTranslator 来决定，添加到模型中的属性名称可以在该方法上用 @ModelAttribute(“attributeName”) 来定义，否则将使用返回类型的类名称的首字母小写形式来表示。使用 @ModelAttribute 标记的方法会在 @RequestMapping 标记的方法执行之前执行。 与Struts2的区别它们两者的实现机制是不同的，所以说区分是很大的，主要是下面几点： SpringMVC 的入口是 Servlet，Struts2 是基于 Filter，所以就说它们的实现机制是不同的 SpringMVC 是基于方法的设计，也就是说传递参数是通过方法的形参，实现是单例（Spring 实例化对象默认就是单例），也推荐单例，这样就省去了创建、销毁对象的过程，提高效率，并且使用方法的形参传值，也不需要担心并发问题。Struts2 是基于类设计，传递参数通过类的属性，只能设置为多例。 参数传递方面，Struts2 因为是用类属性接收，所以不同方法可以共享；但是在 SpringMVC 中多个方法直接不能共享参数，因为是基于方法的嘛 使用JSTL前面学过，JSTL 是个标签库，为什么要使用 JSTL 呢？SpringMVC 没有么？答案是 SpringMVC 有自己的标签库，但是并没有 Struts2 的那么强大，所以就没有必要用它的了，使用经典成熟的 JSTL 或许是更好的选择。在讲框架流程的时候也说了，最终模型里的数据会填充到 Request 域，所以 JSTL 可以直接获取数据来用的。只要你还记得导 C 标签库和会用 JSTL 就行…. 输出&amp;输入为JSON在上一篇的 @ResponseBody 注解中提到了，使用了这个注解就意味着是不正常的输出（其他视图），默认就是 JSON。 123@RequestMapping(value = "/userlist")@ResponseBodypublic List&lt;User&gt; show () &#123;...&#125; 返回值是 List，当加入了 Jackson 的依赖后，会自动注册其对应的转换器，最终响应的就是 JSON 数据格式了，会自动把 List 对象序列化为 JSON 格式的数据。当输入为 JSON 的时候就需要用 @RequestBody 来处理了，它们一个是从对象到 JSON，一个是 JSON 到对象（这里以 JSON 这种格式为例） 1234@RequestMapping(value = "/user")public ModelAndView show (@RequestBody User user) &#123;...&#125;// 获取原始 JSON 数据public ModelAndView show (@RequestBody String user) &#123;...&#125; 这个栗子就是接收一个 JSON 数据，然后反序列为 user 对象。 文件上传这里就用比较简单的方式了，Spring 自带的上传功能效率好像更好些，比较复杂，稍后再说；首先要添加 Apache 的 commons-fileupload 依赖，上传功能依赖于它；然后在 SpringMVC 的配置文件中做相应的配置： 12345 &lt;!-- 上传文件的设置 ，maxUploadSize=-1，表示无穷大。uploadTempDir为上传的临时目录 --&gt; &lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver" p:defaultEncoding="UTF-8" p:maxUploadSize="5400000" p:uploadTempDir="fileUpload/temp"/&gt; 使用了 p 命名空间，不记得？去看 spring 笔记二。可以配合 Spring 提供的 CharacterEncodingFilter 来简单处理下乱码问题。如果怕出异常（如超出限制），可以配下 exceptionResolver 这个 bean。然后定义处理上传文件的控制器即可： 1234567891011121314151617181920212223242526272829303132@Controllerpublic class TestController &#123; @RequestMapping(value="/uploadfile",method=RequestMethod.GET) public String upLoadFile() &#123; return "upload"; &#125; @RequestMapping(value="/uploadfile",method=RequestMethod.POST) public String upLoadFile(@RequestParam("file") MultipartFile[] myfiles, HttpServletRequest request) throws IOException &#123; //如果只是上传一个文件，则只需要MultipartFile类型接收文件即可 for(MultipartFile myfile : myfiles) &#123; if(myfile.isEmpty()) &#123; System.out.println("文件未上传"); continue; &#125; else &#123; System.out.println("文件长度: " + myfile.getSize()); System.out.println("文件类型: " + myfile.getContentType()); System.out.println("文件名称: " + myfile.getName()); System.out.println("文件原名: " + myfile.getOriginalFilename()); System.out.println("==================="); String realPath = request.getSession().getServletContext().getRealPath("/WEB-INF/upload"); // FileUtils.copyInputStreamToFile() 会自动关流 FileUtils.copyInputStreamToFile(myfile.getInputStream(), new File(realPath + "/" +myfile.getOriginalFilename())); // 或者使用下面的方式保存 // myfile.transferTo(new File(realPath + "/" +myfile.getOriginalFilename())); &#125; &#125; return "redirect:/success.html"; &#125; &#125; 可以看到这些方法的返回值都是 String 类型的，根据前面所讲，对应的就是视图名了，如果字符串是以 redirect 开头，那就表示是以重定向的方式跳转到后面的地址。然后，可以看到文件上传多数是通过 MultipartFile 这个对象来处理的，能够理解为什么前面在配置文件中配了个 MultipartResolver 解析器了吧。Spring使用 Jakarta Commons FileUpload 技术实现了一个 MultipartResolver 实现类：CommonsMultipartResolver；如果上传的是超大文件，还是使用流式传输比较好（使用 CommonsMultipartFile 对象来接收），虽然慢，但是省资源。因为 transferTo() 函数调用 write() 函数，而这个函数要求上传文件已经要在内存中，或者是在磁盘里，才能成功执行。 拦截器这一块的学习成本应该是比较低的，据说 SpringMVC 的拦截器思想来源就是 Struts2，所以应该差不多。SpringMVC 拦截器接口（HandlerInterceptor）定义了三个方法： preHandle该方法在目标方法之前被调用（调用 Handler 之前），在该方法中对用户请求 request 进行处理若返回值为 true, 则继续调用后续的拦截器和目标方法.若返回值为 false, 则不会再调用后续的拦截器和目标方法.可以考虑做权限. 日志, 事务等。方法中的 Object 对象，对于静态资源是 ResourceHttpRequestHandler 类型（一般都会进行排除不会拦截的）；对于动态资源是 HandlerMethod 类型，通过它可以获得 Method 等对象。 postHandle调用目标方法之后, 但渲染视图之前.可以对请求域中的属性或视图做出修改，也就是说：这个方法在业务处理器处理完请求后，但是 DispatcherServlet 向客户端返回响应前被调用，在该方法中对用户请求 request 进行处理。 afterCompletion渲染视图之后被调用（在 DispatcherServlet 完全处理完请求后）. 可用来释放资源 自定义拦截器就不贴了，建个类，实现 HandlerInterceptor 接口就行了，就是上面的那三个方法，光写完了是没卵用的，还需要配置进 springmvc 的配置文件，这样它才知道你写了，然后才会执行（还记得框架流程图里的执行链？）： 123456789101112131415&lt;mvc:interceptors&gt; &lt;!-- 配置自定义的拦截器，这样写所有请求都会生效 --&gt; &lt;bean class="com.MySpringMVC.interceptors.FirstInterceptor"/&gt;&lt;/mvc:interceptors&gt;&lt;!-- 下面看比较理性的写法 --&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;!-- 配置拦截器作用的路径；/** 为所有请求 --&gt; &lt;mvc:mapping path="/emps" /&gt; &lt;!-- 若要配置不起作用的路径，则使用 &lt;mvc:exclude-mapping path=""/&gt; --&gt; &lt;!-- 如果使用了 @Component 注解，可以用 ref 来指定，id 默认类名首字母小写 --&gt; &lt;bean class="com.MySpringMVC.interceptors.SecondInterceptor"&gt;&lt;/bean&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 前面说过吧，/** 是指所有目录，也可以认为是所有请求了。注意：即使配置了 mvc:resources 或者 mvc:default-servlet-handler 排除静态资源，但是除了视图（jsp、html）其他静态资源也会被拦截，需要再配置放行规则。 然后下面就是重点的顺序问题，和 Struts 相似，preHandle 按照配置文件的顺序执行，剩余两个按照配置顺序的反序执行，应该能想得通吧。工作流程（建议参考源码）： 当页面发送请求后，DispatcherServlet 会通过其中的 HandlerExecutionChain 对象依次调用各拦截器的的 preHandle() 方法。该 HandlerExecutionChain 对象，其中的 interceptors 属性记录了当前项目配置的所有拦截器，除了我们自定义的两个拦截器以外，还有一个 SpringMVC 内置的 ConversionServiceExposingInterceptor 拦截器。此外，还有一个 int 型参数 interceptorIndex 用于记录当前时刻 preHandle() 方法已经返回 true 的拦截器的最大下标；例如现在我们的 SecondInterceptor 拦截器的 preHandle() 方法返回了 false，那么 interceptorIndex 参数便为1，代表拦截器 ConversionServiceExposingInterceptor 和 FirstInterceptor 的 preHandle() 方法返回了 true 当 HandlerExecutionChain 对象发现某拦截器的 preHandle() 方法返回了 false 后，便执行 triggerAfterCompletion() 方法用于从下标值为 interceptorIndex 开始执行各拦截器的 afterCompletion() 方法，直到下标值减为 -1 为止然后执行链直接 return，终止执行。 如果 HandlerExecutionChain 对象发现某拦截器的 preHandle() 方法返回的是 true ，那么会继续执行下一个的 preHandle，全部执行完后再执行 Handler 的具体逻辑 Handler 执行完毕后，开始倒序执行 postHandle() 各拦截器的后置方法 后置方法调用完成后开始渲染视图（利用 ModelAndView 对象），渲染完成后倒序调用拦截器的完成方法 afterCompletion() 也就是说，无论第二个拦截器的 preHandle 返回的是什么，第一个（已经通过的拦截器）的 afterCompletion 都会执行；但是如果返回的是 false，Handler 和 postHandle 就不会执行了。 和过滤器的区别：简单来说，过滤器什么都能拦下来，而拦截器则不一定，看框架的实现，比如 SpringMVC 的拦截器就不拦视图。其次，过滤器是 Servlet API 里的，其中不能注入 Spring IoC 的对象；拦截器是 Spring 自己的（SpringMVC 中），就可以使用了。 使用RESTful前面最开始的时候页介绍了，SpringMVC 支持 RESTful 风格，原来是使用 @ResponseBody 注解来转成相关的 json 格式，其实不需要这么的复杂。这里必须要说下 ResponseEntity&lt; T&gt; 这个对象了: ResponseEntity 意味着表示整个 HTTP 响应。你可以控制任何进入它：状态码，标题和正文。@ResponseBody 是 HTTP 响应正文的标记，@ResponseStatus 声明 HTTP 响应的状态代码。 来看几个栗子就知道是怎么回事了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384@Controller@RequestMapping("rest/girl")public class RestfulController &#123; private static final Logger LOGGER = LoggerFactory.getLogger(RestfulController.class); @Resource private GirlService girlService; @RequestMapping(value = "&#123;id&#125;", method = RequestMethod.GET) @ResponseBody private ResponseEntity&lt;Girl&gt; getGirl(@PathVariable("id") Integer id) &#123; LOGGER.info("-----------------&gt;" + id); try &#123; Girl girl = girlService.queryGirl(id); if (girl == null) &#123; return ResponseEntity.status(HttpStatus.NOT_FOUND).body(null); &#125; // 200 可以进行简写 return ResponseEntity.ok(girl); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(null); &#125; @RequestMapping(method = RequestMethod.POST) public ResponseEntity&lt;Void&gt; saveGirl(Girl girl) &#123; LOGGER.debug("--------------&gt;" + girl.getName()); try &#123; if (girl.getName().isEmpty() || girl.getAge().toString().isEmpty()) &#123; // 400 错误，参数不正确 return ResponseEntity.status(HttpStatus.BAD_REQUEST).build(); &#125; Boolean flag = girlService.saveGirl(girl); if (flag) &#123; // 新增成功，响应 201,如果返回是 void 可以使用 build 方法 return ResponseEntity.status(HttpStatus.CREATED).build(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; // 新增失败 return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); &#125; @RequestMapping(method = RequestMethod.PUT) public ResponseEntity&lt;Void&gt; updateGirl(Girl girl) &#123; LOGGER.debug("--------------&gt;" + girl.getName()); try &#123; if (girl.getName().isEmpty() || girl.getId().toString().isEmpty()) &#123; // 400 错误，参数不正确 return ResponseEntity.status(HttpStatus.BAD_REQUEST).build(); &#125; Boolean flag = girlService.updateGirl(girl); if (flag) &#123; // 修改成功，响应 204,如果返回是 void 可以使用 build 方法 return ResponseEntity.status(HttpStatus.NO_CONTENT).build(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; // 新增失败 return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); &#125; @RequestMapping(method = RequestMethod.DELETE) public ResponseEntity&lt;Void&gt; deleteGirl(@RequestParam(value = "id",defaultValue = "0") Integer id) &#123; try &#123; if (id == 0) &#123; return ResponseEntity.status(HttpStatus.BAD_REQUEST).build(); &#125; Boolean flag = girlService.deleteGirl(id); if (flag) &#123; // 删除成功，响应 204 return ResponseEntity.status(HttpStatus.NO_CONTENT).build(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; // 新增失败 return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); &#125;&#125; 返回的时候会自动把实体里的属性（凡是有 getter 方法的）转换成 JSON 类型的格式(默认)返回，同时可以设置状态码之类的，总之就是非常的全面了，先说这些，等以后再补充。 其他相关的注解： @RestController：Spring 4 的新注解。 她自动为每个方法加上 @ResponseBody ；@RestController 可以看做是 @Controller 和@ResponseBody 两个注解的组合。 @RequestBody：如果一个方法申明了 @RequestBody 注解, Spring 会基于请求头中的 Content-Type 使用 HTTP Message converters 将 request body 反序列化成对象。 @ResponseBody：是在方法的上注解，如果一个方法申明了@ResponseBody, Spring 会基于请求头中的 Accept 使用 HTTP Message converters 将对象序列化成 response body。在 Spring 4 中如果使用了 @RestController，则可以不用再声明此注解。 关于父/子容器当我们的项目中引入了 Spring + SpringMVC 后，在启动后其实有两个 Spring 容器，一个是 Spring 的容器（处理 Bean），一个是 SpringMVC 的容器（处理 Controller），它们是父子关系，Spring 就是父容器了！它们的关系： 子容器能够访问父容器的资源（Bean）比如说在 Controller 中可以引用（注入） Service 中的 Bean 父容器不能访问子容器中的资源这也就说明了为什么在 Spring 的配置文件中配置了扫描包，而在 SpringMVC 的配置文件中却还要再配置 然后再说下 @Value 这个注解，它就是用来获取配置文件中的值的；时机就是在所有 Bean 初始完成后从当前所在容器中获取值，然后注入；所以说如果在 Controller 中使用可能获取不到（假设配置在 Spring 文件中读取），即使子容器能够访问父容器，但是优先级最高的还是注解自身的特性。解决方案可以采用在 Service 中写一个 propertiesService 专门用来注入配置文件的值，然后在 Controller 中注入这个 Service 来间接的使用配置文件中的值。在它们整合的时候，SpringMVC 其实会扫描是否存在 Spring 容器（存在 Application 域，所以还算好找），如果存在就把它作为父容器，如果不存在它会自己搞一个（具体不是很清楚） 路径匹配在 SpringMVC 中遵循：路径匹配，先最长路径匹配，再最短路径匹配；优先级最高的精确匹配就不说了。所以说 /* 可以匹配所有，如果设置为 &lt;url-pattern&gt;/user/*&lt;/url-pattern&gt; 那么 /abc/def/user/list 也是可以匹配到的，我想说明的就是这个，在以前的文章中应该提到过，不过我没找到呢。 关于放行静态资源当 SpringMVC 拦截 / 的时候，会拦截所有请求，这时候需要配置让其放行静态资源，方式有几种： 激活Tomcat的defaultServlet来处理静态文件 XML 中使用 mvc:resources 标签 XML 中使用 mvc:default-servlet-handler 标签 对于方案一，要写在 DispatcherServlet 的前面， 让 defaultServlet 先拦截，这个就不会进入 Spring 了： 123456789101112&lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;*.jpg&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;*.js&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;*.css&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 关于默认 servlet 是专门处理静态资源的，名字也不是乱写的： Tomcat, Jetty, JBoss, and GlassFish 默认 Servlet的名字 – “default”Google App Engine 默认 Servlet的名字 – “_ah_default”Resin 默认 Servlet的名字 – “resin-file”WebLogic 默认 Servlet的名字 – “FileServlet”WebSphere 默认 Servlet的名字 – “SimpleFileServlet” 方案二配置：&lt;mvc:resources mapping=&quot;/images/**&quot; location=&quot;/images/&quot; /&gt; location 指定静态资源的位置，可以是 web application 根目录下、jar 包里；多个路径可使用 , 分割 mapping 指的是 location 映射成的路径，前端请求访问的就是这个路径，** 表示的是包含多层目录。 由 Spring MVC 框架自己处理静态资源，并添加一些有用的附加值功能。 &lt;mvc:resources /&gt; 允许静态资源放在任何地方，如 WEB-INF 目录下、类路径下等，你甚至可以将JavaScript等静态文件打到JAR包中。通过 location 属性指定静态资源的位置，由于location属性是Resources类型，因此可以使用诸如”classpath:”等的资源前缀指定资源位置。传统 Web 容器的静态资源只能放在 Web 容器的根路径下，&lt;mvc:resources /&gt; 完全打破了这个限制。 其次，&lt;mvc:resources /&gt; 依据当前著名的 Page Speed、YSlow 等浏览器优化原则对静态资源提供优化。你可以通过cacheSeconds属性指定静态资源在浏览器端的缓存时间，一般可将该时间设置为一年，以充分利用浏览器端的缓存。在输出静态资源时，会根据配置设置好响应报文头的Expires 和 Cache-Control值。在接收到静态资源的获取请求时，会检查请求头的Last-Modified值，如果静态资源没有发生变化，则直接返回303相应状态码，提示客户端使用浏览器缓存的数据，而非将静态资源的内容输出到客户端，以充分节省带宽，提高程序性能 。 方案三是用的最多的一种，将静态资源的处理经由 Spring MVC 框架交回Web应用服务器处理 。定义此标签后，会在 Spring MVC 上下文中定义一个 org.springframework.web.servlet.resource.DefaultServletHttpRequestHandler，它会像一个检查员，对进入 DispatcherServlet 的 URL 进行筛查，如果发现是静态资源的请求，就将该请求转由 Web 应用服务器默认的 Servlet 处理，如果不是静态资源的请求，才由 DispatcherServlet 继续处理。一般 Web 应用服务器默认的 Servlet 名称是 “default”，因此 DefaultServletHttpRequestHandler 可以找到它。如果你所有的 Web 应用服务器的默认 Servlet 名称不是 “default”，则需要通过 default-servlet-name 属性显示指定：&lt;mvc:default-servlet-handler default-servlet-name=&quot;所使用的Web服务器默认使用的 Servlet 名称&quot; /&gt; 其他接收数据时用到日期时，默认 SpringMVC 是不会处理的，如果使用的是 po 来接收，那么肯定会转换异常，这时可以使用 @DateTimeFormat 注解来格式化，它有一个 pattern 参数可以指定日期的格式，比如：@DateTimeFormat(pattern=&quot;yyyy-MM-dd&quot;)类似的，SpringMVC 提供其他多个 format，比如 NumberFormat 需要时了解吧 如果只是简单的跳转视图逻辑，那么可以使用 &lt;mvc:view-controller path=&quot;&quot; view-name=&quot;&quot;/&gt; 来配置，省的单独写一个 controller 方法了。 参考http://www.cnblogs.com/fangjian0423/p/springMVC-interceptor.htmlhttp://blog.csdn.net/xiangwanpeng/article/details/53157756http://www.jianshu.com/p/58cf4936c523https://www.cnblogs.com/fangqi/archive/2012/10/28/2743108.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC学习笔记]]></title>
    <url>%2F2017%2F10%2F05%2FSpringMVC%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[SpringMVC 是 Spring 的一个模块，提供 web 层解决方案；和众多其他web框架一样，它基于MVC的设计理念，此外，它采用了松散耦合可插拔组件结构，比其他MVC框架更具扩展性和灵活性。SpringMVC 通过一套 MVC 注解，让 POJO 成为处理请求的控制器，无需实现任何接口，同时，SpringMVC 还支持 REST 风格的 URL 请求。此外，SpringMVC 在数据绑定、视图解析、本地化处理及静态资源处理上都有许多不俗的表现。它在框架设计、扩展性、灵活性等方面全面超越了 Struts、WebWork 等 MVC 框架，从原来的追赶者一跃成为 MVC 的领跑者。SpringMVC 框架围绕 DispatcherServlet 这个核心展开，DispatcherServlet 是 SpringMVC 框架的总导演、总策划，它负责截获请求并将其分派给相应的处理器处理。 框架流程下面说说 SpringMVC 的执行流程，当然和传统的 MVC 模式是非常相似的： 介绍中提到过，DispatcherServlet 是前端控制器，相当于中央调度器，各个组件都和前端控制器进行交互，降低了各个组件之间耦合度；Handler 是后端控制器，当成模型（Model）。那么一个请求的完整步骤可以概况为： 用户发起 request 请求，请求至 DispatcherServlet 前端控制器 DispatcherServlet 前端控制器请求 HandlerMapping 处理器映射器查找 Handler HandlerMapping 处理器映射器，根据 url 及一些配置规则（xml配置、注解配置）查找 Handler，将 Handler 返回给 DispatcherServlet 前端控制器；这里其实会以 HandlerExecutionChain 对象的形式返回，我就称它为执行链了，它包含了 Handler 对象和拦截器列表 DispatcherServlet 前端控制器根据返回的 Handler 调用合适的适配器执行 Handler，有了适配器通过适配器去扩展对不同 Handler 执行方式（比如：原始servlet开发，注解开发） 适配器执行 Handler（成功获得 HandlerAdapter 后，将开始执行拦截器的 preHandler(…) 方法 ），其中会经过一些消息转换器，例如进行数据绑定相关操作（数据转换、格式化、校验等） Handler 执行完成后返回 ModelAndView 给适配器ModelAndView 是 springmvc 的一个对象，对 Model （数据）和 view （仅仅是 View 的名字）进行封装。 适配器将 ModelAndView 返回给 DispatcherServlet DispatcherServlet 通过返回的 View 的名称，调用视图解析器进行视图解析，解析后生成真正的 view也就是说视图解析器根据逻辑视图名解析出真正的视图对象。View：springmvc 视图封装对象，提供了很多 view，比如：jsp、freemarker、pdf、excel。。。 ViewResolver 视图解析器给前端控制器返回真正的 view 对象 DispatcherServlet 调用 view 的渲染视图的方法（根据模型对象的数据），将模型数据填充到 request 域。 View 返回渲染后的视图 DispatcherServlet 向用户响应结果(jsp页面、json数据等) 以上就是 springmvc 处理一个请求的步骤了，然后再贴一张详细一点的流程图吧： 一个简单的栗子然后就来看看具体的开发流程，首先定义 web.xml 文件，也就是定义 SpringMVC 的总导演 DispatcherServlet 这就是一个 servlet： 123456789101112131415161718192021222324&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE web-app PUBLIC "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN" "http://java.sun.com/dtd/web-app_2_3.dtd"&gt;&lt;web-app&gt; &lt;!-- springMVC前端控制器 --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 指定配置文件的位置 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- load-on-startup 表示在web应用启动时，即加载该DispatcherServlet，而不是等到首次请求再中载 --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt;&lt;/web-app&gt; 关于配置文件也有必要说一下，首先这里使用了 init-param 标签来指定配置文件的位置，如果不配置，默认加载的是 WEB-INF/{servlet-name}-servlet.xml ，其中大括号中指的是变量名，还记的 servlet-name 在那设置的么？看第六行！也就是说最终默认找的配置文件是 springmvc-servlet.xml 。然后就是 url-pattern 了，匹配规则，SpringMVC 支持：*.xxx、/、/xxx/* ；不支持 ：/* ；是的，原来在 struts 中写的很爽的 /* 在这里是错误的；使用这种配置，最终要转发到一个 jsp 页面时，仍然会由 DispatcherServlet 解析 jsp 地址，不能根据 jsp 页面找到 handler，会报错。不过 / 表示所有访问的地址都由 DispatcherServlet 解析，对于静态文件的解析需要配置不让 DispatcherServlet 解析，使用此种方法可以实现 RESTful 风格的 url。按照套路接下来就应该是 springmvc 的配置文件了（先使用配置文件的方式），其实就是 spring 的配置文件，因为基本没差嘛： 123456789101112131415161718192021222324252627&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.1.xsd"&gt; &lt;!-- 配置 HandlerMapping（处理器映射器） --&gt; &lt;bean class="org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping"/&gt; &lt;!-- 处理器适配器，所有的处理器适配器都实现HandlerAdapter --&gt; &lt;bean class="org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter"/&gt; &lt;!-- 配置Handler(自己写的) --&gt; &lt;bean name="/hello.action" class="cn.itcast.ssm.controller.HelloController"/&gt; &lt;!-- 定义视图解析器 --&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver" id="internalResourceViewResolver"&gt; &lt;!-- 前缀 --&gt; &lt;property name="prefix" value="/WEB-INF/jsp/" /&gt; &lt;!-- 后缀 --&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;/bean&gt;&lt;/beans&gt; 这些定义其实都没必要看文档，就看上面的那个流程图，定义的这些 bean 完全就是按照图的顺序定义的，因为图中都是接口所以定义的时候要写具体实现的 bean。视图解析器的前缀后缀是什么意思呢？从图中得知，最终要根据一个 View 的名字（在 Handler 的返回值中）返回一个具体的视图对象，视图说白了就是 jsp、json 等这种，所以视图解析器会根据前缀和后缀最后拼出一个真正的视图，也就是找到具体你定义的 jsp 文件。视图就不写了，就是简单的一个 JSP 文件，除了视图 Handler 也需要自己写，看下 Handler 应该怎么写： 1234567891011121314151617//实现Controller接口的映射器public class HelloController implements Controller&#123; @Override public ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; // 返回ModelAndView ModelAndView modelAndView = new ModelAndView(); // 相当于 request 的 setAttribute 方法 // 在jsp页面中就可以通过 EL 表达式获取数据 modelAndView.addObject("msg","HelloWorld!"); //指定视图 modelAndView.setViewName("hello"); return modelAndView; &#125;&#125; 这里一定要实现 Controller 接口，在适配器中会检查是否有实现这个接口，一般 Handler 会放在 controller 包下。视图解析器会根据 setViewName 设置的名字和前缀、后缀进行拼接，最后就转到了我们定义的 jsp 视图上；这样就可以通过访问 /hello.action 来调用 Handler 了。从命名来说，一般如果是 addXXX 那么就可以添加多个，如果是 setXXX 那么就只能设置一个了。另外，Controller（handler）的名字和视图的名字应该是对应的。 优化配置文件我们在 springmvc 的配置文件中配置了一些 bean，但是有些其实是不用配也是可以的（有默认值），比如前两个：处理器映射器和处理器适配器，后面两个肯定不能省了，因为有我们自己配的信息 使用注解在最开始的介绍中也说明了，springmvc 中的注解是很重要的，大多都是用注解吧，毕竟在默认的配置文件中就已经开启了注解，所以注解我们可以直接用。目标就是达到零配置！虽然默认配置已经开启了注解，但是已经过时了（AnnotationMethodHandlerAdapter 和 DefaultAnnotationHandlerMapping），推荐使用新的类（RequestMappingHandlerAdapter 和 RequestMappingHandlerMapping）： 12345678910&lt;!-- spring 3.1之后由RequestMappingHandlerAdapter和RequestMappingHandlerMapping代替 --&gt; &lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter"/&gt; &lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping"/&gt;&lt;!-- 或者直接使用下面的注解驱动，上面的两个就不用写了 --&gt;&lt;mvc:annotation-driven/&gt;&lt;!-- 声明DispatcherServlet不要拦截下面声明的目录 --&gt; &lt;mvc:resources location="/images/" mapping="/images/**"/&gt;&lt;!-- 另一种放行方式 --&gt;&lt;mvc:default-servlet-handler/&gt; 至于注解驱动是如何将最新的映射器和适配器注入的可以去看 org.springframework.web.servlet.config.AnnotationDrivenBeanDefinitionParser 这个类，我就先不研究了….简单说，驱动注解内部其实包含了这些内容： 1234567891011&lt;!-- HandlerMapping --&gt;&lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping "/&gt;&lt;bean class="org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping"&gt;&lt;/bean&gt;&lt;!-- HandlerAdapter --&gt;&lt;bean class="org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter"&gt;&lt;/bean&gt;&lt;bean class="org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter "&gt;&lt;/bean&gt;&lt;bean class="org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter"&gt;&lt;/bean&gt;&lt;!-- HadnlerExceptionResolvers --&gt;&lt;bean class="org.springframework.web.servlet.mvc.method.annotation.ExceptionHandlerExceptionResolver "&gt;&lt;/bean&gt;&lt;bean class="org.springframework.web.servlet.mvc.annotation.ResponseStatusExceptionResolver"&gt;&lt;/bean&gt;&lt;bean class="org.springframework.web.servlet.mvc.support.DefaultHandlerExceptionResolver"&gt;&lt;/bean&gt; @Controller在 SpringMVC 中，控制器 Controller 负责处理由 DispatcherServlet 分发的请求，它把用户请求的数据 经过业务处理层处理之后封装成一个 Model ，然后再把该 Model 返回给对应的 View 进行展示。在 SpringMVC 中提供了一个非常简便的定义 Controller 的方法，你无需继承特定的类或实现特定的接口，只需使用@Controller 标记一个类是 Controller 。然后使用 @RequestMapping 和 @RequestParam 等一些注解用以定义 URL 请求和 Controller 方法之间的映射，这样的 Controller 就能被外界访问到。Controller 不会直接依赖于 HttpServletRequest 和 HttpServletResponse 等 HttpServlet 对象，它们可以通过 Controller 的方法参数灵活的获取到。也就是说，@Controller 用于标记在一个类上，使用它标记的类就是一个 SpringMVC Controller 对象，然后 Spring 怎样才能找到它呢，有两种方式，当然，推荐使用扫描啦： 1234&lt;!--方式一--&gt;&lt;bean class="com.host.app.web.controller.MyController"/&gt;&lt;!--方式二--&gt;&lt;context:component-scan base-package = "com.host.app.web" /&gt; @RequestMapping首先这是一个重点！其次，它非常 NB。RequestMapping 是一个用来处理请求地址映射的注解，可用于类或方法上；访问的时候就是 类上 value + 方法上的 value 拼起来，并且可以不加斜线，会自动处理。RequestMapping 注解有六个属性，可分为五种映射（或者说三种映射两种限制）： 标准URL映射比如：@RequestMapping(&quot;hello&quot;) or @RequestMapping(&quot;/hello&quot;) or @RequestMapping(value=&quot;hello&quot;) ，它们都是一样的。 Ant 风格的 URL 映射Ant 通配符有三种，？- 匹配单个字符；* - 匹配 0 或者任意数量的字符；** - 匹配 0 或者更多的目录。例如：@RequestMapping(&quot;/test/*/hello&quot;) 占位符映射 使用占位符用花括号括起来，可以使用多个占位符。例如： 123456@RequestMapping(value="user/&#123;id&#125;")public ModelAndView show (@PathVariable("id") int pid) &#123; ModelAndView modelAndView = new ModelAndView("hello"); modelAndView.addObject("msg","HelloWorld!" + pid); return modelAndView;&#125; 这样访问 xxx/user/123.action 的时候，会自动把 123 传进 pid 这个参数，这样就又多了一种传值的方式。 限制请求方法映射也就是限制请求方式，只要添加一个属性就可以了，使用到的是 method 属性：@RequestMapping(value=&quot;hello&quot;,method = RequestMethod.GET)需要多个的话用花括号括起来就行了，之间用逗号分割。 限制参数映射也是通过属性指定，params 属性：指定 request 中必须包含某些参数值是，才让该方法处理。过个参数还是用花括号括起来，一般有下面的几种形式：&quot;!user&quot; ：参数不能包含 user；&quot;userID != 1&quot; ：参数必须包含 userID 但是不能为 1；{&quot;user&quot;,&quot;age&quot;} ：参数必须包含 user 和 age； 然后属性这样就已经说了三个了，value、method、params；其它是的：headers： 指定 request 中必须包含某些指定的 header 值，才能让该方法处理请求；举个栗子：@RequestMapping (value= &quot;testHeaders&quot; , headers={ &quot;host=localhost&quot; , &quot;Accept&quot; }) ，这样就表示只有当请求头包含 Accept 信息，且请求的 host 为 localhost 的时候才能正确的访问到此方法。consumes： 指定处理请求的提交内容类型（Content-Type），例如 application/json 、 text/html;produces: 指定返回的内容类型，仅当 request 请求头中的 (Accept) 类型中包含该指定类型才返回； @PathVariable用于将请求 URL 中的模板变量映射到功能处理方法的参数上，即取出 uri 模板中的变量作为参数。上面已经使用过了，需要注意的是 value 参数不能省略！虽然省略了可以执行，但是当正常编译（ IDE 一般为 debug 模式）时，方法的传入参数不会被记录，所以会报错。许多注解都是这个机制，所以最好是写上 value ，万无一失，总之就是不要依赖变量名。Eclipse 中的 Java Compiler 中的 add variable attributes to generated class files 默认是开启的。 @requestParam主要用于在 SpringMVC 后台控制层获取参数，类似一种是 request.getParameter(&quot;name&quot;)，它有三个常用参数：defaultValue = “0”、required = false、 value = “isApp”；defaultValue 表示设置默认值，required 通过 boolean 设置是否是必须要传入的参数（默认为 true），value 值表示接受的传入的参数类型。具体用在哪里可以参考上面的 PathVariable ，差不多的，这样就直接把请求参数放进方法参数里了，在方法里使用非常方便了，对于基本数据类型，记得一定要写，不要省略。它可以直接对参数进行 POJO 实体注入、转换数组、List 等集合，并且不会出现空指针异常，非常的方便！ @CookieValue可以把 Request header 中关于 cookie 的值绑定到方法的参数上，例如获取 cookie 中的 JSESSIONID 的值：public void displayHeaderInfo(@CookieValue(&quot;JSESSIONID&quot;) String cookie) {}和上面两个注解也没啥差别，都是获取数据的。 @RequestHeader和 CookieValue 等注解类似，就是将 HttpServletRequest 头信息到 Controller 的方法参数上，比如 @RequestHeader(&quot;host&quot;) 就是获取 host 头信息；不同的是，在 RequestHeader 中是不区分大小写的，但在 @PathVariable 、 @RequestParam 和 @CookieValue 中都是大小写敏感的。 @ResponseBody作用：该注解用于将 Controller 的方法返回的对象，通过适当的 HttpMessageConverter 转换为指定格式后，写入到 Response 对象的 body 数据区。使用时机：返回的数据不是 html 标签的页面，而是其他某种格式的数据时（如 json、xml等）使用；默认为 JSON；补充：为什么默认是 JSON ，在注解驱动中已经定义了，如果项目中有 Jackson 的依赖，那么会自动注册其转换器做转换输出。 @RequestBody和上面长的很像…..我都看错了，它们是一对。该注解常用来处理请求的 Content-Type 不是application/x-www-form-urlencoded编码的内容，例如 application/json, application/xml 等.它是通过使用 HandlerAdapter 配置的 HttpMessageConverters 来解析 post data body，然后绑定到相应的 bean 上的。因为配置有 FormHttpMessageConverter，所以也可以用来处理 application/x-www-form-urlencoded 的内容，处理完的结果放在一个 MultiValueMap&lt;String, String&gt; 里，这种情况在某些特殊需求下使用，详情查看FormHttpMessageConverter API. @ResponseStatus业务异常可以使用 @ResponseStatus 来注解。当异常被抛出时，ResponseStatusExceptionResolver 会设置相应的响应状态码。DispatcherServlet 会默认注册一个 ResponseStatusExceptionResolver 以供使用。它有两个属性，value 属性是 http 状态码，比如 404，500 等。reason 是错误信息：@ResponseStatus(value=HttpStatus.FORBIDDEN,reason=&quot;用户不匹配&quot;)通常它是修饰类的，然后在抛出异常的时候就会起作用了： 123456789@ResponseStatus(value=HttpStatus.FORBIDDEN,reason="用户不匹配")public class UserNotMatchException extends RuntimeException&#123;&#125;@RequestMapping("/testResponseStatus")public String testResponseStatus(int i)&#123; if(i==0) throw new UserNotMatchException(); return "hello";&#125; 这样，用户看到的异常界面正是我们自己定义的异常，而不再是一大堆用户看不懂的代码，但如果用这个注解来修饰方法，那么无论它执行方法过程中有没有异常产生，用户都会得到异常的界面。而目标方法正常执行 @ModelAttribute和@SessionAttributes该 Controller 的所有方法在调用前，先执行此 @ModelAttribute 方法；可以把这个 @ModelAttribute 特性，应用在 BaseController 当中，所有的 Controller 继承 BaseController，即可实现在调用 Controller 时，先执行 @ModelAttribute 方法。@SessionAttributes 即将值放到 session 作用域中，写在 class 上面有效。 SpringMVC 支持使用 @ModelAttribute 和 @SessionAttributes 在不同的模型（model）和控制器之间共享数据。 @ModelAttribute 主要有两种使用方式，一种是标注在方法上，一种是标注在 Controller 方法参数上。当 @ModelAttribute 标记在方法上的时候，该方法将在处理器方法执行之前执行，然后把返回的对象存放在 session 或模型属性中，属性名称可以使用 @ModelAttribute(&quot;attributeName&quot;) 在标记方法的时候指定，若未指定，则使用返回类型的类名称（首字母小写）作为属性名称。 12345678910111213141516171819202122232425262728293031@Controller@RequestMapping ( "/myTest" )public class MyController &#123; @ModelAttribute ( "hello" ) public String getModel() &#123; System. out .println( "-------------Hello---------" ); return "world" ; &#125; @ModelAttribute ( "intValue" ) public int getInteger() &#123; System. out .println( "-------------intValue---------------" ); return 10; &#125; @RequestMapping ( "sayHello" ) public void sayHello( @ModelAttribute ( "hello" ) String hello, @ModelAttribute ( "intValue" ) int num, @ModelAttribute ( "user2" ) User user, Writer writer, HttpSession session) throws IOException &#123; writer.write( "Hello " + hello + " , Hello " + user.getUsername() + num); writer.write( "\r" ); Enumeration enume = session.getAttributeNames(); while (enume.hasMoreElements()) writer.write(enume.nextElement() + "\r" ); &#125; @ModelAttribute ( "user2" ) public User getUser()&#123; System. out .println( "---------getUser-------------" ); return new User(3, "user2" ); &#125;&#125; 当我们请求 /myTest/sayHello.do 的时候使用 @ModelAttribute 标记的方法会先执行，然后把它们返回的对象存放到模型中。最终访问到 sayHello 方法的时候，使用 @ModelAttribute 标记的方法参数都能被正确的注入值.这时，数据都是放在模型中，而不是存放在 session 属性中，如果想要也同时放进 Session 中，那么需要加 @SessionAttributes 注解： 1234@Controller@RequestMapping ("/myTest")@SessionAttributes (value=&#123; "intValue" , "stringValue" &#125;, types=&#123;User. class &#125;)public class MyController &#123;...&#125; 在上面代码中我们指定了属性为 intValue 或 stringValue 或者类型为 User 的都会放到 Session 中；但是当访问时，Session 中并没有值，第二次访问才有，也就是说：等处理器方法执行完成后 Spring 才会把模型中对应的属性添加到 session 中，所以第二次访问才能拿到值。 绑定Servlet内置对象使用注解后确实简单多了，但有一个问题，怎么才能获得 Servlet 中的内置对象呢，比如 request、response、session 之类的，工具类？接口？完全不需要，需要什么定义什么就 OK 了，直接定义在 handler 中的方法参数中即可。 12345678910111213@RequestMapping(value = "test/show")public ModelAndView show (HttpServletRequest request,HttpServletResponse response,HttpSession session)&#123; ModelAndView mv = new ModelAndView(); mv.setViewName("show"); StringBuilder sb = new StringBuilder(); sb.append("request = " + request); sb.append("&lt;br/&gt;response = " + response); sb.append("&lt;br/&gt;session = " + session); mv.addObject("msg", sb.toString()); return mv;&#125; 似不似非常简单方便，这种做法确实非常棒！ POJO对象绑定对于“实体”对象，也是非常的简单，和上面一样，直接在方法参数里写就行了，会自动把请求中的参数注入到这个 POJO 中去；例如，方法为：public ModelAndView show (User user){} ；请求的时候直接 id=1&amp;name=abc 这样就可以了。然后，还有一个对于我们来说最烦人的问题，中文乱码问题…..，如何解决后面再补充 如果提交的是集合，就是这样：&lt;input name=&quot;users[0].name&quot; /&gt; ；这样提交的不是一个集合或者数组嘛，但是接收不能使用 public ModelAndView show (List&lt;user&gt; users){} 的形式来接收，从上面的简单栗子也可以看得出，填充对象的时候应该是去参数对象中去找相应的 setter 方法进行注入（除非使用 @requestParam 注解，适用于基本数据类型，大概是这样），所以，你需要将 List 进行对象包装后才可以使用。虽然这种使用形式并不会经常用到。 参考http://www.cnblogs.com/leskang/p/5445698.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis学习笔记（二）]]></title>
    <url>%2F2017%2F09%2F30%2FMyBatis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[接笔记一继续，还剩下一些知识点，本篇包含缓存、高级查询、延迟加载（lazy）、插件的使用当然，仅靠这两篇也是不全的（注解就没说），只能说个大概，后面做 SSM 的时候还会再进行补充吧…. 动态SQLMyBatis 的强大特性之一便是它的动态 SQL。如果你有使用 JDBC 或其他类似框架的经验，你就能体会到根据不同条件拼接 SQL 语句有多么痛苦。拼接的时候要确保不能忘了必要的空格，还要注意省掉列名列表最后的逗号。利用动态 SQL 这一特性可以彻底摆脱这种痛苦。MyBatis 采用功能强大的基于 OGNL 的表达式来消除其他元素。 if choose (when, otherwise) trim (where, set) foreach 下面就来详细说说到底怎么用，还是那句话，官方有很详细的文档，详细了解还是去官方比较好，飞机：http://www.mybatis.org/mybatis-3/zh/dynamic-sql.html这里就是演示最基本的使用，首先是 if，最常用的就是根据传入的参数来决定 sql ： 12345678&lt;select id="findActiveBlogWithTitleLike" resultType="Blog"&gt; SELECT * FROM BLOG WHERE state = ‘ACTIVE’ &lt;if test="title != null and title != ''"&gt; AND title like #&#123;title&#125; &lt;/if&gt;&lt;/select&gt; 熟悉 OGNL 的就不用多说了，都看得懂，忽然发现我对 OGNL 并不是多了解o(￣▽￣)ゞ))￣▽￣)o，抽时间单独搞一篇！；需要提一下的是，OGNL 中可以直接使用 Java 中的方法比如 &lt;if test=&quot;title != null and &#39;&#39;.equals(title.trim())&quot;&gt; 是合法的，我以前有说过么？？大概 然后继续向下看，第二个 choose (when, otherwise) ，它比较类似 Java 中的 switch，从其中根据条件选择一种： 1234567891011121314&lt;select id="findActiveBlogLike" resultType="Blog"&gt; SELECT * FROM BLOG WHERE state = ‘ACTIVE’ &lt;choose&gt; &lt;when test="title != null"&gt; AND title like #&#123;title&#125; &lt;/when&gt; &lt;when test="author != null and author.name != null"&gt; AND author_name like #&#123;author.name&#125; &lt;/when&gt; &lt;otherwise&gt; AND featured = 1 &lt;/otherwise&gt; &lt;/choose&gt;&lt;/select&gt; 如果都没匹配到就会执行 otherwise 标签的内容，如果没有定义这个标签就默认什么也不做，当匹配到第一个后就会 break，即使后面的也匹配但是会按照第一个来确定最终的语句； 第三个是 trim (where, set) ，有些时候 WHERE 后面没有默认的语句，就是说条件是通过动态 SQL 确定的，但是如果都不匹配那么 WHERE 语句后面就是空了；比较业余的做法就是加 1=1 这种恒等条件；MyBatis 有一个简单的处理，这在 90% 的情况下都会有用 123456789&lt;select id="findActiveBlogLike" resultType="Blog"&gt; SELECT * FROM BLOG &lt;where&gt; &lt;if test="title != null"&gt; AND title like #&#123;title&#125; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 总结一下：如果 where 标签内部有 sql 语句那么就会在前面自动添加 where 关键字，如果标签内是空那么就不做处理；更 NB 的是，如果内部的语句是 “AND”或“OR”开头的，会自动处理它们… 再来看 set，也能猜到了，是用在 update 语句中的 set 关键字哪里，就是处理逗号的问题了 12345678&lt;update id="updateAuthorIfNecessary"&gt; update Author &lt;set&gt; &lt;if test="username != null"&gt;username=#&#123;username&#125;,&lt;/if&gt; &lt;if test="bio != null"&gt;bio=#&#123;bio&#125;&lt;/if&gt; &lt;/set&gt; where id=#&#123;id&#125;&lt;/update&gt; set 元素会动态前置 SET 关键字，同时也会消除无关的逗号，嗯~很赞 然后，trim 标签其实可以代替它们两个，可以用来做他们两个做不到的一些逻辑： 12345678&lt;!-- 和 where 标签等价 --&gt;&lt;trim prefix="WHERE" prefixOverrides="AND |OR "&gt; ... &lt;/trim&gt;&lt;!-- 和 set 标签等价 --&gt;&lt;trim prefix="SET" suffixOverrides=","&gt; ...&lt;/trim&gt; prefixOverrides 属性会忽略通过管道分隔的文本序列（注意此例中的空格也是必要的）。它带来的结果就是所有在 prefixOverrides 属性中指定的内容将被移除，并且插入 prefix 属性中指定的内容。通俗说就是：如果判断 trim 里面有东西就在前面加 prefix 的值（还可以指定一个 suffix 值，就是在后面输出了），否则不输出；prefixOverrides 的意思就是判断标签里面的内容的最前面有没有设定的值，如果有就去除；suffixOverrides 就是判断最后有没有了，有就去除。 这样还剩最后一个 foreach ，从名字很明显看出这是用来遍历的，一般用在批量删除、更新等，也就是构建 IN 条件语句的时候 ： 123456789&lt;select id="selectPostIn" resultType="domain.blog.Post"&gt; SELECT * FROM POST P WHERE ID in &lt;foreach item="item" index="index" collection="list" open="(" separator="," close=")"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 然后它贴心的提供了几个属性，比如在开始前加入 ( ，结束后加入 )，指定分隔符为 ,；index 是当前迭代的次数;另外不要忘了使用 @Param 注解指定参数名哦，要不然 collection 可能会报 “404” 错误 一级缓存对于缓存和 Hibernate 中的是差不多的，所以这一部分应该是相对比较轻松的；MyBatis 中的一级缓存作用域也是 Session 级别的，如果执行的是相同的 SQL （相同的语句和参数）那么就会从缓存中获取；同样的，一级缓存默认是开启的，并且你是关不掉的…..但是可以使用 sqlSession 的 sqlSession.clearCache() 方法清除缓存；在同一个 Session 中，如果两句相同的 SQL 之间进行了 insert、update、delete 时，会刷新缓存，下次获取还是会重新从数据库中进行获取。 二级缓存二级缓存的作用域是一个 mapper 的 namespace ；同一个 namespace 中查询 sql 是可以从缓存中命中的；二级缓存是跨 session 的；要使用二级缓存首先要开启，开启非常简单，在映射文件 mapper 中加入 &lt;cache/&gt; 标签即可，还有就是别忘了把实体进行序列化，要不然缓存的时候会出错在全局配置文件中，settings 标签中有一个 cacheEnabled 的属性，该配置影响的所有映射器中配置的缓存的全局开关，默认是 true；所以默认情况下只需要配置下 &lt;cache/&gt; 标签就可以使用了；但是如果全局配置文件中的 cacheEnabled 为 false，那么二级缓存依然是不可用的状态。 当然 cache 标签还可以设置很多属性，详细的说明还是看官方文档：http://www.mybatis.org/mybatis-3/zh/sqlmap-xml.html#cache下面是一个比较全的栗子： 12345&lt;cache eviction="FIFO" flushInterval="60000" size="512" readOnly="true"/&gt; 这个更高级的配置创建了一个 FIFO 缓存，并每隔 60 秒刷新（默认情况是不设置,也就是没有刷新间隔,缓存仅仅调用语句时刷新），存数结果对象或列表的 512 个引用（默认值是 1024），而且返回的对象被认为是只读的（默认 false），因此在不同线程中的调用者之间修改它们会 导致冲突。可用的收回策略有: LRU – 最近最少使用的：移除最长时间不被使用的对象。 FIFO – 先进先出：按对象进入缓存的顺序来移除它们。 SOFT – 软引用：移除基于垃圾回收器状态和软引用规则的对象。 WEAK – 弱引用：更积极地移除基于垃圾收集器状态和弱引用规则的对象。 默认的是 LRU。一般这个就够用了 使用第三方缓存比如，都很熟悉的 EHCache、或者 Memcache（高性能 KV 缓存框架，目前被 Redis 所替代）；下面说 MyBatis 如何集成 EHcache ，处理依赖就不用说了，然后是编写配置文件： ehcache.xml ，或者选择集成在 MyBatis 中的配置文件中。Ehcache 默认使用 CLASSPATH 根目录下的 ehcache.xml 作为配置文件，如果没找到，则使用 Jar 包下的 ehcache-failsafe.xml 作为配置文件，该配置文件提供了默认的简单配置；关于它的配置就不说了，现在的功力还达不到…….不知道怎么配，然后就是在 MyBatis 中的映射文件中使用了： 1234567891011&lt;cache type="org.mybatis.caches.ehcache.EhcacheCache"&gt; &lt;property name="timeToIdleSeconds" value="3600"/&gt; &lt;property name="timeToLiveSeconds" value="3600"/&gt; &lt;property name="maxEntriesLocalHeap" value="1000"/&gt; &lt;property name="maxEntriesLocalDisk" value="100000"/&gt; &lt;property name="memoryStoreEvictionPolicy" value="LRU"/&gt;&lt;/cache&gt;&lt;!-- 如果你的配置文件写在了外面，可以直接使用下面的其中一种 --&gt;&lt;cache type="org.mybatis.caches.ehcache.LoggingEhcache"/&gt;&lt;cache type="org.mybatis.caches.ehcache.EhcacheCache"/&gt; 使用外部配置文件需要注意的是文件名一定不要写错，ehcache.xml ！！还有就是两种 type 的区别，第一种可以输出日志，第二种不可以。 一对一查询使用 MyBatis 的好处是性能好，使用也相对简单，毕竟是一个比较轻量级的框架，当然是与 hibernate 相比的，但是相应的也给我带来了个问题，就是要写 SQL 语句了（如果你喜欢写，那就….）首先来看第一种方案：SQL 语句是必须的吗，就不说了，对于一对一可以使用 左连接的方式将其关联的表给查出来，但是这样一来返回的字段肯定比实体中的字段多，并且没有一个实体与其对应，所以可以创建一个新类，继承自原来的实体（第一个一的一方），然后扩展你所需要的其他属性。 第二种方案：这种方式就和 hibernate 有点相似了，采用 OO 的思想，使用组合的方式，在原来的实体中引用另一个实体，这样一来，自动映射就不行了，需要手动指定了，一个例子： 123456789101112&lt;resultMap type="Order" id="orderResultMap" autoMapping="true"&gt; &lt;id column="id" property="id"/&gt; &lt;!-- association: 用于映射java对象 property：Order对象中的属性名（User对象） javaType：属性的java类型（可以使用别名） --&gt; &lt;association property="user" javaType="User" autoMapping="true"&gt; &lt;!-- id：User对象的id --&gt; &lt;id column="user_id" property="id"/&gt; &lt;/association&gt;&lt;/resultMap&gt; 因为使用了 association（定义实体引用的对象类型），所以默认 autoMapping 是关闭的，需要手动开启，这样指定实体的 resultMap 就可以了 一对多查询一对多的实现只有一种那就是 OO 的组合思想，继承是办不到的，和上面类似，需要在实体里加一个集合，比如 List ，然后使用泛型来约束多的一方的类型，就是 hibernate 中的一对多的写法啊；然后来看看映射文件怎么写： 12345678910&lt;resultMap type="Order" id="orderAndUserAndOrderDetailResultMap" autoMapping="true"&gt; &lt;id column="id" property="id"/&gt; &lt;!-- javaType: 属性的 java 类型 ofType：集合中的对象类型 --&gt; &lt;collection property="orderdetails" javaType="List" ofType="Orderdetail" autoMapping="true"&gt; &lt;id column="detail_id" property="id"/&gt; &lt;/collection&gt;&lt;/resultMap&gt; 是的，无论什么时候，既然用 resultMap，那就起码得写个 id 吧，然后用 collection 定义集合，写好后下面就可以用了，这种情况下不用忘了 autoMapping ，默认是 false 的 多对多查询本质上还是写映射文件，展示的就是在集合中还能够再套属性，就是说： 12345678910111213&lt;resultMap type="Order" id="orderAndUserAndOrderDetailAndItemResultMap" autoMapping="true" extends="orderResultMap"&gt; &lt;!-- javaType: 属性的javae类型 ofType：集合中的对象类型 --&gt; &lt;collection property="orderdetails" javaType="List" ofType="Orderdetail" autoMapping="true"&gt; &lt;id column="detail_id" property="id"/&gt; &lt;association property="item" javaType="Item" autoMapping="true"&gt; &lt;id column="iid" property="id"/&gt; &lt;/association&gt; &lt;/collection&gt;&lt;/resultMap&gt; 从上面的配置中，还可以得出一个结论，就是 resultMap 是可以继承的，支持这个特性就可以做到代码复用了；还是不要忘记 autoMapping 。下面写一个示例的 sql 语句，仅供参考： 1234567891011121314151617181920&lt;select id="query...ByOrderNumber" resultMap="orderAndUser...ResultMap"&gt; SELECT o.*, u.user_name, u.name, od.item_id, od.total_price, od.id detail_id, i.id iid, i.item_name, i.item_price, i.item_detail FROM tb_order o LEFT JOIN tb_user u ON o.user_id = u.id LEFT JOIN tb_orderdetail od ON od.order_id = o.id LEFT JOIN tb_item i ON od.item_id = i.id WHERE o.order_number = #&#123;orderNumber&#125;&lt;/select&gt; 如果这样都能看得懂，那应该就没啥问题了，emmmm；关于多对多我还是有点蒙的，订单和商品应该是多对多，但是这个体现方式我还需要消化消化 批量查询在 JDBC 中，批量操作使用 addBatch 的方式就可以实现，但是在 MyBatis 中就只能….想要做到批量查询在 MyBatis 中就只能写相应的 SQL 语句了，或者说拼相应的 SQL 语句；不同的数据库的 SQL 写法会有区别，比如在 MySQL 中：insert into tabName(a,b) values(&#39;A&#39;,&#39;B&#39;),(&#39;D&#39;,&#39;E&#39;)所以在 ParameterType 中直接接收一个 List 就行了，然后用 foreach 遍历出来就行了，注意它们直接有 , 所以要用上 foreach 的特性 separator 属性。 延迟加载就是所谓的 lazy 懒加载，就是说，数据用的时候再加载；要开启延迟加载，需要在主配置文件中的 settings 标签中加入两个开关： 1234&lt;settings&gt; &lt;setting name="lazyLoadingEnabled" value="true"/&gt; &lt;setting name="aggressiveLazyLoading" value="false"/&gt;&lt;/settings&gt; lazyLoadingEnabled：延迟加载的全局开关， 特定关联关系中可通过设置fetchType属性来覆盖该项的开关状态。默认为 false;aggressiveLazyLoading：当开启时，如果有多个延迟加载项调用其中一个都会加载，false 则按需加载，默认为 false (true in ≤3.4.1) 注意：懒加载需要使用动态代理技术（get 的时候需要执行查询逻辑），但是实体应该都是没有接口的，所以记得加入 Cglib 相关的依赖~ 使用懒加载，Java 代码不需要进行更改，改的还是映射文件，栗子： 12345678910111213141516&lt;resultMap type="Order" id="lazyOrderResultMap" autoMapping="true"&gt; &lt;id column="id" property="id"/&gt; &lt;!-- select:延迟加载时执行SQL的StatementId column：指定关联的字段(传入的参数) --&gt; &lt;association property="user" javaType="User" select="queryUserById" column="user_id"/&gt;&lt;/resultMap&gt;&lt;select id="queryUserById" parameterType="Long" resultType="User"&gt; SELECT * FROM tb_user WHERE id = #&#123;id&#125;&lt;/select&gt;&lt;select id="lazyQueryOrderAndUserByOrderNumber" resultMap="lazyOrderResultMap"&gt; SELECT * FROM tb_order WHERE order_number = #&#123;orderNumber&#125;&lt;/select&gt; 因为全局配置中已经开启了懒加载，所以默认 association 就会进行延迟加载，事前就要配置好 第三方插件(实现分页)在 MyBatis 主配置文件里有个 插件（plugins）的说明，插件说白了其实就是拦截器，就是在原有的处理流程上加入自己的逻辑（相当于修改源代码的效果）。然后就以分页来作为栗子吧，分页其实就是使用 limit 来实现的；MyBatis 允许你在已映射语句执行过程中的某一点进行拦截调用。默认情况下，MyBatis 允许使用插件来拦截的方法调用包括： Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) 从上到下可以理解为：拦截执行器的方法、拦截参数的处理、拦截结果集的处理、拦截Sql语法构建的处理；关于自定义插件的步骤先挖坑吧，暂时可参考官方的文档嘛….. 然后来实现分页，使用果然开发的 PageHelper 就可以了，不重复造轮子（我也造不出来）；因为是国人写的所以有详细的中文文档，使用也很简单，就不多说了，给个地址好了：https://github.com/pagehelper/Mybatis-PageHelper下面说个最简单的使用栗子，首先要保证在主配置文件中已经配置好了： 12345678910//获取第1页，10条内容，默认查询总数countPageHelper.startPage(1, 10);List&lt;Country&gt; list = countryMapper.selectAll();//用PageInfo对结果进行包装PageInfo page = new PageInfo(list);//测试PageInfo全部属性//PageInfo包含了非常全面的分页属性assertEquals(183, page.getTotal()); // 数据总数assertEquals(19, page.getPages()); // 总页数assertEquals(1, page.getFirstPage()); 主配置文件的配置，这里按最简单的最基本的设置来： 123456789&lt;plugins&gt; &lt;plugin interceptor="com.github.pagehelper.PageInterceptor"&gt; &lt;!-- 使用下面的方式配置参数，后面会有所有的参数介绍 --&gt; &lt;!-- 设置方言 --&gt; &lt;property name="helperDialect" value="mysql"/&gt; &lt;!-- 查询总页数 --&gt; &lt;property name="rowBoundsWithCount" value="true"/&gt; &lt;/plugin&gt;&lt;/plugins&gt; 嘛，这样就差不多了，基本使用的话 自定义插件使用自定义插件（拦截器）来实现一个批量分页功能，最终就是只需要定义普通的 SQL 语句，id 命名符合一定规范，传入相应的分页参数（对象形式）就可以返回相应的分页数据。所使用的就是上面提到的 StatementHandler — 拦截Sql语法构建的处理。首先来分析一下，除 SQL 语句和配置参数（如何分页，每页几条这样的）不同，其他大部分都是相同的，JSP 页面的重复代码可以使用自定义标签来实现复用，然后一行代码就搞定了；然后解决的就是拿到执行的 SQL 语句然后利用子查询拼成一个分页查询的 SQL 语句再替换回去，SQL 语句在配置文件中，由 MyBatis 进行处理，想要得到要么改源码，要么就是使用 拦截器（插件）了！那么关键点就是： 要拦截住（拦截什么样的对象、对象的什么行为、什么时候拦截） 拦截下来做什么 做完后要交回主权 要实现在执行前完成 SQL 的替换（创建 statement 的时候），需要了解 MyBatis 的源码，这里只说大体，先看看定义的拦截器： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Intercepts(&#123;@Signature(type=StatementHandler.class,method="prepare",args=&#123;Connection.class&#125;)&#125;)public class PageInterceptor implements Interceptor &#123; private String test; // 3. 处理拦截后要干什么，参数就是被拦截的对象 // 只有拦截成功的才会执行这个方法，相当于动态代理中的那个 InvocationHandler @Override public Object intercept(Invocation invocation) throws Throwable &#123; StatementHandler statementHandler = (StatementHandler) invocation.getTarget(); // 通过 MetaObject 能达到使用反射获取属性的效果 MetaObject metaObject = MetaObject.forObject(statementHandler, SystemMetaObject.DEFAULT_OBJECT_FACTORY, SystemMetaObject.DEFAULT_OBJECT_WRAPPER_FACTORY); MappedStatement mappedStatement = (MappedStatement) metaObject.getValue("delegate.mappedStatement"); // 获取配置文件中 SQL 语句的 ID，判断是不是要分页功能，通过 id 命名结尾带有 ByPage String id = mappedStatement.getId(); if(id.matches(".+ByPage$")) &#123; BoundSql boundSql = statementHandler.getBoundSql(); // 原始的SQL语句 String sql = boundSql.getSql(); // 查询总条数的SQL语句 String countSql = "select count(*) from (" + sql + ")a"; Connection connection = (Connection)invocation.getArgs()[0]; PreparedStatement countStatement = connection.prepareStatement(countSql); // 获取并填充 SQL 中的参数 ParameterHandler parameterHandler = (ParameterHandler)metaObject.getValue("delegate.parameterHandler"); parameterHandler.setParameters(countStatement); // 填充 ResultSet rs = countStatement.executeQuery(); // 获取在 service 层调用时传入的参数，也就是配置文件里的 parameterType Map&lt;?,?&gt; parameter = (Map&lt;?,?&gt;)boundSql.getParameterObject(); Page page = (Page)parameter.get("page"); if(rs.next()) &#123; page.setTotalNumber(rs.getInt(1)); &#125; // 改造后带分页查询的SQL语句 String pageSql = sql + " limit " + page.getDbIndex() + "," + page.getDbNumber(); // 将改造后的 SQL 塞回 statementHandler，这样就偷梁换日了 metaObject.setValue("delegate.boundSql.sql", pageSql); &#125; // 返回主权，也就是继续执行 prepare 方法，这样会执行我们换的 statementHandler return invocation.proceed(); &#125; // 2. 负责拦截对象（请求） // 参数为被拦截的对象，如果判断成功返回的是代理类，否则是拦截对象本身 // 这里相当于是拦截所有的要创建 statement 的对象 @Override public Object plugin(Object target) &#123; System.out.println(this.test); return Plugin.wrap(target, this); &#125; // 1. 可以把注册时配置的参数加载进来 @Override public void setProperties(Properties properties) &#123; this.test = properties.getProperty("test"); &#125;&#125; 首先要实现 Interceptor 接口，也就是那三个方法，setProperties 用于注入配置的参数，plugin 负责拦截对象，intercept 负责拦截后的进一步处理（还需要判断下拦下的这个“人”是不是要执行某个“动作”）；注解确定了要拦截那个对象的那个方法，通过这个方法有什么参数来确定是那个重载，达到唯一的目的。MyBatis 获取 statement 是在 StatementHandler 这个类（接口）中定义的，其中的 prepare 方法返回的就是 statement 对象；所以知道注解怎么写了吧。Plugin.wrap() 方法中会根据注解来判断是不是需要拦截的对象，如果是就返回代理类，如果不是就返回拦截对象本身（放行）；因为这个方法第二个参数传入了 this ，所以其实当调用代理类的时候，执行的就是在 Interceptor 中定义的 intercept 方法！MetaObject 对象提供了一个 getValue 的方法可以让我们很方便的访问被包装对象的属性（即使是保护权限的），StatementHandler 有两个实现类，我们拦截到的首先是 RoutingStatementHandler 然后通过里面的一个 delegate 属性拿到另一个实现类 BaseStatementHandler ，从它里面的 mappedStatement 属性拿到 MappedStatement ，也就是映射配置文件的信息。还可以拿到 ParameterHandler 对象，其中保存了填充 SQL 的数据，可以用它来填充一个 statement ，然后这个 statement 就可以执行了。获取处理 SQL 是在 prepare 方法中进行的，所以我们拦截这个方法（在处理这个方法之前会被拦截），更准的说获取 SQL 其实是 instantiateStatement 方法，这个方法是在 prepare 方法中调用的。然后最后的注册插件就不多说了 plugins 标签，上面也是写过的。 其他写 SQL 的时候尽量不用用 select * 这种，就是说不要用星，直接手写上就行了，避免给数据库带来不必要的压力，虽然这样写我们倒是很省事； 在 Mybatis 中映射文件的各种标签是可以写多条 SQL 语句的，比如 INSERT 里可以批量插入几张表的数据，多条语句以分号分割；但是有一个前提，就是在连接数据库的 URL 中要加个参数开启这个功能（默认是关闭的）对于 MySQL：在 URL 的后面再追加 &amp;allowMultiQueries=true并且写在同一标签的多条 SQL 是在同一个事务下的，只要你不 try ，就能回滚 还有事务的具体操作没有提到，待补充….]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-模板方法模式]]></title>
    <url>%2F2017%2F09%2F29%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[首先来看下模板方法的概述：定义一个操作中算法的框架，而将一些步骤延迟到子类中。模板方法模式使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。它是一种类行为型模式，是基于继承的代码复用的基本技术。 这种设计模式非常的简单，同时也是最常见的设计模式之一，但是不能大意，就像单例模式一样，虽然简单，但是有许多细节需要注意 结构&amp;角色模板方法模式需要开发抽象类和具体子类的设计师之间的协作。一个设计师负责给出一个算法的轮廓和骨架，另一些设计师则负责给出这个算法的各个逻辑步骤。代表这些具体逻辑步骤的方法称做基本方法(primitive method)；而将这些基本方法汇总起来的方法叫做模板方法(template method)，这个设计模式的名字就是从此而来。 前面说过，这种模式非常简单，只涉及到两个角色： 抽象模板(Abstract Template)角色定义了一个或多个抽象操作，以便让子类实现。这些抽象操作叫做基本操作，它们是一个顶级逻辑的组成步骤。定义并实现了一个模板方法。这个模板方法一般是一个具体方法，它给出了一个顶级逻辑的骨架，而逻辑的组成步骤在相应的抽象操作中，推迟到子类实现。顶级逻辑也有可能调用一些具体方法。 具体模板(Concrete Template)角色每一个抽象模板角色都可以有任意多个具体模板角色与之对应，而每一个具体模板角色都可以给出这些抽象方法（也就是顶级逻辑的组成步骤）的不同实现，从而使得顶级逻辑的实现各不相同。 一个栗子首先定义抽象模板角色： 12345678910111213141516171819202122232425public abstract class AbstractTemplate &#123; // 模板方法 // 使用 final 关键字保证算法不会被篡改 public final void prepareExecute()&#123; // 调用基本方法 Operation1(); Operation2(); hookMethod(); Operation3(); &#125; // 基本方法声明（由子类实现） protected abstract void Operation1(); private void Operation2() &#123; System.out.println("顶级类中的方法执行...."); &#125; private final void Operation3()&#123; System.out.println("顶级类方法 final"); &#125; // 基本方法，空方法--钩子 void hookMethod() &#123;&#125;&#125; 在方法中，我们可以使用 final 关键字来保证子类继承的时候无法覆盖父类中的算法，也就是说这个抽象类它拥有算法，而且保护这个算法。下面再来看看具体模板角色，就非常简单了，这个栗子中就需要实现一个算法： 1234567891011public class ConcreteTemplate extends AbstractTemplate &#123; @Override protected void Operation1() &#123; System.out.println("子类中的具体实现"); &#125; @Override void hookMethod() &#123; System.out.println("使用钩子...."); &#125;&#125; 然后稍微测试下就可以了，测试代码就不写了，超简单的 关于钩子上面的栗子中，我们还定义了一个叫钩子的方法，什么是钩子？在此设计模式中可以这么理解：一个钩子方法常常由抽象类给出一个空实现作为此方法的默认实现。这种空的钩子方法叫做“Do Nothing Hook”。作为子类，你可以视情况绝对要不要去覆盖它们（也就是说是可选的），更加常用的是通过钩子来控制某个算法要不要执行，比如： 123456789public final void prepareExecute() &#123; // 调用基本方法 Operation1(); Operation2(); if (hookMethod()) &#123; System.out.println("钩子控制的代码被执行...."); Operation3(); &#125;&#125; 这样就可以通过钩子方法的返回值来决定要不要执行某些算法，应该是比较实用的。钩子方法的名字应当以 do 开始，这是熟悉设计模式的 Java 开发人员的标准做法。在上面的例子中，钩子方法 hookMethod() 应当以 do 开头；在 HttpServlet 类中，也遵从这一命名规则，如 doGet()、doPost() 等方法。 根据基本设计原则，我们应该将决策权放在高层模块中 应用实际应用中，JDK 中的排序的功能就是用的模板方法，如果排序一个对象数组（ Object[] ），我们实现两个对象的比较，就要实现 Comparable 接口的 compareTo 方法.Java 的 sort 方法实现就比较像模板方法，首先克隆数组，然后进行一系列的调取算法 在 Java 提供的 Applet 中也使用了大量的钩子来提供行为 与策略模式关系也会会感觉它和策略模式非常相似，但是其实有着本质的不同，虽然它们都是封装算法，但是一个用组合一个用继承。策略模式：是定义一个算法家族（具体实现），并让这些算法可以互换。模板方法：定义一个算法的大纲（算法的步骤），由子类定义其中某些步骤的内容。 模板方法可以定义具体的方法、抽象方法、钩子或者可以说，工厂方法也是模板方法的一种特殊版本。 参考http://www.cnblogs.com/java-my-life/archive/2012/05/14/2495235.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis学习笔记]]></title>
    <url>%2F2017%2F09%2F28%2FMyBatis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[MyBatis 是一个 Java 持久化框架，它通过 XML 描述符或注解把对象与存储过程或 SQL 语句关联起来。MyBatis 是在 Apache 许可证 2.0 下分发的自由软件；MyBatis 的前身是 iBatis ，是 Apache 的一个开源项目由于 MyBatis 是直接基于 JDBC 做了简单的映射包装，所以从性能的角度来看：JDBC &gt; MyBatis &gt; hibernate MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。 整体架构首先来看下 MyBatis 的整体架构，有一个大体的了解，相比 Hibernate 真是简单多了： 可以看出，MyBatis 也是依赖于两类配置文件，一类是主配置文件（只有一个），一般约定命名为 mybatis-config.xml ，配置了运行参数、插件、连接池等信息。还有就是 Mapper.xml 映射文件，可以有多个，里面配置的是 Statement （简单说是 SQL 也行）执行的时候会通过主配置文件构建出 SqlSessionFactory ，然后获得 SqlSession 对象，利用 SqlSession 就可以操作数据库了SqlSession 的底层会通过一个执行器来执行 Statement（SQL），执行器一般有两种实现，一种是基本的，一种是带有缓存功能的前面说过 Statement 可以简单理解为 SQL 语句，一般我们写 SQL 语句都是用 ？占位符，所以需要输入参数，然后执行，返回执行结果，至于输入输出的类型，图上已经说的很清楚了 一个入门栗子了解其的最好方法就是看文档，官方有中文文档哦，首先我们需要配置基本的配置文件： 123456789101112131415161718192021222324&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;!-- 引入外部资源配置文件，以使用 $&#123;&#125; --&gt; &lt;properties resource="jdbc.properties" /&gt; &lt;!-- 配置环境，数据库等信息 --&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;driver&#125;"/&gt; &lt;property name="url" value="$&#123;url&#125;"/&gt; &lt;property name="username" value="$&#123;username&#125;"/&gt; &lt;property name="password" value="$&#123;password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 引入映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource="org/mybatis/example/BlogMapper.xml"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 上面就是主配置文件，如果是 Maven 工程，默认是在工程的 resurces 中查找映射文件，所以可以直接写文件名；然后来看看映射文件应该怎么写，这个映射和 Hibernate 的可不一样，简单的多，其实就是写 SQL 语句： 123456789&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="com.bfchengnuo"&gt; &lt;select id="selectBlog" resultType="com.bfchengnuo.domain.Blog"&gt; select * from Blog where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; resultType 指定的就是结果集映射到的相应的实体类，其他的先不说，看完下面的 Java 代码更好理解，下面是一段基本的 MyBatis 使用： 1234567891011String resource = "org/mybatis/example/mybatis-config.xml";// Resources 是 MyBatis 提供的工具类InputStream inputStream = Resources.getResourceAsStream(resource);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);SqlSession session = sqlSessionFactory.openSession();try &#123; Blog blog = (Blog) session.selectOne("com.bfchengnuo.selectBlog", 101);&#125; finally &#123; session.close();&#125; 然后就可以看出，通过 session 的 selectOne 方法进行查询的时候是用 namespace.id 来定位 Statement 的 添加日志支持按照上面的方法确实是可以执行，但是控制台没任何日志输出，也看不到执行的 SQL，如果想了解就需要添加日志支持，既然是日志，那就用大名鼎鼎的 log4j 了，MyBatis 是支持的，会自动检测，如果发现有 slf 就会加载的~所以只需要写个配置文件了。导入依赖这个就不用说了，只要在 log4j 的配置文件中写入下面的代码即可看到效果： 12345log4j.rootLogger=DEBUG,A1log4j.logger.org.mybatis = DEBUGlog4j.appender.A1=org.apache.log4j.ConsoleAppenderlog4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=%-d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c]-[%p] %m%n 简单说下意思：第一行是设置日志的等级和位置（可以随便起个名字，比如 A1），MyBatis 的许多信息都是用的 Debug 级别，可以去源码看看；第二行是单独指定 MyBatis 的级别，第一行是全局的，相当于个性化设置；第三行就是指定位置（A1）具体是什么，这里是控制台；下面是布局和格式，d-时间；t-线程名；p-显示级别名；n-换行；关于 log4j 的使用，待补充… 进行CRUD操作下面就再深入一点，看下 MyBatis 是如何进行 CRUD 操作的：首先来定义映射文件 UserMapper （习惯上，命名为 xxxMapper）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="user"&gt; &lt;select id="queryUserById" resultType="cn.itcast.mybatis.pojo.User"&gt; SELECT *,user_name userName FROM tb_user WHERE id = #&#123;id&#125; &lt;/select&gt; &lt;select id="queryAll" resultType="cn.itcast.mybatis.pojo.User"&gt; SELECT *,user_name userName FROM tb_user &lt;/select&gt; &lt;insert id="saveUser" parameterType="cn.itcast.mybatis.pojo.User"&gt; INSERT INTO tb_user ( id, user_name, age, sex, updated ) VALUES ( NULL, #&#123;userName&#125;, #&#123;age&#125;, #&#123;sex&#125;, NOW() ); &lt;/insert&gt; &lt;update id="updateUser" parameterType="cn.itcast.mybatis.pojo.User"&gt; UPDATE tb_user SET user_name = #&#123;userName&#125;, age = #&#123;age&#125;, sex = #&#123;sex&#125;, updated = NOW() WHERE id = #&#123;id&#125; &lt;/update&gt; &lt;delete id="deleteUserById" parameterType="java.lang.Long"&gt; DELETE FROM tb_user WHERE id = #&#123;id&#125; &lt;/delete&gt;&lt;/mapper&gt; 一般一个 mapper 文件对应一个 Dao 层中的一些方法，一般来说方法名就是其 id，毕竟方法的执行需要 SQL 语句；注意下它们的标签就行了，下面 dao 层的具体实现也是都差不多，就是方法名和 SQL 语句的区别： 12345678910111213141516171819202122public User queryUserById(Long id) &#123; return this.sqlSession.selectOne("user.queryUserById", id);&#125;public List&lt;User&gt; queryAll() &#123; return this.sqlSession.selectList("user.queryAll");&#125;public void saveUser(User user) &#123; this.sqlSession.insert("user.saveUser", user); this.sqlSession.commit();//提交事务&#125;public void updateUser(User user) &#123; this.sqlSession.update("user.updateUser", user); this.sqlSession.commit();//提交事务&#125;public void deleteUserById(Long id) &#123; this.sqlSession.delete("user.deleteUserById", id); this.sqlSession.commit();//提交事务&#125; 于是就会想，能不能不写这个 dao 的实现，因为感觉都是重复代码啊….最好是连映射文件也不用写…..这是可能的，后面再说（如果连接口都不用写那就更爽了） 解决字段名于属性名不一致的问题：最容易想的是用别名的方式，在 SQL 语句中将字段名起一个和属性名相同的别名就可以了，例如：select *,user_name userName from users 就能正确映射了或者直接用 MyBatis 提供的功能，在主配置文件中加入： 123&lt;settings&gt; &lt;setting name="mapUnderscoreToCamelCase" value="true"/&gt;&lt;/settings&gt; 这样貌似更简单是吧…..但是只限于驼峰命名规则，也就是：A_COLUMN ===&gt; aColumn 动态代理实现DAOMyBatis 提供了使用动态代理的方式来实现 DAO，也就是说只需要写个 dao 的接口就行了，而不需要写具体的实现类；然后先来介绍映射文件 XML 法，让我们只要配置好映射文件就不需要再写实现类： Mapper 中的 namespace 指定为接口 id 对应接口中的方法名 接口方法中的参数必须和 Mapper 中 parameterType 配置的一致（可以省略，会根据传入值进行判断） 接口方法的返回值必须和 Mapper 中 resultType 配置的一致（不可省略） 满足上面四个条件应该就可以使用了，所以 dao 层接口又称为是 mapper 接口，使用的时候直接通过下面一行代码获取代理对象： 1234// 设置事务自动提交SqlSession session = sqlSessionFactory.openSession(true);userDAO = sqlSession.getMapper(UserDAO.class); 这样就省去了写 dao 实现类的时间，如果还想省去 XML 文件那么就需要使用注解了，它就是专门做这个的，关于注解等会再说 那么为什么不需要具体的实现类呢，很显然使用的是动态代理技术；源码中有个 MapperProxy 类来做这件事，它实现了 InvocationHandler 接口，当我们调用 getMapper 方法时，就会创建出相应的一个代理对象，当我们执行这个接口的方法时就会调用 MapperProxy 中的 invoke 方法，从而不需要具体的实现类了。那么它如何找到相关的 SQL 语句呢，在 MyBatis 加载主配置文件的同时，映射文件也一同加载了，如果接口和配置文件是对应的，那么就可以利用动态代理以及反射拿到接口名、方法名等数据通过方法名等找到对应的映射配置文件，也就知道对应的 SQL 语句了。我们使用 getMapper 方法的时候还传入了一个类，内部通过使用泛型做了强转，所以即使是返回的代理对象我们可以直接用 UserDAO 去接收。 这里的 getMapper 方法的实现应该是： 1234@Overridepublic &lt;T&gt; T getMapper(Class&lt;T&gt; type) &#123; return configuration.&lt;T&gt;getMapper(type, this);&#125; 它是在 DefaultSqlSession 中被定义的，可以看出深层次的调用中还传入了 this （DefaultSqlSession），这是为后面调用 selectOne、selectList 等方法做准备。在 MyBatis 加载配置文件的时候（准确说是 build 的时候），就会判断映射文件配置的 namespace 是不是个接口，如果是就做为 key （class 对象），并且根据这个接口创建一个 MapperProxyFactory 对象作为值，put 进一个叫 knownMappers 的 Map 对象中去。在调用 getMapper 的时候，其实就是在从这个 Map 中获取相应的代理工厂，最终通过 MapperProxyFactory 的 newInstance 方法生产出一个具体的代理对象（ MapperProxy ），其中会根据返回值而调用不同的 SQL 查询方法，调用会触发其 invoke 方法 configuration常用配置因为官方有中文文档，所以可以直接去看官方的详细解释，这里写几个比较常用的以便查询下面的配置都是写在主配置文件里的！！ 属性（properties）常用它来读取外部的配置文件（properties 文件），然后用 ${name} 来引用，让配置更加的灵活，例如可以这样定义： 1234&lt;properties resource="org/mybatis/example/config.properties"&gt; &lt;property name="username" value="dev_user"/&gt; &lt;property name="password" value="F2Fa3!33TYyg"/&gt;&lt;/properties&gt; 这样是不是感觉更灵活了，读取顺序是先读取 properties 中定义的，再读取指定的文件，出现重名的情况时，文件中定义的会覆盖 xml 定义的；从 MyBatis 3.4.2 开始，你可以为占位符指定一个默认值，但需要先开启这个功能，详情可看官方，因为感觉用的不多就不贴了 设置（settings）第一个其实已经说过了，就是解决属性名和字段名不同的问题，会自动转换的那个（mapUnderscoreToCamelCase）；然后重点是这几个，后面会说到： 设置参数 描述 有效值 默认值 cacheEnabled 该配置影响的所有映射器中配置的缓存的全局开关。 true / false true lazyLoadingEnabled 延迟加载的全局开关。当开启时，所有关联对象都会延迟加载。 特定关联关系中可通过设置fetchType属性来覆盖该项的开关状态。 true / false false aggressiveLazyLoading 当开启时，任何方法的调用都会加载该对象的所有属性。否则，每个属性会按需加载（参考lazyLoadTriggerMethods). true / false false (true in ≤3.4.1) mapUnderscoreToCamelCase 是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射。 true / false False 类型别名（typeAliases）简单说就是把 Java 类型取一个别名，在其他地方用的时候就不需要写一大堆的包了 1234567&lt;typeAliases&gt; &lt;typeAlias alias="Author" type="domain.blog.Author"/&gt; &lt;typeAlias alias="Blog" type="domain.blog.Blog"/&gt; &lt;!-- 可以写一个包，将会从这个包下找 JavaBean --&gt; &lt;package name="domain.blog"/&gt;&lt;/typeAliases&gt; 别名的首字母是不区分大小写的，但是习惯是大写，毕竟是类；然后 MyBatis 定义了一些默认的别名，直接用就可以了： 别名 映射的类型 _byte byte _long long _short short _int int _integer int _double double _float float _boolean boolean string String byte Byte long Long short Short int Integer integer Integer double Double float Float boolean Boolean date Date decimal BigDecimal bigdecimal BigDecimal object Object map Map hashmap HashMap list List arraylist ArrayList collection Collection iterator Iterator 另外，当传入的是个数组的时候，根据源码的定义要类似这样的形式：parameterType=&quot;Object[]&quot; ，解析配置文件的过程中在 TypeAliasRegistry 这个类中定义了这些别名。 环境（environments）前面刚开始就用到了，用它来配置的数据库信息，因为一般都有有多个环境，比如开发环境、测试环境、生产环境；每个环境的数据库是不一样，这样做是为了便于分离可以使用 new SqlSessionFactoryBuilder().build(reader, environment); 来指定加载某个环境的配置实际上，都是用 Spring 来处理这个，所以了解就好 映射器（mappers）这个是一个重点，mappers 可以通过四种方式引用： 12345678910111213141516171819&lt;!-- Using classpath relative resources --&gt;&lt;mappers&gt; &lt;mapper resource="org/mybatis/builder/AuthorMapper.xml"/&gt; &lt;mapper resource="org/mybatis/builder/BlogMapper.xml"/&gt;&lt;/mappers&gt;&lt;!-- Using url fully qualified paths --&gt;&lt;mappers&gt; &lt;mapper url="file:///var/mappers/AuthorMapper.xml"/&gt; &lt;mapper url="file:///var/mappers/BlogMapper.xml"/&gt;&lt;/mappers&gt;&lt;!-- Using mapper interface classes --&gt;&lt;mappers&gt; &lt;mapper class="org.mybatis.builder.AuthorMapper"/&gt; &lt;mapper class="org.mybatis.builder.BlogMapper"/&gt;&lt;/mappers&gt;&lt;!-- Register all interfaces in a package as mappers --&gt;&lt;mappers&gt; &lt;package name="org.mybatis.builder"/&gt;&lt;/mappers&gt; 第一种是比较熟的了；第二种通过绝对路径的方式基本不会用；第三种通过类引用就要必须将映射文件和接口文件放在一起，名字也要统一；第四种的包扫描方式也是如此，需要放在一起才行这四种并没有想象中的那样优雅，但是好消息是，和 Spring 整合后就能使用更优雅的方式了，不用和类混在一起也不需要配置那么多 mapper，等下篇说吧 Mapper常用配置这里所有的配置都是写在映射文件中的哦。这方面官方文档中写的也是非常详细的，也是挑常用的说，SQL 映射文件有很少的几个顶级元素（按照它们应该被定义的顺序）： cache – 给定命名空间的缓存配置。 cache-ref – 其他命名空间缓存配置的引用。 resultMap – 是最复杂也是最强大的元素，用来描述如何从数据库结果集中来加载对象。 sql – 可被其他语句引用的可重用语句块。 insert – 映射插入语句 update – 映射更新语句 delete – 映射删除语句 select – 映射查询语句 最后的四个其实已经用过了，对应 CRUD 操作，对它们还是有些补充的东西；对于 insert/update，如果设置的是 id 自动增长，插入一条数据后是获取不到 id 的，为了让 id 回填，需要配置一些东西（下表加黑的属性）： 属性 描述 id 命名空间中的唯一标识符，可被用来代表这条语句。 parameterType 将要传入语句的参数的完全限定类名或别名。这个属性是可选的，因为 MyBatis 可以通过 TypeHandler 推断出具体传入语句的参数，默认值为 unset。 flushCache 将其设置为 true，任何时候只要语句被调用，都会导致本地缓存和二级缓存都会被清空，默认值：true（对应插入、更新和删除语句）。 timeout 这个设置是在抛出异常之前，驱动程序等待数据库返回请求结果的秒数。默认值为 unset（依赖驱动）。 statementType STATEMENT，PREPARED 或 CALLABLE 的一个。这会让 MyBatis 分别使用 Statement，PreparedStatement 或 CallableStatement，默认值：PREPARED。 useGeneratedKeys （仅对 insert 和 update 有用）这会令 MyBatis 使用 JDBC 的 getGeneratedKeys 方法来取出由数据库内部生成的主键（比如：像 MySQL 和 SQL Server 这样的关系数据库管理系统的自动递增字段），默认值：false。 keyProperty （仅对 insert 和 update 有用）唯一标记一个属性，MyBatis 会通过 getGeneratedKeys 的返回值或者通过 insert 语句的 selectKey 子元素设置它的键值，默认：unset。如果希望得到多个生成的列，也可以是逗号分隔的属性名称列表。 keyColumn （仅对 insert 和 update 有用）通过生成的键值设置表中的列名，这个设置仅在某些数据库（像 PostgreSQL）是必须的，当主键列不是表中的第一列的时候需要设置。如果希望得到多个生成的列，也可以是逗号分隔的属性名称列表。 databaseId 如果配置了 databaseIdProvider，MyBatis 会加载所有的不带 databaseId 或匹配当前 databaseId 的语句；如果带或者不带的语句都有，则不带的会被忽略。 简单来说是 useGeneratedKeys 开启回填 id；keyColumn 指定数据库中的列（如果和 keyProperty 相同可以省略不写）；keyProperty 指定对象中的属性名： 12345&lt;insert id="insertAuthor" parameterType="Author" keyProperty="id" keyColumn="id" useGeneratedKeys="true" &gt; 这样插入以后获取 id 就是有值的 resultMap官方给出的介绍是：是最复杂也是最强大的元素，用来描述如何从数据库结果集中来加载对象；可以看出这个标签是很重要的，同时也看出了它的作用，就是来处理字段和属性之间的映射关系的，所以它也是可以处理字段名和属性名不同的问题，例如： 123456789&lt;!-- 定义 --&gt;&lt;resultMap id="userResultMap" type="User"&gt; &lt;id property="id" column="user_id" /&gt; &lt;result property="username" column="user_name" /&gt;&lt;/resultMap&gt;&lt;!-- 使用 --&gt;&lt;select id="selectUser" resultMap="userResultMap"&gt; select * from user where id = #&#123;id&#125;&lt;/select&gt; 其中 resultMap 有一个 autoMapping 自动映射的属性，开启后如果没写的字段也会进行映射，也就是不用写全，这个默认应该是开启状态（当使用了集合等标签默认是关闭），但还是写上比较好 sql片段sql 片段就是为了 sql 代码的复用，和 Java 代码类似，如果有很多重复的 sql 语句，可以提取出来，也便于以后的统一维护；一般情况下，我们会把这些 sql 片段统一在一个单独的 mapper 文件中，就像 Java 中的工具类；下面就来看看 sql 片段的写法和使用： 123456789&lt;sql id="userColumns"&gt; $&#123;alias&#125;.id,$&#123;alias&#125;.username,$&#123;alias&#125;.password &lt;/sql&gt;&lt;!-- 使用片段 --&gt;&lt;select id="selectUsers" resultType="map"&gt; select &lt;include refid="userColumns"&gt;&lt;property name="alias" value="t1"/&gt;&lt;/include&gt;, &lt;include refid="userColumns"&gt;&lt;property name="alias" value="t2"/&gt;&lt;/include&gt; from some_table t1 cross join some_table t2&lt;/select&gt; 使用 include 标签来引用，在这其中可以使用 property 标签来指定片段中引用的值 $与#.在 Mapper 中，参数传递有两种方式，一种是 ${} 另一种是 #{} 它们有很大的区别； #{} 是实现的 sql 语句的预处理，之后执行的 sql 中会用 ？来代替，不需要关注数据类型（自动处理），并且能防止 SQL 注入，因为本质是占位符，所以名字其实可以随便写（#{xxx}），最后都会换成 ？，前提是参数是基本数据类型； ${} 是 SQL 语句的直接拼接，不做数据类型转换，需要自行判断数据类型，不能防止 SQL 注入； 当我们的表名不确定时，就可以使用 $ 了，select * from ${tabName} ，表名可以通过参数传递过来，如果使用 # 那么最终的 sql 语句就会变成：select * from ? ，当然会报错；使用 $ 时要注意的是它默认会从传入的对象中找 getter 方法获得配置的名（# 也是类似，但当传入的是基本数据类型时名称其实可以随便写），例如，你传入的是一个字符串类型的表名，但是它会调用 String.getTabName() 方法，自然会报错，所以你知道为什么前面 CRUD 的时候一条 SQL 可以使用多次 #{name} 了吧。解决方法有两种： 使用 ${value} ，这样就会获取默认传入的参数，它是 MyBatis 中提供的默认参数名 代码中使用注解 ( @Param )public List&lt;Map&lt;String,Object&gt;&gt; queryByTableName(@Param(&quot;tabName&quot;) String tabName); 还记得那张架构表么，resultType 的类型是可以设置为 Map 的，不过 Map 是个接口，需要写具体类，比如 HashMap； 多个参数当传入的是多个参数的时候，就和 #{name} 中的名字有关了，但是你直接写肯定是不行的，方法也是有两种： 使用 #{0} 、#{param1} 这样依次增加 代码中使用 @Param 注解，然后就可以使用 #{name} 的形式了 优先选择的当然是注解了，这样看着比较舒服….单个参数的时候 # 是与参数名无关的，最终反正会被替换成 ？ 当我们传入一个对象时，可以在 SQL 中用多个 #{name} 来获取对象中的属性，就像刚开始的栗子里用的一样；这种情况算是只传入了一个参数哦 使用OGNLMyBatis 默认采用的是 OGNL 表达式，所以在这里也是可以使用的，在写 SQL 的时候写的 #{name} 中就可以使用，还可以用在一些标签里面（比如 if 里的 test 表达式）。获取几种值的写法（对大小写敏感），例如获取基本数据类型是 #{_parameter}：获取基本类型：_parameter自定义类型：属性名获取集合：数组（用 array）、List（用 list ）、Map（用 _parameter ）；例如：对于基本数据类型的数组：array[索引]如果是 Map 的话就是 _parameter.key ，自定义的类型（Map&lt;string,obj&gt;） 可以直接 key.属性名 获取 在 OGNL 中可以直接使用 Java 中的操作符（+、-、/、==、!=、||、&amp;&amp;），以及调用 Java 中的一些方法 因为是写在 XML 中，所以有些符号需要转义，于是 OGNL 提供了自己的操作符代替（and、or、in、not in、mod）mod 就是取余 所以，当 parameterType 的类型是字符串或者基本数据类型时（因为值是唯一的），并且只有一个的情况下，可以 #{_parameter} 也可以 #{随便写点啥} 或者还可以使用 OGNL 的方式直接写 _parameter 其他因为 mapper 的定义可以省去写 dao 层接口实现类，所以很多情况下直接把原来的 dao 层命名为 mapper 层，原接口的命名 UserDAO 就写成了 UserMapper ，它们其实是指的一个东西，这样在使用 MyBatis 的情况下看着还是比较规范的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring中的AOP]]></title>
    <url>%2F2017%2F09%2F22%2FSpring%E4%B8%AD%E7%9A%84AOP%2F</url>
    <content type="text"><![CDATA[想了想还是单独来总结一篇吧，关于 AOP 内容不少，分散在两篇文章中也不好看，在 Spring 的两大核心 IoC 和 AOP 中，普遍认为 AOP 是比较难的，因为它的概念比较多吧，并且还都非常抽象，所以单独来一篇。虽然会和以前写的有重复，但也不删了，重就重吧；这篇大多都是说的理论，实践的话可暂时参考 [4] 或者 [0] 中的连接 什么是 AOPAOP (Aspect-Oriented Programming)，即 面向切面编程, 它与 OOP( Object-Oriented Programming, 面向对象编程) 相辅相成, 提供了与 OOP 不同的抽象软件结构的视角.在 OOP 中, 我们以类(class)作为我们的基本单元, 而 AOP 中的基本单元是 Aspect(切面) ，什么是切面下面会说。 关于AspectJAspectJ 实际上是对 AOP 编程思想的一个实践，当然，除了 AspectJ 以外，还有很多其它的 AOP 实现，例如 ASMDex，但目前最好、最方便的，依然是 AspectJ。也许是因为最早做 Java 的 AOP 实现的原因。AspectJ 可以干净地模块化横切关注点，基本上可以实现无侵入，同时学习成本低，功能强大（甚至可以通过修改字节码来实现多继承），可扩展性高。 AspectJ 意思就是 Java 的 Aspect，Java 的 AOP。 它其实不是一个新的语言，它就是一个代码编译器（也就是 AJC ），在 Java 编译器的基础上增加了一些它自己的关键字识别和编译方法。 因此，ajc 也可以编译 Java 代码。它在编译期将开发者编写的 Aspect 程序编织到目标程序中，对目标程序作了重构，目的就是建立目标程序与 Aspect 程序的连接（耦合，获得对方的引用（默认情况下，也就是不使用 this 或 target 来约束切点的情况下，那么获得的是声明类型，不是运行时类型）和上下文信息），从而达到 AOP 的目的（这里在编译期还是修改了原来程序的代码，但是是 AJC 替我们做的）。 AspectJ 是一套独立的面向切面解决方案，你可以在 Eclipse 的官网找到它，然后下载安装，然后使用它的 ajc.exe 进行编译，入门请参考：http://zhoujingxian.iteye.com/blog/667214使用 AspectJ 有两种方法： 完全使用 AspectJ 的语言。这语言一点也不难，和 Java 几乎一样，也能在 AspectJ 中调用 Java 的任何类库。AspectJ 只是多了一些关键词罢了。 使用纯 Java 语言开发，然后使用AspectJ注解，也就是 @AspectJ。 当然不论哪种方法，最后都需要 AspectJ 的编译工具 AJC 来编译。关于 AspectJ 就不多说了，想详细了解的见参考 [1] ；另外 Android 中也有很多人开始使用 AspectJ 了！ SpringAOP与AspectJ首先，Spring 采用的就是 AspectJ，在 Spring 的 AOP 相关包里你能看到 AspectJ 的影子；Spring AOP 采用的动态织入，而 AspectJ 是静态织入。静态织入：指在编译时期就织入，即：编译出来的class文件，字节码就已经被织入了。动态织入又再分静动两种，静则指织入过程只在第一次调用时执行；动则指根据代码动态运行的中间状态来决定如何操作，每次调用 Target Object 的时候都执行。 使用 Spring 自己原生的 AOP，你需要实现大量的接口，继承大量的类，所以Spring AOP一度被人所诟病，这与它的无侵入，低耦合完全冲突，我在笔记一中采用的就是这种方式，确实麻烦。不过 Spring 对开源的优秀框架，组件向来是采用兼容，并入的态度。所以，后来的 Spring 就提供了 Aspectj 支持，也就是我们后来所说的基于纯 POJO 的 AOP。 AOP中的术语通知/增强（Advice）切面的工作被称为通知，通知定义了切面是什么以及何时使用。除了描述切面要完成的工作，通知还解决了何时执行这个工作的问题。它应该应用在某个方法被调用之前？之后？之前和之后都调用？还是只是在方法抛出异常时调用？简单说就是你想要（切入）的具体功能实现(想要干啥)，比如安全，事物，日志操作等。 许多 AOP框架, 包括 Spring AOP, 会将 advice 模拟为一个拦截器(interceptor), 并且在 join point 上维护多个 advice, 进行层层拦截.例如 HTTP 鉴权的实现, 我们可以为每个使用 RequestMapping 标注的方法织入 advice, 当 HTTP 请求到来时, 首先进入到 advice 代码中, 在这里我们可以分析这个 HTTP 请求是否有相应的权限, 如果有, 则执行 Controller, 如果没有, 则抛出异常. 这里的 advice 就扮演着鉴权拦截器的角色了. 连接点(JoinPoint)连接点是在应用执行过程中能够插入切面的一个点。这个点可以是调用方法、抛出异常时、甚至修改一个字段时。切面代码可以利用这些点插入到应用的正常流程之中，并添加新的行为。就是 Spring 允许你使用通知的地方 另一种说法：程序运行中的一些时间点, 例如一个方法的执行, 或者是一个异常的处理；在 Spring AOP 中, join point 总是方法的执行点, 即只有方法连接点.Spring 中是方法、方法、方法，重要的事情说三遍！ 切点(Pointcut)一组连接点的总称，用于指定某个通知（增强）应该在何时被调用。切入点是「在哪干」，你可能在很多地方（连接点）都可以干，但并不是每个地方都要干，要干的地方叫切点 在 Spring 中, 所有的方法都可以认为是 joinpoint, 但是我们并不希望在所有的方法上都添加 Advice, 而 pointcut 的作用就是提供一组规则(使用 AspectJ pointcut expression language 来描述) 来匹配 joinpoint, 给满足规则的 joinpoint 添加 Advice 切面（Aspect）切面是通知和切点的结合。通知和切点共同定义了切面的全部内容——它是什么，在何时和何处完成其功能通知说明了干什么和什么时候干（什么时候通过方法名中的before、after、around 等就能知道），而切入点说明了在哪干（指定到底是哪个方法），这就是一个完整的切面定义。Spring AOP 就是负责实施切面的框架, 它将切面所定义的横切逻辑织入到切面所指定的连接点中.可以简单地认为, 使用 @Aspect 注解的类就是切面 joinPoint 和 pointCut 的区别在 Spring AOP 中, 所有的方法执行都是 join point. 而 point cut 是一个描述信息, 它修饰的是 join point, 通过 point cut, 我们就可以确定哪些 join point 可以被织入 Advice. 因此 join point 和 point cut 本质上就是两个不同纬度上的东西.advice 是在 join point 上执行的, 而 point cut 规定了哪些 join point 可以执行哪些 advice 目标对象(Target)织入 advice 的目标对象. 目标对象也被称为 advised object.因为 Spring AOP 使用运行时代理的方式来实现 aspect, 因此 adviced object 总是一个代理对象(proxied object)注意，adviced object 指的不是原来的类, 而是织入 advice 后所产生的代理类. 织入(weaving)把切面应用到目标对象来创建新的代理对象的过程。专业点的说法是：将 aspect 和其他对象连接起来, 并创建 adviced object 的过程。有3种方式，spring 采用的是运行时（上面提到过）： 编译器织入, 这要求有特殊的Java编译器. 类装载期织入, 这需要有特殊的类装载器. 动态代理织入, 在运行期为目标类添加增强(Advice)生成子类的方式.Spring 采用动态代理织入, 而AspectJ采用编译器织入和类装载期织入. 关注点（concern）对软件工程有意义的小的、可管理的、可描述的软件组成部分，一个关注点通常只同一个特定概念或目标相关联。 通俗来说其实指的就是重复代码；在一些方法中，我们可能需要写许多重复代码，而真正的核心代码就只有几行。比如 DAO 层中的 session 处理的代码，又是事务又是连接的，每个方法都需要写，这些代码就可以看作是关注点，我们要做的就是分离它，使用代理，可以在运行期间，执行核心业务代码的时候动态植入关注点代码AOP 的目标就是让关注点代码和业务代码进行分离不太严谨的讲，切面也可以理解为关注点形成的类，也就是说很多重复的代码就可以形成一个切面 通知(advice)类型Spring 中的通知有五种类型： 前置通知(Before)：在目标方法被调用之前调用通知功能；在 join point 前被执行的 advice. 虽然 before advice 是在 join point 前被执行, 但是它并不能够阻止 join point 的执行, 除非发生了异常(即我们在 before advice 代码中, 不能人为地决定是否继续执行 join point 中的代码) 后置通知(After)：在目标方法完成之后调用通知；不管是否正常退出或者发生了异常都会执行 返回通知/最终通知(After-returning)：在目标方法成功执行之后调用通知，如果有后置通知会在其之后执行；在方法正常返回后执行 异常通知(After-throwing)：在目标方法抛出异常后调用通知； 环绕通知(Around)：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。在 join point 前和 joint point 退出后都执行的 advice. 这个是最常用的 advice. 运行顺序：前置通知/环绕通知–目标方法执行–返回通知/异常通知–后置通知/环绕通知这里需要注意的是：环绕通知由于和前置、后置处于同一个 aspect 内，所以是无法确定其执行顺序的，当然可以通过其他手段来解决实际开发中，一般会将顺序执行的 Advice 封装到不同的 Aspect，然后通过注解或者实现接口的方式控制 Aspect 的执行顺序，二选一（对于在同一个切面定义的通知函数将会根据在类中的声明顺序执行） 关于 AOP ProxySpring AOP 默认使用标准的 JDK 动态代理(dynamic proxy)技术来实现 AOP 代理, 通过它, 我们可以为任意的接口实现代理.如果需要为一个类（没有实现接口）实现代理, 那么可以使用 CGLIB 代理.当一个业务逻辑对象没有实现接口时, 那么Spring AOP 就默认使用 CGLIB 来作为 AOP 代理了. 即如果我们需要为一个方法织入 advice, 但是这个方法不是一个接口所提供的方法, 则此时 Spring AOP 会使用 CGLIB 来实现动态代理. 鉴于此, Spring AOP 建议基于接口编程, 对接口进行 AOP 而不是类. JDK 中的代理一般是 $ 开头，Cglib 一般就是代理对象名开头后面再加一些东西这两种我以前都写过，关于 JDK 动态代理的介绍我放在参考[2]；关于 Cglib 的介绍使用我放在参考[3] @AspectJ 支持@AspectJ 是一种使用 Java 注解来实现 AOP 的编码风格.@AspectJ 风格的 AOP 是 AspectJ Project 在 AspectJ 5 中引入的, 并且 Spring 也支持@AspectJ 的 AOP 风格. 使能 @AspectJ 支持@AspectJ 可以以 XML 的方式或以注解的方式来使用, 并且不论以哪种方式使能 @ASpectJ, 我们都必须保证 aspectjweaver.jar 在 classpath 中.使用 Java Configuration 方式使能 @AspectJ，想深入了解的可见参考[5]： 123@Configuration@EnableAspectJAutoProxypublic class AppConfig &#123;&#125; @Configuration 注解简单说就是把一个类作为 IoC 容器（还可配合 @Bean 使用，写 Spring 注解的时候有解释）；使用 XML 方式使能 @AspectJ ，顺便记得加上命名空间： 1234567891011121314151617181920&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context.xsdhttp://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop.xsdhttp://www.springframework.org/schema/txhttp://www.springframework.org/schema/tx/spring-tx.xsd"&gt; &lt;!-- 自动扫描方式加载对象 --&gt; &lt;context:component-scan base-package="com.bfchengnuo"/&gt; &lt;!-- 启动 @AspectJ 支持 --&gt; &lt;aop:aspectj-autoproxy/&gt;&lt;/beans&gt; 常用注解定义 aspect(切面)当使用注解 @Aspect 标注一个 Bean 后, 那么 Spring 框架会自动收集这些 Bean, 并添加到 Spring AOP 中, 例如: 123@Component@Aspectpublic class MyTest &#123;&#125; 注意：仅仅使用 @Aspect 注解, 并不能将一个 Java 对象转换为 Bean，因此我们还需要使用类似 @Component 之类的注解.如果一个 类被 @Aspect 标注, 则这个类就不能是其他 aspect 的 advised object 了, 因为使用 @Aspect 后, 这个类就会被排除在 auto-proxying 机制之外. 也就是说：Spring 将会把它当作一个特殊的 Bean（一个切面），也就是说不对这个类本身进行动态代理 声明 pointcut一个 pointcut 的声明由两部分组成: 一个方法签名, 包括方法名和相关参数 一个 pointcut 表达式, 用来指定哪些方法执行是我们感兴趣的(即因此可以织入 advice). 在 @AspectJ 风格的 AOP 中, 我们使用一个方法来描述 pointcut, 一般用空方法即可，方法名随意，即: 12@Pointcut("execution(* com.xys.service.UserService.*(..))") // 切点表达式private void dataAccessOperation() &#123;&#125; // 切点前面 这个方法必须无返回值.这个方法本身就是 pointcut signature；pointcut 表达式使用 @Pointcut 注解指定.上面我们简单地定义了一个 pointcut, 这个 pointcut 所描述的是: 匹配所有在包 com.xys.service.UserService 下的所有方法的执行，第一个 * 指的是任何返回值，(..) 指的是此方法的任何参数，多个可以使用 || 分割，其他的应该就更好理解了；需要注意的是最后一定是定义到方法的！ 声明 adviceadvice 是和一个 pointcut 表达式关联在一起的, 并且会在匹配的 join point 的方法执行的前/后/周围 运行.pointcut 表达式可以是简单的一个 pointcut 名字的引用, 或者是完整的 pointcut 表达式.下面我们以几个简单的 advice 为例子, 来看一下一个 advice 是如何声明的. 123456789101112131415161718@Component@Aspectpublic class BeforeAspectTest &#123; // 定义一个 Pointcut, 使用 切点表达式函数 来描述对哪些 Join point 使用 advise. @Pointcut("execution(* com.xys.service.UserService.*(..))") public void dataAccessOperation() &#123; &#125;&#125;/*************************分割线*****************************/@Component@Aspectpublic class AdviseDefine &#123; // 定义 advise @Before("com.xys.aspect.PointcutDefine.dataAccessOperation()") public void doBeforeAccessCheck(JoinPoint joinPoint) &#123; System.out.println("*****Before advise, method: " + joinPoint.getSignature().toShortString() + " *****"); &#125;&#125; 可以看出 @Before 引用的是一个 pointcut 的名字，并且这个 pointcut 并不是和它在同一个包下的，如果在同一个类中那直接写名字就可以了，或者直接把表达式放在 @Before 中就行了 这里再着重说下 around advice，因为 around advice 比较特别, 它可以在一个方法的之前之前和之后添加不同的操作, 并且甚至可以决定何时, 如何, 是否调用匹配到的方法 12345678910111213141516@Component@Aspectpublic class AdviseDefine &#123; // 定义 advise @Around("com.xys.aspect.PointcutDefine.dataAccessOperation()") public Object doAroundAccessCheck(ProceedingJoinPoint pjp) throws Throwable &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); // 开始 Object retVal = pjp.proceed(); stopWatch.stop(); // 结束 System.out.println("invoke method: " + pjp.getSignature().getName() + ", elapsed time: " + stopWatch.getTotalTimeMillis()); return retVal; &#125;&#125; 诺，就是这样，其他的通知就不写了，都是一样的，就是注解不一样而已 切点标志符(designator)AspectJ5 的切点表达式由标志符(designator)和操作参数组成. 如 &quot;execution(* greetTo(..))&quot; 的切点表达式, execution 就是标志符, 而圆括号里的 * greetTo(..) 就是操作参数下面介绍几个标志符： execution匹配 join point 的执行, 例如 &quot;execution(* hello(..))&quot; 表示匹配所有目标类中的 hello() 方法. 这个是最基本的 pointcut 标志符.不写包名默认就是本包内，必须要精确到方法 within匹配特定包下的所有 join point, 例如 within(com.xys.*) 表示 com.xys 包中的所有连接点, 即包中的所有类的所有方法. 而 within(com.xys.service.*Service) 表示在 com.xys.service 包中所有以 Service 结尾的类的所有的连接点. this 与 targetthis 的作用是匹配一个 bean, 这个 bean(Spring AOP proxy) 是一个给定类型的实例(instance of).而 target 匹配的是一个目标对象(target object, 即需要织入 advice 的原始的类), 此对象是一个给定类型的实例(instance of). bean匹配 bean 名字为指定值的 bean 下的所有方法 12bean(*Service) // 匹配名字后缀为 Service 的 bean 下的所有方法bean(myService) // 匹配名字为 myService 的 bean 下的所有方法 args匹配参数满足要求的的方法，一般和其他的标志符连用，例如： 12345678910111213141516171819202122@Pointcut("within(com.xys.demo2.*)")public void pointcut2() &#123;&#125;@Before(value = "pointcut2() &amp;&amp; args(name)")public void doSomething(String name) &#123; logger.info("---page: &#123;&#125;---", name);&#125;/*********************分割线*************************/@Servicepublic class NormalService &#123; private Logger logger = LoggerFactory.getLogger(getClass()); public void someMethod() &#123; logger.info("---NormalService: someMethod invoked---"); &#125; public String test(String name) &#123; logger.info("---NormalService: test invoked---"); return "服务一切正常"; &#125;&#125; 当 NormalService.test 执行时, 则 advice doSomething 就会执行, test 方法的参数 name 就会传递到 doSomething 中，常用的一些栗子有： 1234567891011121314// 匹配只有一个参数 name 的方法@Before(value = "aspectMethod() &amp;&amp; args(name)")public void doSomething(String name) &#123;&#125;// 匹配第一个参数为 name 的方法@Before(value = "aspectMethod() &amp;&amp; args(name, ..)")public void doSomething(String name) &#123;&#125;// 匹配第二个参数为 name 的方法Before(value = "aspectMethod() &amp;&amp; args(*, name, ..)")public void doSomething(String name) &#123;&#125; 也就是说，这个标志符是非常重要的，经常能用到，因为使用方式非常的灵活。 @annotation匹配由指定注解所标注的方法 ，举个栗子： 123// 匹配由注解 AuthChecker 所标注的方法@Pointcut("@annotation(com.xys.demo1.AuthChecker)")public void pointcut() &#123;&#125; 常用的标志符就是上面的这些了，这些基本能满足大部分的需求了，大概…….. 常见的切点表达式标志符知道了，表达式也不在话下，上面说过表达式是由标志符和操作参数组成，那么来看一下常用的表达式吧： 匹配方法签名： 1234567891011121314// 匹配指定包中的所有的方法execution(* com.xys.service.*(..))// 匹配当前包中的指定类的所有方法execution(* UserService.*(..))// 匹配指定包中的所有 public 方法execution(public * com.xys.service.*(..))// 匹配指定包中的所有 public 方法, 并且返回值是 int 类型的方法execution(public int com.xys.service.*(..))// 匹配指定包中的所有 public 方法, 并且第一个参数是 String, 返回值是 int 类型的方法execution(public int com.xys.service.*(String name, ..)) 匹配类型签名： 1234567891011// 匹配指定包中的所有的方法, 但不包括子包within(com.xys.service.*)// 匹配指定包中的所有的方法, 包括子包within(com.xys.service..*)// 匹配当前包中的指定类中的方法within(UserService)// 匹配一个接口的所有实现类中的实现的方法within(UserDao+) 匹配 Bean 的名字： 12// 匹配以指定名字结尾的 Bean 中的所有方法bean(*Service) 常用的表达式组合： 12345// 匹配以 Service 或 ServiceImpl 结尾的 beanbean(*Service || *ServiceImpl)// 匹配名字以 Service 结尾, 并且在包 com.xys.service 中的 beanbean(*Service) &amp;&amp; within(com.xys.service.*) 以上，就是常用的了，应该能满足大部分的需求了吧…. 使用 XML 定义首先应当确保引入了命名空间，也就是 schema 的支持，先来看一段简单的栗子： 123456789101112131415161718&lt;aop:config&gt; &lt;aop:aspect id="myAspect" ref="aBean"&gt; &lt;aop:pointcut id="businessService" expression="execution（* com.xyz.myapp.service.*.*（..）） &amp;&amp; this（service）"/&gt; &lt;aop:before pointcut-ref="businessService" method="monitor"/&gt; ... &lt;/aop:aspect&gt;&lt;/aop:config&gt;&lt;bean id="aBean" class="..."&gt; ... &lt;/bean&gt;&lt;aop:config&gt; &lt;!-- 配置多个切点，&amp;&amp; || ! --&gt; &lt;aop:pointcut id="pc" expression="execution(public * com.*.service.*.*(..)) || execution(public * com.*.*.service.*.*(..))" /&gt; &lt;aop:advisor pointcut-ref="pc" advice-ref="userTxAdvice" /&gt;&lt;/aop:config&gt; 看以看出，所有的相关配置都写在 aop:config 中，aop:aspect 是定义一个切面，aop:pointcut 是定义一个切点，指明一个表达式，然后就是通过 aop:before 等一系列标签来指定通知了看过注解后再看 XML 大部分都是能直接看懂的。 补充一点，在 XML 中如果想定义多个切点，第一种可以在表达式使用 &amp;&amp;/and 或者 ||/or 连接，推荐使用小写字母；第二种可以定义多个 &lt;aop:advisor pointcut=&quot;&quot; /&gt; 关于 Hibernate 事务相关的 AOP 配置：点我跳转 参考&amp;拓展[0]：https://segmentfault.com/a/1190000007469968[1]：http://linbinghe.com/2017/65db25bc.html[2]：JDK动态代理[3]：关于Cglib[4]：https://jacksmiththu.github.io/2017/06/26/Spring%E4%B8%AD%E7%9A%84AOP/[5]：http://blog.csdn.net/ethanwhite/article/details/52050351]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring学习笔记（二）]]></title>
    <url>%2F2017%2F09%2F20%2FSpring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这里的 Spring 指的还是 SpringFramework ；Spring 是做什么的上一篇中已经说的很详细了，也主要是说的怎么用，这篇主要是对上篇的知识补充，多是理论的补充，有些重复内容，写的也很混乱….Spring AOP 详细的整理已出！强行下一篇就是！ 概述Spring 提供了一站式解决方案，它主要包含六大模块： Spring Core核心功能：IoC 容器，解决对象的创建及其依赖关系jar 包组成由：spring-expression-4.3.11.RELEASE.jarspring-beans-4.3.11.RELEASE.jarspring-core-4.3.11.RELEASE.jarcommons-logging-1.2.jarspring-context-4.3.11.RELEASE.jar Spring Web这是 spring 对 web 模块的支持，也就是说可以与 Struts 整合，把 Action 的创建交给 spring；或者可以使用自家的 springMVC，相关的 jar 包有：spring-web-4.3.11.RELEASE.jarspring-webmvc-portlet-4.3.11.RELEASE.jarspring-webmvc-4.3.11.RELEASE.jarspring-websocket-4.3.11.RELEASE.jar Spring DAO这是 spring 对 jdbc 的支持，比如 JdbcTemplate 模板工具类，相关的 jar 包有：spring-jdbc-4.3.11.RELEASE.jarspring-oxm-4.3.11.RELEASE.jarspring-jms-4.3.11.RELEASE.jarspring-tx-4.3.11.RELEASE.jarspring-orm-4.3.11.RELEASE.jar Spring ORM这是 spring 对 ORM 的支持，既可以与 hibernate 整合，也可以选择使用 spring 对 hibernate 操作的封装 Spring AOP上篇中说过了，对 AOP 的支持，也就是面向切面编程，相关的 jar 包有：spring-aop-4.3.11.RELEASE.jarspring-instrument-4.3.11.RELEASE.jarspring-aspects-4.3.11.RELEASE.jarspring-instrument-tomcat-4.3.11.RELEASE.jar Spring EE这是 spring 对 javaEE 其他模块的支持 Bean的创建关于 Bean 的生命周期在上一篇说的非常非常详细了，总结就是：如果你设置了单例模式，也就是默认的，并且懒加载没有开启（默认）那么它会在启动的时候创建（实例化）；但是如果设置的是 prototype 就是在获取的时候才进行实例化 使用构造函数在装配 Bean 的时候，上一篇使用的是 property 标签，通过相应的 Set 方法进行注入，其实还可以使用 constructor-arg 标签来进行初始化，它们的区别是一个是通过 set 方法，一个是通过构造函数（不一定非要是构造，其他带参数的方法也行，比如下面要说的工厂方式），默认用的就是无参的构造函数： 1234567891011121314151617&lt;bean id="student" class="com.rc.sp.Student"&gt; &lt;constructor-arg name="id" value="1"/&gt; &lt;constructor-arg name="name" value="student"/&gt; &lt;constructor-arg name="dream"&gt; &lt;list&gt; &lt;value&gt;soldier&lt;/value&gt; &lt;value&gt;scientist&lt;/value&gt; &lt;value&gt;pilot&lt;/value&gt; &lt;/list&gt; &lt;/constructor-arg&gt; &lt;constructor-arg name="score"&gt; &lt;map&gt; &lt;entry key="math" value="90"/&gt; &lt;entry key="english" value="85"/&gt; &lt;/map&gt; &lt;/constructor-arg&gt;&lt;/bean&gt; 但是需要注意下顺序，如果顺序和构造函数不同要使用 index （从 0 开始）或者 name 属性来指定；name 省略其实也可以 使用工厂创建 bean 的方式中，除了使用构造函数常见的还有就是使用工厂模式了，并且分为静态工厂和实例工厂；如果使用的不是静态方法，那么就需要先创建工厂对象然后通过方法进行获取实例： 1234&lt;!-- 先创建工厂对象 --&gt;&lt;bean id="factory" class="com.bfchengnuo.TestFactory"/&gt;&lt;!-- 使用工厂的方法创建实例 --&gt;&lt;bean id="user" factory-bean="factory" factory-method="getInstance"/&gt; 如果使用的是静态的，那么就不需要创建工厂对象了： 12&lt;!-- 使用工厂的方法创建实例 --&gt;&lt;bean id="user" class="com.bfchengnuo.TestFactory" factory-method="getStaticInstance"/&gt; p命名空间p 命名空间是用来简化属性注入的，就是说让我们少写一些 property 标签，使用之前需要先引入 xmlns，但是没有对应的 Schema 文件，因为没有办法预先知道用户使用的属性名称，所以也就无法定义 Schema 文件。所以说只需要加入 xmlns:p=&quot;http://www.springframework.org/schema/p&quot; 就可以使用了，具体的使用方式是： 12&lt;bean id="rob" class="..TestBean" p:name="Rob Harrop" p:spouse-ref="sally"/&gt;&lt;bean id="sally" class="..." /&gt; 这样是不是简洁多了呢，使用 p:name 直接注入其 name 属性的值，p:spouse-ref 就是注入 spouse 属性，不过是引用的其他 bean，当然使用的还是 set 方法来进行注入的。 AOP编程Spring中的AOP 文章已经整理完，关于 AOP 的详细信息推荐去看看！在 Spring 中的 AOP 处理中，如果加入容器的目标对象有实现接口就使用 JDK 代理，如果目标对象没有实现接口就使用 Cglib 代理。 另外补充下，最早做 AOP 的是 Eclipse 的 Aspect，Spring 也是引用的它 代理对象的创建-Cglib不知道以前是否写过，在创建代理对象的时候，如果目标对象有接口那么用 JavaAPI 的那一套就可以创建代理对象了，但是如果目标对象没有实现接口，那么就白搭了，好在有一个叫 Cjlib 的库，它被许多 AOP 框架使用，它就是为了解决这个问题，具体的介绍自行搜索，这里只说明它的作用它是通过继承的方式来实现的，首先有一个通用的工具类 Enhancer，通过这个工具类设置父类为我们的目标对象，然后设置回调函数，最后创建一个子类返回就可以了。 123456789101112131415161718192021222324252627public class ProxyFactory implements MethodInterceptor&#123; // 维护目标对象 private Object target; public ProxyFactory(Object target)&#123; this.target = target; &#125; public Object getProxy()&#123; // 1.工具类 Enhancer en = new Enhancer(); // 2.设置父类 en.setSuperclass(target.getClass()); // 3.设置回调函数 en.setCallback(this); // 4.创建子类（代理对象） return en.create(); &#125; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println("开始"); // 执行目标对象的方法 Object returnValue = method.invoke(target, args); return returnValue; &#125;&#125; 怎么用就不用说了吧…..因为本质是采用的继承的方式，所以有几个注意事项： 代理的类不能为 final，否则报错 目标类的方法不能为 final 或者 static，否则不会进行拦截 当然不要忘记了代理类实现接口。JDK 中的代理一般是 $ 开头，Cglib 一般就是代理对象名开头后面再加一些东西 关注分离前面也是提到过这个词的，什么关注分离思想，关注点是什么呢？其实指的就是重复代码；在一些方法中，我们可能需要写许多重复代码，而真正的核心代码就只有几行。比如 DAO 层中的 session 处理的代码，又是事务又是连接的，每个方法都需要写，这些代码就可以看作是关注点，我们要做的就是分离它使用代理，可以在运行期间，执行核心业务代码的时候动态植入关注点代码 AOP 的目标就是让关注点代码和业务代码进行分离 对比上一篇“专业”的解释，我们来一把通俗的介绍，当然可能并不准确；通知/增强： 就是 关注点，也就是重复的代码；切面： 就是关注点形成的类，也就是说很多重复的代码就可以形成一个切面；切入点：运行时在哪里织入切面类的代码，拦截的作用； 面向切面编程：对很多重复的代码进行抽取，然后在运行的时候往业务方法上进行织入切面类代码。 Spring中使用AOP提起 AOP 那其实要说下 Aspect 了，它值得去单独去学习，也不少的内容，这里就只是简单的说下 Spring 中常用的几种形式，如果有必要，会单独开一篇写 AspectJ ，并且一篇都不一定写的完，大体浏览了下，内容确实不少，慢慢来吧 首先必要的在 xml 中引入命名空间和相关 jar 包就不细说了，记得配置开启注解，这个比较省事，大都采用这种形式吧 1234567891011121314151617181920&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context.xsdhttp://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop.xsdhttp://www.springframework.org/schema/txhttp://www.springframework.org/schema/tx/spring-tx.xsd"&gt; &lt;!-- 自动扫描方式加载对象 --&gt; &lt;context:component-scan base-package="com.bfchengnuo"/&gt; &lt;!-- 启动 @AspectJ 支持 --&gt; &lt;aop:aspectj-autoproxy/&gt;&lt;/beans&gt; 上一篇中使用的 XML 配置，然后下面简单介绍下注解的使用： @Aspect声明该类是一个切面， Spring 将会把它当作一个特殊的 Bean（一个切面），也就是说不对这个类本身进行动态代理 。 @Pointcut定义切入点，切入点的名称就是此方法名，也就是确定拦截哪些方法，就是为哪些类生成代理对象参数为切入点表达式：@Pointcut(&quot;execution (* com.bfchengnuo.service..*.*(..))&quot;)第一个 * 指的是任何返回值，(..) 指的是任何参数，.. 指的是本包、以及及其子包下，多个可以使用 || 分割，甚至可以在前面加 ! ，其他的应该就更好理解了；需要注意的是最后一定是定义到方法的！ 其他的注解就没什么难度了，来看个栗子就都知道了： 12345678910111213141516171819202122232425@Aspectpublic calss Audience &#123; // 把 Performance 的 perform 方法定义为名为 performance 的切点 @Pointcut("execution(** concert.Performance.perform(..))") public void performance() &#123;&#125; // 表演之前输出 @Before("performance()") public void silencePhones() &#123; System.out.println("Siliencing phones."); &#125; // 表演之后输出 @AfterReturning("performance()") public void clap() &#123; System.out.println("Claping."); &#125; // 表演失败（抛出异常）输出 @AfterThrowing（“Performance()”) public void refund() &#123; System.out.println("Demanding a refund"); &#125;&#125; 详细的理论知识补充可以去参考 Spring中的AOP 一文 对JDBC的支持在刚开始的时候已经看到了，Spring 中有个模块就是 Spring DAO，说明了 Spring 对 JDBC 的支持还是很好的，；例如体现在对 C3P0 连接池的支持上，甚至比 Hibernate 都好，并且有 JdbcTemplate 模板工具类（类似 DBUtils）。下面配置 dataSource 的方式应该都比较熟悉了，使用 C3P0： 123456789101112131415161718192021222324252627&lt;bean id="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource" destroy-method="close"&gt; &lt;property name="driverClass" value="org.gjt.mm.mysql.Driver"/&gt; &lt;property name="jdbcUrl" value="jdbc:mysql://localhost:3306/ssh?useUnicode=true&amp;amp;characterEncoding=UTF-8"/&gt; &lt;property name="user" value="root"/&gt; &lt;property name="password" value="123456"/&gt; &lt;!--初始化时获取的连接数，取值应在minPoolSize与maxPoolSize之间。Default: 3 --&gt; &lt;property name="initialPoolSize" value="1"/&gt; &lt;!--连接池中保留的最小连接数。--&gt; &lt;property name="minPoolSize" value="1"/&gt; &lt;!--连接池中保留的最大连接数。Default: 15 --&gt; &lt;property name="maxPoolSize" value="300"/&gt; &lt;!--最大空闲时间,60秒内未使用则连接被丢弃。若为0则永不丢弃。Default: 0 --&gt; &lt;property name="maxIdleTime" value="60"/&gt; &lt;!--当连接池中的连接耗尽的时候c3p0一次同时获取的连接数。Default: 3 --&gt; &lt;property name="acquireIncrement" value="5"/&gt; &lt;!--每60秒检查所有连接池中的空闲连接。Default: 0 --&gt; &lt;property name="idleConnectionTestPeriod" value="60"/&gt;&lt;/bean&gt;&lt;bean id="jdbcTemplate" class="org.springframework.jdbc.core.JdbcTemplate"&gt; &lt;property name="dataSource" ref="dataSource"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id="userDao" class="com.curd.spring.impl.UserDAOImpl"&gt; &lt;property name="jdbcTemplate" ref="jdbcTemplate"&gt;&lt;/property&gt;&lt;/bean&gt; 并且顺便把 JdbcTemplate 也进行实例化了，它需要一个数据域，可以 set 方法指定，也可以构造函数给它；至于它怎么使用，看个栗子就都知道了，会感觉很熟悉吧…. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class UserDAOImpl extends JdbcDaoSupport implements IUserDAO &#123; public void addUser(User user) &#123; String sql = "insert into user values(?,?,?)"; this.getJdbcTemplate().update(sql, user.getId(), user.getUsername(), user.getPassword()); &#125; public void deleteUser(int id) &#123; String sql = "delete from user where id=?"; this.getJdbcTemplate().update(sql, id); &#125; public void updateUser(User user) &#123; String sql = "update user set username=?,password=? where id=?"; this.getJdbcTemplate().update(sql, user.getUsername(), user.getPassword(), user.getId()); &#125; public String searchUserName(int id) &#123;// 简单查询，按照ID查询，返回字符串 String sql = "select username from user where id=?"; // 返回类型为String(String.class) return this.getJdbcTemplate().queryForObject(sql, String.class, id); &#125; public List&lt;User&gt; findAll() &#123;// 复杂查询返回List集合 String sql = "select * from user"; return this.getJdbcTemplate().query(sql, new UserRowMapper()); &#125; public User searchUser(int id) &#123; String sql="select * from user where id=?"; return this.getJdbcTemplate().queryForObject(sql, new UserRowMapper(), id); &#125; class UserRowMapper implements RowMapper&lt;User&gt; &#123; //rs为返回结果集，以每行为单位封装着 public User mapRow(ResultSet rs, int rowNum) throws SQLException &#123; User user = new User(); user.setId(rs.getInt("id")); user.setUsername(rs.getString("username")); user.setPassword(rs.getString("password")); return user; &#125; &#125;&#125; 用户自己编写DAO 只需要继承 JdbcDaoSupport， 就可以注入 JdbcTemplate，下面就可以直接用了；JdbcTemplate 基本就可以满足一些日常需求了 关于事务事务可分为两种，编程式事务和声明式事务；编程式的我们都用过，比如 JDBC 中的 conn.setAutoCommite(false)，或者 Hibernate 中的 session.beginTransaction()；好处是操作比较灵活，能够实现细粒度的控制（但在 Spring 中一般不提倡使用）；声明式事务就是字面意思，先声明后面直接用，如果不需要细粒度的事务控制那么可以选择，在 Spring 中只需要在配置文件中配置一下就可以了，无侵入性、无耦合的，是基于 AOP 的；下面主要说说声明式事务，基于 AOP 那也就可以得出只能对方法进行事务控制，对方法内的几行实现事务控制是做不到的，可以使用两种方式进行配置，XML 或者 注解； XML配置方式XML 的话这样配： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id="transactionManager" class="org.springframework.orm.hibernate3.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt;&lt;!-- Hibernate版（上）JDBC版（下） --&gt;&lt;bean id="txManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt;&lt;/bean&gt; &lt;!-- 配置事务增强（如何管理事务） --&gt; &lt;!-- 将我们想要施加在事务中的语义封装在&lt;tx:advice/&gt;中， 其中默认的设置为：事务性传播设置是REQUIRED；隔离级别为DEFAULT；事务是读/写； 事务超时默认是依赖于事务系统的，或者事务超时没有被支持； 任何 RuntimeException 将触发事务回滚，但是任何 checked Exception 将不触发事务回滚。--&gt;&lt;tx:advice id="txAdvice" transaction-manager="txManager"&gt; &lt;tx:attributes&gt; &lt;tx:method name="save*" propagation="REQUIRED" read-only="false"/&gt; &lt;tx:method name="add*" propagation="REQUIRED"/&gt; &lt;tx:method name="delete*" propagation="REQUIRED"/&gt; &lt;tx:method name="test*" propagation="REQUIRED"/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; &lt;!-- 设置一个 pointcut 确保由 "txAdvice" bean定义的事务通知在应用中合适的点来执行， 然后使用一个通知器(advisor)将该切面与 txAdvice 绑定到一起， 其中 expression 的属性是织入点语法 --&gt;&lt;aop:config&gt; &lt;aop:pointcut expression="execution(public * com.niu.service..*.*(..))" id="transactionAop"/&gt; &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="transactionAop"/&gt;&lt;/aop:config&gt;&lt;!-- *******************分割线************************ --&gt;&lt;!--指定异常回滚类型--&gt;&lt;tx:method name="save*" propagation="REQUIRED" rollback-for="NoProductInStockException"/&gt;&lt;!--即使遇到没有经过处理的InstrumentNotFoundException异常，也不要回滚事务--&gt;&lt;tx:method name="save*" propagation="REQUIRED" no-rollback-for="InstrumentNotFoundException"/&gt; 除了上面列出的两种事务管理器，其实还有 JpaTransactionManager、JtaTransactionManager 等，就不介绍了，声明事务管理并不是这一种，更多的可以参考最后一个参考连接分割线下的不设置也可以，大概……. ；这里的 txManager 就相当于是个切面了&lt;tx:method&gt;设置： 属性 是否需要 默认值 描述 name 是 与事务属性关联的方法名。 propagation 不 REQUIRED 事务传播行为 isolation 不 DEFAULT 事务隔离级别 timeout 不 -1 事务超时时间，以秒为单位 readonly 不 false 事务是否只读 rollback-for 不 将被触发回滚的Exception，以逗号隔开 no-rollback-for 不 不被触发回滚的Exception，以逗号隔开 注解方式使用注解的方式还是要配下 XML 文件，起码要设置开启扫描、事务管理器之类的： 1234567&lt;bean id="txManager" class="org.springframework.orm.hibernate5.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt;&lt;/bean&gt;&lt;!--启动注解版事务管理开关--&gt;&lt;tx:annotation-driven transaction-manager="txManager"/&gt;&lt;context:component-scan base-package="com.bfchengnuo" /&gt; 然后就可以使用 @Transactional 注解进行设置了，关于这个注解前面文章中讲过（Spring 中的注解） 参考&amp;资料AOP 系列：https://my.oschina.net/itblog/blog/208067https://jacksmiththu.github.io/2017/06/26/Spring%E4%B8%AD%E7%9A%84AOP/https://www.ibm.com/developerworks/cn/education/opensource/os-cn-spring-trans/index.htmlhttp://www.cnblogs.com/hellojava/archive/2012/11/21/2780694.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-适配器模式]]></title>
    <url>%2F2017%2F09%2F18%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[总的来说这是一个非常简单的适配器，但却非常的好用；另外还补充了个和它差不多的模式：外观模式首先来看下适配器的定义：适配器模式把一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口不匹配而无法在一起工作的两个类能够在一起工作。划重点就是：使用对象组合的方式，把一种接口转换成另一种接口；也就是说适配者和被适配者不需要改变任何的代码，只需要在它们中间加一个适配器就可以了来张图就显得非常好理解了： 适配器分类实际上有两种适配器，对象适配器和类适配器对象适配器多是用的组合的方式；类适配器则是用的多继承的方式，因为在 Java 中并不支持多继承，所以类适配器就不演示了，看看对象适配器就好，反正思想都是一样的 对象适配器不仅可以适配某个类，也可以适配该类的任何子类类适配器非要说的话，Java 中也是可以模拟的，就是适配器继承一个类（源）然后实现一个接口（目标）的方式了适配器也不一定是单向的，如果实现了双方的接口，也就成了一个双向的适配器 涉及角色 目标(Target)角色：这就是所期待得到的接口。注意：由于这里讨论的是类适配器模式，因此目标不可以是类。 源(Adapee)角色：现在需要适配的接口。 适配器(Adaper)角色：适配器类是本模式的核心。适配器把源接口转换成目标接口。显然，这一角色不可以是接口，而必须是具体类。 一个栗子既然是把接口转换成另一种接口，那么就先来定义接口吧，就是目标角色和源角色的接口以及它们的实现类： 12345678910111213141516171819202122232425262728293031323334353637383940public interface Boy &#123; void wearClothes(); void eat();&#125;public interface Girl &#123; // 穿女装？？！ void wearDress(); void eat(); void prayer();&#125;/***************实现类们****************/public class LovelyGirl implements Girl &#123; @Override public void wearDress() &#123; System.out.println("穿女装 o(*≧▽≦)ツ"); &#125; @Override public void eat() &#123; System.out.println("少女进食中...."); &#125; @Override public void prayer() &#123; System.out.println("少女祈祷中..."); &#125;&#125;public class ReliableBoy implements Boy &#123; @Override public void wearClothes() &#123; System.out.println("穿“合适”的衣服"); &#125; @Override public void eat() &#123; System.out.println("男孩吃饭中...."); &#125;&#125; 然后下面就是关键的适配器了，在这个简单的栗子里是非常的简单的，没什么技术含量….. 1234567891011121314151617181920212223public class BoyAdapter implements Girl &#123; private Boy boy; public BoyAdapter(Boy boy) &#123; this.boy = boy; &#125; @Override public void wearDress() &#123; System.out.println("适配/转换中..."); boy.wearClothes(); &#125; @Override public void eat() &#123; boy.eat(); &#125; @Override public void prayer() &#123; System.out.println("我不会...."); &#125;&#125; 最后来测试下，按照设计原则，尽量的多使用接口，多使用组合..： 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; Boy boy = new ReliableBoy(); Girl girl = new LovelyGirl(); BoyAdapter boyAdapter = new BoyAdapter(boy); // 使他看起来像个女孩... testGirl(girl); System.out.println("------------------"); testGirl(boyAdapter); &#125; // 这个方法需要一个 Girl public static void testGirl(Girl girl) &#123; girl.eat(); // 先吃饭 girl.wearDress(); // 穿衣服 girl.prayer(); // 做祷告 &#125;&#125; 适配器差不多就这个样子了，就是说当目标调用的方法（接口）和期望的不同时，可以转换一下，达到以假乱真的目的，由于调用者使用的是接口，所以它确实不知道你传进来的其实是一个伪娘不同类型的对象 区分 装饰者：不改变接口，但加入责任将一个对象包装起来以增加新的行为和责任 适配器：将一个接口转成另一个接口 外观：让接口更简单将一群对象“包装”起来以简化其接口当需要简化并统一一个很大的接口或者一群复杂的接口时，使用外观模式 外观模式外观模式也是一个改变接口的模式，但它改变接口的原因是为了简化接口，让其显露出一个干净的外观嘛~~同时它也将客户从组件的子系统中解耦了一个子系统可以定义多个外观；引入外观之后依然可以调用之前的方法 12345678910111213141516class Facade &#123; private SubSystemA obj1 = new SubSystemA(); private SubSystemB obj2 = new SubSystemB(); private SubSystemC obj3 = new SubSystemC(); public void Method() &#123; obj1.MethodA(); obj2.MethodB(); obj3.MethodC(); &#125; public void Method2() &#123; obj1.MethodA(); obj3.MethodC(); &#125;&#125; 由于在外观类中维持了对子系统对象的引用，客户端可以通过外观类来间接调用子系统对象的业务方法，而无须与子系统对象直接交互。另外，它可以让客户端（调用者）的学习成本降低；或者可以配合工厂模式来进行使用引用的 SubSystem 都应该是接口才对，另外可以提供 setter 方法或者在构造函数中进行延迟初始化放在这里因为和适配器模式还是有点关联的，并且它非常非常的简单更多可参考：http://blog.csdn.net/c_son/article/details/51121432 应用举例还是以 JDK 中的栗子，早先，所有的集合都实现了一个 elements 的方法，用于返回一个枚举，使用枚举来进行迭代元素；后来开始使用了 Iterator 这个接口，使用迭代器来迭代元素，它还支持删除元素的能力那么，为了让它和老版本的枚举兼容，就需要使用适配器来进行转换了 在 Android 开发中就有大量的适配器，写过 ListView 的都造是吧？]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-命令模式]]></title>
    <url>%2F2017%2F09%2F14%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[首先，我们来看下命令模式的定义：将一个请求封装成一个对象，从而让你使用不同的请求把客户端参数化，对请求排队或者记录请求日志，可以提供命令的撤销和恢复功能。命令模式是一种行为模式，通俗点说 可以把它看成分离的关注点，可将“动作请求者”从“动作执行者”对象中解耦；对于请求者（客户端）只需要知道调用某个方法就能达到相应的效果，并不需要执行者具体是怎么做的，它们之间用一个命令对象来连接 命令模式结构命令模式涉及到五个角色，它们分别是： 客户端(Client)角色：创建一个具体命令(ConcreteCommand)对象并确定其接收者。通俗理解我们写的测试类就是一个客户端角色，也就是要负责创建那些所需要的对象 命令(Command)角色：声明了一个给所有具体命令类的抽象接口。一般会定义一个 execute 和 undo 方法 具体命令(ConcreteCommand)角色：定义一个接收者和行为之间的弱耦合；实现 execute() 方法，负责调用接收者的相应操作。execute() 方法通常叫做执行方法。 请求者(Invoker)角色：负责调用命令对象执行请求，相关的方法叫做行动方法。 接收者(Receiver)角色：负责具体实施和执行一个请求。任何一个类都可以成为接收者，实施和执行请求的方法叫做行动方法。 一个简单的栗子还是通过代码来看比较好，首先从命令角色开始写吧，就是定义一个接口，所有的命令都要实现它 12345public interface Command &#123; void execute(); void undo();&#125; 下面是接收者角色 ；就是具体做事情的类了，这种栗子大概只有我会看懂….emmm，下面会改的 12345public class LovelyLoli &#123; public void action() &#123; System.out.println("啦啦啦o(*≧▽≦)ツ.....dance"); &#125;&#125; 然后定义具体的命令角色，大部分情况下是一种具体的命令对应一种“行为” 123456789101112131415161718192021public class LoliDanceCommand implements Command &#123; // 命令角色中会持有一个接受者角色 private LovelyLoli loli; // 使用构造函数来取得 LoliDanceCommand(LovelyLoli loli) &#123; this.loli = loli; &#125; @Override public void execute() &#123; System.out.println("准备中....."); loli.action(); System.out.println("谢谢~~"); &#125; @Override public void undo() &#123; System.out.println("这个，，，没法撤销...."); &#125;&#125; 请求者角色 ，这个怎么说….就是执行命令的那个类？？可理解为控制器，其实还是蛮重要的 1234567891011121314public class LoliconControl &#123; // 首先里面包含有一个命令，这个命令控制着一个“功能” private Command command; // 提供一个 set 方法可以随时更改命令 public void setCommand(Command command) &#123; this.command = command; &#125; // 调用命令相关的方法称为行动方法 public void action()&#123; command.execute(); &#125;&#125; 准备的差不多多了，下面就可以进行测试了，测试类也可以说是客户端角色，随意就好，反正在真正用的时候我感觉不会管是什么角色…..知道怎么用就行…额 123456789public static void main(String[] args) &#123; LoliconControl control = new LoliconControl(); // 调用者 LovelyLoli loli = new LovelyLoli(); // 具体执行者 // 创建命令 LoliDanceCommand command = new LoliDanceCommand(loli); control.setCommand(command); // 设置命令 control.action(); // 调用命令&#125; 然后就应该看到效果了，以上就是命令模式的栗子了，可以看出其实还是非常简单的 小总结可对比定义，一个命令对象通过在特定的接受者上绑定一组动作来封装一个请求。要达到这一点，需要命令对象将动作和接收者包进对象中；这个对象（命令对象）只暴露出一个 execute 方法，当这个方法被调用的时候就会执行这些动作。从外面看来，其他对象不知道究竟是那个接收者进行了那些动作，只知道如果调用 execute 方法请求的目的就能达到最终的主要目的之一就是让请求者和接收者之间进行解耦 更全面的栗子这里就参考 HeadFirst 中的栗子了，感觉还是非常好的，还是那几个角色，就是加入了一些别的功能，比如撤销、宏命令命令角色我就不写了，因为是一样的，都是统一的 execute 和 undo 方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/********************* 接收者角色 **********************/public class Light &#123; private String local; public Light(String local) &#123; this.local = local; &#125; public void on() &#123; System.out.println(local + "灯已经打开！"); &#125; public void off() &#123; System.out.println(local + "灯已经关闭！"); &#125;&#125;/********************* 具体命令角色 **********************/public class LightOffCommand implements Command &#123; private Light light; public LightOffCommand(Light light) &#123; this.light = light; &#125; @Override public void execute() &#123; // 这里其实可以执行一系列的方法 // 如果次对象的状态存在多种，可以在执行之前记录当前的状态；以便给后面的撤销操作用 light.off(); &#125; @Override public void undo() &#123; // 如果存在多个状态，在 exec 中记录状态，在这里获取记录的状态，然后再相应的处理 light.on(); &#125;&#125;public class LightOnCommand implements Command &#123; private Light light; public LightOnCommand(Light light) &#123; this.light = light; &#125; @Override public void execute() &#123; light.on(); &#125; @Override public void undo() &#123; light.off(); &#125;&#125;/********************* 请求者角色 **********************/public class RemoteControl &#123; private Command[] onCommand; private Command[] offCommand; // 如果想撤销多次，需要使用一个堆桟进行记录了 private Command undoCommand; // 撤销命令 public RemoteControl() &#123; // 在构造器中进行初始化，具体几个按键就不用太纠结了 onCommand = new Command[4]; offCommand = new Command[4]; // 默认所有的按键都是空的，这样执行的时候就省了判断 null 了 NoCommand noCommand = new NoCommand(); for (int i = 0; i &lt; 4; i++) &#123; onCommand[i] = noCommand; offCommand[i] = noCommand; &#125; undoCommand = noCommand; &#125; public void setCommand(int slot, Command onCommand, Command offCommand) &#123; this.onCommand[slot] =onCommand; this.offCommand[slot] = offCommand; &#125; // 打开 public void onButton(int slot) &#123; onCommand[slot].execute(); // 记录当前的命令 undoCommand = onCommand[slot]; &#125; // 关闭 public void offButton(int slot) &#123; offCommand[slot].execute(); // 记录当前的命令 undoCommand = offCommand[slot]; &#125; public void undoButton() &#123; undoCommand.undo(); &#125;&#125; 与上面的简单例子相比，这个请求者容纳更多的命令，就像遥控器一样各自都有指定的位置，并且还具备撤销操作，其实也非常简单，接收者应该最清楚撤销怎么实现了，所以我们执行某个命令后如果需要撤销操作就再执行这条命令的撤销方法就行了，也就是说只需要记录执行时的这个命令对象就可以了，下面就看下测试的代码吧（或者说客户端？）： 1234567891011121314151617181920212223242526private static void lightTest() &#123; // 先创建好控制器（请求者） RemoteControl remoteControl = new RemoteControl(); // 创建具体的执行者（接受者） Light corridorLight = new Light("走廊"); Light roomLight = new Light("卧室"); // 创建相关的具体命令 LightOnCommand roomOnCmd = new LightOnCommand(roomLight); LightOffCommand rommOffCmd = new LightOffCommand(roomLight); LightOnCommand corridorOnCmd = new LightOnCommand(corridorLight); LightOffCommand corridorOffCmd = new LightOffCommand(corridorLight); // 设置到指定的按键上 remoteControl.setCommand(0,roomOnCmd,rommOffCmd); remoteControl.setCommand(1,corridorOnCmd,corridorOffCmd); // 调用执行 remoteControl.onButton(0); remoteControl.onButton(1); remoteControl.offButton(1); // 撤销 System.out.println("撤销...."); remoteControl.undoButton(); remoteControl.offButton(0);&#125; 就是这样，这样命令模式就应该差不多很全了；关于撤销，如果想撤销多次那么可以使用一个类似堆桟的结构来按顺序存储多个命令，然后就很容易可以实现多次撤销了 使用宏命令所谓的宏命令就是命令的集群，就是说一次性执行一连串的命令达到某个效果，当然也是可以进行撤销的，使用宏命令我们再加两个类，嫌麻烦不用接口大概也是可以的…..就是嘛可能会…. 123456789101112131415161718192021222324252627282930313233343536public interface MacroCommand extends Command &#123; // 最基本的添加和删除 void add(Command command); void remove(Command command);&#125;/******************* 宏命令 **************************/public class MacroLightCommand implements MacroCommand &#123; private List&lt;Command&gt; commands = new ArrayList&lt;&gt;(); private List&lt;Command&gt; undoCommands = new ArrayList&lt;&gt;(); @Override public void add(Command command) &#123; commands.add(command); &#125; @Override public void remove(Command command) &#123; commands.remove(command); &#125; @Override public void execute() &#123; for (Command command : commands) &#123; command.execute(); undoCommands.add(command); &#125; &#125; @Override public void undo() &#123; for (int i = undoCommands.size() - 1; i &gt;= 0; i--) &#123; undoCommands.get(i).undo(); &#125; undoCommands.clear(); &#125;&#125; 并没有太大的差别，具体命令对象中只是改用 List 保存一串命令，增加了 add 和 remove 方法，在执行的时候是将集合里的每个命令按顺序执行，下面是测试类： 123456789101112131415161718public static void main(String[] args) &#123; MacroCommand macroCommand = new MacroLightCommand(); Light light = new Light("卧室"); LovelyLoli loli = new LovelyLoli(); LightOnCommand lightOnCommand = new LightOnCommand(light); LightOffCommand lightOffCommand = new LightOffCommand(light); LoliDanceCommand loliDanceCommand = new LoliDanceCommand(loli); // 添加到宏命令 macroCommand.add(lightOnCommand); macroCommand.add(loliDanceCommand); macroCommand.add(lightOffCommand); macroCommand.execute(); System.out.println("--------------"); macroCommand.undo();&#125; 效果还是不错的，嗯…. 关于空对象注意，这里说的是空对象 ，比如上面所说的 NoCommand 对象，当你不想返回一个有意义的对象时，空对象就显得非常的有用了，将处理 Null 的责任转移给了空对象，这大概会避免写太多的判断 xxx != null 的语句了，并且有效避免了空指针异常 在许多模式中都会看到空对象的使用，甚至在有些时候，空对象本身也视为是一种设计模式 命令模式的应用就像定义中所说，很多语言在设计队列请求、线程池的时候就是用的命令模式，起码我知道在 Android 中的消息机制就是这样实现的；比如，在一个工作队列中，我们在一边进行添加命令，另一端则是线程；线程会从队列中取出一个命令然后执行它的 execute 方法，等待调用完成后就将此命令进行丢弃，再取出下一个命令….这样的实现，工作队列和进行计算的对象之间是完全解耦的，只要实现了命令模式的对象就可以加入到队列中；当线程调用的时候，不管你是具体做什么的，只知道调用其 execute 方法 还有一种比较出名的就是日志请求，通过序列化，当每个命令被执行时全部存储到硬盘中，如果出现宕机，可以从硬盘中取出重新加载，以正确的次序执行，这就相当于是回滚操作了 或者还可以应用于事务系统 命令模式的优点 更松散的耦合命令模式使得发起命令的对象——客户端，和具体实现命令的对象——接收者对象完全解耦，也就是说发起命令的对象完全不知道具体实现对象是谁，也不知道如何实现。 更动态的控制命令模式把请求封装起来，可以动态地对它进行参数化、队列化和日志化等操作，从而使得系统更灵活。 很自然的复合命令命令模式中的命令对象能够很容易地组合成复合命令，也就是宏命令，从而使系统操作更简单，功能更强大。 更好的扩展性由于发起命令的对象和具体的实现完全解耦，因此扩展新的命令就很容易，只需要实现新的命令对象，然后在装配的时候，把具体的实现对象设置到命令对象中，然后就可以使用这个命令对象，已有的实现完全不用变化。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring中的注解使用总结]]></title>
    <url>%2F2017%2F09%2F04%2FSpring%E4%B8%AD%E7%9A%84%E6%B3%A8%E8%A7%A3%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[在实际使用中，Spring 的注解使用还是非常常见的，毕竟省去了那么多罗嗦的配置，前面在看它们整合 SSH 框架的时候栗子都是用注解，忽然发现我并没有学习过 Spring 的注解….这不就马上来补上 准备工作使用注解是在 Spring2.5（2.0以后陆续加入） 以后新加的功能，所以至少要保证版本是 2.5+，并且引入了注解的包然后需要在 Spring 的配置文件中告诉它你使用了注解，最简单的一种方式就是加入下面的一句：&lt;context:component-scan base-package=&quot;com.xxx&quot; /&gt;它的意思就是开启自动扫描，会自动扫描你设置的包路径下的所有类，如果有注解就进行解析这就是所谓的用注解来构造 IoC 容器；base-package 是可以指定多个包的，用逗号分割 @Autowired顾名思义，就是自动装配，其作用是为了消除 Java 代码里面的 getter/setter 与 bean 属性中的 property。当然，getter 看个人需求，如果私有属性需要对外提供的话，应当予以保留。@Autowired 默认按类型匹配的方式，在容器查找匹配的 Bean，当有且仅有一个匹配的 Bean 时，Spring 将其注入 @Autowired 标注的变量中。下面就搞个栗子看看吧，便于理解，首先在 Spring 的配置文件中设置好： 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd "&gt; &lt;context:component-scan base-package="com.spring" /&gt; &lt;bean id="zoo" class="com.spring.model.Zoo" /&gt; &lt;bean id="tiger" class="com.spring.model.Tiger" /&gt; &lt;bean id="monkey" class="com.spring.model.Monkey" /&gt;&lt;/beans&gt; 配置中我们开启了自动扫描，并且配了三个 bean，但是没有给这些 bean 配置 property 属性，因为我们用注解的话是不需要的下面在我们的 zoo.java 文件中这样写： 1234567891011public class Zoo &#123; @Autowired private Tiger tiger; @Autowired private Monkey monkey; public String toString()&#123; return tiger + "\n" + monkey; &#125;&#125; 在这个 bean 中使用了 @Autowired 注解，并且没有设置 getter/setter 方法；因为开启了自动扫描，当 Spring 发现 @Autowired 注解时，将自动在代码上下文中找到和其匹配（默认是类型匹配）的 Bean，并自动注入到相应的地方去。所以，类中的 tiger 和 monkey 都已经注入成功了 这里有个细节，就是如果在配置文件中配置了 property 属性，又使用了注解，会怎么样？Spring 会按照 xml 优先的原则去 Zoo.java 中寻找这两个属性的 getter/setter，如果连 getter/setter 方法也没有那就只能抛异常了 匹配不到 bean 的情况下，如果不想让 Spring 抛异常而是将属性设置位 null，那么可以将 @Autowired 注解的 required 属性设置为 false 即可 @Qualifier这个注解是用来指定注入 Bean 的名称 ；也就是说如果容器中有一个以上匹配的 Bean，则可以通过 @Qualifier 注解限定 Bean 的名称，否则调用的时候会抛异常比如：某个 bean 中引用了一个接口，实现这个接口的 bean 有多个，Spring 在注入的时候就不知道注入那一个了，这样要么删除其他的 bean 要么就是使用 @Qualifier 注解 123@Autowired@Qualifier("bmwCar")private ICar car; 这样通过 id 就指明了注入的是那个 bean @Resource@Resource 注解与 @Autowired 注解作用非常相似，是通过 name/type 进行注入的 123456789101112public class Zoo1 &#123; @Resource(name="tiger") private Tiger tiger; @Resource(type=Monkey.class) private Monkey monkey; public String toString()&#123; return tiger + "\n" + monkey; &#125;&#125; 下面就来看下它的装配顺序和与 @Autowired 的区别 @Resource的装配顺序 @Resource 如果后面没有任何内容，默认通过 name 属性去匹配 bean，找不到再按 type 去匹配 指定了 name 或者 type 则根据指定的类型去匹配 bean 指定了 name 和 type 则根据指定的 name 和 type 去匹配 bean，任何一个不匹配都将报错 与@Autowired注解区别 @Autowired 默认按照 byType 方式进行 bean 匹配，@Resource 默认按照 byName 方式进行 bean 匹配 @Autowired 是 Spring 的注解，@Resource 是 J2EE 的注解，这个看一下导入注解的时候这两个注解的包名就一清二楚了 Spring 属于第三方的，J2EE 是 Java 自己的东西，因此，建议使用 @Resource 注解，以减少代码和 Spring 之间的耦合。 它们都可以使用在“字段”或者 setter 方法上，如果写在了字段上不写 setter 方法也是可以成功注入的 @Service@Service 对应的是业务层 Bean上面一开始的例子，还可以继续简化，因为 Spring 的配置文件里面还定义了三个 bean，下一步的简化是把这三个 bean 也给去掉，使得 Spring 配置文件里面只有一个自动扫描的标签，增强 Java 代码的内聚性并进一步减少配置文件。使用到的当然就是 @Service 注解了，配置文件不用说了，就剩一行扫描了，Java 类要进行稍微的改造了： 123456789101112@Servicepublic class Zoo &#123; @Autowired private Tiger tiger; @Autowired private Monkey monkey; public String toString()&#123; return tiger + "\n" + monkey; &#125;&#125; 确实是没什么大的变化，只是在类上加了一个 @Service 注解，这样，Zoo.java 在 Spring 容器中存在的形式就是 “zoo”，即可以通过 ApplicationContext.getBean(&quot;zoo&quot;) 来得到 Zoo 对象@Service 注解，其实做了两件事情： 声明 Zoo.java 是一个 bean，这点很重要，因为 Zoo.java 是一个 bean，其他的类才可以使用 @Autowired 将 Zoo 作为一个成员变量自动注入。 Zoo.java 在 bean 中的 id 是 “zoo”，即类名且首字母小写。 那么我想指定 id 的名称怎么办？我想设置 bean 为原型怎么办（每次都会 new 一个新的）？ 123@Service("Zoo")@Scope("prototype")public class Zoo&#123;...&#125; @Scope 注解和 bean 标签例的 Scope 属性是一致的；Spring 默认产生的 bean 是单例，也就是说默认是 “singleton” 即单例，需要每次都 new 一个新的就选择 “prototype”struts2 是要求每次次访问都对应不同的 Action @Controller@Controller 对应表现层的 Bean，也就是 Action，用法上倒是和 @Service 并没有什么区别 123@Controller@Scope("prototype")public class UserAction &#123;...&#125; 使用 @Controller 注解标识 UserAction 之后，就表示要把 UserAction 交给 Spring 容器管理，在 Spring 容器中会存在一个名字为 “userAction” 的 action，默认的 bean 名字为这个类的类名首字母小写，如果需要单独指定就手动设置下其注解的 value 值 @Repository@Repository 对应数据访问层 Bean ；和上面类似，本质也是把类交给 Spring 管理而已，只不过对应的不同 1234@Repository(value="userDao")public class UserDaoImpl extends BaseDaoImpl&lt;User&gt; &#123;………&#125; 这样在 Service 层使用 @Resource 进行注入即可 @Repository / @Service / @Controller 这三个没什么功能上的差别，差别只是在语义上，分别代表了特定语义的类（DAO、Service、Web） 这个有点类似于 HTML 5 提出的语义化标签，你说 HTML 5 里面的 “header” 和 “div” 有什么差别呢，其实功能上来说没有，只是语义表达的更清楚 更确切的说，@Service 和 @Controller 功能上和 @Component 是相同的，@Repository 在 2.0 就有了，那时候只能标注在 DAO 层，否则可能抛异常（因为它除了识别 bean 同时它还能将所标注的类中抛出的数据访问异常封装为 Spring 的数据访问异常类型） @Component@Component 是所有受 Spring 管理组件的通用形式，@Component 注解可以放在类的头上，@Component 不推荐使用。标识该类需要 Spring 初始化时自动装配，也许正是因为太通用所以不推荐，实在不好归类的时候再用 Spring常用注解汇总 @Configuration把一个类作为一个配置类，它的某个方法头上如果注册了 @Bean，就会作为这个 Spring 容器中的 Bean。一个带有 @Bean 的注解方法将返回一个对象，该对象应该被注册为在 Spring 应用程序上下文中的 bean，默认它的 id 就是方法名，此注解包含了 @Component；比如，看下面的一段代码： 1234567@Configurationpublic class HelloWorldConfig &#123; @Bean public HelloWorld helloWorld()&#123; return new HelloWorld(); &#125;&#125; 上面的代码如果写成 XML 文件就是： 123&lt;beans&gt; &lt;bean id="helloWorld" class="com.tutorialspoint.HelloWorld" /&gt;&lt;/beans&gt; 这样应该就比较好理解了，然后就可以通过 AnnotationConfigApplicationContext 进行获取定义的 Bean： 1234567891011121314151617public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(HelloWorldConfig.class); HelloWorld helloWorld = ctx.getBean(HelloWorld.class); helloWorld.setMessage("Hello World!"); helloWorld.getMessage();&#125;/*************分割线******************/public static void main(String[] args) &#123; AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(); ctx.register(AppConfig.class, OtherConfig.class); ctx.register(AdditionalConfig.class); ctx.refresh(); MyService myService = ctx.getBean(MyService.class); myService.doStuff();&#125; 上面两种方式都可以获取，用那种就自己看着办吧…. @Scope设置 bean 的作用域 @Lazy(true)表示延迟初始化 @Service用于标注业务层组件 @Controller用于标注控制层组件（如 struts 中的 action） @Repository用于标注数据访问组件，即 DAO 组件。 @Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 @Scope用于指定 scope 作用域的（用在类上） @PostConstruct用于指定初始化方法（用在方法上） @PreDestory用于指定销毁方法（用在方法上） @DependsOn定义 Bean 初始化及销毁时的顺序 @Primary自动装配时当出现多个 Bean 候选者时，被注解为 @Primary 的 Bean 将作为首选者，否则将抛出异常 @Autowired默认按类型装配，如果我们想使用按名称装配，可以结合 @Qualifier 注解一起使用。如下：@Autowired @Qualifier(“personDaoBean”) 存在多个实例配合使用 @Resource默认按名称装配，当找不到与名称匹配的 bean 才会按类型装配。 @PostConstruct初始化注解 @PreDestroy摧毁注解 默认 单例 启动就加载 @Async异步方法调用 补充-关于事务因为上面貌似没有提到过，就单独拿出来补充，除了事务应该还有 AOP，不过那个还没用到以后再说吧事务方面用的还是比较多的，比如常用的 @Transaction 注解，来说下这个注解准备工作就不说了，在 SSH 整合中已经说过了，就是要开启事务管理器以及配置下开启注解，然后来看下它的属性吧： 属性名 说明 name 当在配置文件中有多个 TransactionManager , 可以用该属性指定选择哪个事务管理器。 propagation 事务的传播行为，默认值为 REQUIRED。 isolation 事务的隔离度，默认值采用 DEFAULT。 timeout 事务的超时时间，默认值为-1。如果超过该时间限制但事务还没有完成，则自动回滚事务。 read-only 指定事务是否为只读事务，默认值为 false；为了忽略那些不需要事务的方法，比如读取数据，可以设置 read-only 为 true。 rollback-for 用于指定能够触发事务回滚的异常类型，如果有多个异常类型需要指定，各类型之间可以通过逗号分隔。 no-rollback- for 抛出 no-rollback-for 指定的异常类型，不回滚事务。 此注解可以设置在合适的方法上，也可以设置在类上，当把 @Transactional 注解放在类级别时，表示所有该类的公共方法都配置相同的事务属性信息。当类级别配置了@Transactional，方法级别也配置了@Transactional，应用程序会以方法级别的事务属性信息来管理事务，换言之，方法级别的事务属性信息会覆盖类级别的相关配置信息。 在应用系统调用声明 @Transactional 的目标方法时，Spring Framework 默认使用 AOP 代理，在代码运行时生成一个代理对象，根据 @Transactional 的属性配置信息，这个代理对象决定该声明 @Transactional 的目标方法是否由拦截器 TransactionInterceptor 来使用拦截； 在 TransactionInterceptor 拦截时，会在在目标方法开始执行之前创建并加入事务，并执行目标方法的逻辑, 最后根据执行情况是否出现异常，利用抽象事务管理器 AbstractPlatformTransactionManager 操作数据源 DataSource 提交或回滚事务 propagation属性事务的传播行为大约有七种，需要注意下面三种 propagation 可以不启动事务。本来期望目标方法进行事务管理，但若是错误的配置这三种 propagation，事务将不会发生回滚。 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 其他的一些传播行为有： TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务（默认）。 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于 PROPAGATION_REQUIRED。 isolation属性就是隔离级别的问题，这个在前面其实已经详细说过了 TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。 TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读和不可重复读，因此很少使用该隔离级别。 TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。 TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。即使在多次查询之间有新增的数据满足该查询，这些新增的记录也会被忽略。该级别可以防止脏读和不可重复读。 TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 rollbackFor属性默认情况下，如果在事务中抛出了未检查异常（继承自 RuntimeException 的异常）或者 Error，则 Spring 将回滚事务；除此之外，Spring 不会回滚事务。如果在事务中抛出其他类型的异常，并期望 Spring 能够回滚事务，可以指定 rollbackFor 只有public的方法才有效只有 @Transactional 注解应用到 public 方法，才能进行事务管理。这和使用的 AOP 技术有关，从上面的原理中可以推断出来，详情可见参考 AOP自调用问题在 Spring 的 AOP 代理下，只有目标方法由外部调用，目标方法才由 Spring 生成的代理对象来管理，这会造成自调用问题。若同一类中的其他没有 @Transactional 注解的方法内部调用有@Transactional 注解的方法，有@Transactional 注解的方法的事务被忽略，不会发生回滚。 123456789101112@Servicepublic class OrderService &#123; private void insert() &#123; insertOrder(); &#125; @Transactional public void insertOrder() &#123; //insert log info //insertOrder //updateAccount &#125;&#125; insertOrder 尽管有 @Transactional 注解，但它被内部方法 insert 调用，事务被忽略，出现异常事务不会发生回滚。要解决这个问题需要使用 AspectJ 取代 Spring AOP 代理（具体的配置见参考2）。 补充-Spring中的过滤器临时看到的就写在这里了，在 Spring 中还自带了一些过滤器，比如最常用的设置编码的，避免中文乱码问题，直接在 web.xml 中配置就行 123456789101112131415161718&lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 如果查看源码，其实实现比我们的要简单，没有处理 GET 乱码问题，所以，需要 GET 的还是要自己写(或者直接在 Tomcat 中设置死)，不过在 Tomcat8.x 默认 Get 编码已改为 utf-8 ，Get 乱码就不用操心了。 123456789101112131415//........省略.............. @Override protected void doFilterInternal( HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; if (this.encoding != null &amp;&amp; (this.forceEncoding || request.getCharacterEncoding() == null)) &#123; request.setCharacterEncoding(this.encoding); if (this.forceEncoding) &#123; response.setCharacterEncoding(this.encoding); &#125; &#125; filterChain.doFilter(request, response); &#125; //........省略.............. 然后还有一个很常见的问题就是 No Session 的问题，因为 Spring 已提供了过滤器来解决 nosession or session is closed 这个问题，所以我们直接用就行了，使用的也是过滤器来实现的，以此延长 Session 的生命周期（所以有利有弊，对象层级较多的话会拖慢加载速度），在 web.xml 文件中直接配置就行 12345678910111213&lt;!-- 配置 OpenSessionInViewFilter 解决 nosession 问题 该过滤器必须配置在 struts2 核心控制期之前，否则 action 访问不了--&gt;&lt;filter&gt; &lt;filter-name&gt;openSessionInViewFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.orm.hibernate5.support.OpenSessionInViewFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;openSessionInViewFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 也可以配置它的两个参数：singleSession 和 sessionFactoryBeanNamesessionFactoryBeanName 默认值是 sessionFactorysingleSession 默认值是 true 不过在 SSH 的项目中一般都配了事务管理器，并且启用了注解，人嘛，能省事则省事一般都直接加个 @Transaction 注解就不管了，这个也确实能解决问题，至于是不是 openSessionInViewFilter 的思路目前我还不知道，我懒的也没获取下各自的 session 对比下是不是同一个对象….. 参考http://www.cnblogs.com/xiaoxi/p/5935009.htmlhttps://www.ibm.com/developerworks/cn/java/j-master-spring-transactional-use/index.htmlhttps://www.ibm.com/developerworks/cn/education/opensource/os-cn-spring-trans/index.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH框架整合笔记]]></title>
    <url>%2F2017%2F09%2F01%2FSSH%E6%A1%86%E6%9E%B6%E6%95%B4%E5%90%88%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这一篇说的是纯手动的整合，目前主流的应该是采用 Maven 或者 Gradle 进行整合，这个以后再说吧…关于集成顺序，不是固定的，但是一般是先添加 Spring 支持，再添加 Hibernate 支持，这样 hibernate.cfg.xml 就被集成到 applicationContext.xml（beans.xml） 中，就不用手动配置了。至于 Struts 在他们俩前后都可以了….每添加一个框架建议做一次测试，以保证引入是没有问题的，用简单的单元测试就行 环境搭建先用最基本的手动搭建大法，现在使用构建工具来搭建也很普遍了，也更方便一些，后面看看再说吧，这里采用的是：Struts2-2.3 + Spring4.3 + Hibernate5.2手动导入的时候还要注意一个问题，那就是 jar 包的冲突问题 1.引入Spring&amp;XML知识补充作为第一个引入的来说，应该是最简单的，没什么可说的和平常加入 Spring 是一样的，导入相应的 jar 包，设置好相应的配置文件就行了配置文件的名字和位置其实是随意的，前面都说过，通过用 applicationContext 或者 beans 还有种是 spring-config，一般放在 src 目录下配置方面可以加入 aop 和事务的约束，对于 spring 来说 xsd 文件可以不写版本号，会使用自带的最新的版本，内部做了本地的映射，jar 包中就有 xsd 文件，避免离线不能用的情况 123456789101112131415161718&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context.xsdhttp://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop.xsdhttp://www.springframework.org/schema/txhttp://www.springframework.org/schema/tx/spring-tx.xsd"&gt; &lt;!-- 自动扫描方式加载对象 --&gt; &lt;context:component-scan base-package="com.bfchengnuo"/&gt;&lt;/beans&gt; 这里就顺便补充下关于 xml 的一些知识吧，原来大多使用的是 dtd 约束文件，但是因为其的局限性已经逐步被 XML Schema 取代，对于 Spring ，为了避免冲突，有几种不同的命名空间，相当于 java 中的包了需要那个加那个就可以了 xmlns=声明 xml 文件默认的命名空间，表示未使用其他命名空间的所有标签的默认命名空间。如果没有提供 schemaLocation，那么 Spring 的 XML 解析器会从 namespace 的 URI 里加载 XSD 文件 xmlns:xsi=声明 XML Schema 实例，声明后就可以使用 schemaLocation 属性了说明当前的 xml 是 schema 一个实例文档 xmlns:aop=声明前缀为 aop 的命名空间，后面的 URL 用于标示命名空间的地址，不会被解析器用于查找信息。其惟一的作用是赋予命名空间一个惟一的名称。当命名空间被定义在元素的开始标签中时，所有带有相同前缀的子元素都会与同一个命名空间相关联。 xsi:schemaLocation=这个从命名可以看出个大概，指定 Schema 的位置这个属性必须结合命名空间使用。这个属性有两个值，第一个值表示需要使用的命名空间。第二个值表示供命名空间使用的 XML schema 的位置schemaLocation 提供了一个 xml namespace 到对应的 XSD 文件的一个映射，所以我们可以看到，在 xsi:schemaLocation 后面配置的字符串都是成对的，前面的是 namespace 的 URI，后面是 xsd 文件的 URI 为了防止离线获取不到的问题，Spring 很贴心的将这些文件打包到 jar 文件里了，并且进行了映射，所以说：不要在 Spring 的配置里，配置上 XSD 的版本号，因为如果没有配置版本号，取的就是当前 jar 里的 XSD 文件，减少了各种风险。而且这样约定大于配置的方式很优雅。 如果感觉下载 XSD 文件非常慢，导致IDE的卡顿，可以事先下载好然后本地引用，详细配置可以参考：http://blog.csdn.net/jackphang/article/details/17021241 2.引入Hibernate和一般的引入也是差不多的，倒不如说更简单了，因为 cfg 文件不需要单独配置了(映射的 hbm 文件也可以用 JPA 来代替了)，直接在 Spring 的配置文件里用就行了导入相关的 jar 包不要忘了数据库驱动的包和连接池相关的包（spring 也自带一些） 首先要配置数据源，这里使用的 C3P0： 123456789101112131415161718&lt;bean id="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource" destroy-method="close"&gt; &lt;property name="driverClass" value="org.gjt.mm.mysql.Driver"/&gt; &lt;property name="jdbcUrl" value="jdbc:mysql://localhost:3306/itcast?useUnicode=true&amp;amp;characterEncoding=UTF-8"/&gt; &lt;property name="user" value="root"/&gt; &lt;property name="password" value="123456"/&gt; &lt;!--初始化时获取的连接数，取值应在minPoolSize与maxPoolSize之间。Default: 3 --&gt; &lt;property name="initialPoolSize" value="1"/&gt; &lt;!--连接池中保留的最小连接数。--&gt; &lt;property name="minPoolSize" value="1"/&gt; &lt;!--连接池中保留的最大连接数。Default: 15 --&gt; &lt;property name="maxPoolSize" value="300"/&gt; &lt;!--最大空闲时间,60秒内未使用则连接被丢弃。若为0则永不丢弃。Default: 0 --&gt; &lt;property name="maxIdleTime" value="60"/&gt; &lt;!--当连接池中的连接耗尽的时候c3p0一次同时获取的连接数。Default: 3 --&gt; &lt;property name="acquireIncrement" value="5"/&gt; &lt;!--每60秒检查所有连接池中的空闲连接。Default: 0 --&gt; &lt;property name="idleConnectionTestPeriod" value="60"/&gt;&lt;/bean&gt; 至于为什么要配置数据源，现在基本都是用线程池的吧，要不然效率也太低了并且官方也推荐这样配置，关于连接池的好处前面其实已经说过了然后就是 Hibernate 最重要的对象 sessionFactory 了（单例），并且我们知道它是需要一个数据源的，我们配置的 cfg 配置文件其实就是为了生成它： 123456789101112131415161718192021222324252627&lt;bean id="sessionFactory" class="org.springframework.orm.hibernate4.LocalSessionFactoryBean"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;!-- 使用的注解，或者 packagesToScan --&gt; &lt;property name="annotatedClasses"&gt; &lt;list&gt; &lt;value&gt;com.bfchengnuo.entity.UsersEntity&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="hibernateProperties"&gt; &lt;props&gt; &lt;prop key="hibernate.show_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.hbm2ddl.auto"&gt;update&lt;/prop&gt; &lt;prop key="dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 设置 session 的上下文，解决 getCurrentSession 获取不到的情况 --&gt; &lt;prop key="hibernate.current_session_context_class"&gt;org.springframework.orm.hibernate5.SpringSessionContext&lt;/prop&gt; &lt;prop key="hibernate.connection.url"&gt;jdbc:mysql://localhost:3306/ssh&lt;/prop&gt; &lt;prop key="hibernate.connection.driver_class"&gt;com.mysql.jdbc.Driver&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 上面用的是注解形式，如果使用的是 hbm 映射文件就是这样配 --&gt;&lt;property name="mappingResources"&gt; &lt;list&gt; &lt;value&gt;com/bfchengnuo/domain/Employee.hbm.xml&lt;/value&gt; &lt;/list&gt;&lt;/property&gt; 至于里面的配置是什么意思，我想都看出来了，这也更说明了 Spring 其实就是个快速生成对象（bean）的框架（从这里来看）其他的，可以配置下事务等配置，使用事务管理器最大的好处就是能少些代码，由它来管理 sessionFactory 的事务，其实是使用的类似 AOP 技术的（织入）通知实现的，这样在 Service 层的类中写的 CRUD 方法的时候就不需要写事务相关的代码了（记得在类上加注解） 123456&lt;!-- 配置事务管理器 --&gt;&lt;bean id="txManager" class="org.springframework.orm.hibernate5.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory"/&gt;&lt;/bean&gt;&lt;!-- 使用注解配置事务 --&gt;&lt;tx:annotation-driven transaction-manager="txManager"/&gt; 然后可以创建个实体 bean，编写好对应的映射文件，在上面的配置中写好 hbm 的文件路径（如果用注解就是类名了），然后就可以进行测试了，不出意外的话 hibernate 就集成完毕了 3.引入StrutsStruts 作为 web 层的框架引入也是很简单的，首先当然还是要导入相关的 jar ，还是要注意 jar 包的重复，以及因为要和 spring 整合，不要忘了导入 struts2-spring-plugin 包和 spring-web-x.x.x.RELEASE.jar 包然后顺便配置下相关的配置文件，有 struts 的 Struts.xml ： 12345678&lt;!-- 指定Action对象由谁创建 --&gt; &lt;constant name="struts.objectFactory" value="spring"&gt;&lt;/constant&gt; &lt;!-- 配置Action 不用写全名，因为已经交给Spring管理 --&gt;&lt;package name="employee" namespace="/employee" extends="struts-default"&gt; &lt;action name="list" class="employeeAction"&gt; &lt;result name="list"&gt;/WEB-INF/page/employee.jsp&lt;/result&gt; &lt;/action&gt;&lt;/package&gt; 注意：Action 的管理托管给 Spring 后，在 action 标签配置的 class 就是 spring 配置文件中 bean 的 id在 web.xml 中配置 struts： 123456789&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt; &lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 最后还是要在 Web.xml 中配置实例化 Spring 容器，避免每一次请求就初始化一次，浪费资源；实际上就是把 spring 容器存到 ServletContext 中去了： 12345678910&lt;!-- 指定spring的配置文件，默认从web根目录寻找配置文件，我们可以通过spring提供的classpath:前缀指定从类路径下寻找 --&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:beans.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- 对Spring容器进行实例化 Spring容器放在application范围 --&gt;&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; 到这里基本就整合完了，下面要做的就是测试一下然后进行开发了，测试也非常的简单 4.测试创建个 Action 访问下测试就可以了，比如可以这样写一个测试类 12345678910@Servicepublic class EmployeeAction &#123; // 依赖注入 @Resource EmployeeService employeeService; public String execute()&#123; ActionContext.getContext().put("empList", employeeService.list()); return "list"; &#125;&#125; OK，到这里基本就真的完成了，如果不出问题的话。。。。 所谓整合我们所说的整合一般就是：Spring 整合 Hibernate ：由 IOC 容器（Spring 框架的核心）管理 Hibernagte 的 SessionFactory ；以及让 Hibernate 使用上 Spring 的声明式事务Spring 整合 Struts ：让 spring 来接管 Struts 的 Action 在服务器启动的时候自动创建相应的 Action，如果想要解决高并发问题可以利用 spring 的 scope 属性（scope=&quot;prototype&quot;），让其每一个请求产生独立的 Action，避免数据的混乱，当然它的生命周期是很短的 补充来说一下为什么要在 web.xml 中配置实例化 Spring，如果不实例化那么在 Action 被访问的时候是必须要实例化的（因为 Action 要用啊），如果有很多人访问就会多次执行生成 ApplicationContext 的代码，这个是非常耗费资源的所以不如索性在服务器启动的时候就把 ApplicationContext 给实例化了，存到 ServletContext 中，需要的时候可以使用下面的代码获取： 123ApplicationContext ac = WebApplicationContextUtils.getRequiredWebApplicationContext(this.getServletContext());// 还有一种形式是ApplicationContext ac2 = WebApplicationContextUtils.getWebApplicationContext(ServletContext sc); 更好的，可以通过继承 ApplicationContextAware 来构造一个 spring 的工具类，别忘了让 spring 把这个工具类顺便给实例化了，这样直接调用就可以了 或者可以直接使用 spring 提供的：WebApplicationContext wac = ContextLoader.getCurrentWebApplicationContext();进而使用 wac 来获取 bean WebApplicationContext:其实这个接口不过是 applicationContext 接口的一个子接口罢了，只不过说它的应用形式是 web 罢了. 它在ApplicationContext 的基础上，添加了对 ServletContext 的引用(后面还加入了很多其他作用域的引用)，即 getServletContext 方法.可以说：它是一个是支持 web 特性的 BeanFactory。 相关源码： 123public interface WebApplicationContext extends ApplicationContext &#123; ServletContext getServletContext();&#125; 在配置 Hibernate 数据库信息的时候，就是 datasource 的时候，可以从 prop 文件中装载的，需要配一个：&lt;context:property-placeholder location=&quot;classpath:db.properties&quot;/&gt; 属性而已这样可以在 datasource 中使用：value=&quot;${user}&quot; 进行引用；同理，配置 SessionFactory 的时候也可以引用外部文件： 12345678910111213&lt;property name="configLocations" value="classpath:hinernate.cfg.xml" /&gt;&lt;!-- 上下的作用、内容都是一样的 --&gt;&lt;property name="hibernateProperties"&gt; &lt;props&gt; &lt;prop key="hibernate.show_sql"&gt;true&lt;/prop&gt; &lt;prop key="hibernate.hbm2ddl.auto"&gt;update&lt;/prop&gt; &lt;prop key="dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 一定不要忘记，要不然 getCurrentSession 是获取不到的 --&gt; &lt;prop key="hibernate.current_session_context_class"&gt;org.springframework.orm.hibernate5.SpringSessionContext&lt;/prop&gt; &lt;prop key="hibernate.connection.url"&gt;jdbc:mysql://localhost:3306/ssh&lt;/prop&gt; &lt;prop key="hibernate.connection.driver_class"&gt;com.mysql.jdbc.Driver&lt;/prop&gt; &lt;/props&gt;&lt;/property&gt; 这样 cfg 文件中就没必要写数据库连接的相关东西了，因为会注入一个连接池，只需要写 hibernate 特有的一些配置就 OK 了至于是使用外部文件好还是直接写在一起好，我也不造…. 再补充 classpath 是指 WEB-INF 文件夹下的 classes 目录；classpath 和 classpath* 区别： classpath：只会到你的 class 路径中查找找文件； classpath*：不仅包含 class 路径，还包括 jar 文件中( class 路径)进行查找. 也就是说前者只会从第一个 classpath 中加载，而后者会从所有的 classpath 中加载，用 classpath* 需要遍历所有的 classpath，所以加载速度是很慢的，因此，在规划的时候，应该尽可能规划好资源文件所在的路径，尽量避免使用 classpath* ；另外，&quot;**/&quot; 表示的是任意目录；**/applicationContext-*.xml&quot; 表示任意目录下的以 “applicationContext-“ 开头的 XML 文件。 千万注意 jar 包的重复和未部署 jar 包的巨坑，详细的解决笔记在我的 Github 参考&amp;拓展http://www.cnblogs.com/doit8791/p/5757798.htmlhttp://blog.csdn.net/hzy38324/article/details/44139609spring获取bean的几种方式https://essviv.github.io/2016/07/02/spring/spring-differerce-between-application-context-and-servlet-context/http://www.cnblogs.com/panxuejun/p/5898540.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Hibernate</tag>
        <tag>Spring</tag>
        <tag>Struts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JPA学习笔记（二）]]></title>
    <url>%2F2017%2F08%2F21%2FJPA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[上一次只是说了保存数据的方法，下面就把其他的一些 CRUD 操作补充完整，以及其他的一些细节，有些知识是和上一篇有关联的，但是上一篇感觉写的有点多于是就放在了这里虽然这一篇也非常的多…. 基本使用在很多情况和 hibernate 是差不多的，毕竟作者是主导者，套路都一样保存数据在上一篇已经说了，主要就是那个 persist 方法，Hibernate 中也是一样的，以前可能是 save 方法，persist 这个命名更规范，作用其实是一样的 获取数据在 hibernate 中获取数据是 get/load 简单说分别对应的是正常获取和延迟加载（lazy），在 JPA 中也是类似的，对应的通过 find/getReference 获取；由于它们使用了 Java 泛型方法，无需任何显示的类型转换即可获得特定类型的实体对象。 123456789101112EntityManagerFactory factory = Persistence.createEntityManagerFactory("jpaDemo");EntityManager em = factory.createEntityManager();// 查询其实无需开启事务em.getTransaction().begin();Employee emp = em.find(Employee.class, 1);// 相同的，返回的也是代理对象，延迟加载Employee emp = em.getReference(Employee.class, 1);em.getTransaction().commit();em.close();factory.close(); 与 hibernate 一样，如果使用延迟加载，在 EntityManager 关闭后是无法再向数据库查询数据的，EntityManager 内部其实也是在操纵 session 对象然后下面就当复习了，find 如果找不到会返回 null；getReference 找不到会抛异常 PS：获取的时候不使用事务也是可以的 更新数据继续复习，因为开启了事务，所以修改实体后在最后事务提交的时候会将变化更新到数据库，所以不需要再保存总结下就是只要满足两点，就会在事务提交时自动更新：1.和事务关联，2.处于托管状态 12Employee emp = em.find(Employee.class, 1);emp.setName("new Name"); 可以参考下后面的 JPA 中的对象状态 删除数据删除也很简单，就是调用 EntityManager 的 remove 方法。 12Employee emp = em.find(Employee.class, 1);em.remove(emp); 对象状态在 JPA 中对象有四种状态，分别是：新建、托管、游离、删除 ；在 hibernate 中好像是三种来，其实是差不多的，所以也就不多解释了，可以参考 Hibernate 笔记里的内容 新建当进行 new 对象时，这个对象处于新建状态 托管首先事务要开启，然后比如通过 find 来获取实体，那么这个实体就处于托管状态这个对象发生改变 EntityManager 会感知到，并且在事务提交的时候会自动更新到数据库 游离当调用 entity.clear()；等方法时此时，bean 将变成游离状态（此方法就是把实体管理器中的所有实体变成游离状态）游离状态修改实体不会再同步到数据库中如果此时调用 entity.merge(person); 将会把游离状态的实体变化的数据同步到数据库 删除就是认为删除的对象或是垃圾回收掉的对象 JPQL语句和 HQL 语句非常的相似，它们最大的特点就是采用的面向对象的方式，所以语句中出现的应该都是属性名而不是字段，那就先来看一段简单的 JPQL 使用： 12345// createQuery 中还可以传入第二个参数，这样就不用强转了，当然不写也是可以执行的TypedQuery&lt;Employee&gt; query = em.createQuery("SELECT e FROM Employee e", Employee.class);List&lt;Employee&gt; emps = query.getResultList(); 如果是使用 HQL 的话那就是直接 FROM Employee 就可以了，当然如果使用的 ORM 是 hibernate，那么其实也是可以这样写的，但是为了规范，最好还是这样写：SELECT e FROM Employee e 这里的 e 就是起的别名啦其他的也都差不多，如果后面填充参数所使用的占位符，索引的话用 ?，key 的话用 : ，补充一点的是如果想从指定的索引开始可以这样写：?1 这样就是从1开始的 Hibernate 中获取单一结果（只查一条）是使用 query.uniqueResult 方法；在 JPA 中是使用 getSingleResult 方法，其实也就只是方法名不一样 查询可以不开启事务，增删改就需要开启事务了 如果是执行增删改的语句，那就得执行 query 的 executeUpdate 方法了，和 hibernate 是一样的 其他的多数语句是一样的，遇到特殊的再回来补充吧…..（ORM 如果用的是 hibernate，写 HQL 也是可以跑的嘛） 关于createEntityManager 以 create 开头的有三个（用于创建相关查询语句），分别是 createQuery、createNamedQuery、createNativeQuery createQuery用于执行编写的 jpql 语句，不能用于执行 SQL 语句它有两个重载：Query createQuery(java.lang.String qlString)&lt;T&gt; TypedQuery&lt;T&gt; createQuery(java.lang.String qlString,java.lang.Class&lt;T&gt; resultClass)执行的结果是一样的（返回值略有不同），不同的是前者指定了返回的结果集的类型，后者没有制定，当然这两种查询得到的结果集是一样的，但是建议在使用的时候制定结果集 createNamedQuery用于传入命名查询的名称创建查询，如：createNamedQuery(&quot;findAllUser&quot;);使用 @NamedQuery 注解在实体类中定义命名查询：@NamedQuery(name=&quot;findAllUser&quot;,query=&quot;SELECT u FROM User u&quot;)如果要定义多条命名查询，可以使用这种形式：@NamedQueries({@NamedQuery(xx),...}) createNativeQuery此方法用于执行 SQL 语句，不能用于执行 jpql 语句，一般用于复杂的 sql 语句此方法有三个重载 123Query createNativeQuery(java.lang.String sqlString,java.lang.Class resultClass);Query createNativeQuery(java.lang.String sqlString);Query createNativeQuery(java.lang.String sqlString,java.lang.String resultSetMapping); 前面两个方法中一个指定了结果集一个没有制定，但是其执行结果是不一样的在返回的结果中由于没有指定结果集的类型，返回结果中只有结果，没有结果对应的字段名。也就是说指定结果集意味着原生查询的结果集中的栏将完全匹配实体的 O/R 映射。第三个方法使用一个字符串来指定结果集（主要应用于复杂的查询，返回的结果不能被实体一一映射），和第一个使用类来指定有所不同。 关系模型结合上一篇的注解看，哎╮(╯▽╰)╭又是分开的，太长了写不下了首先记住第一定律：只要注解后面是以 Many 结尾的（比如：@OneToMany）默认统一是延迟加载（lazy），其他的一般是立即加载 关于级联JPA 中的提供的级联有四种，下面会用到 CascadeType.REFRESH级联刷新当你刚开始获取到了这条记录，那么在你处理业务过程中，这条记录被另一个业务程序修改了（数据库这条记录被修改了），那么你获取的这条数据就不是最新的数据，那你就要调用实体管理器里面的 refresh 方法来刷新实体。使用其他获取是获取的缓存的数据，所以…..注意一般配置为：cascade={CascadeType.REFRESH},fetch=FetchType.LAZY，不会影响系统的正常运作。 CascadeType.PERSIST级联持久化（级联保存）比如保存 order 的时候也保存 orderItem，如果在数据库里已经存在与需要保存的 orderItem 相同的 id 记录，则级联保存出错。也就是说相当于是 insert 操作 CascadeType.MERGE级联更新比如当对象 Order 处于游离状态时，对对象 Order 里面的属性作修改，也修改了 Order 里面的 orderItems。 CascadeType.REMOVE级联删除就是删除的时候都删了….不是很推荐，加上逻辑判断更好 CascadeType.ALL具备上面的全部 还有一些需要注意的是： 级联的配置是生效于对应的方法的，比如上面的四个就依次对应实体管理器的 refresh 、persist、merge、remove 所以说，使用 JPQL 语句的时候级联是不会生效的 一对多/多对一大体套路是不变的：多的一方为关系维护端，负责外键记录的更新，关系被维护端没有权利更新外键记录当属性中出现 mappedBy 时，一般就是关系被维护端了，值为是由谁维护的（维护端的属性名） 在一对多的“一”的一方，可以使用注解 @OneToMany 来标识多的一方的集合，以及设置相应的级联（级联是可以设置多个的，用括号套）下面是一个简单的用户和订单的栗子，对于关系被维护端的用户类： 12345678910111213141516171819202122232425262728293031323334353637383940@Entity@Table(name = "users")public class User &#123; private Integer id; private String name; private Set&lt;Order&gt; orders = new HashSet&lt;&gt;(); @Id public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @OneToMany(cascade = CascadeType.ALL,mappedBy = "user") //拥有mappedBy注解的实体类为关系被维护端 public Set&lt;Order&gt; getOrders() &#123; return orders; &#125; public void setOrders(Set&lt;Order&gt; orders) &#123; this.orders = orders; &#125; // 只为方便测试添加 public void addItem(Order order) &#123; order.setUser(this); this.orders.add(order); &#125;&#125; 然后是多的一方，为关系的维护端 Order： 123456789101112131415161718192021222324252627282930313233343536@Entity@Table(name = "orders")public class Order &#123; private Integer id; private String orderName; private User user; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getOrderName() &#123; return orderName; &#125; public void setOrderName(String orderName) &#123; this.orderName = orderName; &#125; // 通过 optional 设置为必填项 @ManyToOne(cascade = CascadeType.ALL,optional = false) @JoinColumn(name = "user_id") public User getUser() &#123; return user; &#125; public void setUser(User user) &#123; this.user = user; &#125;&#125; 一个简单的测试类，其实只需要执行完 createEntityManagerFactory 就会自动建表了 1234567891011121314151617181920public void save() &#123; EntityManagerFactory factory = Persistence.createEntityManagerFactory("jpaDemo"); EntityManager em = factory.createEntityManager(); em.getTransaction().begin(); User user = new User(); user.setName("佳芷"); user.setId(123); Order order1 = new Order(); order1.setOrderName("测试项1"); Order order2 = new Order(); order2.setOrderName("测试项2"); user.addItem(order1); user.addItem(order2); em.persist(user); em.getTransaction().commit(); em.close(); factory.close();&#125; 需要说下的是，如果设置的一对多/多对一关系是单向的，那么不加 mappedBy 也是可以的，在双向关系中是必须的，否则会引发数据一致性的问题什么是单向映射？简单地说就是可以从关联的一方去查询另一方，却不能反向查询；以上面的栗子，在 user 中的 set 集合使用 @OneToMany （不用 mappedBy），在 order 中去掉 user 的属性，什么也不用加，这就是单向的；只能通过 user 获取 order ，不能通过 order 获取 user想深入了解的话可以去：https://www.ibm.com/developerworks/cn/java/j-lo-jparelated/index.html 一对一在一对一中和一对多其实也差不多，都需要设置关系维护端和被维护端，只不过因为是一对一，所以这两个角色是可以互换的，设谁都可以在被维护端记得加 mappedBy 属性，在维护端记得使用 @JoinColumn 注解指定外键约束的那一列的名字，然后因为是一对一，如果在关系维护端使用了 （非空），那么关系被维护端就没必要在设了具体的代码就不贴了，和上面很类似，一对一也区分单向和双向，如果是单向的话，只需要在一方加入 @OneToOne @JoinColumn(name=&quot;PSPACE_ID&quot;) ；在另一方什么都不用了。如果是双向的，那么就需要在被维护方加 @OneToOne(mappedBy=&quot;department&quot;) 注解；在关系维护方加 注解 @OneToOne @JoinColumn(name=&quot;PSPACE_ID&quot;) 多对多会了上面的两种，这种其实也差不多了，会比上面的复杂一点，关于实体的定义就不全部贴了，主要贴下不太一样的东西，以学生和老师为例，这是多对多的关系，假设规定学生的一方为关系的维护端，那么老师就是关系被维护端，学生有权修改和更新中间表的数据，老师则不可以 123456789101112131415@Entitypublic class Student &#123; private Integer id; private String name; private Set&lt;Teacher&gt; teachers = new HashSet&lt;&gt;(); // inverseJoinColumns 即关系被维护端的外键列名 @ManyToMany(cascade = CascadeType.REFRESH) @JoinTable(name = "student_teacher",inverseJoinColumns = @JoinColumn(name = "teacher_id"), joinColumns = @JoinColumn(name = "student_id")) public Set&lt;Teacher&gt; getTeachers() &#123; return teachers; &#125; ....&#125; 然后是被维护端的老师，也是省略了一部分： 1234567891011121314151617181920212223242526@Entitypublic class Teacher &#123; private Integer id; private String name; private Set&lt;Student&gt; students = new HashSet&lt;&gt;(); // 在多对多的关系中，一般不会设置级联删除，太危险 @ManyToMany(cascade = CascadeType.REFRESH,mappedBy = "teachers") public Set&lt;Student&gt; getStudents() &#123; return students; &#125; // 相同的 ID 即认为是相同的对象 @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Teacher teacher = (Teacher) o; return id != null ? id.equals(teacher.id) : teacher.id == null; &#125; @Override public int hashCode() &#123; return id != null ? id.hashCode() : 0; &#125;&#125; 然后是测试类，大部分的管理其实只要操作关系维护端就可以了，因为它才是有权利更新中间表的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 @Test public void save() throws Exception &#123; EntityManagerFactory factory = Persistence.createEntityManagerFactory("jpaDemo"); EntityManager em = factory.createEntityManager(); em.getTransaction().begin(); Student student = new Student("佳芷"); Teacher teacher = new Teacher("冰封承諾"); em.persist(student); em.persist(teacher); em.getTransaction().commit(); em.close(); factory.close(); &#125;// 建立学生与老师之间的关系，也就是在中间表中添加记录// 只有关系维护端才有权利@Testpublic void buildBind() throws Exception &#123; EntityManagerFactory factory = Persistence.createEntityManagerFactory("jpaDemo"); EntityManager em = factory.createEntityManager(); em.getTransaction().begin(); Student student = em.find(Student.class, 1); student.getTeachers().add(em.getReference(Teacher.class, 1)); // 解除关系的话 // student.getTeachers().remove(em.getReference(Teacher.class, 1)); em.getTransaction().commit(); em.close(); factory.close();&#125;// 删除学生，会自动删除相关的“关系”// 如果是删除老师，没有权利删除关系，所以要先解除关系再删除@Testpublic void delStudent() throws Exception &#123; EntityManagerFactory factory = Persistence.createEntityManagerFactory("jpaDemo"); EntityManager em = factory.createEntityManager(); em.getTransaction().begin(); Student student = em.find(Student.class, 1); em.remove(student); em.getTransaction().commit(); em.close(); factory.close();&#125; 联合主键所谓主键就是可以唯一确定该行数据，由此可以知道，当一个字段不能决定该行的值时，就要考虑采用多个字段作为主键。比如，对于学校来说，班号可以决定班级，但是决定不了班级里的某个人，表示班级里的某个人就需要用班号+该学生在该班内的编号.这就可以说是联合（复合）主键了首先要定义一个联合主键的类，也就是里面的属性都是主键，并且这个类有几个规定：首先要实现序列化的接口，然后要重写 hashCode 和 equals，最后标上 @Embeddable 注解就可以了 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Embeddablepublic class StuPK implements Serializable &#123; private Integer classId; private Integer stuId; // 为了下面测试便于初始数据 public StuPK() &#123;&#125; public StuPK(Integer classId, Integer stuId) &#123; this.classId = classId; this.stuId = stuId; &#125; public Integer getClassId() &#123; return classId; &#125; public void setClassId(Integer classId) &#123; this.classId = classId; &#125; public Integer getStuId() &#123; return stuId; &#125; public void setStuId(Integer stuId) &#123; this.stuId = stuId; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; StuPK stuPK = (StuPK) o; if (classId != null ? !classId.equals(stuPK.classId) : stuPK.classId != null) return false; return stuId != null ? stuId.equals(stuPK.stuId) : stuPK.stuId == null; &#125; @Override public int hashCode() &#123; int result = classId != null ? classId.hashCode() : 0; result = 31 * result + (stuId != null ? stuId.hashCode() : 0); return result; &#125;&#125; 下面就是具体的持久化类了，引用 StuPK 作为主键，StuPK 类里的属性在这里是可以直接用的，也就是说最后会保存到数据库的表中 1234567891011121314151617181920212223242526272829@Entitypublic class Stu &#123; private StuPK id; private String name; public Stu() &#123;&#125; public Stu(String name,Integer classId, Integer stuId) &#123; this.name = name; id = new StuPK(classId, stuId); &#125; @EmbeddedId public StuPK getId() &#123; return id; &#125; public void setId(StuPK id) &#123; this.id = id; &#125; @Column(length = 10) public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 测试类就不写了，没什么好写的和上面是一样的，主要是执行后看看数据库对应的表结构对不对 主键生成策略这个算是个补充内容吧，本来是应该写在上一篇中的，但是那篇看了下有点长，就补充在这吧，自带的四种就不说了，上一篇中已经说的很详细了，下面主要说的是使用 hibernate 的情况下如何使用 hibernate 提供的其他注解生成策略，通过 Hibernate 的 @GenericGenerator 实现。首先看下用法吧(以 uuid 为例)： 12345@Id@GeneratedValue(generator = "system-uuid")@GenericGenerator(name="system-uuid", strategy="uuid")@Column(name="uuid", length=32)private String uuid; @GeneratedValue(generator = &quot;system-uuid&quot;) ：用 generator 属性指定要使用的策略生成器。@GenericGenerator(name = &quot;system-uuid&quot;, strategy = &quot;uuid&quot;) ：声明一个策略通用生成器，策略 strategy 为 “uuid”。uuid 可以说是最通用的，适用于所有数据库。 persistence-unit中的类集合一个 persistence unit 将固定数量的一组类映射到关系数据库。缺省情况下，如果你没有在 persistence.xml 中指定任何元数据，persistence provider 就会对包含该 persistence.xml 的 JAR 文件进行扫描，从根目录开始搜寻任何标注有 @javax.persistence.Entity 注解的类，并将这些类添加到由 persistence unit 管理的类集合中。此外，你还可以通过 &lt;jar-file&gt; 元素指定额外的 JAR 文件，以供 persistence provider 搜索。该元素的值不能使用绝对路径，只能是一个以包含 persistence.xml 的JAR文件为基准的相对路径。比如：&lt;jar-file&gt;../lib/customer.jar&lt;/jar-file&gt;JAR 文件的自动扫描在 Java EE 环境下是保证可以正常执行的，但在 Java SE 应用程序中却无法做到可移植。理论上，要决定必须搜索哪些 JAR 文件也许是不太可能的。不过，现实中这不是问题。参与 EJB 3.0 专家组的主要厂商都非正式地宣称过，它们会毫无疑问的在 Java SE 中支持这一特性。无论是否使用自动 JAR 文件扫描，你都可以用 &lt;class&gt; 元素显式的列出 persistence unit 中的类集合。 通常情况下，你无需指定 &lt;class&gt;，&lt;jar-file&gt; 或 &lt;mapping-file&gt; 元素。但是有一种情形你可能需要使用上述元素，即当你需要在两个或多个 persistence unit 中映射同一个类时。 参考https://wizardforcel.gitbooks.io/tutorialspoint-java/jpa/629.htmlhttps://www.w3cschool.cn/java/jpa-entitymanager.html实体管理器和实体管理器工厂JPA中的级联JPQL大全stence-Unit中的类集合]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JPA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JPA学习笔记]]></title>
    <url>%2F2017%2F08%2F14%2FJPA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[JPA( Java Persistence API )是 Sun 官方提出的 Java 持久化规范。 它为 Java 开发人员提供了一种对象/关联映射工具来管理 Java 应用中的关系数据。 他的出现主要是为了简化现有的持久化开发工作和整合 ORM 技术，结束现在 Hibernate，TopLink，JDO 等 ORM 框架各自为营的局面。 注意：JPA 是规范，不是 ORM 框架；是 ORM 框架的规范，JPA 没有实现 ORM，具体实现由 ORM 厂商提供JPA 其实和 Hibernate 使用并没有多少差别，采用的 JPQL 语句和 HQL 差不多呢，毕竟 Hibernate 的作者是主导的 组件构成与优势 标准化: 提供相同的 API，这保证了基于 JPA 开发的企业应用能够经过少量的修改就能够在不同的 JPA 框架下运行。 简单易用，集成方便: JPA 的主要目标之一就是提供更加简单的编程模型，在 JPA 框架下创建实体和创建 Java 类一样简单，只需要使用 javax.persistence.Entity 进行注释；JPA 的框架和接口也都非常简单 可媲美 JDBC 的查询能力: JPA 的查询语言是面向对象的，JPA 定义了独特的 JPQL，而且能够支持批量更新和修改、JOIN、GROUP BY、HAVING 等通常只有 SQL 才能够提供的高级查询特性，甚至还能够支持子查询。 支持面向对象的高级特性: JPA 中能够支持面向对象的高级特性，如类之间的继承、多态和类之间的复杂关系，最大限度的使用面向对象的模型 摘自：http://www.jianshu.com/p/afd588c8951c JPA 由三个不同的组件构成： 实体（Entities）: 在当前版本的 JPA 中实体是普通 Java 对象（POJO）。老版本的 JPA 中实体类需要继承 JPA 提供的实体基类，但是这样的设计导致框架中存在了严重的依赖关系，测试变得更加困难；所以在新版 JPA 中不再要求实体类继承任何框架类。 对象-关系型元数据（Object-relational metadata）: 应用程序的开发者们必须正确设定 Java 类和它们的属性与数据库中的表和列的映射关系。有两种设定方式：通过特定的配置文件建立映射(XML)；或者使用在新版本中支持的注解。 Java持久化查询语句（Java Persistence Query Language - JPQL): 因为 JPA 旨在建立不依赖于特定的数据库的抽象层，所以它也提供了一种专有查询语言来代替 SQL。 这种由 JPQL 到 SQL 语言的转换，为 JPA 提供了支持不同数据库方言的特性，使得开发者们在实现查询逻辑时不需要考虑特定的数据库类型。 PS：Hibernate 针对 JPA 实现的相关包在 required、jpa 下 初探JPA这里还是以 hibernate 为栗子，要使用 JPA，首先要知道它必须的配置文件，按照规范是放在类路径的 META-INF 目录下 ，文件名也是固定的 persistence.xml 在这个文件中配置持久化单元需要指定跟哪个数据库进行交互；需要指定 JPA 使用哪个持久化的框架以及配置该框架的基本属性 下面的步骤可以总结为：创建实体类, 使用 annotation 来描述实体类跟数据库表之间的映射关系；使用 JPA API 完成数据增加、删除、修改和查询操作 创建 EntityManagerFactory (对应 Hibernate 中的 SessionFactory); 创建 EntityManager (对应 Hibernate 中的Session); 下面就展示一份 persistence 的配置文件，差不多是这样配的，transaction-type 事务类型一般都是本地事务吧，全局事务感觉用的很少 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;persistence version="2.1" xmlns="http://xmlns.jcp.org/xml/ns/persistence" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/persistence http://xmlns.jcp.org/xml/ns/persistence/persistence_2_1.xsd"&gt; &lt;persistence-unit name="jpaDemo" transaction-type="RESOURCE_LOCAL"&gt; &lt;!-- 配置 Hibernate JPA作为JPA的实现ORM框架 --&gt; &lt;!-- 若项目只有一个JPA实现产品，那么这个配置可以不写 --&gt; &lt;provider&gt;org.hibernate.ejb.HibernatePersistence&lt;/provider&gt; &lt;!-- 添加持久化类 --&gt; &lt;class&gt;com.jpaDemo.entity.Customer&lt;/class&gt; &lt;class&gt;com.jpaDemo.entity.Order&lt;/class&gt; &lt;!-- 配置二级缓存的策略 ALL：所有的实体类都被缓存 NONE：所有的实体类都不被缓存. ENABLE_SELECTIVE：标识 @Cacheable(true) 注解的实体类将被缓存 DISABLE_SELECTIVE：缓存除标识 @Cacheable(false) 以外的所有实体类 UNSPECIFIED：默认值，JPA 产品默认值将被使用 --&gt; &lt;shared-cache-mode&gt;ENABLE_SELECTIVE&lt;/shared-cache-mode&gt; &lt;properties&gt; &lt;!-- 连接数据库的基本信息 --&gt; &lt;property name="javax.persistence.jdbc.driver" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="javax.persistence.jdbc.url" value="jdbc:mysql:///jpa"/&gt; &lt;property name="javax.persistence.jdbc.user" value="root"/&gt; &lt;property name="javax.persistence.jdbc.password" value="root"/&gt; &lt;!-- 配置 JPA 实现产品的基本属性. 配置 hibernate 的基本属性 --&gt; &lt;property name="hibernate.format_sql" value="true"/&gt; &lt;property name="hibernate.show_sql" value="true"/&gt; &lt;property name="hibernate.hbm2ddl.auto" value="update"/&gt; &lt;!-- 二级缓存相关 --&gt; &lt;property name="hibernate.cache.use_second_level_cache" value="true"/&gt; &lt;property name="hibernate.cache.region.factory_class" value="org.hibernate.cache.ehcache.EhCacheRegionFactory"/&gt; &lt;property name="hibernate.cache.use_query_cache" value="true"/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; 可以看出大部分的配置是在 persistence-unit 中，直译就是持久化单元，简单说就是一堆实体 bean 的集合上面的配置中还开启了二级缓存，那么就需要配相关的信息，下面作为简单演示就不搞了，我只是配了 properties 标签的前两部分而已那么下面就写几个实体吧，需要注意的是如果使用注解不要导错包，以 javax 开头的是 sun 公司定义的标准 123456789101112131415161718192021222324252627282930@Entitypublic class Person &#123; private Integer id; private String name; public Person() &#123; &#125; public Person(String name) &#123; this.name = name; &#125; @Id @GeneratedValue public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 非常简单的一个 bean，下面就弄段测试代码来跑一下，记得导入相关的包比如 JPA 和 Hibernate 的 12345678910111213@Testpublic void save() throws Exception &#123; // 这句话执行后就会创建相应的表，前提是配置了相关配置;相当于 SessionFactory EntityManagerFactory factory = Persistence.createEntityManagerFactory("jpaDemo"); EntityManager em = factory.createEntityManager(); // 开启事务 em.getTransaction().begin(); // 和 save 是一样的，只不过这个名字更规范 em.persist(new Person("佳芷")); em.getTransaction().commit(); em.close(); factory.close();&#125; JPA注解首先，看准包是 javax 开头的，其次，和 hibernate 的注解其实是差不多了 @Table使用格式：@Table(name=&quot;&quot;,catalog=&quot;&quot;,schema=&quot;&quot;)只能标注在实体的 class 定义处 ，表示实体对应的数据库表的信息，当然这三个值都是可选的 @Id必须，定义了映射到数据库表的主键的属性,一个实体只能有一个属性被映射为主键.标注在属性或者 getter 方法前 @GeneratedValue使用格式：@GeneratedValue(strategy=GenerationType, generator=&quot;&quot;)strategy：表示主键生成策略，有 AUTO、INDENTITY、SEQUENCE 和 TABLE 4 种，分别表示让 ORM 框架自动选择、根据数据库的 Identity 字段生成（差不多就是自增长）、根据数据库表的 Sequence 字段生成、以有根据一个额外的表生成主键，默认为AUTOgenerator：表示主键生成器的名称,这个属性通常和 ORM 框架相关，例如 Hibernate 可以指定 uuid 等主键生成方式. @Column同上，一般标注在 getter 方法前，当实体的属性与其映射的数据库表的列不同名时需要使用，一般可以省略不写描述了数据库表中该字段的详细定义，这对于根据 JPA 注解生成数据库表结构的工具非常有作用.name：表示数据库表中该字段的名称,默认情形属性名称一致nullable：表示该字段是否允许为 null，默认为 trueunique：表示该字段是否是唯一标识，默认为 falselength：表示该字段的大小，仅对 String 类型的字段有效insertable：表示在 ORM 框架执行插入操作时，该字段是否应出现 INSETRT 语句中，默认为 trueupdateable：表示在 ORM 框架执行更新操作时，该字段是否应该出现在 UPDATE 语句中，默认为 true；对于一经创建就不可以更改的字段，该属性非常有用，如对于 birthday 字段.columnDefinition：表示该字段在数据库中的实际类型，通常 ORM 框架可以根据属性类型自动判断数据库中字段的类型，但是对于 Date 类型仍无法确定数据库中字段类型究竟是 DATE,TIME 还是 TIMESTAMP，此外，String 的默认映射类型为 VARCHAR，如果要将 String 类型映射到特定数据库的 BLOB 或 TEXT 字段类型（或者使用 @Lob 注解），该属性非常有用 @Lob用于标注字段类型为 Clob 和 Blob 类型，比如标注在 string 类型上就是长文本型Clob(Character Large Ojects) 类型是长字符串类型,实体的类型可为char[]、Character[]、或者 String 类型Blob(Binary Large Objects) 类型是字节类型,实体的类型可为 byte[]、Byte[]、或者实现了 Serializable 接口的类。通常使用惰性加载的方式,，@Basic(fetch=FetchType.LAZY) @Transient表示该属性并非一个到数据库表的字段的映射 ，ORM 框架将忽略该属性.如果一个属性并非数据库表的字段映射，就务必将其标示为 @Transient，否则，ORM 框架默认其注解为 @Basic. @Temporal在核心的 Java API 中并没有定义 Date 类型的精度(temporal precision)。而在数据库中表示 Date 类型的数据有 DATE, TIME 和 TIMESTAMP 三种精度(即单纯的日期,时间,或者两者兼备)。在进行属性映射时可使用 @Temporal 注解来调整精度，比如：@Temporal (TemporalType.TIMESTAMP) @Basic表示一个简单的属性到数据库表的字段的映射，对于没有任何标注的 getXxxx() 方法，默认即为 @Basicfetch：表示该属性的读取策略，有 EAGER 和 LAZY 两种，分别表示主支抓取和延迟加载，默认为 EAGER.optional：表示该属性是否允许为 null，默认为 true @Enumerated用来指定实体持久化属性的为枚举类型如果标记没有显性给出或者 EnumType 没有指定，那枚举类型默认为 ORDINAL 数字(索引)标识或者明确指定其 EnumType 为 STRING，采用的是字符串存到数据库，通常此字段配合 Column 来设置长度和非空 @ManyToOne表示一个多对一的映射，该注解标注的属性通常是数据库表的外键optional：是否允许该字段为 null，该属性应该根据数据库表的外键约束来确定,默认为 truefetch：表示抓取策略，默认为 FetchType.EAGERcascade：表示默认的级联操作策略，可以指定为 ALL、PERSIST、MERGE、REFRESH 和 REMOVE 中的若干组合，默认为无级联操作targetEntity：表示该属性关联的实体类型，该属性通常不必指定，ORM 框架根据属性类型自动判断 targetEntity.此注解一般需要配合 @JoinColumn 来确定外键名（字段名） @JoinColumn@JoinColumn 和 @Column 类似，只不过它描述的不是一个简单字段，而一一个关联字段；例如.描述一个 @ManyToOne 的字段.name：该字段的名称，由于 @JoinColumn 描述的是一个关联字段，则默认的名称由其关联的实体决定.例如，实体 Order 有一个 user 属性来关联实体 User，则 Order 的 user 属性为一个外键其默认的名称为实体 User 的名称 + 下划线 + 实体 User 的主键名称 @OneToMany描述一个一对多的关联，该属性应该为集体类型，在数据库中并没有实际字段.fetch：表示抓取策略，默认为 FetchType.LAZY，因为关联的多个对象通常不必从数据库预先读取到内存cascade：表示级联操作策略，对于 OneToMany 类型的关联非常重要，通常该实体更新或删除时，其关联的实体也应当被更新或删除mappedBy：拥有 mappedBy 注解的实体类为关系被维护端（One），值为维护端的属性名（当前实体在关联实体中的属性名称）；如果类之间是单向关系，不需要提供定义，在双向关系中是必须的，否则会引发数据一致性的问题例如：实体 User 和 Order 是 OneToMany 的关系，则实体 User 被删除时，其关联的实体 Order 也应该被全部删除 @OneToOne描述一个一对一的关联fetch：表示抓取策略，默认为 FetchType.LAZYcascade：表示级联操作策略 @ManyToMany描述一个多对多的关联，多对多关联上是两个一对多关联，但是在 ManyToMany 描述中中间表是由 ORM 框架自动处理targetEntity：表示多对多关联的另一个实体类的全名，例如：package.Book.classmappedBy：表示多对多关联的另一个实体类的对应集合属性名称示例：User 实体表示用户，Book 实体表示书籍，为了描述用户收藏的书籍，可以在 User 和 Book 之间建立 ManyToMany 关联 1234567891011121314151617181920212223242526@Entitypublic class User &#123; private List books; @ManyToMany(targetEntity=package.Book.class) public List getBooks() &#123; return books; &#125; public void setBooks(List books) &#123; this.books=books; &#125;&#125;@Entitypublic class Book &#123; private List users; @ManyToMany(targetEntity=package.Users.class, mappedBy=”books”) public List getUsers() &#123; return users; &#125; public void setUsers(List users) &#123; this.users=users; &#125;&#125; 两个实体间相互关联的属性必须标记为 @ManyToMany ，并相互指定 targetEntity 属性需要注意的是，有且只有一个实体的 @ManyToMany 注解需要指定 mappedBy 属性，指向 targetEntity 的集合属性名称利用 ORM 工具自动生成的表除了 User 和 Book 表外，还自动生成了一个 User_Book 表，用于实现多对多关联 @MappedSuperclass可以将超类的 JPA 注解传递给子类，使子类能够继承超类的 JPA 注解，所以说要写在超类的上面啊 @Embedded将几个字段组合成一个类，并作为整个 Entity 的一个属性.例如 User 包括 id、name、city、street、zip 属性.我们希望 city、street、zip 属性映射为 Address 对象，这样 User 对象将具有 id、name 和 address 这三个属性.Address 对象必须定义为 @Embededable 123456789@Embeddablepublic class Address &#123;city,street,zip&#125;@Entitypublic class User &#123; @Embedded public Address getAddress() &#123; &#125;&#125; Hibernate验证注解 注解 说明 @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(regex=,flag=) 被注释的元素必须符合指定的正则表达式 Hibernate 验证器附加的约束Validator 附加的 constraint 注解 说明 @NotBlank(message =) 验证字符串非null，且长度必须大于0 @Email 被注释的元素必须是电子邮箱地址 @Length(min=,max=) 被注释的字符串的大小必须在指定的范围内 @NotEmpty 被注释的字符串的必须非空 @Range(min=,max=,message=) 被注释的元素必须在合适的范围内 完整版参考：http://www.cnblogs.com/afeng7882999/p/4300032.html 参考http://www.jianshu.com/p/07e8412d577ahttp://blog.csdn.net/u010837612/article/details/47660205]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JPA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建工具之Gradle]]></title>
    <url>%2F2017%2F08%2F09%2F%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7%E4%B9%8BGradle%2F</url>
    <content type="text"><![CDATA[Gradle 是一个基于Apache Ant 和 Apache Maven 概念的项目自动化建构工具。它使用一种基于 Groovy 的特定领域语言来声明项目设置，而不是传统的 XML。 之前其实写过一点 Gradle 的，那是因为写 Android 的时候在 AS 中使用的就是它来构建的，但是并没怎么深入，这次就来好好的看一下，当然深度感觉还是不够，下面的一些介绍直接 copy 过来的，也可以去看看当时的原文 Groovy是一种动态语言。这种语言比较有特点，它和Java一样，也运行于Java虚拟机中。恩？？对头，简单粗暴点儿看，你可以认为Groovy扩展了Java语言。 比如，Groovy对自己的定义就是：Groovy是在 java平台上的、 具有像Python， Ruby 和 Smalltalk 语言特性的灵活动态语言， Groovy保证了这些特性像 Java语法一样被 Java开发者使用。 实际上，由于Groovy Code在真正执行的时候已经变成了Java字节码，所以JVM根本不知道自己运行的是Groovy代码。所以说，对于做Java的来说，可以无缝切换，它是一个构建工具，不是依赖管理工具，如果你的项目不需要复杂的构建流程，请不要尝试 Gradle，否则踩的坑根本补不过来，这个去网上看看就知道了，个人并不太喜欢它，也没用特别出众的地方。 安装和之前的 Maven 差不多，都是下载、添加环境变量、命令行进行测试（gradle -v），环境变量一般命名 GRADLE_HOME 然后在 PATH 里添加 %GRADLE_HOME%\bin ；当然这是 win 的方法，其他系统等用到了再补充吧 每次执行 Gradle 的时候都会把其 init.d 目录下的脚本文件都执行一遍 官网：https://gradle.org/ 另外，Gradle 的版本升级很恶心，各个版本还不兼容，所以每个项目需要特定版本的 Gradle，所以不建议自己下，让 IDEA 通过 wrapper 文件自动下载合适的版本是更好的选择；一般会在用户目录下创建 .gradle 文件，里面缓存了各个版本，记得清理不需要的版本，否则。。。 Groovy的补充Groovy 这门语言展开说还是挺多的，也许以后会详细了解，目前只需要知道几点它的特性就行了，它应该也算是“动态语言了” 完全兼容 Java 语法 分号是可选的 类、方法默认是 public Grooy 编译器会自动给属性添加 getter/setter 方法 属性可以直接使用点号进行获取 最后一个表达式的值会被当作返回值 == 相当于 equals()，所以不会产生空指针异常 类型可选、括号可选、 assert 语句、字符串的定义(单引号、双引号、三个单引号)、闭包、集合API比如可以直接使用 def 来定义变量：def ver = 1 ；println(ver) 和 println ver 是一样的和其他语言类似，字符串的定义上，双引号中可以使用 ${} 引用变量，三个单引号可以换行 关于集合的简单使用： 1234567891011// listdef listTest = ['abc','aaa']// List追加数据listTest &lt;&lt; 'bbb'// mapdef mapTest = ['aaa':12,'bbb':13]// map追加数据mapTest.ccc = 456println mapTest.aaaprintln mapTest['bbb'] 关于闭包的简单使用，闭包简单说就是个代码块，在其他语言中比如 python 中也是了解过的： 12345678910111213141516171819202122// 定义两个“闭包”，一般闭包用于复制给变量// 这是一个带参数的闭包，v 就是参数def c1 = &#123; v -&gt; println v&#125;// 不带参数的闭包def c2 = &#123; println 'hello'&#125;// closure 就是接收闭包函数的，注意不要导入任何 java 的包def method1(Closure closure)&#123; closure('param')&#125;def method2(Closure closure)&#123; closure()&#125;// 调用测试method1(c1)method2(c2) 初探Gradle首先 Gradle 有三要素，GroupId、ArtifactId、VersionGroupId ：一般就是公司域名的倒写，类似包名ArtifactId ：“编号”，要保证在组下面编号是唯一的（当然可以使用英文） 目录结构目录结构类似： |-src|— main|— |— java|— |— resources|— |— webapp|— test|— |— java|— |— resources 构建一个项目我用 IDEA 新建了个 Gradle 项目，自动给我生成的有 build.gradle 和 settings.gradle 这两个文件，第一个就是主要的配置文件了，第二个文件主要是加载模块用的，第一个文件最终会生成一个 Project 对象，之前定义的那三要素就相当于是里面的变量了，需要说下的是，如果有多个模块就要在 settings 文件中使用 include &quot;module1&quot;, &quot;module2&quot;, &quot;module3&quot; 进行导入了，下面会详细说 12345678910111213141516171819group 'com.bfchengnuo.gradletest'version '1.0-SNAPSHOT'apply plugin: 'java'// apply plugin: 'war'sourceCompatibility = 1.8// 定义公共仓库repositories &#123; maven &#123;url 'http://maven.aliyun.com/nexus/content/groups/public/'&#125; mavenLocal() mavenCentral()&#125;// 定义依赖关系dependencies &#123; testCompile group: 'junit', name: 'junit', version: '4.12'&#125; 配置 group、version 等属性除了在 build.gradle 文件中，还可以新建一个 gradle.properties 文件，然后这么写，效果是一样的 12group=&apos;com.bfchengnuo.gradletest&apos;version=&apos;1.0-SNAPSHOT&apos; 关于仓库配置： mavenLocal():直接使用 ~/.m2/ 作为 maven 仓库的路径 mavenCentral():使用 maven 中央仓库 http://central.maven.org/ 作为 maven 仓库的路径 jcenter():使用 jcenter 仓库 http://jcenter.bintray.com/ 作为 maven 仓库路径，在国内通常比 mavenCentral() 快很多，有 CDN。 maven { url: &#39;/path/to/custom/url&#39; }:自定义的 maven 仓库路径 可以同时配置多个仓库，会按顺序查找。 关于构建脚本Gradle 里的任何东西都是基于这两个基础概念: projects ( 项目 ) tasks ( 任务 ) 每一个构建都是由一个或多个 projects 构成的. 一个 project 到底代表什么取决于你想用 Gradle 做什么. 举个例子, 一个 project 可以代表一个 JAR 或者一个网页应用. 它也可能代表一个发布的 ZIP 压缩包, 这个 ZIP 可能是由许多其他项目的 JARs 构成的. 但是一个 project 不一定非要代表被构建的某个东西. 它可以代表一件XX要做的事, 比如部署你的应用. 每一个 project 是由一个或多个 tasks 构成的. 一个 task 代表一些更加细化的构建. 可能是编译一些 classes, 创建一个 JAR, 生成 javadoc, 或者生成某个目录的压缩文件. 我们经常使用的 tasks 多是由插件进行提供的 构建的生命周期总体可分为：初始化 —-&gt; 配置 —-&gt; 执行 初始化阶段：就是看看配置了那些项目，把它们加进来配置阶段：可以说只要不是特别的都属于配置，比如 apply、repositories、task 中的语句（不包括里面的doFirst、doLast ），这些语句都是，主要是用于处理各 task 的依赖关系执行阶段：最简单的栗子就是 task 里的 doFirst、doLast 里面的代码，它们都属于执行阶段运行的 PS:每个阶段之后都会执行一个构造方法，平时一般用不到，就不说了 自定义Task首先定义一个简单的任务试试： 12345678910// 第一种方式，task 是小写task hello &#123; doLast &#123; //实现doLast方法 println 'Hello world!' &#125;&#125;// 第二种方式task hello &lt;&lt; &#123; println 'Hello world!'&#125; &lt;&lt; 就等价于 doLast，doLast 是 gradle 提供访问 task 任务的一个 API，类似的还有 doFirst，当一个 task 被执行的时候，可以通过 doFirst 和 doLast 向 task 中动态添加操作。doFirst 和 doLast 会在 task 本身被执行之后才会被执行 如果使用依赖关系话，定义顺序其实是无所谓的… 123456789101112131415161718task hello &lt;&lt; &#123; println 'Hello world!'&#125;task intro(dependsOn: hello) &lt;&lt; &#123; println "I'm Gradle"&#125;// 或者你这么写task intro(dependsOn: hello)&#123; println 'hello' // 如果你写了这句，那么无论是否有依赖，这句永远第一执行 doLast &#123; println "I'm Gradle" &#125;&#125;// 依赖还可以这样写task intro() &lt;&lt; &#123; dependsOn 'hello' println "I'm Gradle"&#125; 依赖管理对应最新的构建工具，怎么可能没有依赖管理的功能，并且它是兼容 Maven 的（依赖管理主要还是依赖 Maven，它的定位是构建工具，不是 Maven 这种依赖管理工具），也就是说，可以使用 Maven 仓库里的各种 jar 包依赖同样也分为编译时依赖和运行时依赖等，以前写 Maven 时候也有写过，差不多，然后再给一个 飞机链接下面说下 Gradle 的依赖范围，在 Gradle 里，对依赖的定义有6种，他们分别是 compile, runtime, testCompile, testRuntime, providedCompile和 providedRuntime ；首先是 java 插件的依赖标准下，就是说一下常用的几个： compile在所有的 classpath 中可用，同时它们也会被打包。大部分都是这一类 runtime在运行和测试系统的时候需要，但在编译的时候不需要。比如 JDBC 驱动实现，默认地依赖包括 compile 时的依赖 testCompile依赖在编译测试代码时需要，默认地依赖包括产品 class、已经 compile 时的依赖 testRuntime依赖在运行测试时需要，默认地依赖包括 compile、runtime 以及 testCompile 时的依赖 可以看到 test 依赖和程序代码依赖是分开的，也就是说 test 设置的依赖在程序代码里并不依赖它 ，然后下面再来说说 war 插件使用的依赖范围 providedCompile与 compile 作用类似，但不会被添加到最终的 war 包中 ，这是由于编译、测试阶段代码需要依赖此类 jar 包，而运行阶段容器已经提供了相应的支持，所以无需将这些文件打入到 war 包中了，例如 Servlet API 就是一个很明显的例子 providedRuntime同 providedCompile 类似，它表示一些在编译的时候不需要提供的依赖库，但是在运行的时候需要。它们一般也是已经包含到应用服务器中了。 相关的配置主要就是在下面的两个闭包中，定义仓库的瞬间就是查找的顺序 12345678910111213141516// 定义公共仓库repositories &#123; // 如果是自己的私服 maven&#123; url 'xxxx' &#125; mavenLocal() mavenCentral()&#125;// 定义依赖关系dependencies &#123; // 可以使用 : 进行分割，也可以使用全名 compile 'com.hynnet:logback-classic:1.1.3' testCompile group: 'junit', name: 'junit', version: '4.12'&#125; 处理冲突既然有传递依赖，那么肯定就有依赖冲突问题，Gradle 默认的处理方式是选择版本最高的，关于依赖的处理日志可以看 tasks 下的 help 下的 dependencies，当然是对于 IDEA 来说的 Gradle 提供了两种解决版本冲突的策略：Newest 和 Fail. 默认策略是 Newest，如果配置 Fail 模式，就需要加入下面的代码，这样 Gradle 就不会自动的处理冲突了，你也可以强制指定使用某一版本 1234567891011121314151617181920apply plugin: 'java'configurations.all &#123; resolutionStrategy.failOnVersionConflict() // 如果有冲突,强制依赖asm-all的3.31版本和commons-io的1.4版本 // resolutionStrategy.force 'asm:asm-all:3.3.1'&#125;// 第二种写法，取消自动处理冲突configurations.all &#123; resolutionStrategy&#123; failOnVersionConflict() &#125;&#125;// 强制指定某个版本configurations.all &#123; resolutionStrategy&#123; failOnVersionConflict() force 'asm:asm-all:3.3.1' &#125;&#125; 或者使用排除传递中的依赖的方式，一般只使用一个 exclude 进行排除就行了 12345678910111213apply plugin: 'java'dependencies &#123; compile('org.hibernate:hibernate:3.1') &#123; //如果有冲突，强制使用3.1版本 force = true //排除传递中的依赖 exclude module: 'cglib' //通过 artifact 的名字排除 exclude group: 'org.jmock' //通过 artifact 的 group 名字排除 exclude group: 'org.unwanted', module: 'iAmBuggy' //通过artifact的名字和grop名字排除 //禁止该依赖传递 transitive = false &#125;&#125; 多项目构建或者说多模块，新建模块的方法就不多说了，每个模块都含有 src、build.gradle ….等文件或目录，并且有些情况下各个模块之间是互相引用的，建好后如果需要引用其他模块，在 build.gradle 文件中这样引用 1234dependencies &#123; // 这里是有传递依赖的 compile project(':modeName')&#125; 别忘了最外层的 settings.gradle 文件中引入模块：include &quot;module1&quot;, &quot;module2&quot;, &quot;module3&quot;通常，多个模块之间都有一些通用的配置，比如插件、版本等，这些都可以在根目录下的 build.gradle 文件中进行定义 123456789101112131415161718192021222324252627// subprojects 里面可以写的内容和正常的build.gradle文件写法一样// 为所有子模块应用设置subprojects &#123; apply plugin: 'java' apply plugin: 'eclipse-wtp' repositories &#123; mavenCentral() &#125; dependencies &#123; testCompile 'junit:junit:4.11' &#125; version = '1.0' jar &#123; manifest.attributes provider: 'gradle' &#125;&#125;// 写 allprojects 貌似也可以// 比如：所有模块都采用统一的版本号以及 groupNameallprojects &#123; group = 'com.bfchengnuo.test' version = "0.1.0"&#125; 其中几个闭包的作用是： 方法名 描述 allprojects 配置当前模块以及所有子模块行为 subprojects 配置所有子模块行为 project 配置指定子模块行为 其他常用的还有自动测试，稍微提一下是如果写了相关的测试代码，在执行 build 任务的时候是先处理源码，然后处理测试代码，如果测试代码通过则再进行打包或者XXX，如果测试不通过会直接报错，测试相关的日志可以去里看，有 html 格式的，看起来还不错，根据使用的测试框架（JUnit/ TestNG）也会有些差别 类被认为是一个JUnit测试类： 类或父类集成自TestCase或GroovyTestCase 类或父类有@RunWith注解 类或者父类中的方法有@Test注解 再来说说发布，也许会用到要发布到私服或者本地仓库，那么可以使用 maven-publish 这个插件，当然必须是 Maven 仓库 下面是通过 ”maven-publish” 插件来支持发布到 Maven 功能。最终这种新的发布方式会替换掉通过 Upload task 的发布方式 12345678910111213141516171819apply plugin: 'maven-publish'publishing &#123; publications &#123; // mavenJava 可以自定义，可以设置多个，会同步到到 tasks 中去 mavenJava(MavenPublication) &#123; // 若是war包，就写 components.web from components.java &#125; &#125;&#125;publishing &#123; repositories &#123; maven &#123; name 'test' // 好像可以省略 // change to point to your repo, e.g. http://my.org/repo url "$buildDir/repo" &#125; &#125;&#125; 然后找到相关的任务执行就可以了，上面的两个可以写在一个 publishing 中；关于 maven-publish 这个插件就不多说了，想进一步了解的可以去：http://blog.csdn.net/a38017032/article/details/65935573 参考/更多https://dongchuan.gitbooks.io/gradle-user-guide-/build_script_basics/projects_and_tasks.htmlhttp://www.zhaiqianfeng.com/2017/03/love-of-gradle-dependencies-1.htmlhttps://michaelliuyang.github.io/projectsupport/2014/11/04/gradle-multi-project.html一个多项目管理实例Gradle User Guide 中文版Gradle 实战–单元测试]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>构建工具</tag>
        <tag>Gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用JUnit进行测试]]></title>
    <url>%2F2017%2F08%2F07%2F%E4%BD%BF%E7%94%A8JUnit%E8%BF%9B%E8%A1%8C%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[JUnit 是一个 Java 语言的单元测试框架。它由肯特·贝克和埃里希·伽玛（Erich Gamma）建立，逐渐成为源于 Kent Beck 的 sUnit 的 xUnit 家族中为最成功的一个。 JUnit 有它自己的 JUnit 扩展生态圈。 JUnit 是一个回归测试框架，被开发者用于实施对应用程序的单元测试，加快程序编制速度，同时提高编码的质量。JUnit 提供： 断言测试预期结果。 测试功能共享通用的测试数据。 测试套件轻松地组织和运行测试。 图形和文本测试运行。 JUnit用于测试： 整个对象 对象的一部分 - 交互的方法或一些方法 几个对象之间的互动(交互) 我们常用的它的两个特点应该就是注解和断言了吧. 一般来讲，源代码中包含明确的输入和输出的每一个方法被认为是一个可测试的单元 一般来说，有两种类型的测试，一是所谓的黑盒测试（black-box testing），另一种 便是所谓的白盒测试（white-box testing），也有称为透明盒（clear-box/glass-box）或结构（structural）测试的。 在黑盒测试中，我们的测试只针对程序的功能说明，而不管程序的具体实现，即不看程序代码。黑盒测试的最大好处是其测试过程不受实现的 潜在影响。如编码者可能对输入数据的一些不合理的假设，例如，输入参数是整数， 编码者可能假设只有正整数，而对0和负数则没作处理。如果读了代码后再去产生输入数据，很可能会受到这种错误假设的误导。黑盒测试的第二个好处是其测试数据与实现 无关，实现如有改变，只要功能说明不变，测试就保持不变。 与黑盒测试相反，白盒测试则需要审阅程序代码。其主要的目的是要产生一些输入 数据，能够测试代码的每个语句（statement coverage）、分支（decision coverage），使得每条路径都能走到，即所谓的”路径完全（path-complete）”。 每次修改程序代码后，所有的测试都需要重新运行，这通常称为回归测试（regression testing）。 版本区别主要是 JUnit3 和 JUnit4 的使用区别，目前使用的也就这两个版本，多数还是 JUnit4，虽然现在已经有 JUnit5 了，这个…..以后再补充 在 JUnit3 中需要继承 TestCase 类，但在 JUnit4 中已经不需要继承任何类 在 JUnit3 中需要覆盖 TestCase 中的 setUp 和 tearDown 方法，其中 setUp 方法会在测试执行前被调用以完成初始化工作，而 tearDown 方法则在结束测试结果时被调用，用于释放测试使用中的资源，而在 JUnit4 中，只需要在方法前加上 @Before，就代表这个方法用于初始化操作，方法名是随意的 在 JUnit3 中对某个方法进行测试时，测试方法的命令是固定的，例如对 addBook 这个方法进行测试，需要编写名字为 tetAddBook 的测试方法，而在 JUnit4 中没有方法命令的约束，例如对 addBook 这个方法进行测试，那么可以编写 addBookToLibrary 的方法，然后在这个方法的前面加上 @Test,这就代表这个方法是测试用例中的测试方法 编写 JUnit4 的测试用例和编写一个普通的类没有什么区别，只是需要加上 Annotation 指定要测试的方法，这种松偶合的设计理念相当优秀，能很好把测试分离出来.使用 JUnit4 的 Annotation 功能，需要 JDK 1.5 或以上版本 基本使用现在基本都是4版本了，所以我也是使用的 JUnit4，一般的习惯是新建一个 test 文件夹保存测试代码，和 src 源代码目录得以区分，最后发布的时候也好删除；要测试那个类写的测试类后面加 Test，比如 MainTest 是 Main 类的测试类，测试类中的方法也是对应的，比如测试类中的 testAdd 方法就是对原始类中 add 方法的测试方法 测试方法一般是使用 public void 来进行修饰的，并且不需要任何参数，各个方法直接也没有关联，稍微想一下就能理解这样设计是很合理的进行测试的时候可以一起执行也可以单个方法执行，只要你加了 @Test 注解 测试 web 项目也是差不多，只是在 @BeforeClass 的方法中初始化一些对象方便下面测试方法的使用，比如初始化 Spring、Hibernate 等 常用注解 @Test:将一个普通的方法修饰成为一个测试方法测试方法我认为最主要的特点是不是 main 也可以执行 @Test(expected=XXX(异常类).class)：确定会抛出该异常，设定后出现该异常也是绿条通过 @Test(timeout=毫秒 )：设置执行的时间，用于结束死循环或是性能测试 @Ignore:所修饰的测试方法会被测试运行器忽略 @RunWith:可以更改测试运行器默认为 org.junit.runner.Runner @BeforeClass:他会在所有的方法运行前被执行，并且只会执行一次，static 修饰，所以是单实例适合配置文件的加载或者初始化 @AfterClass:他会在所有方法运行结束后被执行，只执行一次，static 修饰;一般用于释放资源、关闭连接等 @Before：会在每一个测试方法被运行前执行一次 @After：会在每一个测试方法被运行后执行一次 测试套件测试套件简单说就是把一堆测试类整合起来，一起执行，主要使用了 @RunWith 注解更改默认的测试运行器，并且作为入口类，是不允许这个类中有其他方法的，具体的使用方法在下面： 12345@RunWith(Suite.class) //将测试类改为测试套件类@Suite.Suite.class(&#123;TaskTest1,TaskTest2...&#125;) //用数组的形式将测试的类添加到测试套件中public classSuiteTest&#123; //要用public修饰，套件测试类要为空。不能有方法。&#125; 然后可以进行参数化设置，也就是批量测试，测试一堆数据，一般放在数组或者集合里，使用前也需要使用 @RunWith 注解更改默认的测试运行器 1234567891011121314151617181920212223242526272829@RunWith(Parameterized.class)public class ParameterTest &#123; // 声明变量用来存预期值和结果值 int expected = 0; int input1 = 0; int input2 = 0; // 声明一个返回值为 Collection 的公共静态方法，并使用 @Parameters 进行修饰 @Parameterized.Parameters public static Collection&lt;Object[]&gt; t()&#123; return Arrays.asList(new Object[][]&#123; &#123;3,2,1&#125;, &#123;4,2,2&#125; &#125;); &#125; // 设置构造函数，作用就是进行循环赋值了，注意顺序和构造的数组顺序是一致的 public ParameterTest(int expected, int input1, int input2) &#123; this.expected = expected; this.input1 = input1; this.input2 = input2; &#125; //测试方法 @Test public void testAdd() &#123; assertEquals(expected,new MainTest().add(input1,input2)); &#125;&#125; 这样 testAdd 会循环执行两次，每次的参数就是数组设置的那个 使用断言Java 中的断言就不说了，用的很少，也是比较简单的，感兴趣的可以看看：http://lavasoft.blog.51cto.com/62575/43735/ JUnit 中的所有的断言都包含在 Assert 类中，这个类提供了很多有用的断言方法来编写测试用例，一些常用的断言看下面的表格，当然最全的还是去看官方的 API 描述 方法 检查两个变量或者等式是否平衡 void assertEquals(boolean expected, boolean actual) 检查条件为真 void assertTrue(boolean expected, boolean actual) 检查条件为假 void assertFalse(boolean condition) 检查对象不为空 void assertNotNull(Object object) 检查对象为空 void assertNull(Object object) 检查两个相关对象是否指向同一个对象 void assertSame(boolean condition) 检查两个相关对象是否不指向同一个对象 void assertNotSame(boolean condition) 检查两个数组是否相等 void assertArrayEquals(expectedArray, resultArray) 扩展如果 JUnit 单元测试满足不了，可以试试基于此的自动化测试 TestNG，它可以配合 extentreports 来进行生成测试报告，使用 API 上跟 JUnit 倒是挺像，没什么学习难度。当然了，JUnit 也可以使用 extentreports 来生成测试报告，其他的例如使用 ant 也可以；PS：单元测试、回归测试、冒烟测试、自动化测试、黑盒测试、白盒测试。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建工具之Maven]]></title>
    <url>%2F2017%2F07%2F27%2F%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7%E4%B9%8BMaven%2F</url>
    <content type="text"><![CDATA[Maven 是一个强大的 Java 项目构建工具。 对于其他语言也是可以的，但是主要是 Java，是由 Apache 开发的。主要作用就是：项目的构建和依赖管理项目构建体现在：在项目编码完成后，要对项目进行编译、测试、打包、部署；这些工作 Maven 都可以做到依赖管理它的好处体现于：不需要再导入 jar 包，大大降低了项目文件的占用大小（它会根据配置的“坐标”去 Maven 仓库里去寻找对应的 jar 包） 安装Maven 是用 Java 开发的，所以它的运行需要 JDK 支持，和 Java（JAVA_HOME） 类似，只需要把解压后的目录放进环境变量就行了，名称可以叫 MAVEN_HOME，然后再配进 path 里然后在 CMD 命令行中输入 mvn -v 测试下是否正常 配置本地仓库Maven 仓库存的是各种各样的 jar 包，可以在本地搭建也可以在局域网中搭建，或者直接用互联网上的中央仓库配置本地仓库去修改 Maven 目录下的 conf 目录下的 settings.xml 文件，将 localRepository 标签配好即可 123&lt;settings&gt; &lt;localRepository&gt;D:\maven_new_repository&lt;/localRepository&gt;&lt;/settings&gt; 如果是 Eclipse 然后可以找到 Maven 的视图生成下索引，这样有利于 jar 包的快速获取 新建一个Maven项目首先选择 Create a simple project (skip archetype selection) 简单来说它的作用是给你创建好相应的文件目录，也就是骨架，如果不选择，你要手动选择一个模型，然后说下新建项目中需要填写的几个： Group Id针对一个项目的普遍唯一识别符。是项目组织唯一的标识符，实际对应 JAVA 的包的结构，相当于我们日常使用的包名，例如：com.bfchengnuo Artifact Id要新建的项目的名字，就是项目的唯一的标识符，实际对应项目的名称，就是项目根目录的名称。 Version版本号，默认 0.0.1-SNAPSHOT ，也就是测试版本 Packing要将该项目生成什么类型，有 jar、war、ejb、rar、pom（父工程）、eclipse-plugin 等等 Name名字(估计是模板的名字) Description说明 Maven项目目录结构和一般的项目也没啥区别，都有 src 源码文件夹，只是多了一个 pom.xml 文件，这个是 Maven 项目的核心配置文件，里面可以配置 JDK 的版本、jar 包的相关信息。另外，Maven 下的 src 文件夹是有一定结构的，里面分为两个文件夹，main 和 test，从名字可以看出一个是主要，一个是测试；main 里面继续分，分为 java、resources(项目所需的配置文件) 和 webapp 文件夹；没有 lib 了，因为没有 jar 包什么事了 |-ProjectName|– pom.xml|– src|–|– main|–|–|– java|–|–|– resources|–|–|– [ webapp/WEB-INF/web.xml ]||–|– test|–|–|– java|–|–|– resources 最终 java 、resource 目录下的文件默认还是会发布在一起，和原来的结构类似，所以 classpath 后可以直接引用 resource 下的文件（貌似是可以手动设置 classpath 的位置的，和 Gradle 应该类似，待研究） Maven常用命令 mvn clean 清理编译后的文件，也就是 targen 目录 mvn compile 编译；编译后的文件在 targen 目录下 mvn test 进行测试；运行测试目录下类的所有测试方法注意类名的规范：xxxTest.java mvn package 打包；默认会先编译再进行测试，最后执行打包，如果是 web 项目会自动打成一个 war 包，放在 targen 目录下 mvn install 安装；把 jar 包安装到本地仓库中去，便于其他项目使用过程还是先编译再测试，然后打包，最后再安装。也可以说是打包到本地仓库 mvn tomcat:run 部署到 Tomcat （通过 war 包），并且启动 Maven项目生命周期在 Maven 中存在三套生命周期，它们相互独立，互不影响（不同周期里的命令不会互相调用），在一套生命周期中，执行后面的命令，前面的命令会自动执行 CleanLifeCycle：清理生命周期包含命令：clean DefaultLifeCycle：默认生命周期包含命令：compile –&gt; test –&gt; package –&gt;install –&gt;deploy（上传到私服） siteLifeCycle：站点生命周期包含命令：site POM常见配置比如我们需要设置 JDK 的版本： 12345678910111213141516171819202122232425&lt;build&gt; &lt;!-- 名字可以省略的 --&gt; &lt;finalName&gt;myweb&lt;/finalName&gt; &lt;plugins&gt; &lt;!--JDK版本 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;showWarnings&gt;true&lt;/showWarnings&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;&lt;!-- 指定 JDK 版本的简便方式，编译插件会自动读取这两个值 --&gt;&lt;properties&gt; &lt;!-- 指定 JDK 的版本 --&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;&lt;/properties&gt; 其实就是添加了一个插件，通常所有的项目都要定义一个共同的父工程，这个父工程没有代码仅仅是为了统一版本号和 jar 等问题，所以最终要的是 pom.xml 文件，还有一个空壳的 src 文件夹，那么就来看看常用的一些配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.itcast.parent&lt;/groupId&gt; &lt;artifactId&gt;itcast-parent&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;!-- 集中定义依赖版本号 --&gt; &lt;properties&gt; &lt;junit.version&gt;4.10&lt;/junit.version&gt; &lt;spring.version&gt;4.1.3.RELEASE&lt;/spring.version&gt; &lt;mybatis.version&gt;3.2.8&lt;/mybatis.version&gt; &lt;/properties&gt; &lt;!-- 设置deploy地址 --&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus&lt;/name&gt; &lt;url&gt;http://maven.taotao.com:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus&lt;/name&gt; &lt;url&gt;http://maven.taotao.com:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 单元测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;$&#123;druid.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 省略.... --&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;plugins&gt; &lt;!-- 资源文件拷贝插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- java编译插件省略 --&gt; &lt;/plugins&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- 配置Tomcat插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;prot&gt;80&lt;/prot&gt; &lt;!-- 这样就不需要输 webAPP 的 name 了 --&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt;&lt;/project&gt; 最重要的是 dependencyManagement 标签，它起到了统一管理依赖版本号的作用，使用这个标签后在子项目也必须配置 dependency 标签后相关的 jar 才会被导入到项目中，但是不用指定其版本，如果不加 dependencyManagement 在父工程定义的依赖都会自动导入到子工程中…..子工程继承父工程非常简单，只需要加入： 12345&lt;parent&gt; &lt;groupId&gt;cn.itcast.parent&lt;/groupId&gt; &lt;artifactId&gt;itcast-parent&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt; 这样就相当于是继承了~解决了指定版本号的问题；另外 Maven 会自动处理 jar 的依赖，也就是说如果此 jar 依赖其他 jar ，会自动把所需的 jar 导入，不用再配。 依赖范围依赖范围可分为：编译依赖范围、测试依赖范围、已提供依赖范围、运行依赖范围、系统依赖范围、导入依赖范围；默认的依赖范围是 compile，手动指定就是在 dependency 中加一个 scope 标签，例如： &lt;scope&gt;test&lt;/scope&gt; 依赖范围 对于编译有效 对于测试有效 对于运行有效 栗子 compile √ √ √ spring-core test - √ - junit provided √ √ - servlet-api runtime - √ √ JDBC 驱动 system √ √ - Maven仓库之外的 import - - - - 系统依赖范围通过 systemPath 显式指定；再说导入依赖范围，它不会对 classpath 产生影响。servlet 相关的 API 一定要设置为 provided，否则会和 web 服务器中的 jar 冲突。 依赖范围除了控制 classpath，还会对依赖传递产生影响。如果 A 依赖 B，B 依赖 C，则 A 对于 B 是第一直接依赖。B 对于 C 是第二直接依赖。 A 对于 C 是传递性依赖。结论是：第一直接依赖的范围和第二直接依赖的范围决定了传递性依赖的范围。 第一依赖\第二依赖 compile test provided runtime compile compile - - runtime test test - - test provided provided - provided provided runtime runtime - - runtime 第一列是第一直接依赖，第一行是第二直接依赖，中间表示传递性依赖范围。可以理解为：第一行为 B 和 C 的范围；第一列为 A 和 B 的范围；中间围起来的区域就是 A 和 C 的范围 传递依赖冲突根据上面的传递依赖，比如项目同时依赖 A 和 B，A 又依赖于 C 并且版本是 1.0；B 也依赖于 C ，但是版本是 2.0；如果都导入肯定会发生冲突Maven 会自动处理一些冲突，其遵循两个原则： 第一声明者优先就是谁先定义的就用谁的 路径近者优先例如，项目中引入了 B 依赖，B 又间接引入了 C 依赖；如果后面单独定义了一个 C 的依赖，那么就会按照这个独立的依赖（第一依赖）的版本来 或者可以手动排除掉其中一个，这样就相当于只有一个，具体在 pom.xml 文件中配置，使用 exclusion 标签，比如： 1234567891011&lt;dependency&gt; &lt;groupId&gt;com.alibaba.china.shared&lt;/groupId&gt; &lt;artifactId&gt;alibaba.apollo.webx&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;org.slf4j.slf4j-api&lt;/artifactId&gt; &lt;groupId&gt;com.alibaba.external&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 还可以进行版本锁定这样就能确保只使用某一个版本：使用的是 dependencyMangedent 标签；它只起一个锁定的作用，不会进行导入包操作【推荐使用】 1234567891011121314&lt;properties&gt; &lt;javaee-api.version&gt;2.1&lt;/javaee-api.version&gt; &lt;junit.version&gt;4.7&lt;/junit.version&gt;&lt;/properties&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;version&gt;$&#123;javaee-api.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 一般情况下 dependencyManagement 都会定义在父工程下的，子工程引用时就不需要指定版本了。 注意：在 pom.xml 文件中可以定义属性，并且可以使用 OGNL 表达式，详细的栗子不表 概念模型用一幅图片来表示就是： 上面的部分是处理依赖关系，下面的就是项目构建了…可以看出项目构建依赖的是我们配置的插件 项目拆分Maven 两个重要的特性就是继承和聚合（它们的类型都是 pom 工程），对于传统的 SSH 架构，可以对每一层进行拆分，然后还需要一个父工程来进行统一，例如可以把 Dao 层和相关配置文件拆成一个工程，这样可以进一步降低耦合度。要知道 Spring 配置文件可以拆的，所以就好办了，可以把 dao 层相关的配置放到 spring-dao.xml 中，最后在 web.xml 配置中使用通配符配下就行了（classpath*:spring-*.xml）。如果拆成多个子工程，子工程之间有依赖，那么记得先安装到本地仓库（install）然后加入其依赖。 因为依赖的传递性，所以 web 层引用 service 层的 jar 后，就不需要再单独加 dao 层的依赖了。并且所有的工程一般都会有一个顶级父工程，它并不含有代码，最大的作用就是进行版本的统一了吧，按照上面的拆分，它就有了三个孩子，对应那三层（Dao 和 Service 打成 jar 包）。 关于父工程创建父工程和创建普通的 Maven 项目一样，就是选择打包的时候选择 pom父工程不进行编码，只有一个 pom.xml 文件既然不进行编码，那么它的作用是什么呢？ 定义所需要的依赖信息，子模块可以直接继承使用 将各个子模块聚合到一起 一般都是把父工程安装到版本库中去的，就是前面的 install 命令，因为最终项目运行的时候第一步会去本地仓库找依赖，如果父工程不发布子工程也没法玩（找不到“依赖”），POM 文件中一般都会用上面说的解决冲突中的版本锁定方式来锁定版本。 注意：创建子工程的时候就要选择 Module 了，新建模块！ 对于 service 和 dao 层，打成 jar 包就可以了，为了避免报错在 service 中引用 dao 层的依赖就行了，在 pom.xml 文件里配置，和一般的加依赖一样。执行的 install 命令其实就是打包到本地仓库啦~对于 web 层，打包选择打 war 包；部署测试的时候，记得先把父工程给安装（install ）到本地仓库，要不就提示 “404” 错误 ；第一次运行会下载一些东西，后面就不会了 根据上面所说，父工程只有一个，但是很多情况下并不能满足需求，例如目前最常用的 SB、SC 系列需要使用父工程做版本仲裁，但是我们自己的工程也需要一个父工程，那么就可以这样解决，在我们自己的父工程中： 12345678910111213141516171819&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Dalston.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 简单说就是通过 dependencyManagement 来导入其他的父工程。 运行运行调试的时候可以直接运行父工程，也可以运行 web 层的子工程，还可以传统方式部署到 Tomcat 运行Maven 项目的运行在右键菜单有两个选项，build 和 build… ，它们的区别是第一个是执行历史运行记录；第二个是手动输入指令运行，一般在 Goals 输入 tomcat:run 即可。或者在 RunConfigurations 设置里直接 new 一个 Maven build 进行配置，比如最新的 tomcat7：tomcat7:run当进行 debug 调试的时候，需要在 source 选项卡下关联本项目才行，否则断点进不了（Eclipse），所以一般还是部署到 Tomcat 调试比较省事，虽然 Maven 提供 build 方式 properties(Maven属性)通过 &lt;properties&gt; 元素用户可以自定义一个或多个 Maven 属性，然后在 POM 的其他地方使用 ${属性名} 的方式引用该属性，这种做法的最大意义在于消除重复和统一管理。Maven 总共有 6 类属性：内置属性、POM属性、自定义属性、Settings属性、java系统属性和环境变量属性 内置属性两个常用内置属性 ${basedir} 表示项目跟目录，即包含 pom.xml 文件的目录；${version} 表示项目版本 POM属性用户可以使用该类属性引用 POM 文件中对应元素的值。如 ${project.artifactId} 就对应了 &lt;project&gt; 中 &lt;artifactId&gt; 元素的值； 1234567891011121314151617$&#123;project.build.sourceDirectory&#125;:项目的主源码目录，默认为 src/main/java/$&#123;project.build.testSourceDirectory&#125;:项目的测试源码目录，默认为src/test/java/$&#123;project.build.directory&#125; ： 项目构建输出目录，默认为target/$&#123;project.outputDirectory&#125; : 项目主代码编译输出目录，默认为target/classes/$&#123;project.testOutputDirectory&#125;：项目测试主代码输出目录，默认为target/testclasses/$&#123;project.groupId&#125;：项目的groupId$&#123;project.artifactId&#125;：项目的artifactId$&#123;project.version&#125;：项目的version,与$&#123;version&#125; 等价$&#123;project.build.finalName&#125;：项目打包输出文件的名称，默认为$&#123;project.artifactId&#125;-$&#123;project.version&#125; 自定义属性上面版本锁定的时候用过了，就是 properties 标签里定义的那些；子模块继承后也是可以继续使用的。 Settings属性与 POM 属性同理，用户使用以 settings. 开头的属性引用 settings.xml 文件中的 XML 元素的值 Java系统属性所有 java 系统属性都可以用 Maven 属性引用，如 ${user.home} 指向了用户目录 环境变量属性所有环境变量属性都可以使用以 env. 开头的 Maven 属性引用，如 ${env.JAVA_HOME} 指代了 JAVA_HOME 环境变量的的值 配置默认JDK版本在 Maven 的配置文件中加入： 123456789101112&lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt;&lt;/profile&gt; 这样在创建项目的时候就是指定的版本了，默认好像是 1.5 来 私服搭建关于局域网（私服）的搭建工具可以使用 nexus ；谷歌搜一下就有了。nexus install/start 可以安装、启动服务（nexus.properties 中可更改端口）启动后可以通过浏览器来访问管理界面，默认账户、密码为：admin / admin123；仓库类型分为四种，一种已经废弃了，就不说了 hosted 宿主仓库一般存放本公司开发的 jar 包，包括正式版、测试版、第三方版等 proxy 代理仓库中央仓库（主仓库）以及 Apache 下的测试版本 jar 包 group 组仓库组嘛，可以包含其他的仓库，比如上面的两个 上传命令：mvn deploy ;当然在上传的前面需要先配置下 pom.xml 文件确定上传路径，还有在 Maven 客户端的配置文件中 写入认证信息： 123456789101112&lt;!--pom 文件！将ssm_dao上传私服 --&gt;&lt;distributionManagement&gt; &lt;!--pom.xml这里 &lt;id&gt; 和 settings.xml 配置 &lt;id&gt; 对应 --&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 会根据项目设置的版本上传到对应的仓库；如果版本为release则上传到私服的release仓库，如果版本为snapshot则上传到私服的snapshot仓库。私服设置好后需要在本地 Maven 客户端中（settings.xml）配置，让其能连接到私服： 123456789101112131415161718192021222324252627&lt;mirrors&gt; &lt;!--配置仓库镜像--&gt; &lt;mirror&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://127.0.0.1:8081/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt;&lt;servers&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;thirdparty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 然后这样 jar 包就会优先从私服寻找，找不到的时候再去互联网仓库；&lt;server&gt; 节点配置服务的账户密码，用于发布构件时进行身份和权限的认证。下载构件这里使用的是 mirrors ，还有其他方案，但是推荐这种，算是简单的吧，mirror 相当于一个拦截器，它会拦截 maven 对 remote repository （包括私服和中央仓库）的相关请求，把请求里的 remote repository 地址，重定向到 mirror 里配置的地址。这样一来，Maven 在找不到的情况下也不会访问中央仓库了，那本是 nexus 该做的事情。最好也配置下 profiles： 123456789101112131415161718192021222324252627282930313233343536373839&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;crop-nexus&lt;/name&gt; &lt;url&gt;http://你的NexusIP:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;!-- true表示开启仓库发布版本下载，false表示禁止 --&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;!-- true表示开启仓库快照版本下载，false表示禁止 --&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt; http://你的NexusIP:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;!-- 禁止快照版本，防止不稳定的插件影响项目构建 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt;&lt;/profiles&gt;&lt;!-- 激活nexus私服 --&gt;&lt;activeProfiles&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 然后来解释下上面出现的几个标签的意思： Profiles 作用：根据环境参数来调整构建配置的列表。 settings.xml 中的 profile 元素是 pom.xml 中 profile 元素的裁剪版本。 它包含了 id、activation、repositories、pluginRepositories 和 properties 元素。 如果一个 settings.xml 中的 profile 被激活，它的值会覆盖任何其它定义在 pom.xml 中带有相同 id 的 profile。 Repositories 作用：远程仓库列表，它是 maven 用来填充构建系统本地仓库所使用的一组远程仓库。 pluginRepositories 作用：发现插件的远程仓库列表。 和 repository 类似，只是 repository 是管理 jar 包依赖的仓库，pluginRepositories 则是管理插件的仓库。 maven 插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories 元素的结构和 repositories 元素的结构类似。每个 pluginRepository 元素指定一个 Maven 可以用来寻找新插件的远程地址。 ActiveProfiles 作用：手动激活 profiles 的列表，按照 profile 被应用的顺序定义 activeProfile。 最后来补充个 repositories 的详细配置： 12345678910111213141516171819202122232425262728&lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!--远程仓库唯一标识 --&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;!--远程仓库名称 --&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;!--如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的选项是：always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。 --&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!--当Maven验证构件校验文件失败时该怎么做-ignore（忽略），fail（失败），或者warn（警告）。 --&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!--如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素 --&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;!--远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;!--用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。Maven 2为其仓库提供了一个默认的布局；然而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt;&lt;/repositories&gt; 关于 Maven 的其他配置就自行搜索吧 使用阿里云镜像当你没有私服，中央仓库由于一些原因特别的慢，不能浪费青春，可以换用阿里的镜像，能大幅提高下载速度，只需要加入： 123456789&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;!-- 被镜像的服务器的id，这里选择中央仓库相同的 id --&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 这样就相当于“替换”了中央仓库，速度是不是明显加快了？ 参考木杉http://www.cnblogs.com/youzhibing/p/5427130.html]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>构建工具</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring学习笔记]]></title>
    <url>%2F2017%2F07%2F21%2FSpring%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Spring Framework 是一个开源的 Java/Java EE 全功能栈（full-stack）的应用程序框架，以 Apache 许可证形式发布，也有 .NET 平台上的移植版本。关键字：轻量级、非侵入式、一站式、模块化，其目的是用于简化企业级应用程序开发。可以说 Spring 是一个容器框架，用于配置 bean、处理和维护 bean 之间的关系的框架注意：Spring 中的 bean 概念可以是 java 中的任何一种对象，比如可以是 javabean、service、action、数据源、dao、ioc（控制反转）、di（依赖注入）等可以说 Spring 贯穿各个层，上至 web 下至持久层，很厉害… 快速入门一般来说，当 Spring 加载的时候，会自动创建配置的 bean 对象（当作用域是 singleton 的时候），并加载到内存中去；使用 property 标签来注入对象（属性）最简单的栗子，使用 Spring 获取一个对象，不需要再 new 了，首先定义一个简单的 bean，这里不多说就写一个 12345678910111213141516171819202122232425public class User &#123; private String name; private int age; @Override public String toString() &#123; return "Name --&gt;" + name + "Age --&gt;" + age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 然后是在 xml 进行配置，文件名一般写 applicationContext 放在 src 目录下，如果 bean 对象引用了其他对象，可以使用 property 的 ref 属性设置 123456789101112131415&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="user" class="com.bfchengnuo.domain.User"&gt; &lt;property name="name" value="佳芷"/&gt; &lt;property name="age" value="12"/&gt; &lt;/bean&gt; &lt;bean id="loli" class="com.bfchengnuo.domain.Loli"&gt; &lt;property name="desc" value="lovely"/&gt; &lt;property name="user" ref="user"/&gt; &lt;/bean&gt;&lt;/beans&gt; 进行测试下，在启动时 Spring 就给你创建好对象了，并且是单例的，默认情况下；最好是使用接口，这样以后就不需要改动代码了，只改配置文件即可 12345// 解析配置文件并且创建其中的 bean 到内存中，非常耗费资源，一般写成工具类 单例模式ApplicationContext ac = new ClassPathXmlApplicationContext("applicationContext.xml");Loli loli = (Loli) ac.getBean("loli");System.out.println(loli); bean的作用域相关配置 bean 的时候有一个属性是 scope ，用来配置作用域，有五种可选值 singleton在 IOC 容器中，每一个 bean 定义对应一个对象实例，也就是单例 【默认】 prototype一个 bean 定义可以对应多个实例，并且在获取（getBean）的时候才会被实例化 requestrequest 表示该针对每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP request 内有效 sessionsession 作用域表示该针对每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP session 内有效 global session类似于标准的 HTTP Session 作用域，不过它仅仅在基于 portlet 的 web 应用中才有意义。Portlet 规范定义了全局 Session 的概念，它被所有构成某个 portlet web 应用的各种不同的 portlet 所共享。在 global session 作用域中定义的 bean 被限定于全局 portlet Session 的生命周期范围内。如果你在 web 中使用 global session 作用域来标识 bean，那么 web 会自动当成 session 类型来使用。 然后获取 bean 的方法上面提到了一种，就是使用 ac 的 getBean 方法，除了这种还可以通过工厂的方式获得，但是不推荐使用，因为通过工厂获取会进行懒加载，也就是说当 get 的时候才会创建相应的实例 123BeanFactory bf = new ClassPathXmlApplicationContext("applicationContext.xml");User user = (User) bf.getBean("user");System.out.println(user); bean的生命周期在默认情况下，也就是当作用域是 singleton 的时候，bean 对象是单例的，这种情况生命周期相对复杂一点，下面的顺序 首先是实例化，当配置文件加载的时候，默认调用无参的构造函数 根据配置文件中的 property 进行属性注入，必须要有相应的 set 方法 如果 bean 对象实现了 BeanNameAware 接口，会调用其 setBeanName 方法，这个方法必须复写，通过这个方法可以获得配置的 id 名称和前面的 struts2 有点相似，实现这个接口可以获得配置的 id 名称 如果 bean 对象实现了 BeanFactoryAware 接口，会调用其 setBeanFactory 方法，通过这个方法可以获得 bean 工厂 如果 bean 对象实现了 ApplicationContextAware 接口，会调用其 setApplicationContext 方法，通过这个方法可以获得上下文对象 调用 BeanPostProcessor (后置处理器)的 postProcessBeforeInitialization 方法，如果配置了处理器的话后置处理器类似过滤器，每个 bean 实例化时（前、后）都要调用此处理器，有点像拦截器设置处理器需要建一个类实现 BeanPostProcessor 接口即可，再配置到 xml 文件中（一切皆是 bean） 如果 bean 对象实现了 InitializingBean 接口，会调用其 afterPropertiesSet 方法 如果在 xml 配置文件中的 bean 标签定义了 init-method 属性，那么会在这时调用其指定的方法使用这种方式耦合性比较低 调用 BeanPostProcessor (后置处理器)的 postProcessAfterInitialization 方法 然后就可以使用 bean 了！！ 容器关闭…. 如果 bean 对象实现了 DisposableBean 接口，这时会调用其 destroy 方法 如果在 xml 配置文件中的 bean 标签定义了 destroy-method 属性，那么会在这时调用其指定的方法使用这种方式耦合性比较低 如果不是用上下文获取的 bean，而是采用的工厂方法，那么就简单一些了，上面的5、6、9 就没有了然后下面是两幅图，第一张比较详细，第二张比较简洁 使用注解指定在配置文件中加入：&lt;context:annotation-config/&gt; 来启用注解比如第八点自定义的 init 方法可以使用 @PostConstruct 注解来指定，不需要在 xml 文件中配置了destroy 方法可以通过 @PreDestroy 注解来指定更多注解的使用后面再说 装载bean普通属性就不说了，主要是对于数组、集合的装填，首先看下配置的 xml 文件，后面用注解可能会更轻松 1234567891011121314151617181920212223242526272829303132333435&lt;bean id="collectionTest" class="com.bfchengnuo.domain.CollectionTest"&gt; &lt;!--装载数组数据--&gt; &lt;property name="strs"&gt; &lt;list&gt; &lt;value&gt;str1&lt;/value&gt; &lt;value&gt;str2&lt;/value&gt; &lt;value&gt;str3&lt;/value&gt; &lt;value&gt;str4&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--装载 List 数据，根据特性可以重复引用，并且是有顺序的--&gt; &lt;property name="users"&gt; &lt;list&gt; &lt;ref bean="user"/&gt; &lt;ref bean="user2"/&gt; &lt;ref bean="user"/&gt; &lt;ref bean="user2"/&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--装载 Map 集合数据，根据特性不能有重复，后面会覆盖前面，当然只是 key 不同就行，无序--&gt; &lt;property name="map"&gt; &lt;map&gt; &lt;entry key="user" value-ref="user"/&gt; &lt;entry key="user2" value-ref="user2"/&gt; &lt;entry key="test" value-ref="user2"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!--装填 set 集合--&gt; &lt;property name="userSet"&gt; &lt;set&gt; &lt;ref bean="user"/&gt; &lt;ref bean="user2"/&gt; &lt;/set&gt; &lt;/property&gt;&lt;/bean&gt; 然后可以跑一下，测试看看 12345678910111213141516171819202122232425262728private static void collectionTest() &#123; ApplicationContext ac = ApplicationContextUtil.getAc(); CollectionTest collectionTest = (CollectionTest) ac.getBean("collectionTest"); // 遍历数组 System.out.println("-----------数组遍历-----------"); for (String s : collectionTest.getStrs()) &#123; System.out.println(s); &#125; // 遍历 List System.out.println("-----------List遍历-----------"); for (User user : collectionTest.getUsers()) &#123; System.out.println(user.getName() + "::" + user.getAge()); &#125; // 遍历 Map System.out.println("-----------Map遍历-----------"); for (Map.Entry&lt;String, User&gt; entry : collectionTest.getMap().entrySet()) &#123; System.out.println(entry.getKey() + "::" + entry.getValue().getName() + "-" + entry.getValue().getAge()); &#125; // 遍历 Set System.out.println("-----------Set遍历-----------"); for (User user : collectionTest.getUserSet()) &#123; System.out.println(user.getName() + "::" + user.getAge()); &#125;&#125; property 标签中还可以继续再套一个 bean 标签，这样就相当于内部类，在其他地方就不能通过 ref 进行引用了 如果有继承关系可以使用 bean 的 parent 属性指定父 bean，配置的相关属性会被继承过来，当然你可以进行覆盖 如果需要设置值为空，可以使用 &lt;null/&gt; 标签 自动装配使用 bean 标签中的 autowire 属性为一个 bean 定义指定自动装配模式，也就是说指定了自动装配就不需要注入对象了，它会自动根据设置去 IoC 容器中寻找合适的 bean 然后自动注入。但是并不推荐使用，仅作为了解；但是我看经常配合注解使用，尤其是在 SSH 的项目中 模式 描述 no 这是默认的设置，它意味着没有自动装配，你应该使用显式的bean引用来连线。你不用为了连线做特殊的事。 byName 由属性名自动装配。当找不到相应的连接关系时会尝试使用属性名去搜寻对应的 bean （id 和 name 相匹配）；如果找不到或者找到多个则抛异常 byType 由属性数据类型自动装配。当找不到相应的连接关系时会尝试使用属性的类型搜寻对应的 bean （class 相匹配）；如果找不到或者找到多个则抛异常 constructor 类似于 byType，但该类型适用于构造函数参数类型。如果在容器中没有一个构造函数参数类型的 bean，则一个致命错误将会发生。 autodetect Spring 首先尝试通过 constructor 使用自动装配来连接，如果它不执行，Spring 尝试通过 byType 来自动装配。 默认值也是可以设置的，在 beans 标签的 defualt-autowire 属性指定 分散配置使用 Spring 的特殊 bean 来达到分散配置的目的，占位符使用 ${name} 的形式；前面说生命周期的时候说的那几个接口就是体现的这个功能首先新建个 properties 文件，然后引入到配置文件中： 1234567&lt;!--分散配置，引入 prop 文件；如果引入多个文件可以用逗号分割--&gt;&lt;context:property-placeholder location="temp.properties"/&gt;&lt;!-- 可以使用占位符来引用 --&gt;&lt;bean id="user3" class="com.bfchengnuo.domain.User"&gt; &lt;property name="name" value="$&#123;name&#125;"/&gt; &lt;property name="age" value="$&#123;age&#125;"/&gt;&lt;/bean&gt; 获取ApplicationContext首先要明确的是 ApplicationContext 是个接口；其实它也是 BeanFactory 的子类，获取 ApplicationContext 实现类常用的有下面几种方式： ClassPathXmlApplicationContext 通过类路径，上面写过了，通常用在“桌面”系统开发 FileSystemXmlApplicationContext 通过文件系统获取，需要指明绝对路径，用的很少 XmlWebApplicationContext 通过 web 方式获取，传统项目这个用的还是比较多的 AnnotationConfigApplicationContext 使用 Java 配置的方式来构建 ApplicationContext，例如 SpringBoot AnnotationConfigWebApplicationContext 使用 Java 配置方式的 Web 应用，可以在 web.xml 中指定。 AOP编程和IoCAOP 和 IoC 是 Spring 的两大特点 AOP（Aspect Orient Programming），也就是面向切面编程。 可以这样理解，面向对象编程（OOP）是从静态角度考虑程序结构，面向切面编程（AOP）是从动态角度考虑程序运行过程。这种在运行时，动态地将代码“切入”到类的指定方法、指定位置上的编程思想就是面向切面的编程。或者说在不增加代码的基础上增加新的功能；面向（作用于）全部对象，或者一类对象（想一下学过的监听器、拦截器、过滤器） 下面就说说常见的几个术语： 通知/增强（Advice）切面的工作被称为通知，通知定义了切面是什么以及何时使用。除了描述切面要完成的工作，通知还解决了何时执行这个工作的问题。它应该应用在某个方法被调用之前？之后？之前和之后都调用？还是只是在方法抛出异常时调用？简单说就是你想要（切入）的具体功能实现(想要干啥)，比如安全，事物，日志操作等。 连接点(JoinPoint)连接点是在应用执行过程中能够插入切面的一个点。这个点可以是调用方法、抛出异常时、甚至修改一个字段时。切面代码可以利用这些点插入到应用的正常流程之中，并添加新的行为。就是 Spring 允许你使用通知的地方 切点(Pointcut)一组连接点的总称，用于指定某个通知（增强）应该在何时被调用。切入点是「在哪干」，你可能在很多地方（连接点）都可以干，但并不是每个地方都要干，要干的地方叫切点 切面（Aspect）切面是通知和切点的结合。通知和切点共同定义了切面的全部内容——它是什么，在何时和何处完成其功能通知说明了干什么和什么时候干（什么时候通过方法名中的before、after、around 等就能知道），而切入点说明了在哪干（指定到底是哪个方法），这就是一个完整的切面定义。 织入(weaving)把切面应用到目标对象来创建新的代理对象的过程。有3种方式，spring 采用的是运行时 Spring 中的通知有五种类型： 前置通知(Before)：在目标方法被调用之前调用通知功能；[接口： MethodBeforeAdvice] 后置通知(After)：在目标方法完成之后调用通知，此时不会关心方法的输出是什么；[接口： AfterReturningAdvice] 返回通知/最终通知(After-returning)：在目标方法成功执行之后调用通知，如果有后置通知会在其之后执行； 异常通知(After-throwing)：在目标方法抛出异常后调用通知；[接口： ThrowsAdvice] 环绕通知(Around)：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。[接口： MethodInterceptor] 运行顺序：前置通知/环绕通知–目标方法执行–返回通知/异常通知–后置通知/环绕通知这里需要注意的是：环绕通知由于和前置、后置处于同一个 aspect 内，所以是无法确定其执行顺序的，当然可以通过其他手段来解决实际开发中，一般会将顺序执行的 Advice 封装到不同的 Aspect，然后通过注解或者实现接口的方式控制 Aspect 的执行顺序，二选一（对于在同一个切面定义的通知函数将会根据在类中的声明顺序执行）关于 AOP 的更详细介绍：[1]:http://cometzb-xujun.iteye.com/blog/1537274；[2]:http://www.jianshu.com/p/66d21dae6a68 简单解释下什么是 IoC（控制反转 Inversion of Control），它是一种设计思想，和依赖注入有些相似之处（或者说不同名称，相同含义），就是说把创建对象和维护对象关系的控制权从程序中转移到了 Spring 容器中（applicationContext.xml），程序本身不再维护，换句话说就是从代码移到了配置文件中 ，对象的创建交给外部容器处理 你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制 ,所以说这个容器很重要，IoC 是有专门一个容器来创建这些对象，即由 Ioc 容器来控制对象的创建简单总结：控制反转解决对象的创建问题（交给别人创建），依赖注入解决在创建对象后，对象关系的处理问题（通过 set 方法依赖注入） 参考：http://luanxiyuan.iteye.com/blog/2279954 AOP快速入门Spring 中“使用” AOP 的步骤一般是： 定义接口 编写对象（目标对象、被代理对象） 编写通知 XML 文件中进行配置，包括被代理对象、通知、代理对象（ProxyFactoryBean） 通过代理对象进行使用 如果看过动态代理相关的知识，还是很好理解的，那就搞一个简单的前置通知来玩玩，前两部就省略了，很简单；主要是通知和 XML 的配置，这种 XML 的配置好像很 low 了，随便看看就好，稍后我会专门更新一篇新版的. 123456789101112public class MyMethodBeforeAdvice implements MethodBeforeAdvice &#123; /** * 在被代理对象的前面执行 * @param method 方法对象--被调用方法的方法名 * @param objects 传入被调用方法的参数 * @param o 目标对象（被代理对象） */ @Override public void before(Method method, Object[] objects, Object o) throws Throwable &#123; System.out.println("Before...记录..." + method.getName()); &#125;&#125; 然后是相关的配置文件，如果代理了多个接口记得要在 proxyInterfaces 都写进去，这样就可以在 getBean 后随意进行切换 12345678910111213141516171819202122232425&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;!--配置目标对象（被代理对象）--&gt; &lt;bean id="standardLoli" class="com.bfchengnuo.aop.StandardLoli"&gt; &lt;property name="flag" value="欧尼酱"/&gt; &lt;/bean&gt; &lt;!--配置通知--&gt; &lt;bean id="myMethodBeforeAdvice" class="com.bfchengnuo.aop.MyMethodBeforeAdvice"/&gt; &lt;!--配置代理对象，Spring 提供--&gt; &lt;bean id="proxyFactoryBean" class="org.springframework.aop.framework.ProxyFactoryBean"&gt; &lt;!--代理接口集--&gt; &lt;property name="proxyInterfaces"&gt; &lt;list&gt; &lt;value&gt;com.bfchengnuo.aop.Loli&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--将通知织入代理对象,相当于把通知和代理对象建立关系--&gt; &lt;property name="interceptorNames" value="myMethodBeforeAdvice"/&gt; &lt;!--配置被代理对象--&gt; &lt;property name="target" ref="standardLoli"/&gt; &lt;/bean&gt;&lt;/beans&gt; 然后测试了下，还是挺不错的，如果之前写了多个接口，可以直接进行各种强转，当然你的通知要都实现了这些接口，终究还是多态的特性 1234567public static void main(String[] args) &#123; ApplicationContext ac = new ClassPathXmlApplicationContext("com/bfchengnuo/aop/beans.xml"); Loli loli = (Loli) ac.getBean("proxyFactoryBean"); loli.hug(); System.out.println("----------------"); System.out.println(loli.speak("bfchengnuo"));&#125; 最后再来补充一个，就是定义切点，是自定义，也就是说指定某个通知在指定的方法切入，配置完后记得在代理对象中将通知换成这个就可以了 123456789101112&lt;!--自定义切入点--&gt;&lt;bean id="myPointcutAdvisor" class="org.springframework.aop.support.NameMatchMethodPointcutAdvisor"&gt; &lt;!--设置的是前置通知的切入点--&gt; &lt;property name="advice" ref="myMethodBeforeAdvice"/&gt; &lt;!--设置在那个方法进行切入--&gt; &lt;property name="mappedNames"&gt; &lt;list&gt; &lt;!--是可以使用通配符的--&gt; &lt;value&gt;hug&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>AOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hibernate学习笔记(二)]]></title>
    <url>%2F2017%2F07%2F09%2FHibernate%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[对于数据量大，性能要求高的系统，是不适合使用 Hibernate 的；主要用于事务操作比较多的系统（比如 OA/CRM 行业软件）忽然想起来，现在已经是 Hibernate5.x 了，有些配置方面已经做了更改，比如 SessionFactory 的获取方式，我也刚刚才意识到，这个版本的问题我准备以后单独搞一篇来说 级联操作级联是什么学过数据库的都懂，应用在 Hibernate 下就是：当对主对象进行操作的时候，是否对从对象也进行类似的操作在 Hibernate 中只需要在配置文件里配一下就可以了比如可以在 set 标签设置 cascade 属性，一般有五个可选值，如果设置多个可以用逗号分割： save-update级联保存或者更新(也就是说 load 以后如果子对象发生了更新,也会级联更新数据库). 但它不会级联删除 delete级联删除, 但不具备级联保存和更新 all-delete-orphan在解除父子关系时,自动删除不属于父对象的子对象, 也支持级联删除和级联保存更新. all能够级联删除和级联更新，但解除父子关系时不会自动删除子对象. delete-orphan删除所有和当前对象解除关联关系的对象 以上设在哪一段就是指对哪一端的操作而言，比如 delete，如果设在 one 的一端的 &lt;set&gt; 属性里，就是当 one 被删除的时候，自动删除所有的子记录；如果设在 many 一端的 &lt;many-to-one&gt; 标签里，就是在删除 many 一端的数据时，会试图删除 one 一端的数据，如果仍然有 many 外键引用 one，就会报“存在子记录”的错误；如果在 one 的一端同时也设置了cascade＝&quot;delete&quot; 属性，就会发生很危险的情况：删除 many 一端的一条记录，会试图级联删除对应的 one 端记录，因为 one 也设置了级联删除 many，所以其他所有与 one 关联的 many 都会被删掉。 所以，千万谨慎在 many-to-one 的 many 一端设置 cascade＝&quot;delete&quot; 属性。故此 cascade 一般用在 &lt;one-to-one&gt; 和&lt;one-to-many&gt; 中 懒加载关于这个问题在上一篇中提到过，就是在获取 Session 的时候，get 和 load 方式；load 方式就是使用的懒加载所带来的问题就是：如果 domain 对象中存在其他对象的引用，默认这个引用的对象数据是空的，它以为你不会用，所以并没有去数据库查；等你用的时候，它会试图去数据库查，但是用数据的时候 Session 可能已经关闭了，所以只能抛异常了 常见的解决方案有： 关闭懒加载，就是在配置文件设置 lazy 属性为 false 显式的进行加载初始化代理对象 Hibernate.initialize(bean.getBean); 就是相当于手动调用了一下 domain 对象中的”对象“ 使用 openSessionInViewFilter前面貌似也说过，通过过滤器实现拦截所有请求，扩大 Session 的范围，在请求开始的时候开启 Session 与事务，在请求结束的时候提交事务、关闭连接过滤器最好配置在 Struts2 核心过滤器的前面 在 SSH 项目中可以使用注解来解决 然后再多说点关于第一点的，因为如果配置不好反而效率会降低，Hibernate 的配置很关键！在 many-to-one 也就是多对一的 many 方，如果配置了 &lt;class ... lazy=&quot;false&quot;&gt; ，那么 Hibernate 就会在查询 many 方的时候把它相互关联的对象也进行查询，对 select 的影响并不是很大，毕竟是多对一在 one-to-many 的 one 这一方，如果配置了 &lt;set ... lazy=&quot;false&quot;&gt; ，那么当你查询这个 one 的时候，会把相关联的 many 全部查询回来，不管你是否使用（都会发一大堆的 select 语句） 所以说，不要在 one-to-many 的 many 的一方设置 lazy 为 false ，效率会大大降低，可以用的是在 set、list 标签中设置属性 lazy 为 extra；这样会比较智能，在使用 list.size() 方法的时候会发送 select count(*)... 语句，而不是一条条的查回来如果要使用第一种方案，少在 many-to-one 和 one-to-one 标签里设置 lazy；不要在 class 和 property 标签里设 lazy 懒加载其实是依赖的动态代理，前面也提到过，返回的其实是个代理对象，也就是说如果要使用懒加载在设计 domain 对象的时候就不能设置为 final 懒加载除了设置 true 和 false，还有一个值：extra 它一般设置在配置文件中集合标签的属性中（如 set 标签中），意义是当使用集合的 size、isEmpty 等函数时，并不把所有的数据查询回来，而是使用相应的 sql 函数进行查询 extra 也是使用懒加载，只不过是更智能的懒加载，一般设置这个就行 关于缓存前面提到过一点，Hibernate 的缓存常用的就是一级和二级缓存，一级缓存又称 Session 缓存（存在于内存），因为它伴随着 Session，会话结束了缓存也就不存在了对于 Session 的那些操作会使其放进一级缓存？save、update、saveOrUpdate、load、get、list、lock、iterate在执行上面的这些方法的时候会先将对象加入到一级缓存，然后在事务提交的时候发送相应的 sql 语句，同步到数据库都知道使用 get、load 方法会从一级缓存中获取数据，但是 list（或者 uniqueResult）并不会从缓存获取数据，它只放进缓存不会主动取缓存数据然后如果频繁的调用上面的方法，为了防止内存溢出，记得调用 evict 或者 clear 来清除缓存 ；分别对应的是清除单个对象和所有对象；此外还有一个 flush 的方法，用来刷新缓存，让一级缓存和数据库进行同步。所以说，一级缓存只有在短时间内频繁的操作数据库的情况下，效果才比较明显；处于持久化的对象一般都存在于 Session 缓存中 关于 List 和 iterate 这里简单说下，list 会一次性把所有的记录都查出来；而 iterate 会查询 n +1 次，也就是如果有五条数据，那么会查询六次。 第一次会查询全部的 id ，然后每一次的 next 会根据这个 id 进行查询，所以总共是 n +1 次查询；但是它会从缓存中获取，List 不会从缓存中获取 所以说，当你第一次用 List 获取后会放入缓存，如果第二次你用 iterate 获取，他会从缓存中获取（List 放进去的数据）数据，这样也验证了 List 是会把数据放进缓存中去的 然后再来说说二级缓存，又叫 SessionFactory 缓存，为什么应该也能猜到了，肯定和其的生命周期有关；不同于一级缓存（生命周期短，大小限制），二级缓存必须配置后才可以使用，并且其是交给第三方进行处理的 ；当然 hibernate 也带一个。它是全局性的，应用中的所有 Session 都共享这个二级缓存。常用的二级缓存插件有：Ehcache、OSCache、JBossCache；这些缓存有可能存在内存中，也有可能存在于硬盘中开启二级缓存首先在 cfg 配置文件中（可以从 hibernate.properties 文件中查找）： 12345678910111213141516&lt;!-- 启动二级缓存 --&gt;&lt;property name="cache.use_second_level_cache"&gt;true&lt;/property&gt;&lt;!-- 启动命中率统计 --&gt;&lt;property name="hibernate.generate_statistics"&gt;true&lt;/property&gt;&lt;!-- 指定使用哪种二级缓存 --&gt;&lt;property name="hibernate.cache.region.factory_class"&gt;org.hibernate.cache.ehcache.EhCacheRegionFactory&lt;/property&gt;&lt;!-- 指定哪个domain启用二级缓存 特别说明二级缓存策略: 1. read-only 2. read-write 3. nonstrict-read-write 4. transcational--&gt;&lt;class-cache usage="read-write" class="com.bfchengnuo.ssh.domain.User"/&gt;&lt;!-- 集合缓存 --&gt;&lt;collection-cache usage="read-write" collection="com.bfchengnuo.ssh.domain.User.set" /&gt; 缓存策略的配置除了 class-cache 标签还可以在 hbm 映射配置文件中的 cache 标签里配，但是为了便于管理多数还是在 cfg 文件中配置。 read-only只读缓存，提供 serializable 事务隔离级别，对于从来不会修改的数据，可以采用这种访问策略，可以避免脏读，不可重复读和幻读。 read-write读写缓存，提供 read committed 事务隔离级别。对于经常读但是很少被修改的数据，可以采用这种隔离类型，它可以防止脏读。（通常选用的策略） nonstrict-read-write不严格的读写，不保证缓存与数据库种数据的一致性。提供 read uncommitted 事务隔离级别。对于极少被修改，而且允许脏读的数据，可以采用这种策略。 transcational仅在受管理环境下适用。它提供了repeatable read 的事务隔离级别。对于经常读，但是很少被修改的数据，可以采用这种隔离类型，它可以防止脏读和不可重复读。 在查询数据的时候，会放进二级缓存和一级缓存（前提是你配置过这个实体），就是都放一份，还可以通过 statistics 查看命中率 (通过 SessionFactory 获取)如果配置了集合缓存，记得集合里面的实体也必须是配置过 class-cache 的，否则是没有效果的。如果使用的是 EHcache 就要在 src 目录下加入 ehcache.xml 文件来具体配置缓存策略，具体的配置不写了，也不是很常用，也就配个大小放入二级缓存的数据必须是很少被修改的内容 其实还有一个查询缓存…..他可以让 query.list() 方法从二级缓存中取数据。查询缓存的开启是：&lt;property name=&quot;hibernate.cache.use_query_cache&quot;&gt;true&lt;/property&gt;；需要定义在 class-cache 之前，并且 list 如果要使用的话需要手动调用一下 q.setCacheable(true) 才行。 主键增长策略至于在那配置我想都是知道的，就是在 id 标签里的 generator 标签的 class 属性中设置；貌似总共有16+，常用的也就 8、9种，比如： increment用于 long、short 或 int 类型（下面就称数值型吧），适用于所有数据库，每次增量为1。只有当没有其他进程向同一张表中插入数据时才可以使用，不能在集群环境下使用。适用于代理主键。实质其实就是用 sql 的 max 函数计算出最大的 id 然后 +1，所以自然不能用字符串 identity采用底层数据库本身提供的主键生成标识符，每次增长1，条件是数据库支持自动增长数据类型（MySQL、SQL server）。在 MySQL 数据库中可以使用该生成器，该生成器要求在数据库中把主键定义成自增长类型。适用于代理主键。Oracle 数据库并不适用。 sequenceHibernate 根据底层数据库序列生成标识符。条件是数据库支持序列。适用于代理主键。在 oracle 数据库中可以使用该生成器。类型也是数值型 native根据底层数据库对自动生成表示符的能力来选择 identity、sequence、hilo 三种生成器中的一种，适合跨数据库平台开发。适用于代理主键。 主键类型一般是数值型 uuid基于128 位(bit)唯一值算法，根据当前 IP，时间，jvm 启动时间，内部自增量产生等多个参数生成 16 进制数值（编码后以长度为 32 的字符串表示）作为主键，注意类型是字符串哦 assigned使用这种方法，主键要提前设置，就是交给用户自己定义，可以是数值型或者字符串 foreign使用另外一个相关联的对象的主键作为该对象主键。主要用于一对一关系中。或者说是用其他表的主键来决定自己的 id hilo高低位算法生成，类型为数值型，生成方法不依赖数据库，所以适用于全部数据库，但是需要新建表并且在配置文件配置，不太常用 Oracle：数值型的话，可以使用 sequence；如果是字符串类型使用 uuid 或者 assignedMySQL：数值型的话，可以使用 increment ；字符串类型使用 uuid 或者 assignedSQL Server：数值型的话，可以使用 identity 或者 native；字符串类型使用 uuid 或者 assigned 使用注解都知道从 JAVA5.0 之后，可以利用注解来简化配置，所以自然可以用在 Hibernate 上，可以不再写 hbm 文件了，但是 cfg 文件还是要写的…其实大部分实体对象都是用工具生成的，我感觉看懂就行了，并且现在多是用 JPA 的注解了，它们基本是一致的： 12345678910111213141516171819202122232425262728293031323334@Entity //如果我们当前这个bean要设置成实体对象，就需要加上Entity这个注解@Table(name="t_user") //设置数据库的表名public class User&#123; private int id; private String username; private String password; private Date registerDate; // Column中的name属性对应了数据库的该字段名字，里面还有其他属性，例如length，nullable等等 @Column(name="register_date") public Date getRegisterDate() &#123; return registerDate; &#125; public void setRegisterDate(Date registerDate) &#123; this.registerDate = registerDate; &#125; // 定义为数据库的主键ID // 建议不要在属性上引入注解，因为属性是private的，如果引入注解会破坏其封装特性，所以建议在getter方法上加入注解 @Id @GeneratedValue //ID的生成策略为自动生成 public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; ............&#125; 采用了注解就不需要写映射文件了，但是需要让 hibernate 知道这是一个实体，所以还要在 cfg 配置文件中这样写： 1234&lt;!-- 基于annotation的配置 --&gt;&lt;mapping class="com.xiaoluo.bean.User"/&gt;&lt;!-- 基于hbm.xml配置文件 --&gt;&lt;mapping resource="com/xiaoluo/bean/User.hbm.xml"/&gt; 当然这是最简单的，后面还有更多的对应关系的注解，移步这里吧：http://www.cnblogs.com/xiaoluo501395377/p/3374955.html 关于连接池Hibernate 有自带的连接池，也支持第三方的连接池，推荐使用 C3P0，无论使用那个，记得导包（hibernate 中就有）默认情况下（即没有配置连接池的情况下），Hibernate 会采用内建的连接池。但这个连接池性能不佳，且存在诸多 BUG，因此官方也只是建议仅在开发环境下使用。比如配置 C3P0 连接池，除了必须的数据库连接信息，只需要在 cfg 配置文件中添加一小部分配置信息即可 123456789101112131415161718192021222324252627282930&lt;?xml version="1.0" encoding="UTF-8" ?&gt; &lt;!DOCTYPE hibernate-configuration PUBLIC "-//Hibernate/Hibernate Configuration DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd"&gt; &lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;property name="hibernate.connection.driver_class"&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name="hibernate.connection.url"&gt;jdbc:mysql:///test&lt;/property&gt; &lt;property name="hibernate.connection.username"&gt;root&lt;/property&gt; &lt;property name="hibernate.connection.password"&gt;123&lt;/property&gt; &lt;property name="hibernate.dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/property&gt; &lt;!-- 配置c3p0连接池 --&gt; &lt;property name="hibernate.connection.provider_class"&gt;org.hibernate.connection.C3P0ConnectionProvider&lt;/property&gt; &lt;!--在连接池中可用数据库连接的最小数目--&gt; &lt;property name="c3p0.min_size"&gt;5&lt;/property&gt; &lt;!--在连接池中所有数据库连接的最大数目--&gt; &lt;property name="c3p0.max_size"&gt;30&lt;/property&gt; &lt;!--设定数据库连接的超时时间--&gt; &lt;property name="c3p0.time_out"&gt;1800&lt;/property&gt; &lt;!--可以被缓存的PreparedStatement的最大数目--&gt; &lt;property name="c3p0.max_statement"&gt;50&lt;/property&gt; &lt;property name="hibernate.show_sql"&gt;true&lt;/property&gt; &lt;property name="hibernate.format_sql"&gt;true&lt;/property&gt; &lt;property name="hibernate.hbm2ddl.auto"&gt;update&lt;/property&gt; &lt;mapping resource="domain/Customer.hbm.xml"/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; 上面说的那个是连接池的最大连接数目，不要当成是连接的最大数目，如果不知道怎么配可以从 hibernate 的 project/etc 目录下的 hibernate.properties 文件中抄记得一定要配置 provider_class ，这一句用于指定 Hibernate 的连接提供方式，如果没有将不会使用 c3p0 作为 Hibernate 的连接池。那么如何知道连接池是否生效了呢，比如可以在 MySQL 中使用 SHOW PROCESSLIST 来查看活跃的连接数。或者可以尝试下评价比较高的 Proxool 连接池，对于它，配置可能会有些复杂，还需要配置单独的 proxool.xml 文件，这里不多说了，或者看看阿里的 druid ？ 另外还有一种比较特殊的，就是使用 Tomcat 自带的（JNDI），这种用法应该比较少见，关于自带连接池的配置我在 这篇文章 说过了，配置好了在 cfg 文件中加入 12345678&lt;property name="hibernate.connection.datasource"&gt;java:comp/env/jdbc/name&lt;/property&gt;&lt;property name="show_sql"&gt;true&lt;/property&gt;&lt;property name="dialect"&gt;com.huatech.sysframe.webapp.common.dao.hibernate.dialet.BaseInformixDialect&lt;/property&gt;&lt;property name="hibernate.generate_statistics"&gt;true&lt;/property&gt; jdbc/name 就是你配置的数据源（连接池）的名字，后面其实还有 Spring 中配置的方式，这个下次再说吧，可以看看参考的最后一个连接，挺全的 关于映射文件hbm 映射文件我感觉大部分都是自动生成的，因为我们多数还是习惯于先设计数据库，hbm 配置文件也很少见了，大多都使用 JPA 的注解了；所以我就没写这方面的 但还是要提一点，在一对多或者多对一等这种遇到使用集合的时候，我们有四种选择：数组、Set、List、Map；配置文件中也有对应的标签（在一对多对象引用时选择 one-to-many 标签）。数组和 List 差不多，都是要保证顺序，所以在配置的时候需要指定一个 index 额外字段来标识顺序，它们的区别也就是可变于不可变的差别；Set 应该是最常用的，它不需要保证顺序，所以不需要额外的字段，配置比较简单，只需要配置个外键字段和相应的列（对象）即可；Map 虽然没顺序，但是需要配置一个 key，这个 key 也会存在于数据库的表中。 但是，一般不会在配置文件中进行配置这些集合，而是只维护多的一方（一对多、多对一）；因为如果多的一方很多，在查询“一” 的一方时会大量的消耗资源来查询多的对象，并且还不一定用得到 inverse，可翻译为控制反转，就是说把控制器交出去了设置位 false 代表有控制器；设置位 true 说明交出了控制权所谓的控制权就是维护关系（外键）的权利了，比如只能在一对多的一的一方设置，如果设置了为 true ，那么它就没有权利更新外键了（数据都是保存的，只是它内部的集合中的实体不会保存引用了）此属性对获取数据没有影响 关于组合与继承映射组合简单说就是一个类中包含有另一个类，映射到数据库中对应的是一张表；继承就继承了…..额组合使用 component 标签，name 属性为组合类类型的属性名，使用子标签 property 指定到底是组合了那些属性 参考http://blog.csdn.net/sinlff/article/details/7342527缓存机制学习注解大全http://www.cnblogs.com/oumyye/p/4442764.htmlhttps://teakki.com/p/57df75a21201d4c1629b86c1]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Hibernate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hibernate学习笔记]]></title>
    <url>%2F2017%2F07%2F01%2FHibernate%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[SSH 框架系列笔记，看到第二个了，Struts 原来是最简单的，后面的 Hibernate 和 Spring 都很有道道呢，尤其是 Spring ，连接了 Struts 和 Hibernate，本来是想看 Spring 的，但是发现 Hibernate 是前提，于是…..学习 Hibernate 其实就是在学习怎么配 xml 文件，因为大部分都是通过配配置文件来实现的，所以说，官方文档很重要！英语很重要！ ╮(╯▽╰)╭ 简介Hibernate 是一种 Java 语言下的对象关系映射（object relation mapping）解决方案，或者说框架。它是使用 GNU 宽通用公共许可证发行的自由、开源的软件。它为面向对象的领域模型到传统的关系型数据库的映射，提供了一个使用方便的框架。它的设计目标是将软件开发人员从大量相同的数据持久层相关编程工作中解放出来。无论是从设计草案还是从一个遗留数据库开始，开发人员都可以采用 Hibernate。Hibernate 不仅负责从 Java 类到数据库表的映射（还包括从 Java 数据类型到 SQL 数据类型的映射），还提供了面向对象的数据查询检索机制，从而极大地缩短的手动处理 SQL 和 JDBC 上的开发时间。 数据持久化…简单说就是把数据（比如对象）保存起来，保存到文件也好数据库也好，都是持久化的体现，也许你会想到序列化，它们两个很相似，但是是两个完全不同的应用场景，持久持久，久….序列化多是为了能将对象更好的交换、传输（比如从这个线程到那个线程） 开发方式Hibernate 一般有三种开发方式： 由 Domain object –&gt; Mapping –&gt; db ；这是官方推荐的方式Domain 对象设计好了，Mapping 配置好了可以直接自动生成数据表 由 DB 开始，然后用工具生成 Domain object 和 Mapping，这种方式用的比较多同样，数据库设计好了，也有相关的工具可以自动生成 Domain 对象和映射文件 由映射文件开始 然后具体解释下，Domain object 其实指的就是 javabean，也叫做 Domain 对象；Mapping 是映射文件；db 就是数据库了它们的关系是：每一个 Domain 对象对应数据库中的一张表，每一个实例化的 Domain 对象就是数据库中对应表中的一条数据，将 Domain 对象和数据库联系起来的是数据持久层，它依赖于ORM-对象关系映射文件（Mapping）；该文件会说明表和对象的关系，以及对象的属性和表字段的对应关系对象关系映射嘛，使用了 Hibernate 基本都是操作对象了….. Mapping 映射文件在命名上有一定规范：对象名.hbm.xml 一般放在 Javabean/Domain 对象目录（包）下，原理也都知道，就是反射，反射必须依赖于这个配置文件 简单使用Javabean 对象和数据库就跳过了，这个很简单没啥可说的，导入相关 jar 包也不说了，关键就剩下那个 Mapping 映射文件了，当然这个完全可以自动生成，实际开发没必要手写，我第一次就手写吧 12345678910111213141516171819202122232425262728293031&lt;?xml version='1.0' encoding='utf-8'?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC "-//Hibernate/Hibernate Mapping DTD 3.0//EN" "http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd"&gt;&lt;hibernate-mapping package="com.bfchengnuo.domain"&gt; &lt;!-- 指定 Javabean 对应那张表 --&gt; &lt;class name="User" table="user"&gt; &lt;!-- id 是指定主键 --&gt; &lt;id name="id" column="id" type="java.lang.Integer"&gt; &lt;!--指定主键值的生成策略，用于主键的自增长，一般有7种方式，mysql用下面的这个就行--&gt; &lt;generator class="identity"/&gt; &lt;/id&gt; &lt;!--其他属性的映射--&gt; &lt;property name="name" type="java.lang.String"&gt; &lt;column name="name" not-null="false"/&gt; &lt;/property&gt; &lt;property name="age" type="java.lang.Integer"&gt; &lt;column name="age" not-null="false"/&gt; &lt;/property&gt; &lt;property name="email" type="java.lang.String"&gt; &lt;column name="email"/&gt; &lt;/property&gt; &lt;property name="hiredate" type="java.util.Date"&gt; &lt;column name="hiredate"/&gt; &lt;/property&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; 关于主键的策略，想深入的可移步：http://www.cnblogs.com/kakafra/archive/2012/09/16/2687569.html上面设置的 type 除了设置 Java 类型，还可以设置 Hibernate 类型，不过一般习惯于 Java 类型 除了配置映射文件，还有一个重要的配置文件 hinernate.cfg.xml ，它用来配置数据库的类型、驱动、用户名、密码、URL 、连接池等信息；放在工程目录的根目录下，就是 Src 下，前面说的 Struts 也是hinernate.cfg.xml 它同时还负责管理 Mapping 映射文件 1234567891011121314151617181920&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;!DOCTYPE hibernate-configuration PUBLIC "-//Hibernate/Hibernate Configuration DTD//EN" "http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd"&gt;&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;property name="connection.driver_class"&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name="connection.username"&gt;root&lt;/property&gt; &lt;property name="connection.password"&gt;******&lt;/property&gt; &lt;property name="connection.url"&gt;jdbc:mysql://localhost:3306/hibernatetest?useUnicode=true&amp;amp;characterEncoding=UTF-8 &lt;/property&gt; &lt;!-- 设置方言,让 Hibernate 明确使用的是那种数据库 --&gt; &lt;property name="dialect"&gt;org.hibernate.dialect.MySQLDialect&lt;/property&gt; &lt;!--显示 sql 语句--&gt; &lt;property name="show_sql"&gt;true&lt;/property&gt; &lt;!--要管理的映射文件，放在最后--&gt; &lt;mapping resource="com/bfchengnuo/domain/User.hbm.xml"/&gt; &lt;/session-factory&gt;&lt;/hibernate-configuration&gt; 这样操作数据库就不需要再使用 JDBC 相关的对象了，都已经被封装了；现在我们操作数据库使用的是 Hibernate 给提供的一些类和接口，最常用的有 Configuration、SessionFactory、Session、Transaction；除了第一个是类其他全是接口使用的时候注意别导错包了，是 org.hibernate 下的包！！！下面举个简单的栗子，使用步骤就那么几步 12345678910111213141516171819202122232425public static void main(String[] args) &#123; // 1. 创建 Configuration，用来读取配置文件，并完成初始化 // 如果文件名没按照规范，可以手动指定 Configuration configuration = new Configuration().configure(); // 2. 创建 SessionFactory 会话工厂，重量级，耗资源，最好要保证只有一个 SessionFactory sessionFactory = configuration.buildSessionFactory(); // 3. 创建会话（Session），相当于 JDBC 中的 connection Session session = sessionFactory.openSession(); // 4. 开启事务，对 Hibernate 而言，要求程序猿在进行 增删改 的时候使用事务 Transaction transaction = session.beginTransaction(); // 添加一条数据 User user = new User(); user.setName("佳芷"); user.setAge(12); user.setEmail("loli@163.com"); user.setHiredate(new Date()); session.save(user); transaction.commit(); session.close();&#125; 就是这四步走，要注意的就是进行增删改一定记得使用事务，否则不会生效，最后操作完了记得关闭连接（session）如果不想用一个单独的变量来保存事务可以直接从 session 中获得，因为它们是绑定在一起的：session.getTransaction().commit(); 按照规范，所定义的 Javabean 需要进行序列化以便可以唯一的标识该对象，并且可以进行传输 核心类和接口上面提到过的，一个类和三个接口，此外还有一个非常重要的接口 Query ，非常强大 Configuration类这是唯一的一个类了，它的作用就是读取（加载）配置文件的，当然也包括映射文件，加载一些驱动啥的这个类还是很简单的，就不多说了 SessionFactory接口它的一些特点： 缓存 SQL 语句和某些数据（一级缓存，又叫 Session 缓存） 在程序初始化的时候创建，因为特别吃资源，一般采用单例模式保证一个应用中只有一个 如果一个应用要使用多个数据库，那么可以一个数据库对应一个 SessionFactory 通过 SessionFactory 获取 Session 实例有两个方法， openSession() 和 getCurentSession()openSession 获取的是一个全新的 SessiongetCurentSession 获取和当前线程绑定的 Session，利于事务的控制，可以猜出，第一次调用的时候其实也是调用的 openSession 方法，只不过会做一些处理 如果要使用 getCurentSession 的方式获得 Session，需要事先在 cfg 配置文件中配置 &lt;property name=&quot;current_session_context_class&quot;&gt;thread&lt;/property&gt; 使用 getCurentSession 创建的 Session，在 commit 或者 rollback 后会自动关闭，但是最好还是手动关一下 使用 openSession 查询数据不需要使用事务，但是如果使用的是 getCurentSession 查询也需要开事务 事务通常有两种，本地事务和全局事务本地事务：针对一个数据库的事务（比如上面所配的 thread）全局事务：跨数据库的事务（可以配 jta）简单说说全局事务的实现，其实就是利用了一个 List ，每次执行数据库操作都会进行检测，如果发现里面含有未完成事务（会有一个标识），无论是那个数据库的事务没有成功都会进行回滚，这样就达到了全局事务的功能，不过一般都是在 web服务器中进行配置 Session接口看过 SessionFactory 再看 Session 就简单了，就是用来操作数据库的，和 JDBC 中的 connection，进行 CRUD 操作Session 可以看作是持久化管理器，它是与持久化操作相关的接口 删除一个对象（记录）：delete 方法 查询一个对象（记录）：get/load 方法说说这两个方法的区别：get 方法：直接返回实体类，如果查不到数据就返回 Nullget 先到缓存（Session 缓存/二级缓存）去查找，如果没找到就立即向 DB 发送 SQL 去查​load 方法：会返回一个实体代理对象 ，可以自动转换成实体对象；但当代理对象被调用的时候，如果数据不存在就会抛异常load 也先到缓存（Session 缓存/二级缓存）去查找，如果没有就返回一个代理对象，等后面使用到这个代理对象的时候再去 DB 查，所以是支持延迟加载的（lazy）所以，如果你能确定数据库中有这个对象就用 load ，否则就用 get，这样效率高通过修改配置文件可以取消延迟加载（懒加载）在 class 标签里设属性 lazy 修改一个对象（记录）：update 方法 保存一个对象（记录）：save 方法 Query接口查询简单的可以使用 get/load 方法，如果需要复杂的查询，就要用 Query 这个接口了先简单说下 hql 语言，是用来操作数据库的，不用担心数据库的类型，会进行自动转换，所以，只需要会写 hql 大部分操作就可以搞定了，多使用在业务逻辑层下面是一个简单的栗子： 123456789Session session = HibernateUtil.openSession();// 获取 Query 引用，使用 hql 语句， User 是实体类名不是表名，id 是属性不是字段// id 位置也可以使用字段名，但是推荐使用属性Query query = session.createQuery("from User where id=1");// 可以使用泛型，会被自动封装List&lt;User&gt; list = query.list();for (User user : list) &#123; System.out.println(user.getName() + "---" + user.getAge());&#125; 拓展下，除了 Query 还有一个用于查询的是 Criteria，它是纯面向对象的，但是并不常用，不多说，还有一种叫本地化 SQL 查询，说白了就是使用原生的 SQL 语句（返回的也是每一行的记录数组，可以使用 addEntity 方法来注入到对象中），也是不推荐使用，因为和数据库耦合了，除非是非常复杂的 SQL 语句，HQL 无法胜任的时候使用 query.executeUpdate() 可以执行更新、删除操作 分页查询使用 Query 可以很简单的达到分页查询的效果，类似： 1234567// 第一条开始取，取 2 条，索引从 0 开始哦（MySQL 中使用的就是 limit）session.createQuery("from User").setFirstResult(0).setMaxResult(2).list();// 获取总记录数ScrollableResults scroll = q.scroll(); // 得到滚动的结果集scroll.last(); // 滚动到最后一行int totalCount = scroll.getRowNumber() + 1; // 得到滚动的记录数，即总记录数 HQL语句HQL（Hibernate Query Language）官方推荐使用的，功能强大首先补充知识：在设计数据表的时候，尽量保证每一张表都有一个主键，有利于对象的标识；如果表有主外键联系，先搞主表关于主从表的设计，前面说过，简洁表示就是：从表依赖于主表（一般在从表建立外键，依赖主表的主键），从表对象直接引用从表对象，主表使用 set 集合引用从表，因为主表可能对应多个从表HQL 语句的形式类似是：Select/update/delete…… from …… where …… group by …… having …… order by …… asc/desc 这样；另外就是 HQL 语句是区分大小写的，毕竟是面向对象的和传统 SQL 不同，使用 Hibernate 的时候，建议是把表中的所有数据都查回来 使用 uniqueResult 默认只查一条，也就是说找到一条就不会再找了，提高效率：session.createQuery(&quot;...&quot;).uniqueResult() ；返回的也就是单个对象而不是 List 了 大部分语句和 SQL 还是很相似的，举几个常用的：from Userdistinct from Userdelete Person as p where p.id=?from User where age between 12 and 16 // 范围查询from User where age in (12,13,14) ；in /not inselect avg(age),name from User group by namefrom Lolicon where loli.age=16 // 因为 HQL 是代指的对象，可以直接用 . 来调用，前面的语句条件就是 Lolicon 对象里引用的 loli 对象的 age 属性 如果是部分查询的话，就是不查所有字段，那么 Hibernate 不会给你封装成 Bean 对象的，返回的 List 是一个 Object[] 的 List；获取数据需要先通过 list.get(index) 获取 Object 数组，然后再从数组里取出值 特别注意：如果查询返回的数据只有一列 ，那么 List 中存的就不是 Object 数组了，是单个的 Object 对象 因为只有一列，List 中存的每一个对象表示一行数据，既然只有一列就不需要数组了 查询外的语句（增删改）需要使用 executeUpdate 方法才行 Hibernate 还支持把 HQL 语句配置到 xml 文件（一般是 hbm 配置文件）中去，使用 query 标签存储，获取的时候使用 getNamedQuery 方法获取，这样可以在不修改源码的情况下更改查询语句，实际中用的并不多 参数绑定在 JDBC 中都用过，使用参数绑定有三个很大的好处，1.可读性强 ；2.效率高 ；3.防止 SQL 注入 参数绑定可分为两种： 1234567891011121314151617181920212223242526// 参数绑定 1private static void parameterBind() &#123; Session session = HibernateUtil.openSession(); // :xxx 形式，xxx 是自定义的，一般和前面保持一致 Query query = session.createQuery("from User where id=:id") .setParameter("id",1); // query.executeUpdate(); 执行更新、删除操作 // 可以使用泛型，会被自动封装 List&lt;User&gt; list = query.list(); for (User user : list) &#123; System.out.println(user.getName() + "---" + user.getAge()); &#125;&#125;// 参数绑定 2private static void parameterBind2() &#123; Session session = HibernateUtil.openSession(); // 和 JDBC 中保存一致，使用 ？ 占位符 Query query = session.createQuery("from User where id=?") .setParameter(0,1); // 可以使用泛型，会被自动封装 List&lt;User&gt; list = query.list(); for (User user : list) &#123; System.out.println(user.getName() + "---" + user.getAge()); &#125;&#125; 总结来说就是 :key 和 ? 的区别，第一个设置参数只能使用 key，第二个只能使用索引 连接查询内连接：如果使用的对象中已经引用了其他对象（通过映射文件可以确定其表）是可以直接用的：from User u inner join u.boss左外连接和右外连接也是类似就不说了，左外连接就是始终显示左表的数据（Null 不会被过滤） 其他的，还有就是迫切内连接 ；他会把右表的数据填充到左表中去，使用 fetch 关键字：from User u inner join fetch u.boss迫切左外连接和右外连接也是类似，记住 右表填充到左表 就行了 其他用法关于 HQL 还有些神奇的用法，比如可以这样用：select new User(user.name,user.age) from User user但是这里有一个问题必须注意，那就是这时所返回的 User 对象，仅仅只是一个普通的 Java 对象而以，除了查询结果值之外，其它的属性值都为 null（包括主键值id），也就是说不能通过 Session 对象对此对象执行持久化的更新操作，并且需要有相应的构造函数 其实没必要这么麻烦更多关于 HQL 的内容参见：http://www.hongyanliren.com/1901.html 缓存Hibernate 使用到的缓存也就两级，Session 缓存也被称为是一级缓存，存在内存里除此外还有个二级缓存，存放于内存和硬盘之间 使用 load 查询只是在缓存里查，等使用的时候才去 DB 里找，并且会返回一份存在二级缓存中，如果再进行反复查询，到达一定频率后会被转移到一级缓存中去 关于二级缓存，可以在配置文件中配置是否开启，使用缓存能减少对数据库的访问次数 对象状态Hibernate 中的对象分为三种状态： 瞬时（transient）数据库中没有数据与之对应，超过作用域会被 JVM 回收；一般是 new 出来的，并且与 Session 没有关联的对象 持久（Persistent）数据库中有数据与之对应，与 Session 有关联，并且相关联的 Session 没有关闭，事务没有提交持久状态对象发生变化，在事务提交时会影响数据库中的数据（Hibernate 会自动检测到） 脱管/游离（detached）数据库中有数据与之对应，但是当前没有 Session 与之关联；当对象发生变化时，Hibernate 并不能检测到 比如，现在创建了一个 Domain 对象，那么就是处于瞬时状态，随时有可能消失；当调用了 Session 的 save 方法后，就变成了持久态，这时数据库已经有一些数据了；也就是说 save 以后还是可以更改数据的，最后事务提交的时候还是会反映到数据库中当调用了 commit、close 方法后就变成了游离态（commit 后一般会自动关闭连接），当数据改变后必须进行 update 才能反映到数据库中去 和线程的几种状态比较类似，可运行啊、阻塞啊 补充pojo前面说的 Javabean 也好、Domain对象也好都是一个东西，它其实还有个名字叫 pojo不过它比较特殊，它在 bean 的基础上有几个要求，我可能重复了一些： 和一张表对应 需要一个主键属性，用来标识一个 pojo 对象 除了主键属性，还有其他的属性 有一个空的构造方法 连接/会话如何判断目前开了几个数据库连接，可以看看进程，查查数据库端口的进程有几个，比如在 win 或者 linux 都可以使用命令：netstat -an 来判断 关于表设计这个我记得在前面是说过的，尤其注意下多对多的情况，实际中如果出现了多对多的情况，通常都会转换成两个一对多或者多对一的情况，也就是往两张表中间再加一个映射表，其实就是数据库设置中的那三范式；在 Hibernate 的 hbm 配置文件中可以看到相关的配置 多对一(或者一对多)和一对一的主要区别就是一对一的外键多了个唯一约束，并且外键列还多用于设置为主键（基于主键） 自动建表开始提到的有种开发方式是先建立 Domain 对象，然后 Hibernate 会自动帮你建表，这个需要在 cfg 配置文件中进行配置 1&lt;property name="hbm2ddl.auto"&gt;update&lt;/property&gt; create ：每次加载配置文件都会创建，如果存在就会先删除，然后重新创建 update ：如果表不存在就会创建；如果表存在，并且表结构没变化就保留，如果表结构有变化就更新 create-drop ：在显式关闭 SessionFactory 的时候会把表给删除，用的很少 validate ：每次在插入数据之前都会检查表结构是否一致，用的很少]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Hibernate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL报Invalid default value错误]]></title>
    <url>%2F2017%2F06%2F26%2FMySQL%E6%8A%A5Invalid-default-value%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[今天在还原数据库的时候给我抛了这样一个错误： Invalid default value for ‘upload_time’ 因为我是用 Navicat Premium 进行还原的，在我同学的 PC 上用 Navicat for MySQL 进行还原是可以的，所以我以为是版本的问题，后来换了 Navicat for MySQL 依然无果 还搜到了一堆说将 DATETIME 改为 TIMESTAMP；对于我来说这就是误导…. 原因最终发现是 MySQL 版本的问题，我所用的版本太高了，备份文件是在低版本中备份的，在高版本中（从 5.6.17 这个版本开始）就默认设置了不允许插入 0 日期了；术语是 NO_ZERO_IN_DATE 、 NO_ZERO_DATE 可以在 MySQL 的命令行中使用下面的命令进行查询：SHOW VARIABLES LIKE &#39;sql_mode&#39;;看看有木有 ：NO_ZERO_IN_DATE、NO_ZERO_DATE 这两个参数，这两个参数限制不能为 0 解决方案方案1这是我采用的，实测有效！在命令行或者 Navicat 下新建查询(必须是在 mysql 数据库下)： 12SET GLOBAL sql_mode = '';commit; 这就是临时取消全部了…然后再执行还原备份操作就 OK 了 方案2在 MySQL 的配置文件中的 [mysqld] 下面添加如下列：sql_mode=ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION 这样就永久生效了不过此方法我未测试，因为我也就是临时改改 参考http://blog.csdn.net/myboyliu2007/article/details/50583088http://www.youyong.top/article/1158cf96a49]]></content>
      <categories>
        <category>我是修电脑的</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Struts2学习笔记(二)]]></title>
    <url>%2F2017%2F06%2F16%2FStruts2%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[主要说了些拦截器相关的内容，还有大量的 OGNL 相关的东西，最后补充了个注解，不过看到有人说 sturt2 几乎现在很少用了，大多数公司都用 springMVC ，还得抓紧时间继续学呐还有一个模型驱动拦截器放在了笔记一中，因为这篇已经太长了…. 注入对象Action 既然只是普通的一个 java 类，如果想要获得 request 等内的数据应该怎么做的，这时候就用到反射技术来注入对象了；具体的注入实现是 ServletConfigInterceptor 这个拦截器，支持注入那些对象可以进这个的源码看看，就是一连串的 ifAction 中注入对象只需要实现特定的接口即可：实现 RequestAware 接口注入 request 中的 map 集合，大多数都是用的这个吧实现 ServletRequestAware 注入 request 对象实现 ApplicationAware 注入 Application 对象实现 ServletContextAware 注入 ServletContext 对象另外还有 session 的，等等…. 那么什么时候用注入对象的方式呢，ActionContext 和 ServletActionContext 都不能在构造器中初始化，因为拦截器是在实例化 Action 之后执行的，如果用的很频繁每个方法都来一遍太 low 了，于是可以使用这种注入的方式保存成全局变量在后期优化的时候对 BestAction 进行注入也是不错的选择 Token拦截器在表单防止重复提交中，这个是必用的吧；一般会用到两个域：一个是 page ，一个是 session，对应客户端和服务端吧相比 Struts1 在 Struts2 中使用更加的简单：首先在 JSP 页面添加 Token 标签，它会在被访问的时候创建令牌，是在 form 里面哦，另外最后也加个显示错误的标签，因为抛的错误是 Action 错误，所以… 1234567891011&lt;body&gt; &lt;s:debug /&gt; 这是一个测试页面 &lt;br&gt; &lt;%--显示 Token 认证错误信息--%&gt; &lt;s:actionerror/&gt; &lt;br&gt; &lt;s:form namespace="/reg" action="TokenAction_reg" method="POST"&gt; &lt;s:token/&gt; &lt;s:textfield name="name" label="用户名"/&gt; &lt;s:submit value="提交"/&gt; &lt;/s:form&gt;&lt;/body&gt; 其次配置 Action 的时候需要指定下栈排除一些不需要校验的方法，然后添加个校验失败跳转的页面 12345678910111213&lt;action name="TokenAction_*" class="com.bfchengnuo.web.action.TokenAction" method="&#123;1&#125;"&gt; &lt;result name="success"&gt;/reg/success.jsp&lt;/result&gt; &lt;result name="regView"&gt;/reg/tokenReg.jsp&lt;/result&gt; &lt;!--指定 Token 验证失败后怎么办--&gt; &lt;result name="invalid.token"&gt;/reg/tokenReg.jsp&lt;/result&gt; &lt;!--指定拦截器栈，这样会覆盖默认栈，所以需要手动再添加一次--&gt; &lt;interceptor-ref name="token"&gt; &lt;!--排除方法--&gt; &lt;param name="excludeMethods"&gt;toReg&lt;/param&gt; &lt;/interceptor-ref&gt; &lt;interceptor-ref name="defaultStack"/&gt;&lt;/action&gt; 嗯….基本就是这样了，另外；如果想要替换错误信息为中文，可以在当前目录建立 Action 相关文件，修改 struts.messages.invalid.token 为你想要的值即可 看网上的帖子说还需要在 struts.xml 中修改 struts.custom.i18n.resources 常量；但是我测试不用修改也完全有效，版本好像是 2.3 execAndWait拦截器常叫做为执行等待拦截器，从名字也可以看出了，就是在执行耗时操作的时候跳转到一个预设的界面，等耗时操作结束后再跳回；这个过程是个伪异步，因为在等待页面是通过定义 meta 的方式进行定时刷新检测的，下面来简单的模拟下，首先是 Action ，非常简单就是让线程休眠下… 123456789101112public class WaitAction &#123; // 模拟耗时操作 public String execute() &#123; try &#123; Thread.sleep(8000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return "success"; &#125;&#125; 然后就是配套的配置文件了，execAndWait 是不在默认栈里的，以及需要配一个等待页面 wait： 1234567&lt;action name="WaitAction" class="com.bfchengnuo.web.action.WaitAction"&gt; &lt;result&gt;/reg/success.jsp&lt;/result&gt; &lt;result name="wait"&gt;/wait.jsp&lt;/result&gt; &lt;!--执行等待拦截器不在默认栈中，一般添加到最后就行--&gt; &lt;interceptor-ref name="defaultStack"/&gt; &lt;interceptor-ref name="execAndWait"/&gt;&lt;/action&gt; 那个等待页面和成功页面就不写了，就是很简单的一句话而已，这样就能看出效果了 自定义拦截器搞一个简单的拦截器测试看看，自定义的拦截器要实现 Interceptor 接口，注意导包： 123456789101112131415161718192021222324252627public class LoginInterceptor implements Interceptor &#123; // 用来反序列化的，写不写无所谓 private static final long serialVersionUID = 5175604343895716587L; @Override public void destroy() &#123;&#125; @Override public void init() &#123;&#125; @Override public String intercept(ActionInvocation actionInvocation) throws Exception &#123; Object action = actionInvocation.getAction(); if (action instanceof LoginAction) &#123; // 直接放行 return actionInvocation.invoke(); &#125; Map&lt;String, Object&gt; sessionMap = actionInvocation.getInvocationContext().getSession(); if (sessionMap.get("username") == null) &#123; // 没有登陆，跳转到登陆 return "login"; &#125; else &#123; return actionInvocation.invoke(); &#125; &#125;&#125; 看以看出，逻辑非常简单，判断是否是 LoginAction 如果是就直接放行了，如果不是就判断 session 是否存有用户信息，没有就跳转到登陆界面上去，但是总感觉这样写不是很优雅呢……嘛~先这样吧优雅一点的可以通过 ActionInvocation 对象获取 Action 的代理对象（ac.getProxy()），然后通过代理对象可以拿到当前执行 Action 的方法名等信息，然后进行相应的拦截拦截器的 init 方法是在服务器启动的时候就会执行的（过滤器也是如此），并且一般只执行一次，而 intercept 方法会执行多次；如果你不需要初始化或清理代码，可以继承自扩展的 AbstractInterceptor 类，它提供了一个对 init() 和 destroy() 方法的默认的无操作实现。接下来是测试 Action 了 12345678910111213141516171819202122232425262728293031323334public class LoginAction extends ActionSupport implements SessionAware &#123; private static final long serialVersionUID = -9178854276859257157L; private String name; // 实现了 SessionAware 接口，以此来获得 session 中的 map private Map&lt;String,Object&gt; sessionMap; @Override public void setSession(Map&lt;String, Object&gt; map) &#123; sessionMap = map; &#125; // 简单模拟，只要用户名对就通过 public String reg() &#123; if ("admin".equalsIgnoreCase(name)) &#123; sessionMap.put("username", name); &#125;else &#123; return LOGIN; &#125; return SUCCESS; &#125; public String toView() &#123; return "toLoginView"; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 与之相关的配置文件： 123456789101112131415161718192021222324252627&lt;package name="Login" extends="struts-default" namespace="/login"&gt; &lt;!--配置自定义拦截器--&gt; &lt;interceptors&gt; &lt;!--注册拦截器--&gt; &lt;interceptor name="loginInterceptor" class="com.bfchengnuo.interceptor.LoginInterceptor" /&gt; &lt;!--配置自定义拦截器栈--&gt; &lt;interceptor-stack name="loginStack"&gt; &lt;interceptor-ref name="loginInterceptor"/&gt; &lt;interceptor-ref name="defaultStack"/&gt; &lt;/interceptor-stack&gt; &lt;/interceptors&gt; &lt;!--可以定义默认的栈（所有的 Action 都执行这个默认的栈）--&gt; &lt;default-interceptor-ref name="loginStack"/&gt; &lt;!--定义全局的返回结果--&gt; &lt;!--&lt;global-results&gt;--&gt; &lt;!--&lt;result name="login"&gt;/login.jsp&lt;/result&gt;--&gt; &lt;!--&lt;/global-results&gt;--&gt; &lt;action name="Login_*" class="com.bfchengnuo.interceptor.LoginAction" method="&#123;1&#125;"&gt; &lt;result&gt;/reg/success.jsp&lt;/result&gt; &lt;result name="login"&gt;/login.jsp&lt;/result&gt; &lt;result name="toLoginView"&gt;/login.jsp&lt;/result&gt; &lt;/action&gt;&lt;/package&gt; 嗯….这样应该就差不多了吧….需要注意的是：配置自定义拦截器（interceptors）要放在 package 中的最前面，如果使用 default-interceptor-ref 那么所有的 Action 都会执行其中定义的栈，如果只需要特定的 Action 执行自定义的栈，可以在相应的 Action 中进行配置，这样其他的 Action 就还是执行默认的栈。 OGNL表达式OGNL 也是一个开源的项目，Struts2 融合了 OGNL 并且将它作为默认的表达式语言，它必须依赖于 Struts 的标签才能使用OGNL 由表达式语言和类型转换器组成，就是为了方便数据的访问；总体来说和 EL 表达式是很像的，并且兼容 EL 表达式，大多数表达式不需要进行转义，如果用到了使用 %{xxx} 进行转义，也叫做强制表达式解析（在不自动进行 OGNL 解析的地方强制解析），多用于非 String 对象举个简单使用的栗子，首先是主要的 JSP 页面部分： 12345678910111213&lt;body&gt; &lt;s:form namespace="/ognl" action="OgnlAction" method="POST"&gt; &lt;s:textfield name="names[0]" label="names[0]"/&gt; &lt;s:textfield name="names[1]" label="names[1]"/&gt; &lt;%--&lt;s:textfield name="names" label="names"/&gt;--%&gt; &lt;s:textfield name="nameList[0].name" label="nameList-name"/&gt; &lt;s:textfield name="nameList[0].age" label="nameList-age"/&gt; &lt;s:textfield name="user" label="user-conver"/&gt; &lt;s:textfield name="nameMap.key.name" label="Map-name"/&gt; &lt;s:submit/&gt; &lt;/s:form&gt;&lt;/body&gt; 在测试的时候还掉进了一个坑，就上面所示： 如果直接写 names 默认会存一个值（数组大小也被置为1），但是不要 names 和 names[0] 同时出现，这样会被覆盖，当时就被坑了好久…..竟然犯这样的低级错误数组、集合都可以和 EL 一样使用，属性链也 OK；当然 map 也是可以的：map 的 key 如果是字符串可以直接用 . 连接：map.key.name ；否则就使用 map[&#39;key&#39;].name 这样的形式我是把把表单提交后的页面再指向回此 JSP ，这样数据会回显，以此来观察效果 相关的 Action 没啥好说的，就是定义上面所述的属性，List、Map 可以不指定泛型，在 OgnlAction-conversion.properties Action 相关文件中指定，如果指定了泛型，那么就会自动进行转换的，转换并不只是八种基本类型，还要强大些，关于 conversion 文件，这是指定前台提交的数据如何进行转换的，毕竟前台提交的都是 String： 123456789# 指定 List 内装的是什么类型Element_nameList=com.bfchengnuo.domain.UserElement_nameMap=com.bfchengnuo.domain.User# 还可以定义 Map 指定类型的 key#Key_nameMap=java.lang.Integer# 指定转换器user=com.bfchengnuo.ognl.UserConverter 通过这个文件就可以确定前面表单中的 nameList[0].name 指的就是 List 中第一个 user 对象中的 name 属性；表单中还有一个 name 叫 user 的，这个是我编写的一个简单的 javabean，正常来说需要这样写：user.name 才能正确的赋值，总不能把一个字符串赋值给一个 user 对象吧，如果硬要这样那就必须定义一个转换器了，就是上面 properties 文件指定的那个类 1234567891011121314151617181920212223public class UserConverter extends StrutsTypeConverter &#123; // 字符串转成对象，客户端提交时 @Override public Object convertFromString(Map map, String[] strings, Class aClass) &#123; if (strings != null &amp;&amp; strings.length &gt; 0) &#123; String[] ss = strings[0].split("-"); if (ss.length &gt; 0) &#123; User user = new User(); user.setName(ss[0]); user.setAge(ss[1]); return user; &#125; &#125; return null; &#125; // 对象转成对象，服务器返回时 @Override public String convertToString(Map map, Object o) &#123; // 直接调用自定义的 tostring 方法 return o.toString(); &#125;&#125; 定义转换器需要继承 StrutsTypeConverter ，复写两个方法，一个是用于客户端提交数据时转换，一个是服务器返回数据回显时转换；它和值栈挨着，提交数据时经过转换后存到值栈，回显数据时经过转换输出到客户端 对于转换器来说，每个 Action 都搞一个太累，可以在 src 根目录下定义 xwork-conversion.property 全局的类型转换器 OGNL中重要的3个符号#、% 和 $ 符号在 OGNL 表达式中经常出现，他们的意义嘛 #它的用途一般有三种，最常用的就是用来访问非根对象属性 ，比如：#session/#request/#application 等；# 相当于 ActionContext.getContext() 默认会自动搜索各个域第二种是用于过滤和投影（projecting）集合第三种是用来构造集合，Map 和 List ：#{key1:val1,key2:val2}/{1,2,3} %% 符号的用途是在标志的属性为字符串类型时（或者说值类型的标签中），计算 OGNL 表达式的值，相当于提供了一个 OGNL 的解析环境，就是所谓的强制（暴力）解析了；如果本来默认解析 OGNL 的标签（对象类型标签）想让其单纯的输出值，就需要在双引号里加一层单引号 $主要用途有两个：在国际化资源文件中，引用 OGNL 表达式；在 Struts2 配置文件中，引用 OGNL 表达式比如在使用重定向时，传参只能通过 url；通过这个就可以获得 Action 中的数据 关于值栈OGNL 和值栈有密切的关系，既然用户要获取一些必要的数据（JSP 中）那么怎么才能把 Action 中的数据传给用户呢，就是用的值栈Action 创建的同时也会创建值栈以及 ActionContext，然后会把 Action 对象封装进值栈中（在根元素），通过 Request 传给用户（key 为 struts.ValueStak）；获取值栈可以通过 ActionContext，也可以通过 Request值栈当中维护了一个 OgnlContext 对象（request、session 等对象的数据就存在这里，Action 对象在根元素），就是 OGNL 表达式所用的，其实是一个 Map从上面可以看出，Action 是放进了值栈的根节点，那么 Action 中的属性也毫无疑问的放在了根节点，而 OGNL 表达式取根节点的值是不需要加 # 符号的，所以可以使用 # + 属性名 直接取 OGNL标签如同 JSTL ，OGNL 也自带了一些标签供使用；首先还要明确一点：struts2 接收到请求后马上创建一个 ActionContext，一个 ValueStak 和一个 Action 对象；Action 对象立即放到值栈上便于 OGNL 的访问顶层对象会覆盖底层对象，也就说从上往下找，找到了就不会继续往下了默认是从栈顶的对象里寻找，是栈顶对象！所以，一般是 Action 在栈顶；但是如果 user 对象在栈顶（前面所说的模型驱动），可以直接用其属性OGNL 不仅可以访问 bean 的属性，还可以访问方法，但是不建议用 Struts2 的标签库都是使用 OGNL 表达式来访问 ActionContext 中的对象数据的；并且将 ActionContext 设置为 OGNL 上下文，默认的根对象为值栈 s:property首先是有关获取数据的几个标签，也就 ActionContext 中的六个“域”，我事先在 Action 初始化了一波数据，后面会用到 12345678910111213141516171819202122232425262728public String execute() &#123; initData(); return SUCCESS;&#125;private void initData() &#123; // 往各个域中存储数据 ServletActionContext.getRequest().setAttribute("name","request-name"); if (ServletActionContext.getRequest().getSession() == null) &#123; System.out.println("session--Null"); &#125; ServletActionContext.getRequest().getSession().setAttribute("name","session-name"); ServletActionContext.getServletContext().setAttribute("name","application-name"); ServletActionContext.getContext().getValueStack().set("name","vs-name"); popList();&#125;// 填充 Listprivate void popList() &#123; nameList = new ArrayList(); for (int i = 0; i &lt; 10; i++) &#123; User user = new User(); user.setName("Loli-No" + i); user.setAge((i+8) + ""); nameList.add(user); &#125;&#125; 然后在 JSP 页面中直接获取就可以了 12345678910111213141516&lt;body&gt; &lt;s:debug/&gt; &lt;br&gt; request.name:&lt;s:property value="#request.name"/&gt; &lt;br&gt; session.name:&lt;s:property value="#session.name"/&gt; &lt;br&gt; application.name:&lt;s:property value="#application.name"/&gt; &lt;br&gt; &lt;%-- 从小到大依次搜索 --%&gt; attr.name:&lt;s:property value="#attr.name"/&gt; &lt;br&gt; parameters.name:&lt;s:property value="#parameters.name"/&gt; &lt;br&gt; &lt;%-- 值栈 --%&gt; vs.name:&lt;s:property value="name"/&gt; &lt;br&gt; 强制表达式解析： &lt;s:textfield label="%&#123;#request.name&#125;"/&gt; &lt;br&gt; &lt;%-- 使用单引号输出常量 --%&gt; 输出常量：&lt;s:property value="'is constant'"/&gt; &lt;br&gt; &lt;%-- 关闭自动转义 --%&gt; &lt;s:property value="'&lt;h1&gt;取消了转义&lt;/h1&gt;'" escape="false"/&gt; &lt;br&gt;&lt;/body&gt; 强制表达式解析前面已经提过了，就是用在默认不会进行 OGNL 解析的地方，让其强制进行解析；还有那个输出常量，加单引号，就是不让其进行 OGNL 解析，但是内容会被 Html 编码，这里我就被坑了…..请记得：在默认进行解析的属性中，如果不使用 OGNL 的时候记得加单引号！ s:set它是用来存储数据的，并且可以存储到指定的域中；如果没有指定范围则默认保存在 ActionContext 的大 Map 中，可以使用 #name 来获取，也可以不加 # ，那样就会先从值栈中搜寻，找不到了再去大 Map 中去找 12&lt;s:set var="s_setName" value="'loli'"/&gt;&lt;s:property value="#s_setName"/&gt; &lt;br&gt; 无论是从那个域取数据，如果找不到最终都会到这个大 Map 自身中去找,当然还可以存取 List：&lt;s:set name=&quot;miloList&quot; value=&quot;{‘java’,’php’,’C#’}&quot;/&gt;# 可以理解为是 ActionContext.getContext() s:push同样是用来存储数据的，将对象放到值栈的栈顶，标签结束后会自动删除，不过不能指定存储位置，必须是值栈 123&lt;s:push value="'Lolicon'"&gt; &lt;s:property/&gt;&lt;/s:push&gt; property 不指定 value 的话会自动获取栈顶的对象（值栈），并且会调用其的 toString 方法；另外，不使用 OGNL 的话千万记得加单引号！ s:bean创建新的 javabean 到栈顶(说的当然是值栈)；如果指定了 var 属性，同时还会保存引用到 ActionContext 中，如果不指定，在标签结束后从值栈移除引用后就无法取得数据了 1234567&lt;s:bean name="com.bfchengnuo.domain.User" var="mybean"&gt; &lt;%-- 属性赋值 --%&gt; &lt;s:param name="name" value="'小奏'"/&gt; &lt;s:property value="name"/&gt;&lt;/s:bean&gt;&lt;br&gt;存储的 name 为（ActionContext 中）：&lt;s:property value="#mybean.name"/&gt; 在 s:bean 标签中的 property 可以直接写 属性名 获取数据…. s:action作用就是在 JSP 中直接调用某个 Action，executeResult 属性可以指定是否把页面包含进来 12345&lt;s:action name="HelloWorldAction" namespace="/one" executeResult="true"&gt; &lt;s:param name="user.name" value="'Lolicon'"/&gt; &lt;s:param name="name" value="'Loli'"/&gt; &lt;s:param name="age" value="'14'"/&gt;&lt;/s:action&gt; 在调用的时候我还传了几个参数过去…..在调用 Action 的时候是在一个线程中，也是一个栈中，后来的 Action 在栈顶 （HelloWorldAction） s:iterator迭代器….已经很熟了吧，在 JSTL、Struts1 中都见过，就是用来迭代集合的，包括 Map；Struts2 中的迭代器也很好用，非常强大每次迭代都会把数据放在栈顶 12345678910111213141516171819&lt;table border="0" cellspacing="1" bgcolor="#db7093"&gt; &lt;tr&gt; &lt;td&gt;姓名&lt;/td&gt; &lt;td&gt;年龄&lt;/td&gt; &lt;td&gt;编号&lt;/td&gt; &lt;td&gt;索引&lt;/td&gt; &lt;td&gt;是否偶数&lt;/td&gt; &lt;/tr&gt; &lt;s:iterator value="nameList" status="st"&gt; &lt;tr class="&lt;s:property value="#st.even?'even':'odd'"/&gt;"&gt; &lt;td&gt;&lt;s:property value="name"/&gt;&lt;/td&gt; &lt;td&gt;&lt;s:property value="age"/&gt;&lt;/td&gt; &lt;td&gt;&lt;s:property value="#st.count"/&gt;&lt;/td&gt; &lt;td&gt;&lt;s:property value="#st.index"/&gt;&lt;/td&gt; &lt;td&gt;&lt;s:property value="#st.even"/&gt;&lt;/td&gt; &lt;%-- 状态还可以是 isFirst isLast 等 --%&gt; &lt;/tr&gt; &lt;/s:iterator&gt;&lt;/table&gt; status 这个对象确实能省不少事呢 s:if/s:elseif/s:else这个标签就爽了，相比 JSTL 里的简直太爽，可想像在写 java 一样 12345&lt;s:set var="age" value="14"/&gt;age:&lt;s:property value="#age"/&gt; &lt;br&gt;&lt;s:if test="#age &lt; 10"&gt;小于10&lt;/s:if&gt;&lt;s:elseif test="#age &gt; 20"&gt;大于20&lt;/s:elseif&gt;&lt;s:else&gt;10-20&lt;/s:else&gt; 以后做逻辑判断就简单多了需要注意的是，第一行代码的 set 没有用把数字放单引号里，这说明它是一个整型，在使用的时候记得加 # s:url在前面的标签库里也见过，就是用来构件 url 的，避免重复写那繁琐的代码，同样；如果加 var 属性就会存到大 Map 中去；最简单的：&lt;s:url /&gt; 表示当前的地址 123456 当前地址（带参数）：&lt;s:url includeParams="all"/&gt; &lt;br&gt;&lt;s:url action="WaitAction" namespace="/reg" var="waitUrl"&gt; &lt;%-- 如果有参数的话可以这样赋值 --%&gt; &lt;%--&lt;s:param name="key" value="lalala"/&gt;--%&gt;&lt;/s:url&gt;&lt;a href="&lt;s:property value="#waitUrl"/&gt;"&gt;跳转等待页&lt;/a&gt; 除了访问 Action 可以直接在 value 属性里写 http 地址 其他在迭代的时候，可以快速的定义集合，使用 {} 123&lt;s:iterator value="&#123;'loli','2','ll','gt'&#125;"&gt; &lt;s:property /&gt; &lt;br&gt;&lt;/s:iterator&gt; 对于 Map，也差不多，不过需要加 # ，也就是这样：#{&#39;key&#39;:&#39;val&#39;,&#39;key2&#39;:&#39;val&#39;}使用 property 标签取的时候直接 value=key 、 value=value 这样就行了，因为放在栈顶的就是 Entry 啊，其中就有 getKey、getValue 这样的方法啊 为了证明可以调用方法，可以这样调用看看： 123nameList.size: &lt;s:property value="nameList.size"/&gt; &lt;br&gt;nameList.isEmpty: &lt;s:property value="nameList.isEmpty"/&gt; &lt;br&gt;nameList.isEmpty: &lt;s:property value="methodName('para')"/&gt; 如果需要调用静态内容，需要指定包名，类似：@java.util.Locale@CHINA调用静态对象的有参方法：@java.util.Locale@CHINA.getDisplayName(@java.util.locale@CHINA)注意：静态方法使用 @ 连接，如果非静态用 . 就可以了，上面栗子是调用的静态对象 CHINA 中的非静态方法 getDisplayName由于 Math 比较常用，默认是可以省略的，比如：@@floor(10.09) 除此之外，还有一些 UI 标签，比如上面用到的 &lt;s:form action=&#39;&#39;&gt; ，总的来说其实和 html 标签并没有什么区别，并且应该是更方便的，上面的这个 form 标签直接在 action 中填 /actionName 就好了，不需要那一串了但是在使用 s:a 标签的时候地址写上 / 反而不正确了，而上面的 form 标签就不会；至于到底加不加 / 最好的方法就是试一下了…..emmmm使用这些标签的时候总是会带一些比较迷的样式（虽然可以设置 theme 属性为 simple），所以一般情况下这类标签是不用的，除非用到了其特别的功能 &lt;s:textfield name=&#39;&#39; /&gt; 标签会自动回显数据，也就是会自动根据 name 属性来从值栈（只有根元素中才是栈，其他的域都是在 Map 中）获取相应的值，某些时候，只需要在值栈中 push 相应的数据就可以了 有一点还是在这里写下吧，大概：OGNL 中还可以使用类似 [1].user.name 来获取数据，意思就是栈中第二个对象的 user 对象的 name 属性 OGNL过滤和投影不想说太多，因为我还没看到…. ?：选择满足条件的所有元素 ^：选择满足条件的第一个元素 $：选择满足条件的最后一个元素 语法为：collection.{?expression} 或 collection.{^expression} 或 collection.{$expression}；栗子大概是这样的： 123&lt;s:property value="array.&#123;?#this &gt; 5&#125;"/&gt;&lt;s:property value="array.&#123;^#this &gt; 5&#125;"/&gt;&lt;s:property value="array.&#123;$#this &gt; 5&#125;"/&gt; 如果把集合中的数据想象成是数据库表中的数据，投影简单说就是从这表中选取某一列所构成的一个新的集合，感觉和 VO 差不多；投影的语法：collection.{expression} ；比如： &lt;s:property value=&quot;personList.{name}&quot;/&gt; 多数情况下，过滤和投影是联合起来用的，使用起来还算简单： 123&lt;s:property value="personList.&#123;?#this.sex.equals('female')&#125;.&#123;name&#125;"/&gt;&lt;s:property value="personList.&#123;^#this.sex.equals('female')&#125;.&#123;name&#125;"/&gt;&lt;s:property value="personList.&#123;$#this.sex.equals('female')&#125;.&#123;name&#125;"/&gt; 使用注解如果厌倦了在配置文件中写一大堆的配置，可以选择使用注解，当然有利有弊，在前面的文章已经说过，下面来看看常用的注解：另：使用注解需要 struts2-convention-plugin 的 jar 包 123456789101112131415161718192021222324252627@ParentPackage("struts-default")@Namespace("/test")@Results( &#123; @Result(name = "success", location = "/main.jsp"), @Result(name = "error", location = "/error.jsp") &#125;)@ExceptionMappings( &#123; @ExceptionMapping(exception = "java.lange.RuntimeException", result = "error") &#125;)public class LoginAction extends ActionSupport&#123; ... @Action("login1") public String test1() throws Exception&#123;...&#125; @Action( //表示请求的Action及处理方法 value="login2", //表示action的请求名称 results=&#123; //表示结果跳转 @Result(name="success",location="/success.jsp",type="redirect"), @Result(name="login",location="/login.jsp",type="redirect"), @Result(name="error",location="/error.jsp",type="redirect") &#125;, interceptorRefs=&#123; //表示拦截器引用 @InterceptorRef("defaultStack"), @InterceptorRef("timer") &#125;, exceptionMappings=&#123; //映射映射声明 @ExceptionMapping(exception="java.lang.Exception",result="error") &#125; ) public String test1() throws Exception&#123;...&#125;&#125; 大体就是这样使用了….. @Action(url) 中的url，如果以 / 开头那是绝对路径，访问的时候不需要加命名空间，否则就要加命名空间，应该是吧，未测试 常用的注解： Namespace：指定命名空间。 ParentPackage：指定父包。 Result：提供了Action结果的映射。（一个结果的映射） Results：“Result”注解列表 ResultPath：指定结果页面的基路径。 Action：指定Action的访问URL。 Actions：“Action”注解列表。 ExceptionMapping：指定异常映射。（映射一个声明异常） ExceptionMappings：一级声明异常的数组。 InterceptorRef：拦截器引用。 InterceptorRefs：拦截器引用组。 其他的一些用到再补充吧，比如那些拦截注解 Before 、After 之类的，类型转换注解啊。。。总之；注解能做很多事 其他补充当我们需要把返回的内容转成 JSON 时，可以使用 Struts 提供的插件完成，导包就不说了（struts2-json-plugin）配置文件中的 package 继承 json-default，返回值的 type 填 json ；在 Action 中对应的方法中，把相应的数据保存到集合里，通常使用 List，这个集合要是全局的才行，因为要提供 getter/setter 方法，这样 Struts 就会自动进行处理了 参考http://wiki.jikexueyuan.com/project/struts-2/annotations-types.htmlhttp://www.blogjava.net/fancydeepin/archive/2014/03/17/struts-ognl.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Struts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Struts2学习笔记]]></title>
    <url>%2F2017%2F06%2F07%2FStruts2%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这两天看 Struts 真是看的一脸懵逼，一方面不禁感叹牛人的思想就是超前啊，这篇笔记写的很急，其中的理解可能有些错误，想以后有了深入认识再回来修正的…..不禁感叹 Struts2 与 Struts1 真是天差地别啊…. webwork 真厉害呢… Struts1和Struts2这个网上随便一搜就有N多的文章，分析的都挺详细的，我呢就说说主要的（我知道的）几点关于 Struts1 我也写了一篇，不过没放在博客上，因为毕竟现在基本已经没人用了啊…..地址在这：Github Struts1-基于Servlet为什么会被淘汰，这当然是因为有缺点的，抛开它奇葩的命名不说，重要的是架构不是很好，体现在下面的几点 ActionServlet 的任务过于集中，压力大，不容易扩展和维护它主要做的有：处理请求、实例化 formBean，往里封装数据、然后根据配置文件决定是否进行校验，对结果进行处理，最终把结果封装到 request 中、实例化相应的 Action ，调用相应的方法，传递相关的对象、对 Action 返回的结果进行处理,如转发到指定页面…..;如此多的功能集中在一起必然扩展性很差，也容易出问题 Action 是单例的，线程不安全的Action 被创建出来后会一直存在，并且只存在一个，就像 Servlet，一个 Action 处理所有的请求（数据不安全，不要在 Action 中声明实例变量） ActionForm 造成类的爆炸因为一个表单就对应一个，虽然可以使用动态 formbean ；一个用户（请求）可能对应好几个…因为 formBean 是随 Action 存在的，一个 Action 可能会处理多个表单 耦合性高从 execute(ActionMapping mapping,ActionForm form,HttpServletRequest request, HttpServletResponse response) 这个从方法的参数就可以看出，需要传入四个参数呐！ Struts2-基于Filter新一代，然而和一代并没多大关系，倒是和 webwork 的关系挺大 分离关注思想和 aop 挺像，嗯？反正就是不像一代那样集中在一个组件上了；将 web 开发中的常规任务剥离开来，分别交给不同的组件（拦截器）进行处理，比如：文件上传、表单的处理、国际化、参数传递、类型转换等等 Action 是原型、独占的，不共享意思就是说一个请求对应一个 Action，所以可以存放客户端的状态信息了！ 取消了 ActionForm ，使用 pojo（action、javabean）接收数据 松耦合，可维护性高action 就是个普通的 pojo 也就是普通的 javabean，也就是说 struts2 api 和 原生的 servlet api 关系不大【非侵入性，不用实现你的接口也不用继承你的类，这是理想条件下】 Struts2的使用搭建就不说了，就是拷进去相应的 jar包，创建的 action 不需要继承任何类或者实现任何的接口下面说下配置，Struts2 是基于过滤器的，所以需要在 web.xml 文件里配一个过滤所有请求的过滤器，我用 IDEA 建了个项目，发现都给配好了 12345678&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 先来写个简单的 Action，相比 Struts1 就简单多了，不需要继承任何类，方法名默认还是用 execute （应该是 webWork 规定的） 1234567891011public class HelloWorld &#123; public HelloWorld() &#123; // 每一次请求都会实例化哦 System.out.println("实例化啦！！"); &#125; public String execute() &#123; System.out.println("Hello World"); return null; // 不进行任何跳转；返回值也叫做路由串 &#125;&#125; 然后是 Struts 的配置文件，默认在 src 下名字为 struts.xml ；至于这个文件怎么写，不知道，那就抄吧，在 Struts 的核心包里有相关的配置文件，照着写写就差不多（struts2-core.jar 下的 struts-default.xml 有返回值等大部分配置；struts2-core.jar/org.apache.struts2/default.properties 文件下有绝大部分的变量说明） 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE struts PUBLIC "-//Apache Software Foundation//DTD Struts Configuration 2.3//EN" "http://struts.apache.org/dtds/struts-2.3.dtd"&gt;&lt;struts&gt; &lt;!-- 配置常量，这个是设置请求后缀的，默认是 .action ；最后的 “,,” 表示结尾空也可以 --&gt; &lt;constant name="struts.action.extension" value="do,,"/&gt; &lt;!-- 配置开发模式，可以自动重新加载国际化资源文件和配置文件 --&gt; &lt;constant name="struts.devMode" value="true"/&gt; &lt;!--可以继承，它配置了很多默认值，如 type 等--&gt; &lt;package name="HelloWorld" namespace="/one" extends="struts-default"&gt; &lt;!--配置默认的 action 也就是在当前的命名空间下，如果找不到 action 的时候访问这个--&gt; &lt;default-action-ref name="HelloWorldAction"/&gt; &lt;!--设置默认的 class 引用，也就是 action 可以不用配 class 了--&gt; &lt;default-class-ref class="com.bfchengnuo.web.action.HelloWorld"/&gt; &lt;action name="HelloWorldAction" class="com.bfchengnuo.web.action.HelloWorld"&gt; &lt;!--配置返回值相关，name 就是返回值，如果是 success 可以省略--&gt; &lt;!--父类中指定了默认的 type 为 dispatcher 也就是转发--&gt; &lt;result name="success"&gt;/index.jsp&lt;/result&gt; &lt;/action&gt; &lt;action name="ActionNoClass"&gt; &lt;result&gt;/index.jsp&lt;/result&gt; &lt;/action&gt; &lt;!--使用通配符来实现动态方法的调用；避免使用 ！--&gt; &lt;action name="HWAction_*" class="com.bfchengnuo.web.action.HelloWorld" method="&#123;1&#125;"&gt; &lt;!--每一个返回值对应一个方法--&gt; &lt;result name="success"&gt;/index.jsp&lt;/result&gt; &lt;result name="save"&gt;/index.jsp&lt;/result&gt; &lt;result name="test"&gt;/index.jsp&lt;/result&gt; &lt;/action&gt; &lt;/package&gt;&lt;/struts&gt; 关于 namespace 命名空间的作用，就是在输入网址的时候用的，比如： http://xxx:8080/webAppName/namesPace/Action最后配置的那个 action 是动态调用，它对应多个返回值，也就是对应多个方法，可以接受很多的请求，并且使用了通配符，通配符匹配到的会传给后面的 {1} 中，比如这样用：http://xxx:8080/webAppName/namesPace/HWAction_save ；下划线后面跟的是方法名，如果省略将会匹配默认的 execute 方法 对于多个方法的访问，除了写多个 action 外 还可以使用这样的地址访问：http://xxx:8080/webAppName/namesPace/Action!methodName 这样的话只需要在 method 里指明方法名就行了，不过用叹号分割总感觉怪怪的，并且在 Struts 的配置文件中开启动态方法调用才会生效。所以还是像上面那样配置吧 上面提到了可以不配 class 走默认设置的全局 class，这个具体是应用在请求 WEB-INF 目录下的文件的，因为默认的 class 是 ActionSupport ，它实现了 Action 的接口，所有有一个 execute 方法，这个方法返回字符串 success，所以说只需要配置：&lt;result name=&quot;success&quot;&gt;/WEB-INF/index.jsp&lt;/result&gt; 就可以实现请求保护目录下的文件，而不需要写相应的 Action 获取参数url 可以传参，但是 action 中并没有继承任何类，如何获取 request 等对象呢，是通过 ServletActionContext 的静态方法获取的；但是获取 url 中的值并不需要这么麻烦，前面说过，action 其实就是一个 javabean，那么就可以设置属性，属性名对应传进来的参数名，调用的时候会自动用反射技术进行设置的；所以说在方法中直接用就行了如果需要数据回显，还是用标签库（其实是从值栈中获取）： 123456789101112&lt;%@ page contentType="text/html;charset=UTF-8" language="java" %&gt;&lt;%@taglib prefix="s" uri="/struts-tags" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;测试页&lt;/title&gt;&lt;/head&gt;&lt;body&gt; 这是一个测试页面 name:&lt;s:property value="name"/&gt; &lt;br&gt; age:&lt;s:property value="age"/&gt; &lt;br&gt;&lt;/body&gt;&lt;/html&gt; 再来说说 ServletActionContext ，它可以获取到绝大数需要的对象，它其实是 ActionContext 的孩子，下面会详细说 数据处理我们知道开发的 web 应用中存储数据有四个域，从小到大依次是：page 、 request 、 session 、 application 在 struts 有个比较常用的是 ActionContext 说到 Context 也就是上下文，指的其实就是环境，也就是当前的环境，同时还可以说是数据中心在 ActionContext 中封装了其他域的引用，具体有： request session application valuestack(值栈)值栈肯定是个栈，然而其实是通过 arrayList 进行模拟的；值栈其实存在于 request 一个特殊的属性 struts.ValueStak 独立出来是为了方便s:property 标签默认就是去值栈里进行寻找 parameters attr会自动从 session/request/application 域中去寻找 ActionContext 于内部的六个“域”是双向引用！ActionContext 属于 threadlocal 俗称线程本地化；ServletActionContext 是 ActionContext 的子类，进行了一些简单的封装；所以有了很多的 get 的方法获取到相应的对象通常还是推荐尽量使用 ActionContext 来获取数据，因为获取的是 Map 集合，不需要加入 servlet API 的 jar 包，也就是说解耦了；但是有些功能实现不了的话还是需要用 servlet 中的对象（比如获取路径）另外，ActionContext 还可以这样用：Map&lt;K,V&gt; map = ActionContext.get(&quot;request&quot;) 这样获取到的是 request 的 Map 集合 ActionContext 的 put 方法默认是直接存到 request 域 get(“request”).put 方式是存到 request 的 map 集合中去 所以说，在用 OGNL 表达式取的时候，第一种可以直接 #key 这样取；而第二种要使用 #request.key 或者 #attr.key 的方式来取值 put 方法像是 Struts 的一种优化，为了方便取值，因为值栈说白了就是存在于 request 域中的 通常，Action 会被放在值栈根节点的栈顶（双向引用哦），这个操作是在执行过程中执行的（下面的工作原理），表单的提交、数据的回显都是依赖于值栈 然后再说下属性链，Action 其实是个 javabean，在 Action 中还可以再引用一个 bean 作为属性，传参的时候就需要指明了：http://xxx:8080/webName/action?user.name=loli 这就是给 action 中的 user 属性的 name 属性赋值，额…说的这么别扭这里有个坑，当你在 action 用 javabean 类型的属性时，千万别用一个小写字母开头，比如：mUser因为这样在生成属性的时候会变成这样：getMUser() ；这就很尴尬了….这是不符合规范的，所以，别这么搞 补充一个专业术语：action 相关说的是很多资源文件需要和 action 放在一起，文件名相同，扩展名不同，至于什么用处，相当于一代的国际化资源文件吧，下面的表单校验中用得到 工作原理 图画的不是很准确，差不多这意思，请求来了都要进入 StrutsPreparreAndExecuteFilter 这个核心过滤器，StrutsPreparreAndExecuteFilter 主要有两个操作（对象），一个是准备（prepare 图的右边）；另一个是执行（excute 图的左边）；prepare 会注册国际化资源文件，并且会创建 ActionContext 对象，顺便绑定到当前线程中去….(创建的过程还是蛮复杂的，想知道的看源码….)接下来会查找 ActionMapping，如果没有就创建（通过 ActionMapper 来寻找/创建），最后存到 request 中去另外还会参与过滤模式的处理，就是过滤出 Action 的请求，其他的请求（jsp、img等）直接放行 在核心过滤器中，会发现刚开始是加载配置文件，如果没有特别的指定会自动加载3个配置文件，第一个是 Struts2 核心包下的 struts-default.xml 文件，最后一个是我们自己配置的 struts.xml 文件 struts-default.xml 文件里配了一些东西，比如刚开始的一堆 bean，后面就是定义了一些 Type（result-types）比如最常用的 dispatcher 和 redirect，其中可以看到 dispatcher 被设置为了默认，所以在我们自己配的配置文件中如果设置转发不需要配 Type 再下面就是初始化拦截器（拦截器只能拦截 Action，是 Struts 中的概念），这应该就是 Struts2 的核心了；定义默认的拦截器、默认执行的 Action 上面的操作完成后最终会到达 excute；判断如果 mapping 不为空，就调用 excute 执行 Action 会把 mapping 一起传过去，执行玩后到此终止，也就是说它没放行，后面的过滤器不会自动执行！在调用 Action 的方法中 excute 会创建出 ActionProxy ，它依赖于 ConfigurationManager 把配置文件（Struts.xml）读取出来进行一些处理，然后会请求一个中间人（Action Invocation）来调用那 N 个过滤器（应该是13个，具体看配置文件，叫做过滤器栈），使用的是递归方式，也就是上图的虚线，调用 1 然后返回（返回时再次调用此方法，也就是调用第二个拦截器），再继续调用 2 再返回….在 Action 调用之前处理的叫预处理，之后处理的叫后处理，大部分后处理是空实现，文件上传的过滤器比较特殊，因为需要在后处理里删除缓存文件 应该是在调用拦截器之前 excute 会把 action 置为栈顶（值栈） 如果配置自己的拦截器，一定要放在 Struts 拦截器的上面，因为 Struts 拦截器并不会放行 其他拓展ActionSupport这个类实现了 action 和其他几个有用的接口，比如数据的校验、国际化；你的 action 继承它后会自动获得这些能力 基本校验&amp;文件上传主要是通过拦截器和接口实现的 ActionSupport 实现了两个接口和默认栈中一个拦截器配合使用DefaultWorkflowInterceptor 提供了基本的校验功能（其实它只是判断是否有错误，有就跳转到 input 设定的页面，而不会再执行 Action 了）关键的调用在校验拦截器；所以 DefaultWorkflowInterceptor 就是工作流嘛，应该可以理解为重新定义路由….额 先来写个注册页面，使用到了标签库，很贴心的会自动显示校验错误，不需要任何标签： 12345678910111213141516&lt;%@ page contentType="text/html;charset=UTF-8" language="java" %&gt;&lt;%@taglib prefix="s" uri="/struts-tags" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;注册的测试页&lt;/title&gt;&lt;/head&gt;&lt;body&gt; 这是一个测试页面 &lt;br&gt; &lt;s:form namespace="/reg" action="RegAction_reg" method="POST" enctype="multipart/form-data"&gt; &lt;s:textfield name="name" label="用户名"/&gt; &lt;s:textfield name="age" label="年龄"/&gt; &lt;s:file name="img" label="上传文件"/&gt; &lt;s:submit value="提交"/&gt; &lt;/s:form&gt;&lt;/body&gt;&lt;/html&gt; 为了兼容直接访问 JSP 的情况（虽然极少）最好把 namespace 写上，这里顺便使用了文件上传的过滤器，文件的信息会自动装填到相应的字段，比如文件的类型会自动装填到下面的 imgContentType 字段，类型是 MIME 的，在服务器的 web.xml 中有相应的定义，相应处理请求的 Action ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class RegAction extends ActionSupport &#123; private String name; private String age; // 上传文件相关，以及获取的文件名和类型 private File img; private String imgFileName; private String imgContentType; // 到达注册页面,本方法不应该被校验 @SkipValidation public String toReg() &#123; return "regView"; &#125; public String reg() &#123; // 获取上传的文件 String dir = ServletActionContext.getServletContext().getRealPath("/upload"); System.out.println(dir); String ext = imgFileName.substring(imgFileName.lastIndexOf(".")); long l = System.nanoTime(); File file = new File(dir, l + ext); // 或者使用 FileUtils.copyFile(a,b); img.renameTo(file); return "success"; &#125; @Override public void validate() &#123; if (name == null || name.isEmpty()) &#123; addFieldError("name",getText("error.name.empty")); &#125; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getAge() &#123; return age; &#125; public void setAge(String age) &#123; this.age = age; &#125; public File getImg() &#123; return img; &#125; public void setImg(File img) &#123; this.img = img; &#125; public String getImgFileName() &#123; return imgFileName; &#125; public void setImgFileName(String imgFileName) &#123; this.imgFileName = imgFileName; &#125; public String getImgContentType() &#123; return imgContentType; &#125; public void setImgContentType(String imgContentType) &#123; this.imgContentType = imgContentType; &#125;&#125; Action 中，validate 校验方法优先于其他方法执行，但是校验过滤器是在参数过滤器之后的，没有参数校什么校；相应的配置文件： 123456789&lt;package name="regPkg" namespace="/reg" extends="struts-default"&gt; &lt;action name="RegAction_*" class="com.bfchengnuo.web.action.RegAction" method="&#123;1&#125;"&gt; &lt;result name="success"&gt;/reg/success.jsp&lt;/result&gt; &lt;result name="error"&gt;/reg/error.jsp&lt;/result&gt; &lt;!-- 校验失败自动跳回的路由 --&gt; &lt;result name="input"&gt;/reg/reg.jsp&lt;/result&gt; &lt;result name="regView"&gt;/reg/reg.jsp&lt;/result&gt; &lt;/action&gt;&lt;/package&gt; 关于 addFieldError 方法，它是先由校验拦截器调用的，然后才是 DefaultWorkflowInterceptor 进行判断，就是判断是否有错误消息啦；如果有就路由到 input 设置的界面了 addFieldError 方法会将错误信息加入到一个 Map 集合中去，然后在后续的 JSP 页面中就可以从这个集合中取出这些信息 关于配置文件，可以采用模块化思想，就是有多份文件，在主文件中使用 include 引用； 校验&amp;文件上传的补充上面使用的注解的方式避免 toReg 方法被校验，这种方法只限于在这个环境（拦截器）下使用；除了这一种还有其他两种方式，并且这两种方式是通用的，在其他的拦截器也可以使用 加后缀法指定校验方法 123456// 看方法名，后面跟需要校验的方法名，或者使用 validateDoReg 也可以public void validateReg() &#123; if (name == null || name.isEmpty()) &#123; addFieldError("name",getText("error.name.empty")); &#125;&#125; 配置文件法：在需要处理的 Action 下覆盖默认的配置 1234&lt;!-- 去校验的第三种方式：覆盖法指定跳过的方法，这个写法在配置文件中也可以找到 --&gt;&lt;interceptor-ref name="defaultStack"&gt;&lt;param name="validation.excludeMethods"&gt;input,back,cancel,toReg&lt;/param&gt;&lt;/interceptor-ref&gt; 在文件上传的时候，我们可能要限制文件上传的大小，这个在 Struts 的配置文件中设置下 struts.multipart.maxSize 的值就可以了还比较常用的是限制文件类型，也就是指定某些类型的文件才能上传，这个用 fileUpload 拦截器就可以做到，这种方式类似是注入了，可以找到其类中具体对应的属性，具体配置是： 12345&lt;interceptor-ref name="defaultStack"&gt; &lt;!-- 覆盖原拦截器的属性 --&gt; &lt;!-- allowedTypes 就是设置限制的 MIME 了，更严格 --&gt; &lt;param name="fileUpload.allowedExtensions"&gt;txt,jpg,jpeg&lt;/param&gt;&lt;/interceptor-ref&gt; 这里提一下，一旦配置了 Action 的拦截器（interceptor-ref）就不会继承包配置的默认的拦截器栈了，所以如果配置其他的拦截器别忘了加入 defaultStack 这些内容在测试代码中都有，还有文件的下载，篇幅太长就不写了，详见 Github 使用验证框架上面所写的是实现 validate 方法，其实还可以用 Struts2 提供的框架，配置下 xml 文件就行了，具体的约束在：lib/xwork-core.jar!/xwork-validator-1.0.2.dtd ；嗯….是的，在 xwork 下 然后创建 Action 相关文件，在需要进行校验的 Action，名字类似：ValidateAction-validation.xml ；其他的都一样，都是用 ValidationAware 接口存储错误信息，用 workflow 拦截器导向 input 界面，一个例子： 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE validators PUBLIC "-//Apache Struts//XWork Validator 1.0.2//EN" "http://struts.apache.org/dtds/xwork-validator-1.0.2.dtd"&gt;&lt;validators&gt; &lt;field name="name"&gt; &lt;field-validator type="requiredstring"&gt; &lt;message&gt;用户名不能为空&lt;/message&gt; &lt;/field-validator&gt; &lt;field-validator type="stringlength"&gt; &lt;param name="minLength"&gt;5&lt;/param&gt; &lt;param name="maxLength"&gt;10&lt;/param&gt; &lt;message&gt;name:$&#123;name&#125; 必须 $&#123;minLength&#125; - $&#123;maxLength&#125;&lt;/message&gt; &lt;/field-validator&gt; &lt;!--&lt;field-validator type="email"&gt;--&gt; &lt;!--&lt;message&gt;xxxx&lt;/message&gt;--&gt; &lt;!--&lt;/field-validator&gt;--&gt; &lt;/field&gt; &lt;field name="age"&gt; &lt;field-validator type="required"&gt; &lt;message&gt;年龄不能为空&lt;/message&gt; &lt;/field-validator&gt; &lt;field-validator type="int"&gt; &lt;param name="min"&gt;12&lt;/param&gt; &lt;param name="max"&gt;16&lt;/param&gt; &lt;message&gt;必须 12 - 16&lt;/message&gt; &lt;/field-validator&gt; &lt;/field&gt; &lt;!-- 使用 OGNL 表达式校验两次密码是否相同 --&gt; &lt;validator type="expression"&gt; &lt;param name="expression"&gt;pwd==confirmPwd&lt;/param&gt; &lt;message&gt;两次密码不同&lt;/message&gt; &lt;/validator&gt;&lt;/validators&gt; msg 可以使用其 key 属性从国际化资源文件中提取，当然在其中也是可以使用 OGNL 的如果验证指定方法文件名就要改成：ClassName-ActionName-validation.xml 主题与国际化这两方面简单说下吧，我没仔细看呢….Orzstruts2 默认带了 4 套主题；可以通过设置常量的方式来指定框架使用那个主题：struts.ui.theme=xhtml ；默认就是 xhtml不同的主题在生成 html 的时候会有所差异。这几个主题什么区别自行搜索吧如果某个标签 Struts 渲染的你不爽，那么是可以修改的，比如说错误的那个标签；因为 JVM 是先加载 jar 包，然后加载 src 里面的代码，如果在 src 里有相同的包，相同的文件，那么就会覆盖之前的（改 ftl 模板文件就行） 关于国际化拦截器，他会检测请求是否传入了 request_locale 参数，如果有就将其存储到 session 中，并且更改其对应的语言（就是加载相应的国际化资源文件）并且其实还存了一份到 ActionContext 中，就是那个大 Map，便于数据的回显资源文件按照：文件名_语言_地区 的形式，比如：name_zh_CN ，类型当然是 properties；加载国际化资源使用的是常量的方式，键是 struts.custom.i18n.resources 值就是你写的 prop 文件的位置了，同一类加载一个就行（默认的那个 name.properties），多个用逗号分割；记得在 JSP 的标签中使用 &lt;s:text&gt; 标签的 name 属性获取值 模型驱动拦截器也许配合 笔记2 效果更好，因为哪一篇太长了（这一篇好像更长了），写在这里了….先说它的作用，其一：OGNL 表达式是从栈顶的对象里找数据，对象里的对象里的值是没办法直接用的，只能使用属性链，使用模型驱动后，会在栈中压入一个自定义的对象，在 Action 的上面，所以在 JSP 中可以直接用这个对象里的属性，表单提交的数据也是进这个对象其二，也是主要作用：对所有的 Action 模型对象进行批处理；比如：在一个可以拦截所有 Action 的拦截器中判断(instanceof)是否是 ModelDriven，如果是就强转成 ModelDriven，调用其 getModel 方法就可以获得其要操作的对象；然后就可以利用反射进行实例化….等 使用模型驱动需要实现接口 ModelDriven&lt;T&gt;,以及实现 getModel 方法，这个方法返回的就是要压入栈顶的那个对象更进一步使用：模型驱动拦截器是在参数拦截器之前的，如果想要在模型驱动将对象压入栈顶之前初始化（填充）对象，可以使用准备拦截器（翻译不是很准确）；需要实现 Preparable 接口，此方法会在模型驱动拦截器之前调用如果需要传进的参数数据，要用 paramsPrepareParamsStack 栈，因为如果用默认栈，准备拦截器是优于参数拦截器的，那样就会取不到参数 具体的测试代码我放在 Github 其他配置发现一个标签对于调试很爽，并且使用简单：&lt;s:debug /&gt;加上后会显示一个连接，点击会展示值栈、ActionContext 等的情况 Action 类中，如果加上 private static final long serialVersionUID = 1006766693264599611L; 是反序列化时用的，具体还没研究 关于校验详细的流程写的不太详细，先这样吧…. Struts2 的根配置文件除了前面所说的，可能还会用到的有： 1234567891011121314151617&lt;struts&gt; &lt;!-- 请求参数的编码方式 --&gt; &lt;constant name="struts.i18n.encoding" value="UTF-8"/&gt; &lt;!-- 指定被struts2处理的请求后缀类型。多个用逗号隔开 --&gt; &lt;constant name="struts.action.extension" value="action,do,htm"/&gt; &lt;!-- 当struts.xml改动后，是否重新加载。默认值为false(生产环境下使用),开发阶段最好打开 --&gt; &lt;constant name="struts.configuration.xml.reload" value="true"/&gt; &lt;!-- 是否使用struts的开发模式。开发模式会有更多的调试信息。默认值为false(生产环境下使用),开发阶段最好打开 --&gt; &lt;constant name="struts.devMode" value="false"/&gt; &lt;!-- 设置浏览器是否缓存静态内容。默认值为true(生产环境下使用),开发阶段最好关闭 --&gt; &lt;constant name="struts.serve.static.browserCache" value="false" /&gt; &lt;!-- 指定由 spring 负责action对象的创建 &lt;constant name="struts.objectFactory" value="spring" /&gt; --&gt; &lt;!-- 是否开启动态方法调用 --&gt; &lt;constant name="struts.enable.DynamicMethodInvocation" value="false"/&gt; &lt;/struts&gt; 最后补充：关于转发，如果是使用链式（type=chain）转发到另一个 Action，那么它们两个都是在一个线程中]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Struts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS3学习笔记(二)]]></title>
    <url>%2F2017%2F05%2F27%2FCSS3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[CSS3 增加的东西确实不少，额，起码我感觉不少，如果真的用好了能做出非常炫酷的东西；然而我感觉是非常难的，到最后差点都勇气看完了3D 方面的东西没说，要使用高级特效，首先把矩阵学好吧…..还有，浏览器兼容问题真是一大璀璨，像我，从不考虑 IE10 以下的，毕竟看前端只是兴趣果然还是 ctrl + c and ctrl + v 大法好啊！这篇笔记从9号开始记录的，一直到….27，虽然是想起来才看一点这样的”策略“ 变形 旋转 rotate() 函数通过指定的角度参数使元素相对原点进行旋转。它主要在二维空间内进行操作，设置一个角度值，用来指定旋转的幅度。如果这个值为正值，元素相对原点中心顺时针旋转；如果这个值为负值，元素相对原点中心逆时针旋转。transform: rotate(45deg); 扭曲 skew() 函数能够让元素倾斜显示。它可以将一个对象以其中心位置围绕着X轴和Y轴按照一定的角度倾斜。这与 rotate() 函数的旋转不同，rotate() 函数只是旋转，而不会改变元素的形状。skew() 函数不会旋转，而只会改变元素的形状。skew 有三种形式：1.skewX(x) ; 2.skewY(y) ; 3.skew(x,y) (如果只写一个第二个参数默认0)分别是让其在水平方向扭曲；垂直方向扭曲；在水平和垂直方向都扭曲，单位写 deg 即可 缩放 scale()函数 让元素根据中心原点对对象进行缩放，同样有三种形式，和上面的扭曲类似，就不多说了，但是如果使用 scale(x,y) 的时候只写一个值，指的是水平、垂直都是这个值注意： scale() 的取值默认的值为1，当值设置为0.01到0.99之间的任何值，作用使一个元素缩小；而任何大于或等于1.01的值，作用是让元素放大。 位移 translate()函数可以将元素向指定的方向移动，类似于 position 中的 relative。或以简单的理解为，使用 translate() 函数，可以把元素从原来的位置移动，而不影响在X、Y轴上的任何Web组件。同样有三种形式，可以使用百分比，比如：transform:translate(-50%,-50%); 就算参数一样也不要省略 原点 transform-origin任何一个元素都有一个中心点，默认情况之下，其中心点是居于元素 X 轴和 Y 轴的 50% 处，也就是所谓的中心。在没有重置 transform-origin 改变元素原点位置的情况下，CSS 变形进行的旋转、位移、缩放，扭曲等操作都是以元素自己中心位置进行变形。比如：transform-origin: left top; 矩阵 matrix()它是一个含六个值的 (a,b,c,d,e,f) 变换矩阵，用来指定一个2D变换，上面的几种变换其实还是调用的它，2D 变换是个 3X3 的矩阵，3D 是 4X4 的矩阵http://www.zhangxinxu.com/wordpress/2012/06/css3-transform-matrix-%E7%9F%A9%E9%98%B5/ 动画-过渡属性它可以通过一些简单的 CSS 事件来触发元素的外观变化，让效果显得更加细腻。简单点说，就是通过鼠标的单击、获得焦点，被点击或对元素任何改变中触发，并平滑地以动画效果改变CSS的属性值。 在CSS中创建简单的过渡效果可以从以下几个步骤来实现： 在默认样式中声明元素的初始状态样式； 声明过渡元素最终状态样式，比如悬浮状态( hover )； 在默认样式中通过添加过渡函数，添加一些不同的样式。 CSS3的过度 transition 属性是一个复合属性，主要包括以下几个子属性： transition-property : 指定过渡或动态模拟的CSS属性 transition-duration : 指定完成过渡所需的时间 transition-timing-function : 指定过渡函数 transition-delay : 指定开始出现的延迟时间 简单的栗子最直接： 12345678910111213141516171819202122div &#123; width: 200px; height: 200px; background: red; margin: 20px auto; -webkit-transition-property: width; transition-property: width; -webkit-transition-duration:.5s; transition-duration:.5s; -webkit-transition-timing-function: ease-in; transition-timing-function: ease-in; -webkit-transition-delay: .18s; transition-delay:.18s;&#125;div:hover &#123; width: 400px;&#125;/* 合在一起也是可以的 */span&#123; -webkit-transition: background-color .5s ease .1s; transition: background-color .5s ease .1s;&#125; “transition-property” 属性设置为 all 时，表示的是所有中点值的属性，也就是你初始状态设的什么属性就代指的什么属性。关于过渡函数： 值 描述 linear 规定以相同速度开始至结束的过渡效果（等于 cubic-bezier(0,0,1,1)）。 ease 规定慢速开始，然后变快，然后慢速结束的过渡效果（cubic-bezier(0.25,0.1,0.25,1)）。 ease-in 规定以慢速开始的过渡效果（等于 cubic-bezier(0.42,0,1,1)）。 ease-out 规定以慢速结束的过渡效果（等于 cubic-bezier(0,0,0.58,1)）。 ease-in-out 规定以慢速开始和结束的过渡效果（等于 cubic-bezier(0.42,0,0.58,1)）。 cubic-bezier(n,n,n,n) 在 cubic-bezier 函数中定义自己的值。可能的值是 0 至 1 之间的数值。 有时我们想改变两个或者多个 css 属性的 transition效果 时，只要把几个 transition 的声明串在一起，用逗号（“，”）隔开，然后各自可以有各自不同的延续时间和其时间的速率变换方式。但需要值得注意的一点：第一个时间的值为 transition-duration，第二个为 transition-delay。例如：a{ transition: background 0.8s ease-in 0.3,color 0.6s ease-out 0.3;} 如果需要鼠标点击触发，那就只能通过设置 onclick 事件，使用 JS 来改变其 style 的属性 关键帧Keyframes 被称为关键帧，其类似于 Flash 中的关键帧。在 CSS3 中其主要以 “@keyframes” 开头，后面紧跟着是动画名称加上一对花括号 “{…}”，括号中就是一些不同时间段样式规则。 123456789101112131415161718192021222324252627@keyframes wobble &#123; 0% &#123; margin-left: 100px; background:green; &#125; 40% &#123; margin-left:150px; background:orange; &#125; 60% &#123; margin-left: 75px; background: blue; &#125; 100% &#123; margin-left: 100px; background: red; &#125;&#125;div &#123; width: 100px; height: 100px; background:red; color: #fff;&#125;div:hover&#123; animation: wobble 5s ease .1s;&#125; 在 @keyframes 中定义动画名称时，其中 0% 和 100% 还可以使用关键词 from 和 to 来代表，其中 0% 对应的是 from，100% 对应的是 to。 Chrome 和 Safari 需要前缀 -webkit-； Foxfire 需要前缀 -moz-。 调用分解通过上面的 css 代码也能看出来，是使用 animation 进行调用的，animation-name 属性主要是用来调用 @keyframes 定义好的动画。需要特别注意: animation-name 调用的动画名需要和 “@keyframes” 定义的动画名称完全一致（区分大小写），如果不一致将不具有任何动画效果。其实和 transition 是差不多的，比如 animation-duration 主要用来设置 CSS3 动画播放时间animation-timing-function 属性主要用来设置动画播放方式。主要让元素根据时间的推进来改变属性值的变换速率，就是上面表格中的那些函数 另有几个特有的属性：控制播放次数，毕竟是动画嘛；animation-iteration-count 属性主要用来定义动画的播放次数。这个值通常为整数，但是小数也是可以的，默认为一次，如果是 infinite 表示无限循环设置动画播放方向 ，就是使用 animation-direction 了 ，其主要有两个值： normal默认值，动画的每次循环都是向前播放； alternate动画播放在第偶数次向前播放，第奇数次向反方向播放。 控制元素动画的播放状态：使用的是 animation-play-state 属性；其主要有两个值：running 和 paused。从命名也能看出是这是什么作用了，只是如果暂停了动画的播放，元素的样式将回到最原始设置状态（应该是形状和颜色，像背景、位置都是会保留的）。设置动画时间外属性：animation-fill-mode 属性定义在动画开始之前和结束之后发生的操作。主要具有四个属性值： 属性值 效果 none 默认值，表示动画将按预期进行和结束，在动画完成其最后一帧时，动画会反转到初始帧处 forwards 表示动画在结束后继续应用最后的关键帧的位置 backwards 会在向元素应用动画样式时迅速应用动画的初始帧 both 元素动画同时具有forwards和backwards效果 布局样式多列布局应用多列布局非常简单，通过 columns 属性，语法：columns：&lt;column-width&gt; || &lt;column-count&gt;第一个为列宽，第二个是分为多少列，直接应用在某个 div 上就行，div 里面可以使用 p 或者 span 等标签来写文字 当然也可以拆开来写，比如 123column-width:200px;-webkit-column-count:3;column-count:3; column-width 的值还可以设置为 auto 或者不写，那么就会按照其他属性自动适应，比如参考列数，列数如果设置为 auto 那么默认就当作是 1 列还可以使用 column-gap 来设置列间距，语法：column-gap: normal || &lt;length&gt; ；normal 为默认值，为 1em（如果你的字号是px，其默认值为你的 font-size 值）如需跨列的情况，可以使用：column-span: none | all 除了这两种，可以指定跨多少列，一般用于第一段，设置位 allcolumn-rule 主要是用来定义列与列之间的边框宽度、边框样式和边框颜色。简单点说，就有点类似于常用的 border 属性。但 column-rule 是不占用任何空间位置的，在列与列之间改变其宽度不会改变任何列的位置。 style 包括：none、hidden、dotted、dashed、solid、double、groove、ridge、inset、outset。 如果不希望显示边框的颜色，也可以将其设置为 transparent (透明色) 使用如：column-rule: 2px dotted green; 盒模型通过设置 box-sizing 可以指定其解析方式：box-sizing: content-box | border-box | inherit我的简单理解为，第一种为 W3C 标准的盒模型，也是默认值 （element width/height = border + padding + content width / height）第二种是 IE6 以下的盒模型 （元素的宽度或高度等于元素内容的宽度或高度；这里的内容宽度或高度包含了元素的border、padding、内容的宽度或高度（盒子的宽度或高度 - 边框 - 内距））第三种是继承父元素的盒模型 总之，最大的用处就是 在自适应布局当中，在元素基础上添加内距 padding，按照标准盒模型解析，往往会将布局撑破，但使用 box-sizing 的 border-box 值，可以让你轻松完成 伸缩布局CSS3 引入了一种新的布局模式——Flexbox布局，即伸缩布局盒模型（Flexible Box），用来提供一个更加有效的方式制定、调整和分布一个容器里项目布局，即使它们的大小是未知或者动态的。 Flexbox 布局常用于设计比较复杂的页面，可以轻松的实现屏幕和浏览器窗口大小发生变化时保持元素的相对位置和大小不变，同时减少了依赖于浮动布局实现元素位置的定义以及重置元素的大小。Flexbox 布局在定义伸缩项目大小时伸缩容器会预留一些可用空间，让你可以调节伸缩项目的相对大小和位置。例如，你可以确保伸缩容器中的多余空间平均分配多个伸缩项目，当然，如果你的伸缩容器没有足够大的空间放置伸缩项目时，浏览器会根据一定的比例减少伸缩项目的大小，使其不溢出伸缩容器 Flexbox 规范版本众多，浏览器对此语法支持度也各有不同 生成内容这个属性我感觉很重要，用的很频繁！ 在 Web 中插入内容，在 CSS2.1 时代依靠的是 JavaScript 来实现。但进入 CSS3 进代之后我们可以通过 CSS3 的伪类 :before，:after和 CSS3 的伪元素 ::before、::after来实现，其关键是依靠 CSS3 中的 content 属性来实现。 不过这个属性对于 img 和 input 元素不起作用。 他们两者的区别可以参见：https://www.qianduan.net/before-and-before-the-difference-between/ content 的属性一般有： 功能 功能说明 none 不生成任何内容 attr 插入标签属性值 url 使用指定的绝对或相对地址插入一个外部资源（图像，声频，视频或浏览器支持的其他任何资源） string 插入字符串 在 CSS 中有一种清除浮动的方法叫“clearfix”。而这个“clearfix”方法就中就使用了“content”，只不过只是在这里插入了一个空格。 12345678910.clearfix:before,.clearfix:after &#123; content:””; display:table;&#125;.clearfix:after &#123; clear:both; overflow:hidden;&#125; 自由缩放为了增强用户体验，CSS3 增加了很多新的属性，其中 resize 就是一个重要的属性，它允许用户通过拖动的方式来修改元素的尺寸来改变元素的大小。到目前为止，可以使用 overflow 属性的任何容器元素。resize 的取值： 属性值 取值说明 none 用户不能拖动元素修改尺寸大小。 both 用户可以拖动元素，同时修改元素的宽度和高度 horizontal 用户可以拖动元素，仅可以修改元素的宽度，但不能修改元素的高度。 vertical 用户可以拖动元素，仅可以修改元素的高度，但不能修改元素的宽度。 inherit 继承父元素的resize属性值。 外轮廓属性外轮廓 outline 在页面中呈现的效果和边框 border 呈现的效果极其相似，但和元素边框 border 完全不同，外轮廓线不占用网页布局空间，不一定是矩形，外轮廓是属于一种动态样式，只有元素获取到焦点或者被激活时呈现。取值为： 属性值 属性值说明 outline-color 定义轮廓线的颜色，属性值为CSS中定义的颜色值。在实际应用中，可以将此参数省略，省略时此参数的默认值为黑色。 outline-style 定义轮廓线的样式，属性为CSS中定义线的样式。在实际应用中，可以将此参数省略，省略时此参数的默认值为none，省略后不对该轮廓线进行任何绘制。 outline-width 定义轮廓线的宽度，属性值可以为一个宽度值。在实际应用中，可以将此参数省略，省略时此参数的默认值为medium，表示绘制中等宽度的轮廓线。 outline-offset 定义轮廓边框的偏移位置的数值，此值可以取负数值。当此参数的值为正数值，表示轮廓边框向外偏离多少个像素；当此参数的值为负数值，表示轮廓边框向内偏移多少个像素。 inherit 元素继承父元素的 outline 效果。 例如：outline: red solid 2px; 媒体类型简单说就是为了适应不同的设备而弄出来的，在 CSS2 中也是有的，叫 CSS3 Media Queries，目前大多数浏览器已经支持，使用方法类似： 123456789/* media 是在 css3 中新增的 */@media screen and (min-width:600px) and (max-width:900px)&#123; body &#123;background-color:#f5f5f5;&#125;&#125;/* 其他方式还有 link 和 import 这些在 css2 中就可以使用 */&lt;link rel="stylesheet" type="text/css" href="style.css" media="screen" /&gt;&lt;link rel="stylesheet" type="text/css" href="print.css" media="print" /&gt;@import url(reset.css) screen; @import url(print.css) print; 意思就是宽度在 600-900px 之间，中间的样式才会生效，这部分没怎么仔细去看，突然感觉前端真的是不容易Media Queries 在其他浏览器中不要像其他 CSS3 属性一样在不同的浏览器中添加前缀。 这一部分最热的应该就是 Responsive 了，也就是常说的响应式布局，不过…..我没足够的耐心看下去了使用 Responsive 的话就必须要使用 meta 标签：&lt;meta name=”viewport” content=”” /&gt; ；它用来处理可视区域，content 的取值嘛…. 最后附一张相关的表，W3C 总共列出了10种媒体类型： 值 设备类型 All 所有设备 Braille 盲人用点字法触觉回馈设备 Embossed 盲文打印机 Handheld 便携设备 Print 打印用纸或打印预览视图 Projection 各种投影设备 Screen 电脑显示器 Speech 语音或音频合成器 Tv 电视机类型设备 Tty 使用固定密度字母栅格的媒介，比如电传打字机和终端 其中Screen、All和Print为最常见的三种媒体类型。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java知识补充]]></title>
    <url>%2F2017%2F05%2F26%2FJava%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[回过头发现有些 Java 基础并没有学好，也许是忘了也许是学的时候就没仔细看，那就在这里补回来本篇介绍的是：迭代器、可变参数、枚举、反射、注解、动态代理的简单使用 迭代器与forIterator迭代器Java 提供一个专门的迭代器 Iterator，它是一个接口，我们可以对某个序列实现该 interface，来提供标准的 Java 迭代器。Iterator 接口实现后的功能是“使用”一个迭代器，比如定义了 next 等方法也就是说，实现了这个接口就可以称为是一个迭代器了 Iterable可迭代对象Java 中还提供了一个 Iterable 接口，Iterable 接口实现后的功能是“返回”一个迭代器,我们常用的实现了该接口的子接口有: Collection、Deque、List、Queue、Set 等，该接口的 iterator() 方法返回一个标准的 Iterator 实现。实现这个接口允许对象成为 For each 语句的目标。就可以通过 For each 语法遍历你的底层序列。也就是说：实现这个接口允许对象成为 “foreach” 语句的目标，实现接口就成为了可以被迭代的对象。Iterable 接口包含一个能够产生 Iterator 的 iterator() 方法，并且 Iterable 接口被 foreach 用来在序列中移动。因此如果创建了任何实现 Iterable 接口的类，都可以将它用于 foreach 语句中。 for eachforEach 不是关键字，关键字还是 for，语句是由 iterator 实现的，他们最大的不同之处就在于 remove() 等方法上。但是，如果在循环的过程中调用集合的 remove() 方法，就会导致循环出错，因为循环过程中 list.size() 的大小变化了，就导致了错误。 所以，如果想在循环语句中删除集合中的某个元素，就要用迭代器 iterator 的 remove() 方法，因为它的 remove() 方法不仅会删除元素，还会维护一个标志，用来记录目前是不是可删除状态，例如，你不能连续两次调用它的 remove() 方法，调用之前至少有一次 next() 方法的调用。 forEach 就是为了让用 iterator 循环访问的形式简单，写起来更方便。当然功能不太全，所以但如有删除等操作，还是要用它原来的形式。 Iterator与for特点采用 ArrayList 对随机访问比较快，而 for 循环中的 get() 方法，采用的即是随机访问的方法，因此在 ArrayList 里，for 循环较快 采用 LinkedList 则是顺序访问比较快，iterator 中的 next() 方法，采用的即是顺序访问的方法，因此在LinkedList 里，使用 iterator 较快 从数据结构角度分析, for 循环适合访问顺序结构，可以根据下标快速获取指定元素；而 Iterator 适合访问链式结构，因为迭代器是通过 next() 和 Pre() 来定位的，可以访问没有顺序的集合. 而使用 Iterator 的好处在于可以使用相同方式去遍历集合中元素，而不用考虑集合类的内部实现（只要它实现了 java.lang.Iterable 接口），如果使用 Iterator 来遍历集合中元素，一旦不再使用 List 转而使用 Set 来组织数据，那遍历元素的代码不用做任何修改，如果使用 for 来遍历，那所有遍历此集合的算法都得做相应调整,因为List 有序, Set 无序,结构不同,他们的访问算法也不一样.清空 List 的时候（单条删除），除了使用迭代器，用 for 条件倒序删除就不会出现越界问题，想出这个方法的真是人才… 可变参数可变参数完全可以当作数组来用，是在 Java5 加入的，如果有多个参数可变参数必须在最后因为可变参数基本等同于数组，所以甚至可以直接传一个数组给他，关于可变参数在 API 文档中随处可见，比如我想到了 Arrays.asList() 这个方法，你可以传出多个参数也可以直接传入一个数组（这个方法接收的是对象参数，不要传基本类型的数组啊，要不然整个数组会被当作一个参数） 1234567891011121314public static void main(String[] args)&#123; int sum = testSum(1,2,3,4,5,6,7); System.out.println(sum); System.out.println(testSum(new int[]&#123;1,2,3,4,5,6,7&#125;));&#125;private static int testSum(int ...nums) &#123; int sum = 0; for (int i : nums) &#123; sum += i; &#125; return sum;&#125; 枚举使用枚举的情况不算少，就是那种需要传入一些固定的参数的方法，比如一个方法允许传入的参数值就两种字符 A 和 B 吧，但是如果定义为 char 类型的话调用者可以瞎鸡巴传的，传个 D 、E、F 都是合法的，枚举就是用来做这个的当然也可以自定义一个类，然后把构造方法私有了，内部制造出指定的几个，这样就太麻烦了，看下面定义个简单的枚举，相当于是个类 123public enum TestEnum &#123; A, B, C, D, E;&#125; 使用起来非常简单，比如： 123456789public class Main &#123; public static void main(String[] args) &#123; enumTest(TestEnum.A); &#125; private static void enumTest(TestEnum e) &#123; System.out.println("name ---&gt;" + e.name()); &#125;&#125; 上面的是最最简单的使用，既然枚举相当于一个类，那么它也有构造函数，也有字段、方法，还可以定义抽象方法，不过要在声明枚举项的时候也就是上面的 A B C D 的时候给复写了，这样就可以直接通过 TestEnum.A.methodName() 的形式调用具体的栗子代码放在了这里Github 反射反射技术非常的重要，一般用于来做框架，虽然做框架都是大牛，但是不懂反射也看不懂框架啊，那就没法用啊，所以还是很有必要的，可以不用精通嘛反射就是加载类，并解剖出类的各个组成部分；别人扔给你一个编译后的类，你不知道里面是啥东西吧，然后别人可能会告诉你里面有个什么方法，这时候想要执行就得用反射，首先把类给搞进去，弄出个对象来，然后检测有没有这个方法，有就调用呗 获取类的字节码文件，也就是把类加载到内存中去，一般有三种方式： 123Class&lt;Student&gt; clazz = (Class&lt;Student&gt;) Class.forName("com.bfchengnuo.reflect.Student"); // 完整路径//Class clazz = new Student().getClass();//Class clazz = Student.class; 通过反射加载了类下一步就是创建对象了，也就是要获取构造函数了，另外还可以获得方法、字段等，反正啥都可以得到，这就反射的厉害之处详细的反射不在这写了，在 Github 上有 内省内省可以当作是反射技术中的一类特殊的情况，它是专门来处理 JavaBean 类的，Java 还提供了相关的 API简单说就是：内省就是用来访问某个属性的 getter/setter 方法的 一般在 JavaBean 中会从 Object 中继承一个属性，Object 中有个 getClass 方法啦 关于内省 Java 提供的 API 中，最重要的是 Introspector 类，它确定了到底“省”谁，被省了以后就会把 bean 的信息封装到一个 BeanInfo 中去；下面基本就是执行其 set 方法填充数据，或者 get 方法获取数据 1234567891011121314151617181920public static void main(String[] args) throws IntrospectionException &#123; // 获取javabean的所有属性 //BeanInfo beanInfo = Introspector.getBeanInfo(Stu.class); // 除去从 obj 继承的属性 BeanInfo beanInfo = Introspector.getBeanInfo(Stu.class,Object.class); PropertyDescriptor[] pds = beanInfo.getPropertyDescriptors(); // 得到属性描述符 for (PropertyDescriptor pd : pds) &#123; // 获得属性名 System.out.println(pd.getName()); // 相当于是拿到set方法 Method writeMethod = pd.getWriteMethod(); // 执行方法，填充数据 writeMethod.invoke(new Stu(),"loli"); // 拿到get方法 Method readMethod = pd.getReadMethod(); readMethod.invoke(new Stu(),null); &#125;&#125; 注解注解的出现，就是为了代替配置文件（比如 XML）的。注解是给程序看的，所以如果想要用自定义注解，那就要必须让程序（编译器）看明白才行，关键字 @interface 意思就是定义一个注解 123456789101112131415// 元注解，给注解加注解，分别是：指定保留域、作用在那、可继承@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)@Inheritedpublic @interface MyAnnotation &#123; // 可以定义八种基本数据类型、字符串、其他注解、类、枚举；当然也可以是数组 String name(); int age(); // 可以配置缺省值 Class clazz() default String.class; // value 是比较特殊的，只有一个属性 value 的话，赋值的时候可以省略属性名 String[] value();&#125; 这样定义后就可以使用了 123456789101112131415161718192021public class Main &#123; private static String name; static &#123; try &#123; // 通过反射，获取注解的信息，首先获取到方法，这个是获取公共方法的.... Method method = Main.class.getMethod("test"); // 获取到注解信息 MyAnnotation annotation = method.getAnnotation(MyAnnotation.class); name = annotation.name(); System.out.println(name); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; &#125; @MyAnnotation(name = "loli", age = 12, value = &#123;"test"&#125;) public static void test() throws NoSuchMethodException &#123; // TODO: 2017/5/24 &#125;&#125; 使用了注解虽然不需要再配置 XML 了，也非常的直观，但是，弊端也体现出来了，如果想要改动配置，必须要修改 java 文件….还得重新进行编译高级的用法就是，有可能加了一个注解就拥有了一个对象，说到底还是通过反射技术来实现的，通过反射解析出创建那个类，以及相应的数据，然后再利用反射设置到这个变量上去就行了（注入对象） 完整代码参考：Github 动态代理动态代理的意义在于：拦截对真实业务对象的访问 ；拦截以后可以干嘛呢，这个就看需求了，比如可以对方法进行增强啥的，就不需要使用包装模式了 动态代理是利用 JavaAPI 在内存中动态的构建代理对象，套路一般是从被代理者上抽取一个接口，这个接口在后面有大用处，也是 Java 的规定，相关的 API 在反射包下 12345678910111213141516171819// 抽取的接口public interface Loli &#123; void hug(String name); String eat();&#125;// 具体的被代理者public class StandardLoli implements Loli &#123; @Override public void hug(String name) &#123; System.out.println("(づ｡◕‿‿◕｡)づ"); &#125; @Override public String eat() &#123; System.out.println("开吃...."); return "吃饱了"; &#125;&#125; 然后就是代理者了，一般内部维护一个具体被代理的对象，然后通过 Java 自带的 API ( Proxy.newProxyInstance() ) 返回给调用者一个代理者，也就是一个接口，当调用这个接口的方法时，会自动携带相关参数到 invoke 方法，嗯？！就是回调，然后在这个方法中进行一些判断或者增强啥的 123456789101112131415161718192021222324252627public class StandardLoliProxy &#123; private StandardLoli mLoli = new StandardLoli(); public Loli createProxy() &#123; // 固定写法 // 1.第一个参数是类加载器，用当前的类获取就可以，或者 mLoli // 2.第二个为被代理者的接口，是接口 // 3.第三个为核心，返回的代理调用时就是调用这个方法 // 在这个方法里判断是调用的那个方法，然后执行相应的处理 return (Loli) Proxy.newProxyInstance(StandardLoliProxy.class.getClassLoader(), mLoli.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if (method.getName().equalsIgnoreCase("hug")) &#123; System.out.println("确认身份..."); if (args[0].equals("bfchengnuo")) &#123; return method.invoke(mLoli, args); &#125; &#125; else if (method.getName().equalsIgnoreCase("eat")) &#123; System.out.println("代理人..."); return method.invoke(mLoli, args); &#125; System.out.println("其他方法不允许执行！"); return null; &#125; &#125;); &#125;&#125; 如果把内部维护的对象设置为 Object（使用泛型可能会更好），当然返回值也要是 Object 了，那么就成为了一个通用的代理对象了；最后简单测试一下： 123456789101112public class MainTest &#123; public static void main(String[] args) &#123; StandardLoliProxy proxy = new StandardLoliProxy(); Loli loli = proxy.createProxy(); loli.hug("bfchengnuo"); System.out.println("------------------------"); String result = loli.eat(); System.out.println("----------"+ result +"--------------"); // loli.hashCode(); &#125;&#125; 其实除了动态代理还有静态代理，静态代理需要实现和代理对象相同的接口，在内部维护一个代理对象的实例，然后最后调用的是代理的方法（也就是 proxy.xxx 的方式），在调用代理类的方法中会调用内部实例的方法，有点类似装饰模式了，不建议使用 List家族两大支柱当然就是 ArrayList 与 LinkedList 了，以往我们都是说 ArrayList 采用连续存储随机查找、读取快，增删慢；LinkedList 采用链表，增删快，随机访问慢。这样说确实没问题，不过实际上还是有点差别，主要是我们忽略了“定位”的时间来说的，如果算上这个时间，直观上 ArrayList 可以完胜了，当然是在性能上。 拜读了这个系列的文章，对我的认知产生了不小的冲击，原文地址另外也可以看看我的笔记本里对应地址 确实有点出乎意料，按照这样的观点，LinkedList 性能上岂不是没有任何优势根据第二篇介绍， LinkedList 因为双向链表，添加元素可以两边查找，所以在中间插入最耗时，Array 当然是开头插入最耗时，因为越是靠前需要进行移动的元素越多；然而，无论哪一种，作者测试都是 ArrayList 遥遥领先…..但是 LinkedList 的耗时其实全部都用在了定位元素上，真正的插入和删除只是修改几个链接，是不耗时的；无奈 ArrayList 使用 native 提速后一般的增删和链表差不多了，甚至反超…..看来维护双向链表的成本确实很大啊~ PS：与上面的迭代器相关内容没有影响。 其他所有的包装类都默认实现了 Serializable 接口也就是说，如果你不确定传过来的值是 String 还是 Long 或者 Integer 等对象，就可以使用 Serializable 来接收了！ 参考迭代器与for相关]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的事件处理]]></title>
    <url>%2F2017%2F05%2F20%2FJava%E4%B8%AD%E7%9A%84%E4%BA%8B%E4%BB%B6%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[紧接着上一篇说，学习观察者模式的时候看到了这个事件处理机制，大体浏览了一下就是用的观察者模式嘛，并且是惊人的相似，既然这样那就顺便写写啦 Java 的事件机制中一共设计到3个角色，分别是：事件源、事件对象、事件监听器事件源上发生某操作的时候，会调用事件监听器里的相应的监听方法，并在调用这个方法的时候把事件对象传递过去，事件监听器由开发人员编写所以在事件监听器中可以通过事件对象拿到相应的事件源 举个栗子，比如把一个窗口当作是事件源，当点击这个窗口上的某个按钮的时候就相当于触发了一个事件，然后会被事件监听器捕获到，交给相应的方法去处理（会携带事件对象） 实现步骤事件源首先定义/确定一个事件源，我呢，就继续厚颜无耻的改造下前面的代码用了 12345678910111213141516171819202122232425262728293031public class StandardLoli &#123; private String name = "佳芷"; private LoliListeners listeners; // 当事件触发的时候通知相应的监听器 public void speak(String name) &#123; System.out.println("大哥哥" + name + "快来呀！！"); if (listeners != null) &#123; Event event = new Event(this); listeners.speakListeners(event); &#125; &#125; public void hug() &#123; System.out.println("(づ｡◕‿‿◕｡)づ"); if (listeners != null) &#123; Event event = new Event(this); listeners.hugListeners(event); &#125; &#125; public String getName() &#123; return name; &#125; // 提供一个注册监听器的方法，由事件源对象调用 public void registerListeners(LoliListeners listeners) &#123; this.listeners = listeners; &#125;&#125; 当然是允许注册多个监听器的，那就要定义一个集合来存储了，和前面的观察者模式一样，我就简单起见，只写了一个的情况 事件对象我认为事件对象就是用于传递对象或者说是数据的，当事件触发的时候通知事件监听器的时候总是会带点东西过去吧….最起码也得把自己带过去啊 12345678910111213public class Event &#123; private StandardLoli mStandardLoli; public Event() &#123;&#125; public Event(StandardLoli standardLoli) &#123; mStandardLoli = standardLoli; &#125; public StandardLoli getStandardLoli() &#123; return mStandardLoli; &#125;&#125; 我这里还真的就只带了自己过去，或者可以加一个 status 的属性 事件监听器与其相关的有两个类，一个是接口，一个是它的实现类；接口的作用嘛，除了规范主要就是用来在事件源中调用未来的方法的！这个说法很好啊，因为事件源中触发事件后调用监听器的方法时确实不知道你是怎么实现的 12345public interface LoliListeners &#123; // 一个监听方法对应一个事件源中的需要被监听方法 void speakListeners(Event event); void hugListeners(Event event);&#125; 废话不说，接着是它的实现类，也就是真正的事件监听器 123456789101112public class MyListeners implements LoliListeners &#123; @Override public void speakListeners(Event event) &#123; StandardLoli loli = event.getStandardLoli(); System.out.println(loli.getName() + "我来了！！"); &#125; @Override public void hugListeners(Event event) &#123; System.out.println("抱抱.."); &#125;&#125; 我都感觉我的命名和方法有些丧心病狂….. 测试类最后一步就是进行测试了，也就是实际用的时候应该怎么用 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; // 事件源 StandardLoli loli = new StandardLoli(); // 注册监听器 loli.registerListeners(new MyListeners()); // test loli.speak("bfchengnuo"); System.out.println("----------------"); loli.hug(); &#125;&#125; 使用Java默认实现巧了，在 Java 中的 util 包中也提供了相关的类和接口，但是我感觉吧，比较鸡助啊，也许是我还没有领会到其中的精髓吧，那就再把上面的代码改把改把吧，先来看看 Java 给提供了那些东西： public interface EventListener所有事件监听器接口必须扩展的标记接口。 public class EventObject extends Object implements Serializable所有事件状态对象都将从其派生的根类。 所有 Event 在构造时都引用了对象 “source”，在逻辑上认为该对象是最初发生有关 Event 的对象。 事件对象这次从事件对象写起，必须继承 EventObject 类，这样就不用定义保存事件源的对象了，此外只是定义了一个 status 的属性而已 1234567891011121314151617181920212223242526public class Event extends EventObject &#123; private String status; /** * Constructs a prototypical Event. * * @param source The object on which the Event initially occurred. * @throws IllegalArgumentException if source is null. */ public Event(Object source) &#123; super(source); &#125; public Event(Object source, String status) &#123; super(source); this.status = status; &#125; public String getStatus() &#123; return status; &#125; public void setStatus(String status) &#123; this.status = status; &#125;&#125; 至于继承的作用，应该也看出来了，就是那个构造方法 监听接口是的，是定义一个接口，和上面的类似，只不过这个接口要继承 EventListener ，这是一个标记接口，就算打了标记，我目前认为没什么太大的作用，不过看看 API 手册又有另一番感慨… 12345public interface LoliListeners extends EventListener &#123; // 一个监听方法对应一个事件源中的需要被监听方法 void doSpeak(Event event); void doHug(Event event);&#125; 监听接口的实现和上面一样的套路，代码也基本一样，不多说，当然，可以定义多个实现 1234567891011121314public class MyListeners implements LoliListeners &#123; @Override public void doSpeak(Event event) &#123; // 拿到事件源对象 StandardLoli loli = (StandardLoli) event.getSource(); System.out.println(loli.getName() + "我来了！！"); &#125; @Override public void doHug(Event event) &#123; String status = event.getStatus(); System.out.println("抱抱，状态" + status); &#125;&#125; 其他除了上面的类，还需要一个 事件源对象和测试的类，这个两个完全可以照搬前面的代码，都不用改的 所以，使用 Java 的默认实现我也没感觉有多大的好处，代码也没少些嘛，也许是我功底不够理解不到，等我理解到了再回来写上哈 完整测试代码在 Github EventListener 接口，这是由几十个其他接口扩展的 Java API，你可以使用一个标记接口来建立一组接口的父接口。 例如：当一个接口继承了 EventListener 接口，Java 虚拟机 (JVM) 就知道该接口将要被用于一个事件的代理方案。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-观察者模式]]></title>
    <url>%2F2017%2F05%2F19%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[什么是观察者模式，Wiki 上的定义为： 观察者模式是软件设计模式的一种。在此种模式中，一个目标对象管理所有相依于它的观察者对象，并且在它本身的状态改变时主动发出通知。 这通常透过呼叫各观察者所提供的方法来实现。此种模式通常被用来实时事件处理系统。 通过定义也可以看出，一个对象是可以对应多个观察者的，当这个对象发生变化时，会通知所有的观察者，让它们能够自动更新自己观察者还是 JDK 中使用最多的模式之一，相当于定义了对象之间的一对多依赖 角色观察者模式涉及到了4个角色： 抽象目标(Subject)角色：此抽象类提供一个界面让观察者进行添附与解附作业。通俗理解就是主要定义三个功能的方法，注册、删除、更新（通知）观察者 具体目标(ConcreteSubject)角色：将有关状态存入具体观察者对象；在具体目标的内部状态改变时，给所有登记过的观察者发出通知。 抽象观察者(Observer)角色：为所有的具体观察者定义一个接口，在得到目标（被观察者）的通知时更新自己，这个接口叫做更新接口。 具体观察者(ConcreteObserver)角色：具体观察者角色实现抽象观察者角色所要求的更新接口，以便使本身的状态与目标（被观察者）的状态相协调。如果需要，具体观察者角色可以保持一个指向具体目标对象的引用。每个观察者类别都要实做它自己的更新函式，以应对状态更新的情形。 可以看出，我们常说的“回调机制”就是它的一种体现形式嘛~~~回调的对象其实就是一个观察者嘛 （: 雾 一个栗子下面就开始具体写这四个角色了，具体的写法不是固定的，我也看到了好多不同的写法，思路是差不多的观察者模式的代表人物就是 MVC 了！！ 抽象目标角色首先写抽象目标角色吧，正如上面所说，主要定义的是对观察者的一些列操作，我呢，继续偷懒尽可能复用前面的代码了 o(*≧▽≦)ツ或者你可以按照 HeadFirst 中的叫法：主题（抽象）；想象成出版社？负责内容的更新以及通知相应的订阅者 12345678910public interface StandardLoli &#123; //注册一个观察者 void register(Lolicon observer); //移除一个观察者 void remove(Lolicon observer); //通知所有观察者 void notifyObservers(StandardLoliImpl data);&#125; 看到网上的一些其他实现是把这个类定义为抽象类，然后把上面的方法给实现了，因为所有的被观察者的实现基本都是一个套路 具体目标角色也就是所说的 被观察者 了，我是在这里实现的抽象目标的方法，register 等方法是给 new 出的具体的对象调用的就是上面所说的主题的具体的实现了。相当于具体的某家出版社这个类通常会有一个属性表示状态….这里没写 1234567891011121314151617181920212223242526272829303132333435363738public class StandardLoliImpl implements StandardLoli &#123; // 用来保存所有的已注册的观察者 // 如果要频繁的增删那要使用 LinkedList // 如果要多线程同步操作，要把 list 转化为安全的 Collections.synchronizedList(list); private List&lt;Lolicon&gt; list = new ArrayList&lt;&gt;(); private String name = "佳芷"; // 此方法就当作是更新状态的方法吧 public void speak(String name) &#123; System.out.println("大哥哥" + name + "快来呀！！"); // 更新完后通知所有观察者 notifyObservers(this); &#125; public String getName() &#123; return name; &#125; @Override public void register(Lolicon observer) &#123; list.add(observer); &#125; @Override public void remove(Lolicon observer) &#123; list.remove(observer); &#125; @Override public void notifyObservers(StandardLoliImpl data) &#123; // 通知所有的已注册的观察者更新自己 // 如果是用的 LinkedList 或者 Map 之类的要使用迭代器进行迭代 for (Lolicon lolicon : list) &#123; lolicon.update(data); &#125; &#125;&#125; notifyObservers 的参数应该是接口的，我就是图方便直接写的实现类啦~如果是使用的抽象类的方式，这里只要定义一个状态标志变量和 change 方法（相当于我的 speak 方法）就行了，具体的可以见参考里的第二个连接 抽象观察者角色这个才是最简单的一个接口，就一个方法，用来更新观察者状态的 12345public interface Lolicon &#123; // 更新的时候可以接受一个被观察者对象，用于获取相关信息 // 这里直接传实现类了 void update(StandardLoliImpl loli);&#125; 观察者角色被观察者状态变化后调用，用来通知观察者进行相应的更新自己 12345678910public class LoliconImpl implements Lolicon &#123; private String status = "我来了！！！"; // 当被观察者更新时会被调用 @Override public void update(StandardLoliImpl loli) &#123; // 更新自己..... System.out.println(status + loli.getName()); &#125;&#125; 测试类好了，所有的角色都定义完毕，下面就开始操练起来了 123456789101112131415public class MainTest &#123; public static void main(String[] args) &#123; // 创建被观察者 StandardLoliImpl standardLoli = new StandardLoliImpl(); // 创建观察者 LoliconImpl lolicon = new LoliconImpl(); // 将观察者注册到被观察者上 standardLoli.register(lolicon); // 更新被观察者，观察者也会自动更新自己的状态 standardLoli.speak("bfchengnuo"); &#125;&#125; 还有一点，很多人 （反正我是…..）都不会这样严格的定义，定义四个类太麻烦啦，我嘛，简单的功能就会省略掉抽象目标角色了…果然是不是非常像我们常用的“回调机制” 使用Java的默认实现其实在 java 中实现观察者模式还有一种更简单的方式，它已经给我们提供好类使用了。需要用到 java.util 包中提供的 Observable 类和 Observer 接口，分别对应 被观察者、观察者 被观察者需要继承 Observable ；它已经给你写好注册、取消等方法了，省事了不少，直接 super 调就行，不过需要注意的是 setChanged 方法，如果你不调用这个告诉它数据已经变化，notifyObservers 是不会执行的这样其实有个好处就是避免了频繁的更新 1234567891011121314151617181920public class StandardLoli extends Observable &#123; private String name = "佳芷"; // 此方法就当作是更新状态的方法吧 public void speak(String name) &#123; System.out.println("大哥哥" + name + "快来呀！！"); // 标记此 Observable 对象为已改变的对象；现在 hasChanged 方法将返回 true super.setChanged(); // 更新完后通知所有观察者 // 如果 hasChanged 方法指示对象已改变，则通知其所有观察者，并调用 clearChanged 方法来指示此对象不再改变。 // 还可以携带一个对象传递 super.notifyObservers(); // super.notifyObservers("data"); &#125; public String getName() &#123; return name; &#125;&#125; Observable 内部保存多个观察者的是一个 AbstractList 集合，看一下源码应该已经保证了线程安全，但是因为它和 ArrayList 的存储结构是一样的，如果需要频繁操作还是要用链表notifyObservers 方法可以传一个 data 数据，这相当于是“推”；如果不传就相当于“拉”；所以，Java 的实现是支持这两种方式的 然后就是观察者了，需要实现 Observer 接口，重写 update 方法 1234567891011121314151617public class Lolicon implements Observer &#123; private String status = "我来了！！！"; public Lolicon() &#123;&#125; public Lolicon(String status) &#123; this.status = status; &#125; // 会传递过来被观察者的对象实体，和数据（如果有的话） @Override public void update(Observable o, Object arg) &#123; // 更新自己..... StandardLoli loli = (StandardLoli) o; System.out.println(status + loli.getName()); &#125;&#125; 其实在这里的构造函数中可以接收一个被观察者，直接将本对象注册，这好像才是比较正确的使用姿势……然后写个测试类来测试下，看看效果： 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; // 创建被观察者 StandardLoli loli = new StandardLoli(); // 创建观察者 Lolicon lolicon1 = new Lolicon(); Lolicon lolicon2 = new Lolicon("飞奔而来!!"); Lolicon lolicon3 = new Lolicon("从天而降!!"); // 注册观察者，顺序是先加入的后执行 loli.addObserver(lolicon1); loli.addObserver(lolicon2); loli.addObserver(lolicon3); // 更新被观察者状态 loli.speak("bfchengnuo"); &#125;&#125; 嗯，这样看的话确实简单了不少，只需要两个类呢 使用 Java 的实现时；不要依赖于观察者被通知的次序 因为 notifyObservers 是 Java 它实现的，具体的实现思路可能和我们自己的并不相同 参考这里推荐一篇文章：https://www.zybuluo.com/pastqing/note/191632http://www.cnblogs.com/java-my-life/archive/2012/05/16/2502279.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构中的树]]></title>
    <url>%2F2017%2F05%2F17%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%AD%E7%9A%84%E6%A0%91%2F</url>
    <content type="text"><![CDATA[今天刚好有机会复习了下数据结构里的树，悲哀的发现基本快忘光啦！看来是需要做点记录什么的了！正好前几天学 JDBC 的时候在数据库设计的时候还有点这方面的东西没说，这次一并补上！ 二叉树性质 非空二叉树的第 n 层上至多有 2^(n-1) 个元素。 深度为 h 的二叉树至多有 2^h-1 个结点。 满二叉树在满二叉树中若其深度为h，则其所包含的结点数必为 2^h-1 。所有终端都在同一层次，且非终端结点的度数为2。通俗讲就是： 要么是叶子结点(结点的度为0)，要么结点同时具有左右子树 (结点的度为 2 )。 完全二叉树完全二叉树是由满二叉树而引出来的。对于深度为 K 的，有 n 个结点的二叉树，当且仅当其每一个结点都与深度为 K 的满二叉树中编号从 1 至 n 的结点一一对应时称之为完全二叉树。就是说：每层结点都完全填满，在最后一层上如果不是满的，则只缺少右边的若干结点，就是不能出现只有右边没有左边的情况，你可以都没有或者只有左边，或者都有。对于完全二叉树，设一个结点为 i 则其父节点为 i/2 ，2i 为左子节点，2i+1 为右子节点。 二叉树的遍历主要说的是深度优先的遍历方法，使用的均是递归的思想 先序遍历根-左-右 中序遍历左-根-右 后序遍历左-右-根 前序遍历：abdefgc中序遍历：debgfac后序遍历：edgfbca层次遍历(广度优先了)：abcdfeg 根据序列画二叉树这应该是考试最喜欢的题目了，给两个遍历序列让你还原二叉树啥的要还原出二叉树，必须要给一个 中序遍历 的序列才行，另外一个无所谓了 套路就是根据先序遍历或者后序遍历确定根的位置，然后根据中序遍历确定左右分支来个栗子：已知：先序遍历为： abdgcefh ；中序遍历为：dgbaechf；方便起见，先序遍历编号为1号，另一个为2号 下面开始画，首先根据先序遍历确定根是 a；然后再看2号，dgb |a| echf ；这样就分开了，左边的是左半枝，右边的是右半枝；先画左半枝吧，那要先确定左边的根是谁，也就是 a 的孩子是谁，拿着左半边的 dgb 去1号找，发现 b 在最前面，先序遍历最前面的就是根啦！所以就确定 a 的左孩子就是 b 了，然后再用 b 根据2号确定左半枝是 dg，右半枝是空，然后依次类推…最后得出： 树的转换树和二叉树之间可以转换，森林于二叉树之间也可以转换，二叉树就是吊 树与二叉树的转换一般的套路就是 画线、抹线、旋转（调整），这个方法就不说了，一搜一大把，利用规律可以不用画线快速写出来，比如下面的一颗简单的树：遵循的套路是：如果此节点有孩子，把最左边（或者叫第一个）的孩子挂在左孩子的位置，如果有直接兄弟，就挂在右孩子的位置按照上面的图画的步骤是：首先 A 是根节点，最左边的孩子是 B ，就把 B 放在 A 的左孩子位置，A 没有兄弟所有右孩子的位置为空；这样 A 就画完了，继续画 B ，B 的左孩子位置应该是第一个孩子 E , 右孩子应该是 B 的兄弟 C ，依次画下去….最后：如何把这个二叉树还原成树呢，也很简单，就是逆操作！套路是：此节点如果有左孩子，就变成其的孩子，它的所有的右分支就是其兄弟还是上图：A 有左孩子 B ，所以 B 是它（还原成树后的）的孩子，A 没有右孩子，所有没有兄弟，就是所谓的 根 了！然后再看 B，B 有左孩子 E ，所以 E 是它的孩子，同时 B 的右分支有 C、D（就是顺着右边那条线下去）；所以 C 和 D 是它的兄弟节点，依次画下去…. 由于树根没有兄弟，故树转化为二叉树后，二叉树的根结点的右子树必为空。 森林与二叉树森林就是由多棵树组成的，所以套路一样，首先把森林里的每棵树按照上面的套路弄成二叉树 ，树转化为二叉树有个特点就是根节点的右孩子是空的！所以，找棵树的根作为主根，其他树往主根的右孩子节点上挂就是了，然后最后旋转调整下，其实相当于抓住主根向上拎一下，然后自然就是一个二叉树了！ 顺着右孩子数，每一个节点就是一棵树，也能很容易看出这个森林由多少树组成的！至于拆，也一个样，顺着右孩子的每一个节点拆下来就是一棵树，然后按照上面的套路再还原回去就行了 数据库无限分类设计在前面说数据库表的设计的时候，有个叫自连接的方案，是给关于分类这种场景用的，使用一张表存，用一个字段指向本表中的父节点的 id ，应该还记得…这样有个很明显的问题是，当层次很深的时候查询非常慢，因为要递归！ 于是，就有好事之人画了这这图：按照这样的结构在数据库存储，很明显这是二叉树，并且还是先序遍历，按照上图设计每一个节点也就是分类、对象的表结构，除了名称和 id 要加一个左值和右值了 通过上面的二叉树的图可以看出有几个特点： 找某个分类下的条目就是左值和右值之间的条目比如：找笔记本分类下的所有商品就是在 12-17 之间找 层次（深度）就是看属于几个根而决定的，或者说看有几个爸爸比如：西门子有冰箱和商品两个爸爸，那深度就是3 区分父子节点爸爸的左值小于孩子的左值；爸爸的右值大于孩子的右值 层次的顺序问题顺序和左值相关，左值越小的应该排在最前面，当然是对某一分类而言 增加某个节点只需要把后面的节点的左值右值全部统一加 2 就可以了，用 SQL 语句非常容易就能解决 在设计对象的时候，对象除了表里的“属性”还应有一个层次或者叫深度的属性，这个属性的值可以根据上面的特点算出来，这个很关键，知道层次后才能用代码“画”出这样的树状层次结构 关键在于查询语句的写法了，首先要算出深度，因为父和子都在一张表里存着，哪就把这一张表想象成两张表，一张存父，一张存子，比如下面的 SQL 语句就能查到每个节点的爸爸： 12345select * from test as parent,test as child where parent.lft &lt; child.lft and parent.rgt &gt; child.rgt;# ThinkPad 笔记本# ThinkPad 商品# ... 也就是说根据某个节点的名称出现多少次就能确定有几个爸爸，也就能确定其的层次，层次就是父节点个数+1；为了方便可以直接把 &gt; 改成 &gt;= 这样就不需要再+1了想要计算层次（深度），就是计算某个名称（分类）出现了多少次，可以使用 group 进行归组，然后用 count 函数统计下就可以了，完整语句： 12select child.id, child.name,count(child.name) as depth from test as parent,test as child where parent.lft &lt; child.lft and parent.rgt &gt; child.rgt group by(child.name) order by (child.lft); 最后还对左值进行了排序，这样查询出来的数据就是按顺序的：id、名称、深度（层次） 了，也就是说每一类的父节点和子节点挨着，封装到对象中进行使用就可以了 用心去去体会！]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet技术之过滤器]]></title>
    <url>%2F2017%2F05%2F13%2FServlet%E6%8A%80%E6%9C%AF%E4%B9%8B%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[在 Servlet 规范 2.3 中定义了过滤器，它能够对 Servlet 容器的请求和响应对象进行检查和修改。Servlet 过滤器本身并不生成请求和响应对象，只是提供过滤功能。Servlet 过滤器能够在 Servlet 被调用之前检查 Request 对象，并修改 Request Header 和 Request 内容；在 Servlet 被调用之后检查 Response 对象，修改 Response Header 和 Response 的内容。 常用的应用如：设置统一编码、设置静态资源的缓存时间、控制权限访问、敏感词拦截等 Servlet 过滤器可以过滤的 Web 组件包括 Servlet，JSP 和 HTML 等文件。Filter 类似于 IO 中的过滤流，实现也类似于 Servlet。 了解Filter使用过滤器非常的简单，新建一个类，实现 Filter 接口就可以了，也就是需要实现三个方法，从名字来看，有两个是和生命周期有关 init(FilterConfig filterConfig) doFilter(ServletRequest request, ServletResponse response, FilterChain chain) destroy() 说说其生命周期，随服务器（或者说随 web 应用）的启动而执行 init 方法，随服务器的关闭而 执行 destroy 方法，也就是生命周期和服务器保持一致，某些只需要执行一次的方法可以放在 init 里面啦！ 拦截处理的操作一般都写在 doFilter 方法中，在这里可以对 request 和 response 进行一些处理，不过 ServletRequest 和 ServletResponse 一般需要转换成具体的 Servlet 实现对于的对象，如：HttpServletRequest 和 HttpServletResponse。 web 服务器在调用 doFilter 方法时，会传递一个 filterChain 对象进来，filterChain 对象是 filter 接口中最重要的一个对象，它也提供了一个 doFilter 方法，开发人员可以根据需求决定是否调用此方法，调用该方法，则 web 服务器就会调用 web 资源的 service 方法，即 web 资源就会被访问，否则 web 资源不会被访问。 在一个 web 应用中，可以开发编写多个 Filter，这些 Filter 组合起来称之为一个 Filter 链。不过需要注意下顺序的问题，web 服务器根据 Filter 在 web.xml 文件中的注册顺序，决定先调用哪个 Filter比如如果有两个过滤器，那么顺序类似：用户发起请求 –&gt; 请求被 Filter1 拦截 –&gt; 请求被 Filter2 拦截 –&gt; 响应被 Filter2 拦截 –&gt; 响应被 Filter1 拦截 –&gt; 响应返回给用户一般把 Filter 配置在所有的 Servlet 之前。 通过注解配置 filter 时，没有专门的指令来配置 filter 执行顺序，确定 filter 执行的先后是根据 filter 类名的字母表顺序。这非常不便于以后的维护，所以用不用，自己决定吧…. 设置全站编码的栗子这样是为了解决乱码的问题，我们的汉字太吊，乱码问题时常让我爆炸….通过简单的栗子，应该是学习或者复习比较快速的一个方法正好这个问题可以使用过滤器来解决，以后再也不用担心乱码的问题了！ 123456789101112131415161718192021222324252627282930313233public class MainFilter implements Filter &#123; private FilterConfig mConfig; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; mConfig = filterConfig; System.out.println("初始化成功！"); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; // 获取配置文件中设置的编码 String charset = mConfig.getInitParameter("charset"); if (charset.isEmpty()) charset = "utf-8"; HttpServletRequest req = (HttpServletRequest) request; HttpServletResponse resp = (HttpServletResponse) response; req.setCharacterEncoding(charset); // 下面两句是设置输出的编码，不设置也可以，因为一般不会在 Servlet 中进行输出，而是跳到 JSP 进行输出 resp.setCharacterEncoding(charset); resp.setContentType("text/html;charset=UTF-8"); // 放行 chain.doFilter(req, resp); &#125; @Override public void destroy() &#123; System.out.println("关闭...."); &#125;&#125; 要想生效，还需要在 web.xml 文件中进行响应的配置： 123456789101112131415&lt;filter&gt; &lt;filter-name&gt;MainFilter&lt;/filter-name&gt; &lt;filter-class&gt;filter.MainFilter&lt;/filter-class&gt; &lt;!-- 配置初始化参数 --&gt; &lt;init-param&gt; &lt;param-name&gt;charset&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;!-- 一个过滤器可以设置多个 mapping --&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;MainFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 可以看到，在 XML 中配置的初始化参数可以在 init 方法中拿到，就是 FilterConfig 这个对象；可以通过 getInitParameter 来获取设置的值 但是呢，这样只能解决 Post 方式提交的乱码问题，Get 方式的乱码还是会有，想要完全的解决需要进行增强 Request 的方式，在获取参数的时候进行判断，分别对 Get 和 Post 进行不同的处理，参考下面增强 Request 目录下的内容，关键代码为： 1234567891011121314151617@Overridepublic String getParameter(String name) &#123; String value = mRequest.getParameter(name); // 非 Get 方式直接返回，不处理 if (!mRequest.getMethod().equalsIgnoreCase("get")) &#123; return value; &#125; if (value == null) &#123; return null; &#125; try &#123; return value = new String(value.getBytes("iso-8859-1"),mRequest.getCharacterEncoding()); &#125; catch (UnsupportedEncodingException e) &#123; throw new RuntimeException(e); &#125;&#125; 控制静态资源缓存这次使用注解的方式来配置，如果配置多个过滤器的话，顺序方面会比较头疼，按名字的字母来排序，上面也提到过这个问题，但是使用注解确实方便了很多： 1234567891011121314151617181920212223242526272829303132333435363738@WebFilter(filterName = "CacheFilter", urlPatterns = &#123;"*.jpg", "*.png", "*.css", "*.js"&#125;, initParams = &#123; @WebInitParam(name = "jpg", value = "10"), @WebInitParam(name = "css", value = "10"), @WebInitParam(name = "js", value = "10") &#125;)public class CacheFilter implements javax.servlet.Filter &#123; private FilterConfig mConfig; public void destroy() &#123;&#125; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws ServletException, IOException &#123; HttpServletResponse resp = (HttpServletResponse) response; HttpServletRequest req = (HttpServletRequest) request; // 获取到用户访问的资源 String uri = req.getRequestURI(); // 获取配置的资源的缓存时间 int expires = 0; if (uri.endsWith(".jpg")) &#123; expires = Integer.parseInt(this.mConfig.getInitParameter("jpg")); &#125; else if (uri.endsWith(".js")) &#123; expires = Integer.parseInt(this.mConfig.getInitParameter("js")); &#125; else if (uri.endsWith(".css")) &#123; expires = Integer.parseInt(this.mConfig.getInitParameter("css")); &#125; // 设置缓存时间，注意格式 resp.setDateHeader("expires", System.currentTimeMillis() + expires * 60 * 1000); // 放行 chain.doFilter(req, resp); &#125; public void init(FilterConfig config) throws ServletException &#123; mConfig = config; &#125;&#125; 这样就没有必要去设置 web.xml 文件了，上面的过滤器只会拦截设置的那些静态文件的请求，比如 JSP 中有个 img 标签，当加载 JSP 页面的时候，JSP 本身请求不会被拦截，但是发送的图片请求会被捕获到，然后设置个缓存时间还有就是：刷新代表着重新执行上次操作，所以就算设置了缓存，刷新操作还是会重新进行获取 增强Request和Response前面也说过，增强功能一般有三种方式：1、继承 2、装饰/包装 3、动态代理 第一种用的基本很少，因为在服务器相关的对象中往往都封装着很多的信息，如果使用继承的话这些信息就会丢失第二种装饰模式确实可以，但是方法比较多的话会很复杂；即使 Apache 给提供了 HttpServletRequestWrapper 类来方便开发，这个类默认就是经过装饰了的，但是内部全部调用的是构造函数中传入的构件中的方法，我们用的时候直接继承自它，需要增强什么方法再单独覆盖，这样就能大大简化步骤了第三种还没有看到，挖个坑，以后单独开一篇 关于 Request 的具体的应用可以是处理编码来解决乱码问题，敏感词过滤审核，进行数据转义防止 XXS 等关于 Response 的具体应用最直接的就是用来搞压缩，大大节省流量啊，gzip 主要是压缩文本数据，还有就是缓存数据，在过滤器定义一个 Map 缓存那些不经常更新的数据（byte[]） 具体代码就不写了，有点长，可以参考我的笔记：Github 其他补充存下在过滤器中常用的一些对象使用的套路 获取ServletContext通过 request 或者 filterConfig 都可以得到 ServletContext 对象: 1234HttpServletRequest req = (HttpServletRequest)request;ServletContext context = req.getSession().getServletContext();ServletContext context = filterConfig.getServletContext(); web.xml节点配置除了上面那几个，补充两个比较常用的 &lt;servlet-name&gt;指定过滤器所拦截的Servlet名称。 &lt;dispatcher&gt;拦截方式，可以设定五个值（可以配置多个，默认 REQUEST）：REQUEST：当用户直接访问页面时，Web 容器将会调用过滤器。如果目标资源是通过 RequestDispatcher 的 include() 或 forward() 方法访问时，那么该过滤器就不会被调用。INCLUDE：如果目标资源是通过 RequestDispatcher 的 include() 方法访问时，那么该过滤器将被调用。除此之外，该过滤器不会被调用。FORWARD：如果目标资源是通过 RequestDispatcher 的 forward() 方法访问时，那么该过滤器将被调用，除此之外，该过滤器不会被调用。ERROR：如果目标资源是通过声明式异常处理机制调用时，那么该过滤器将被调用。除此之外，过滤器不会被调用。ASYNC：异步处理的请求 最后一些代码可以参考我的笔记：Github]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-策略模式]]></title>
    <url>%2F2017%2F05%2F10%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Wiki 上说，策略模式指对象有某个行为，但是在不同的场景中，该行为有不同的实现算法。比如每个人都要“交个人所得税”，但是“在美国交个人所得税”和“在中国交个人所得税”就有不同的算税方法。 策略模式我感觉是非常简单的一种设计模式，应该也是一种比较基本的设计模式，在其它的设计模式中也能看到它的影子 模式定义策略模式(Strategy Pattern)：定义一系列算法，将每一个算法封装起来，并让它们可以相互替换。策略模式让算法独立于使用它的客户而变化，也称为政策模式(Policy)。 策略模式是一种对象行为型模式。 模式动机完成一项任务，往往可以有多种不同的方式，每一种方式称为一个策略，我们可以根据环境或者条件的不同选择不同的策略来完成该项任务。在软件开发中也常常遇到类似的情况，实现某一个功能有多个途径，此时可以使用一种设计模式来使得系统可以灵活地选择解决途径，也能够方便地增加新的解决途径。 在软件系统中，有许多算法可以实现某一功能，如查找、排序等，一种常用的方法是硬编码(Hard Coding)在一个类中；如需要提供多种查找算法，可以将这些算法写到一个类中，在该类中提供多个方法，每一个方法对应一个具体的查找算法当然也可以将这些查找算法封装在一个统一的方法中，通过 if…else… 等条件判断语句来进行选择。这两种实现方法我们都可以称之为硬编码，如果需要增加一种新的查找算法，需要修改封装算法类的源代码；更换查找算法，也需要修改客户端调用代码。该类代码将较复杂，维护较为困难。 可以得出策略模式的使用场景 针对同一类型问题的多种处理方式，仅仅是具体行为有差别时。 需要安全的封装多种同一类型的操作时。 出现同一抽象多个子类，而又需要使用 if-else 或者 switch-case 来选择时。 通俗来说就是： 如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 一个系统需要动态地在几种算法中选择一种。 如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。 不希望客户端知道复杂的、与算法相关的数据结构，在具体策略类中封装算法和相关的数据结构，提高算法的保密性与安全性。 策略模式： 定义了一族算法（业务规则） 封装了每个算法 这族的算法可互换代替（interchangeable） 使用策略模式其实策略模式我感觉在很多地方见过，说明应用还是很广泛的，貌似常用的回调技术也是这个模式的一种体现；这关键是依赖了一个设计原则：面向接口而不是面向实现 Context: 环境类 Strategy: 抽象策略类 ConcreteStrategy: 具体策略类 下面的栗子继续使用上次的吧，就设置情景为我要交朋友，但是交什么朋友是不确定的吧，有好多种（实现），虽然不太恰当，无所谓啦~~~ 首先是抽象策略类，就是定义一个接口了，就是我要交的朋友的抽象： 123public interface Loli &#123; void hug();&#125; 具体的策略类，实现上面的接口： 1234567891011121314// No.1public class StandardLoli implements Loli &#123; @Override public void hug() &#123; System.out.println("要抱抱"); &#125;&#125;// No.2public class LegitimateLoli implements Loli &#123; @Override public void hug() &#123; System.out.println("(づ｡◕‿‿◕｡)づ"); &#125;&#125; 下面是环境类，也就是使用场景，通常还可以在这个类中加入接口的一个 setter 方法，这样就可以动态的改变“策略对象”了： 1234567891011public class MakeFriends &#123; private Loli mLoli; public MakeFriends(Loli loli) &#123; mLoli = loli; &#125; public void start() &#123; mLoli.hug(); &#125;&#125; 然后就是测试了，也就是应该如何去使用，应该越看越熟悉了： 12345678910111213public class Main &#123; public static void main(String[] args) &#123; MakeFriends makeFriends; makeFriends = new MakeFriends(new StandardLoli()); makeFriends.start(); System.out.println("---------------------------------"); makeFriends = new MakeFriends(new LegitimateLoli()); makeFriends.start(); &#125;&#125; 评价策略模式主要用来分离算法，根据相同的行为抽象来做不同的具体策略实现。优点： 结构清晰明了、使用简单直观。 耦合度相对而言较低，扩展方便。 操作封装也更为彻底，数据更为安全。 策略模式提供了对“开闭原则”的完美支持，用户可以在不修改原有系统的基础上选择算法或行为，也可以灵活地增加新的算法或行为。 策略模式提供了管理相关的算法族的办法。 策略模式提供了可以替换继承关系的办法。 使用策略模式可以避免使用多重条件转移语句。 缺点： 随着策略的增加，子类也会变得繁多。可以通过使用享元模式在一定程度上减少对象的数量。 客户端必须知道所有的策略类，并自行决定使用哪一个策略类。 策略模式中的设计原则 开闭原则（Open-Closed Principle，缩写为OCP） 一个软件实体应当对扩展(例如对抽象层的扩展)开放，对修改(例如对抽象层的修改)关闭。即在设计一个模块的时候，应当使这个模块可以在不被修改的前提下被扩展。 开闭原则的关键，在于抽象。策略模式，是开闭原则的一个极好的应用范例。 里氏替换原则（Liskov Substitution Principle，缩写为LSP） 一个软件实体如果使用的是一个基类的话，那么一定适用于其子类，而且它根本不能察觉到基类对象和子类对象的区别。比如，假设有两个类，一个是 Base 类，一个是 Derived 类，并且 Derived 类是 Base 类的子类。那么一个方法如果可以接受一个基类对象b的话：method1(Base b)，那么它必然可以接受一个子类对象d，也即可以有 method1(d)。反之，则不一定成立 里氏替换原则讲的是基类与子类的关系。只有当这种关系存在时，里氏替换关系才存在，反之则不存在。 策略模式之所以可行的基础便是里氏替换原则：策略模式要求所有的策略对象都是可以互换的，因此它们都必须是一个抽象策略角色的子类。在客户端则仅知道抽象策略角色类型，虽然变量的真实类型可以是任何一个具体策略角色的实例 参考https://www.zybuluo.com/pastqing/note/198333https://github.com/simple-android-framework/android_design_patterns_analysis/tree/master/strategy/gkerisonhttp://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/strategy.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS3学习笔记(一)]]></title>
    <url>%2F2017%2F05%2F08%2FCSS3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[CSS3 是 CSS2 的升级版本，3 只是版本号，它在 CSS2.1 的基础上增加了很多强大的新功能。在编写 CSS3 样式时，由于现时不同浏览器支持程度不同，需要加上不同的浏览器前缀来区分。它表示该 CSS 属性或规则尚未成为 W3C 标准的一部分，是浏览器的私有属性，虽然目前较新版本的浏览器都是不需要前缀的，但为了更好的向前兼容前缀还是少不了的。 前缀 浏览器 -webkit chrome 和 safari -moz firefox -ms IE -o opera 总是为兼容性操碎了心呐….后来才知道，原先写的 CSS 都是 3-，真正 3 的魅力完全没体现出来 CSS3的特性简单的说就是带来的好处，CSS3 把很多以前需要使用图片和脚本来实现的效果、甚至动画效果，只需要短短几行代码就能搞定。比如圆角，图片边框，文字阴影和盒阴影，过渡、动画等。 CSS3 简化了前端开发工作人员的设计过程，加快页面载入速度。 先大体总结一下比较让人眼前一亮的特性： 选择器以前我们通常用 class、 ID 或 tagname 来选择 HTML 元素，CSS3 的选择器强大的难以置信。它们可以减少在标签中的 class 和 ID 的数量更方便的维护样式表、更好的实现结构与表现的分离。 圆角效果以前做圆角通常使用背景图片，或繁琐的元素拼凑，现在很简单了 border-radius 帮你轻松搞定。 块阴影和文字阴影可以对任意 DIV 和文字增加投影效果。 色彩CSS3 支持更多的颜色和更广泛的颜色定义。新颜色 CSS3 支持 HSL ， CMYK ，HSLA and RGBA。 渐变效果以前只能用 Photoshop 做出的图片渐变效果，现在可以用 CCS 写出来了。IE 中的滤镜也可以实现。 个性化字体网页上的字体太单一？使用 @Font-Face 轻松实现定制字体。还可以“画”出一些图标来 多背景图一个元素上添加多层背景图片。 边框背景图 变形处理你可以对 HTML 元素进行旋转、缩放、倾斜、移动、甚至以前只能用 JavaScript 实现的强大动画。 多栏布局可以让你不用使用多个 div 标签就能实现多栏布局。浏览器解释这个属性并生成多栏，让文本实现一个仿报纸的多栏结构。 媒体查询针对不同屏幕分辨率，应用不同的样式。 …… 边框首先，就从边框开始吧！ 圆角效果这个效果主要使用了 border-radius 属性：是向元素添加圆角边框。 1234567891011121314151617181920div.test&#123; /* 四个半径值分别是左上角、右上角、右下角和左下角，顺时针 */ border-radius: 5px 4px 3px 2px; /* 所有角都使用半径为10px的圆角 */ border-radius:10px;&#125;/* 实心圆 */div.circle&#123; height:100px;/*与width设置一致*/ width:100px; background:#9da; border-radius:50px;/*四个圆角值都设置为宽度或高度值的一半*/ &#125;/*实心左半圆*/ div.semi-circle&#123; height:100px; width:50px; background:#9da; border-radius:50px 0 0 50px; &#125; 它使用的单位不止是 px，也可以使用百分比之类的，但是效果不太理想 阴影使用的是 box-shadow 属性：是向盒子添加阴影。支持添加一个或者多个。语法：box-shadow: X轴偏移量 Y轴偏移量 [阴影模糊半径] [阴影扩展半径] [阴影颜色] [投影方式];前两个是必填的，其他的可以不写，阴影颜色默认是黑色，投影方式可以写在任何位置，当是 inset 时，是内部阴影；省略为外阴影 123456789/*外阴影效果，比较有立体感啦*/.box_shadow&#123; box-shadow:4px 2px 6px #333333; &#125;/*多阴影效果，用逗号分割*/.box_shadow&#123; box-shadow:4px 2px 6px #f00, -4px -2px 6px #000, 0px 0px 12px 5px #33CC00 inset;&#125; 其实上面参数的介绍挺抽象的，反正我看不懂 阴影模糊半径：此参数可选，其值只能是为正值，如果其值为 0 时，表示阴影不具有模糊效果，其值越大阴影的边缘就越模糊； 阴影扩展半径：此参数可选，其值可以是正负值，如果值为正，则整个阴影都延展扩大，反之值为负值时，则缩小； 至于X轴偏移量和Y轴偏移量值可以设置为负数 ；可以理解为正负就是方向 边框背景图顾名思义就是为边框应用背景图片，它和我们常用的 background 属性比较相似，使用的是 border-image 属性 12345#border-image&#123; background:#F4FFFA; width:210px; height:210px; border:70px solid #ddd; border-image:url(borderimg.png) 70 repeat &#125; 一个矩形，有四个边框。如果应用了边框图片， 图片会自动被切割分成四等分。用于四个边框。所以要设置下切割高度，比如上面的 70px图片的延伸方式有三种：round（平铺）、 repeat（重复）、 stretch（拉伸）难度没多少，效果自己写下看看就知道了 颜色相关CSS3 支持 RGBA，是的，后面的 A 是透明度，用法：background-color:rgba(100,120,60,0.5); 下面再来说说渐变色彩：CSS3 Gradient 分为线性渐变(linear)和径向渐变(radial)。由于不同的渲染引擎实现渐变的语法不同，这里只针对线性渐变的 W3C 标准语法来分析其用法，使用 linear-gradient 属性 123p&#123; background-image:linear-gradient(to left, red, orange,yellow,green,blue,indigo,violet);&#125; 可以看出是支持多种颜色的，两个一组，对应开始和结束；那么第一个参数是啥呢？肯定和方向（角度）有关 角度 英文 作用 0 deg to top 从下向上 90 deg to right 从左向右 180 deg to bottom 从上向下 270 deg to left 从右向左 - to top left 右下角到左上角 - to top right 左下角到右上角 文字与字体text-overflow 用来设置是否使用一个省略标记（…）标示对象内文本的溢出。但是 text-overflow 只是用来说明文字溢出时用什么方式显示，要实现溢出时产生省略号的效果，还须定义强制文本在一行内显示（white-space:nowrap）及溢出内容为隐藏（overflow:hidden），只有这样才能实现溢出文本显示省略号的效果 123456/* ellipsis:显示省略标记；clip:表示剪切 */p&#123; text-overflow: ellipsis; overflow: hidden; white-space: nowrap; &#125; word-wrap 也可以用来设置文本行为，当前行超过指定容器的边界时是否断开换行。它有两个可以设置的属性：normal : 控制连续文本换行（测试应该遇到长单词是不换行）break-word: 表示内容将在边界内换行（就算是整个单词，太长也会换行）normal 为浏览器默认值，break-word 设置在长单词或 URL 地址内部进行换行，此属性不常用，用浏览器默认值即可。 嵌入字体@font-face@font-face 能够加载服务器端的字体文件，让浏览器端可以显示用户电脑里没有安装的字体。语法： 12345678910@font-face &#123; font-family : 字体名称; src : 字体文件在服务器上的相对或绝对路径;&#125;p &#123; font-size :12px; font-family : "My Font"; /*必须项，设置 @font-face 中 font-family 同样的值*/&#125; 其实利用它可以完成 字体图标 的效果，这个我感觉还是非常实用的，不需要再用 PS 切图了，具体的实现打算专门写一篇，毕竟看了下比较复杂的….(其实是功力不够) 发现一篇写的很好的文章，很详细：https://www.w3cplus.com/content/css3-font-face 文本阴影text-shadow 可以用来设置文本的阴影效果。语法为：text-shadow: X-Offset Y-Offset blur color;X-Offset：表示阴影的水平偏移距离，其值为正值时阴影向右偏移，反之向左偏移Y-Offset：是指阴影的垂直偏移距离，如果其值是正值时，阴影向下偏移，反之向上偏移Blur：是指阴影的模糊程度，其值不能是负值，如果值越大，阴影越模糊，反之阴影越清晰，如果不需要阴影模糊可以将 Blur 值设置为 0Color：是指阴影的颜色，其可以使用 rgba 色。 背景相关CSS3 支持设置元素背景图片的原始起始位置。语法：background-origin ： border-box | padding-box | content-box;参数分别表示背景图片是从边框，还是内边距（默认值），或者是内容区域开始显示。需要注意的是，如果背景不是 no-repeat，这个属性无效，它会从边框开始显示。 裁剪可以使用属性 background-clip语法：background-clip ： border-box | padding-box | content-box | no-clip参数分别表示从边框、或内填充，或者内容区域向外裁剪背景。no-clip 表示不裁切，和参数 border-box 显示同样的效果。backgroud-clip 默认值为 border-box。 还有可以设置背景图片的大小，以长度值或百分比显示，还可以通过 cover 和 contain 来对图片进行伸缩。语法：background-size: auto | &lt;长度值&gt; | &lt;百分比&gt; | cover | contain参数：auto：默认值，不改变背景图片的原始高度和宽度；&lt;长度值&gt;：成对出现如 200px 50px，将背景图片宽高依次设置为前面两个值，当设置一个值时，将其作为图片宽度值来等比缩放；&lt;百分比&gt;：0％~100％之间的任何值，将背景图片宽高依次设置为所在元素宽高乘以前面百分比得出的数值，当设置一个值时同上；cover：顾名思义为覆盖，即将背景图片等比缩放以填满整个容器；contain：容纳，即将背景图片等比缩放至某一边紧贴容器边缘为止。 然后就是多重背景，常见的写法一般有两种： 12345678910111213141516171819.demo&#123; width: 300px; height: 140px; border: 1px solid #999; background-image: url(http://img.mukewang.com/54cf2365000140e600740095.jpg), url(http://img.mukewang.com/54cf238a0001728d00740095.jpg), url(http://img.mukewang.com/54cf23b60001fd9700740096.jpg); background-position: left top, 100px 0, 200px 0; background-repeat: no-repeat, no-repeat, no-repeat;&#125;.task &#123; width: 300px; height: 140px; border: 1px solid #999; background:url(http://static.mukewang.com/static/img/logo_index.png) no-repeat, url(http://static.mukewang.com/static/img/logo_index.png) no-repeat; &#125; 如果有 size 值，需要紧跟 position 并且用 “/“ 隔开 ([/background-size])；background-color 只能设置一个。 选择器在 CSS2 中引入了一些属性选择器，而 CSS3 在 CSS2 的基础上对属性选择器进行了扩展，新增了3个属性选择器，使得属性选择器有了通配符的概念，这三个属性选择器与 CSS2 的属性选择器共同构成了 CSS 功能强大的属性选择器 关于这个我曾经整理过一篇文章见：CSS/jQuery选择器总结 简单说这三种选择器是： 123456789101112/* 选择 a 标签中 class 以 column 开头的 */a[class^=column]&#123; background:red;&#125;/* 选择 a 标签中 href 属性以 doc 结尾的 */a[href$=doc]&#123; background:green;&#125;/* 选择 a 标签中 title 属性包含 box 的 */a[title*=box]&#123; background:blue;&#125; CSS3 还增加了一些伪类选择器，下面列举一些，全套的参考上面那篇文章： :root 选择器从字面上我们就可以很清楚的理解是根选择器，他的意思就是匹配元素E所在文档的根元素。在 HTML 文档中，根元素始终是 &lt;html&gt;可以用来设置页面的背景颜色 :not 选择器否定选择器，和 jQuery 中的 :not 选择器一模一样，可以选择除某个元素之外的所有元素。就拿 form 元素来说，比如说你想给表单中除 submit 按钮之外的 input 元素添加红色边框，CSS 代码可以写成 123input:not([type="submit"])&#123; border:1px solid red;&#125; :empty 选择器表示的就是空。用来选择没有任何内容的元素，这里没有内容指的是一点内容都没有，哪怕是一个空格也算不为空。 :target 选择器目标选择器，用来匹配文档(页面)的 url 的某个标志符的目标元素。比如经常用的锚链接，在段落设置相应的 id 属性；在连接使用 #id 来定位（使用 href） 12345678910111213/* 点击#id连接后id为brand元素的样式 */#brand:target &#123; background: orange; color: #fff;&#125;#jake:target &#123; background: blue; color: #fff;&#125;#aron:target &#123; background: red; color: #fff;&#125; 如果只定义一个 :target{} 默认对应的是全部的链接是 #id 元素，也就是说点击他们反映在各自 id 元素上的样式是一致的，但最多只能标记一个使用样式，好像描述的不太清楚，自己测试一下就知道了测试地址：http://www.w3school.com.cn/tiy/t.asp?f=css_sel_target :nth-child(n) 选择器用来定位某个父元素的一个或多个特定的子元素。其中“n”是其参数，而且可以是整数值(1,2,3,4)，也可以是表达式(2n+1、-n+5)和关键词(odd、even)，但参数 n 的起始值始终是 1，而不是 0。也就是说，参数 n 的值为 0 时，选择器将选择不到任何匹配的元素。类似的还有 nth-last-child(n) 就是倒着来的，以及类似的 E:nth-of-type 指定子元素的，比如匹配奇数行这样写： 1234567ol &gt; li:nth-child(2n+1)&#123; background: green;&#125;/* 选择倒数第五个 */ol &gt; li:nth-last-child(5)&#123; background: green;&#125; :enabled 选择器在 Web 的表单中，有些表单元素有可用（“:enabled”）和不可用（“:disabled”）状态，比如输入框，密码框，复选框等。在默认情况之下，这些表单元素都处在可用状态。那么我们可以通过伪选择器 :enabled 对这些表单元素设置样式。 ::selection 伪元素它是用来匹配突出显示的文本(用鼠标选择文本时的文本)。浏览器默认情况下，用鼠标选择网页文本是以“深蓝的背景，白色的字体”显示的不过在 Firefox 浏览器还需要添加前缀：::-moz-selection 12345678::selection&#123; background: orange; color: white;&#125;::-moz-selection&#123; background: orange; color: white;&#125; ::before 和 ::after这两个主要用来给元素的前面或后面插入内容，这两个常和 “content” 配合使用，使用的场景最多的就是清除浮动。我也看到好多配合 @font-face 来使用字体图标的 其他使用 input 定义 button 的时候，要想按钮处于禁用状态可以直接在标签上加 disabled 属性，类似 checked 这样的标识，如果使用 css 的方式实现可以试试下面两种方案： 1234567891011.disabled &#123; opacity: 0.6; cursor: not-allowed;&#125;.btn.disabled, .btn[disabled], fieldset[disabled] .btn &#123; pointer-events: none; cursor: not-allowed; box-shadow: none; opacity: .65;&#125;]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优雅的JDBC写法]]></title>
    <url>%2F2017%2F05%2F06%2F%E4%BC%98%E9%9B%85%E7%9A%84JDBC%E5%86%99%E6%B3%95%2F</url>
    <content type="text"><![CDATA[介绍下在数据库的相关操作中什么是元数据，以及如果自己写工具类的时候应该如何写，或者可以使用别人写好的工具类，比如 Apache 的 dbutils 工具类，就是不知道现在是不是还在用，也当作是为以后学习框架做准备吧！ 元数据首先，来了解下什么是元数据，当然主要说的是数据库相关的，简单说它就是：数据库、表、列的定义信息所以，指的基本就是数据库一些“对象”的信息，基本可以分为三类 DatabaseMetaData数据库的元数据，封装了数据库的相关信息，比如版本、名字、驱动、URL、用户名等等 ParameterMetaData参数元数据，简单说就是：获得预编译 SQL 语句中 “?” 信息。比如：参数的个数、类型等，但是并不是所有的数据库(驱动)都支持 ResultSetMetaData结果集的元数据，封装了结果集中的列数、列名称、列类型等信息 工具类写法下面就来优雅的写 JDBC 的工具类，嗯….起码算比较优雅 以前我们在工具类就写了两个方法，一个是获取连接，一个是释放相关资源；下面就来扩展一下，让其变得更加通用一些增加两个方法，一个用于增删改(update)，一个用于查询(query) 12345678910111213141516171819202122232425262728293031323334353637public static void update(String sql, Object[] params) throws SQLException &#123; Connection conn = null; PreparedStatement st = null; try &#123; conn = getConnection(); st = conn.prepareStatement(sql); // 预编译 // 填充参数 for (int i = 0; i &lt; params.length; i++) &#123; st.setObject(i + 1, params[i]); &#125; st.executeUpdate(); &#125; finally &#123; JdbcUtils.release(conn, st, null); &#125;&#125;public static Object query(String sql, Object[] params, ResultSetHandler handler) throws SQLException &#123; Connection conn = null; PreparedStatement st = null; ResultSet rs = null; try &#123; conn = getConnection(); st = conn.prepareStatement(sql); // 预编译 // 填充参数 for (int i = 0; i &lt; params.length; i++) &#123; st.setObject(i + 1, params[i]); &#125; rs = st.executeQuery(); // 回调 return handler.handler(rs); &#125; finally &#123; JdbcUtils.release(conn, st, rs); &#125;&#125; 基本思路应该看出来了，增删改的时候统一调用 update 方法，传入相应的 SQL 语句和参数数组就可以了；然后查询的时候单独一个方法 query ，除了需要 SQL 和参数数组还要传入一个 ResultSetHandler 对象，这个是个接口，是自己定义的，主要用于回调来处理结果集，因为 SQL 语句不确定，结果集也不确定如何处理，最妥的方式就是交给用户处理，既然是你写的 SQL 所以你肯定知道怎么处理结果集嘛！ 123public interface ResultSetHandler &#123; Object handler(ResultSet rs);&#125; 当然，如果让用户去实现接口多少还是有些麻烦的，为了更好的用户体验我们可以给用户准备几个常用的，比如把查询出来的数据封装到一个 Javabean 中去： 12345678910111213141516171819202122232425262728293031323334public class BeanHandler implements ResultSetHandler &#123; private Class&lt;SimpleTab&gt; clazz; public BeanHandler(Class&lt;SimpleTab&gt; clazz) &#123; this.clazz = clazz; &#125; @Override public Object handler(ResultSet rs) &#123; try &#123; if (rs == null || !rs.next()) &#123; return null; &#125; Object bean = clazz.newInstance(); // 得到结果集的元数据 ResultSetMetaData metaData = rs.getMetaData(); int count = metaData.getColumnCount(); for (int i = 0; i &lt; count; i++) &#123; // 获取每一列的名字 String name = metaData.getColumnName(i + 1); // 获取每一列的值 Object value = rs.getObject(name); // 利用反射技术得到相应的属性，使数据拷贝到 javabean 里 Field field = bean.getClass().getDeclaredField(name); field.setAccessible(true); field.set(bean, value); &#125; return bean; &#125; catch (InstantiationException | IllegalAccessException | SQLException | NoSuchFieldException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 这里就用到了之前说的元数据，还使用到了反射技术，调用的时候直接 new 这个实现类就可以了，传入一个 class 用来指定 bean 的类型；当然如果查询返回了多条记录，一般是封装到 List 集合中去，所以再写一个实现类吧： 12345678910111213141516171819202122232425262728293031public class BeanListHandler implements ResultSetHandler &#123; private Class clazz; public BeanListHandler(Class clazz) &#123; this.clazz = clazz; &#125; @Override public Object handler(ResultSet rs) &#123; List&lt;Object&gt; mList = new ArrayList&lt;&gt;(); try &#123; while (rs.next()) &#123; Object bean = clazz.newInstance(); ResultSetMetaData metaData = rs.getMetaData(); for (int i = 0; i &lt; metaData.getColumnCount(); i++) &#123; String name = metaData.getColumnName(i + 1); Object value = rs.getObject(name); // 反射，使用 Declared 说明获取的是私有变量 Field field = bean.getClass().getDeclaredField(name); field.setAccessible(true); field.set(bean, value); &#125; mList.add(bean); &#125; return mList; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 然后调用的时候就很简单了： 12345678910private static void extendedTest2() throws SQLException &#123; String sql = "SELECT * FROM simpletab"; Object[] params = &#123;&#125;; List list = (List) JdbcUtils.query(sql, params, new BeanListHandler(SimpleTab.class)); System.out.println(list.size()); // String sql = "insert INTO simpletab VALUES (?,?)"; // Object[] params = &#123;2, "测试"&#125;; // JdbcUtils.update(sql,params);&#125; 使用DBUtils上面的工具类是我们自己定义的，然后 Apache 其实也写了一个，就叫 dbutils 不过我看了下最新的也已经是几年前了，不知道在实际开发中还用不用，不过嘛，这种工具类应该一般也不需要更新dbutils 的使用和自定义的工具类很类似，实现原理可能也差不多它只是对 JDBC 进行简单封装，学习成本低，并且不会影响程序的性能。它不是一个框架 简单使用使用很简单，毕竟就是为了简化开发，导入相应的包后直接用就行了，主要用到的是 QueryRunner 对象，具体的方法可以去看 API，说点常用的，比如执行增删改操作： 123456private static void test1() throws SQLException &#123; QueryRunner qr = new QueryRunner(JdbcUtilsC3P0.getDataSource()); String sql = "INSERT INTO simpletab(id,name) VALUES (?,?)"; Object[] param = &#123;3, "hello"&#125;; qr.update(sql, param);&#125; 和我们自己写的非常相似，new 的时候接收一个 conn 对象，直接从连接池里取了再说查询，也是支持封装到 bean 和 List 中去，就举个封装到 List 中的栗子： 123456private static void test2() throws SQLException &#123; QueryRunner qr = new QueryRunner(JdbcUtilsC3P0.getDataSource()); String sql = "SELECT * FROM simpletab"; List&lt;SimpleTab&gt; list = qr.query(sql, new BeanListHandler&lt;&gt;(SimpleTab.class)); System.out.println(list);&#125; dbutils 还给了一些实现了 ResultSetHandler 的类，这里不多说了 API 写的很明白，比如可以封装到数组中去。如果返回的是单条数据可以使用 ScalarHandler 对象，用于获取结果集中第一行某列的数据并转换成 T （指定的泛型）表示的实际对象，更多的返回类型可以参考：http://www.cnblogs.com/myit/p/4272824.html 批量操作就是批量执行 SQL 了，当然是相同的 sql 类型，比如可以批量删除、添加 123456789private static void test3() throws SQLException &#123; QueryRunner qr = new QueryRunner(JdbcUtilsC3P0.getDataSource()); String sql = "INSERT INTO simpletab(id,name) VALUES (?,?)"; Object[][] params = new Object[5][]; for (int i = 0; i &lt; 5; i++) &#123; params[i] = new Object[]&#123;i + 4, "test" + i&#125;; &#125; qr.batch(sql, params);&#125; 主要区别就是参数的一维数组换成了二维数组，也就是说数组中的每一个数组中封装了批量操作中的一条数据，然后执行 batch 方法执行 事务操作事务这是一个重要的操作，就当是操作吧，既然要使用事务，那么就不能让它执行完一条 SQL 就关闭连接了，所以只能我们自己传入 conn 了，并且是关闭了自动提交的 conn 123456789101112131415161718192021private static void test4() throws SQLException &#123; QueryRunner qr = new QueryRunner(); Connection conn = null; try &#123; conn = JdbcUtilsC3P0.getConnection(); conn.setAutoCommit(false); String sql1 = "UPDATE simpletab SET name='loli' WHERE id=1"; qr.update(conn, sql1); String sql2 = "UPDATE simpletab SET name='loli' WHERE id=2"; qr.update(conn, sql2); conn.commit(); &#125; finally &#123; if (conn != null) &#123; conn.close(); &#125; &#125;&#125; 当然这样的写法是有问题的，如果是按照三层架构开发的话，dao 层是不允许有业务逻辑的，只能有执行 sql 的相关代码，类似这样的： 1234567891011121314151617181920// 由构造方法获得private static Connection conn = null;private static int updateExample(SimpleTab bean) throws SQLException &#123; QueryRunner qr = new QueryRunner(); String sql = "UPDATE simpletab SET name=? WHERE id=?"; Object[] param = &#123;bean.getName(), bean.getId()&#125;; return qr.update(conn, sql, param);&#125;private static SimpleTab findExample(int id) &#123; try &#123; QueryRunner qr = new QueryRunner(); String sql = "SELECT * FROM simpletab WHERE id=?"; Object[] param = &#123;id&#125;; return qr.query(conn, sql, new BeanHandler&lt;&gt;(SimpleTab.class), param); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125;&#125; 这样写的话，如果使用到事务，在 Server 层的时候获取连接后关闭自动提交，然后再传进来，最后还要记得手动 commit 和 close 但是这样写还是不太好，更优雅的写法可以考虑使用 ThreadLocal ，将 conn 绑定到线程上，这样也不用考虑高并发的问题了 使用ThreadLocal优化工具类ThreadLocal 可以简单理解为线程管理数据的类，他有两个经常用的方法，get、set 都是静态的，分别表示着往当前线程获取、保存数据下面就来改造下工具类，主要是添加几个管理 conn 的方法，其他方法不变，就省略了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class JdbcUtilsC3P0 &#123; private static ComboPooledDataSource ds = null; // 设置为静态，随类加载而创建，内部其实是一个 Map 集合 private static ThreadLocal&lt;Connection&gt; tl = new ThreadLocal&lt;&gt;(); ...... public static Connection getConnectionThread() &#123; try &#123; // 便于 Dao 层的获取，一般来说获取的到的是开启事务的连接，获取不到就从连接池拿一个普通的 // 得到当前线程上绑定的连接 Connection conn = tl.get(); // 判断是否为空，如果为空就从连接池获取一个，绑定到当前线程 if (conn == null) &#123; conn = getConnection(); tl.set(conn); &#125; return conn; &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; // 便于 Server 层获取 public static void startTransaction() &#123; try &#123; // 得到当前线程上绑定的连接 Connection conn = tl.get(); // 判断是否为空，如果为空就从连接池获取一个，绑定到当前线程 if (conn == null) &#123; conn = getConnection(); tl.set(conn); &#125; conn.setAutoCommit(false); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; public static void commintTransaction() &#123; try &#123; Connection conn = tl.get(); if (conn != null) &#123; conn.commit(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static void closeConnection() &#123; try &#123; Connection conn = tl.get(); if (conn != null) &#123; conn.close(); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally &#123; // 千万记得释放资源，否则 Map 集合会越来越大；只会移除当前线程的 tl.remove(); &#125; &#125;&#125; 这样改造后基本就比较优雅了，效率也比较好了 多表操作这个也不是太难，大体思路是：接受一个 bean 根据里面的具体数据来构造相应的 sql ，比如如果传入的 bean 包含有多个子 bean ；其实对应的就是一对多的关系，先把主 bean 中的基本数据存进去，再把子 bean 拆出来一个个的存，最后加上外键连接起来就 OK 了当然每一个 bean 都应该对应一个具体的 dao 层操作的方法 PS：set 集合的 addAll() 方法可以把传入的集合拆了，然后再存进自己的 set 代码就不写了….]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JDBC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-装饰模式]]></title>
    <url>%2F2017%2F05%2F05%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A3%85%E9%A5%B0%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[来继续之前的设计模式之旅~~这次介绍的是装饰（包装）模式，还是使用 Java 来进行说明，首先来简单了解下装饰模式： 装饰模式又名包装( Wrapper )模式。装饰模式以对客户端透明的方式扩展对象的功能，是继承关系的一个替代方案。 Wiki 上的解释为： 是面向对象编程领域中，一种动态地往一个类中添加新的行为的设计模式。就功能而言，装饰模式相比生成子类更为灵活，这样可以给某个对象而不是整个类添加一些功能。 介绍通过使用装饰模式，可以在运行时扩充一个类的功能。原理是：增加一个装饰类包裹原来的类，包裹的方式一般是通过在将原来的对象作为修饰类的构造函数的参数。装饰类实现新的功能，但是，在不需要用到新功能的地方，它可以直接调用原来的类中的方法。装饰类必须和原来的类有相同的接口。 装饰模式是类继承的另外一种选择。类继承在编译时候增加行为，而装饰模式是在运行时增加行为。 当有几个相互独立的功能需要扩充时，这个区别就变得很重要。在有些面向对象的编程语言中，类不能在运行时被创建，通常在设计的时候也不能预测到有哪几种功能组合。这就意味着要为每一种组合创建一个新类。相反，装饰模式是面向运行时候的对象实例的,这样就可以在运行时根据需要进行组合。一个装饰模式的示例是 JAVA 里的 Java I/O Streams 的实现。 然而大多数的装饰模式实际上是半透明的装饰模式（介于装饰模式和适配器模式之间的），这样的装饰模式也称做半装饰、半适配器模式。 使用前使用场景一般是： 在不影响其他对象的情况下，以动态，透明的方式给单个对象添加职责。 处理那些可以撤销的职责。 当不能采用生成子类的方法进行扩充时。 优点 装饰模式与继承关系的目的都是要扩展对象的功能，但是装饰模式可以提供比继承更多的灵活性。装饰模式允许系统动态决定“贴上”一个需要的“装饰”，或者除掉一个不需要的“装饰”。继承关系则不同，继承关系是静态的，它在系统运行前就决定了。 通过使用不同的具体装饰类以及这些装饰类的排列组合，设计师可以创造出很多不同行为的组合。 缺点使用装饰模式会产生比使用继承关系更多的对象。更多的对象会使得查错变得困难，特别是这些对象看上去都很相像。额，所以说命名很关键 简单示例在装饰模式中的角色有： 抽象构件(Component)角色：给出一个抽象接口，以规范准备接收附加责任的对象。所有的装饰者和被装饰对象都要继承它（如果是接口就是实现） 具体构件(ConcreteComponent)角色：定义一个将要接收附加责任的类（将要被装饰的类）。继承上面的抽象构建 装饰(Decorator)角色：持有一个构件( Component )对象的实例，并定义一个与抽象构件接口一致的接口。也可以是一个实现 interface 的抽象类，也就是说要继承（广义上的）抽象构件 具体装饰(ConcreteDecorator)角色：负责给构件对象“贴上”附加的责任。继承自装饰角色（抽象）；在内部维护一个被装饰对象 下面用代码来说明，先来 抽象构件角色，所有的装饰者、被装饰对象都要实现这个接口： 1234public interface Loli &#123; String speak(String name); void hug();&#125; 然后是具体构件角色，也就是将要被装饰的具体对象，这解释的是啥玩意，说白了就是被装饰对象呗这里我就写一个了： 1234567891011public class LegitimateLoli implements Loli &#123; @Override public String speak(String name) &#123; return "(￣^￣)" + name; &#125; @Override public void hug() &#123; System.out.println("(づ｡◕‿‿◕｡)づ"); &#125;&#125; 装饰角色（引入构件类， 给具体构件类增加职责，但是具体职责在其子类中实现），一般是写成抽象的，当然也可以下面的这样写法吧，大概….： 1234567891011121314151617181920public class Decorator implements Loli &#123; // 持有的构件实例 private Loli mLoli; public Decorator(Loli loli) &#123; mLoli = loli; &#125; @Override public String speak(String name) &#123; // 这里具体实现直接委派给构件实例了... return mLoli.speak(name); &#125; @Override public void hug() &#123; // 同上 mLoli.hug(); &#125;&#125; 具体装饰角色（主要就是对功能进行扩展），嗯，就是装饰者了，大部分会在这里维护被装饰对象，不过也看到过在装饰角色中直接就弄好的，比如说现在的这个栗子： 123456789101112131415161718192021public class LovelyLoli extends Decorator &#123; public LovelyLoli(Loli loli) &#123; super(loli); &#125; @Override public String speak(String name) &#123; return super.speak(name) + " +++装饰+++ 我很可爱~~"; &#125; @Override public void hug() &#123; // 执行原来的构件的功能 super.hug(); // 进行扩展功能 // TODO System.out.println("+++装饰+++ 我很可爱~~"); &#125;&#125; 最后就是来调用测试啦，怎么感觉好羞耻(:捂脸.. 12345678910111213public class MainTest &#123; public static void main(String[] args) &#123; // 选择了标准 Loli Loli loli = new LegitimateLoli(); //对标准 Loli 进行了装饰（增加功能） loli = new LovelyLoli(loli); loli.hug(); System.out.println("----------"); System.out.println(loli.speak("bfchengnuo")); &#125;&#125; 嗯，大体思想就是这样啦~~当然也不一定都是这样的写法，比如可以把构件实例放在具体装饰角色里面，怎么喜欢怎么来吧~~还是要看实际情况嘛~~ 小小总结 装饰者和被装饰对象有相同的超类 你可以用一个或者多个装饰者包装一个对象 既然装饰者和被装饰对象有相同的超类型，所以在任何需要原始对象（被包装的）的场合可以用装饰过的对象代替他 装饰者可以在所委托的被装饰者的行为之前或者之后，加上自己的行为，已达到特定的目的 对象可以在任何时候被装饰，所以可以在运行时动态的、不限量的用你喜欢的装饰者来装饰对象 在 Java 的 I/O 实现中就使用了这个模式，最原始的 InputStream 是抽象的，相当于是抽象组件；FileInputStream 继承自它，相当于是具体构件，也就是被装饰对象的角色；我们在使用的时候大多使用 BufferedInputStream 进行装饰，它继承自 FilterInputStream ，它是一个抽象装饰者，或者在外面再套一层….. 透明与半透明装饰模式对客户端的透明性要求程序不要声明一个 ConcreteComponent 类型的变量，而应当声明一个 Component 类型的变量。上面这句标准解释反正我看不懂，按照上面的栗子就是，这是透明的，是正确的： 12Loli loli = new LegitimateLoli();loli = new LovelyLoli(loli); 这样干是不对的，不透明的： 12Loli loli = new LegitimateLoli();LovelyLoli loli2 = new LovelyLoli(loli); 前面也说过，纯粹的装饰模式是很难找到的，按照透明模式来，我如果在 LovelyLoli 里面进行扩展了方法是调用不到的，这就导致了大多数的装饰模式的实现都是“半透明”的，而不是完全透明的。 换言之，允许装饰模式改变接口，增加新的方法。这意味着客户端可以声明 ConcreteDecorator 类型的变量，从而可以调用 ConcreteDecorator 类中才有的方法，说白了就是类似上面说的错误做法。 半透明的装饰模式是介于装饰模式和适配器模式之间的。适配器模式的用意是改变所考虑的类的接口，也可以通过改写一个或几个方法，或增加新的方法来增强或改变所考虑的类的功能。 为什么是半透明？也就是说，对于客户端而言，具体构件类型无须关心，是透明的；但是具体装饰类型必须指定，这是不透明的 扩充资料如果不是很明白可以看：http://www.cnblogs.com/java-my-life/archive/2012/04/20/2455726.htmlhttp://blog.csdn.net/zhshulin/article/details/38665187http://blog.csdn.net/lovelion/article/details/7425873]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDBC学习之事务和连接池]]></title>
    <url>%2F2017%2F05%2F04%2FJDBC%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BA%8B%E5%8A%A1%E5%92%8C%E8%BF%9E%E6%8E%A5%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[上一篇写了一些基本数据库操作，以及批处理、调用存储过程等内容，这一次主要是使用事务以及使用连接池来提高效率，当然最好还是选用比较成熟的连接池，这些在框架中应该都已经集成了，但是现在不是没学嘛~多了解一点也是有好处的到这里 JDBC 基本就差不多了，如果还看到什么知识点的话就再补充一篇 使用事务关于什么是事务，以及有什么特点，我在这篇文章中更新过了，下面就主要说说在 JDBC 中如何使用事务，还是以 MySQL 为例，其他的也都一致，就是注意下隔离级别的问题，像 Oracle 就只支持 2 种；首先来看下最基本的使用: 123456789101112131415161718192021222324public static void main(String[] args) &#123; Connection conn = null; PreparedStatement st = null; ResultSet rs = null; try &#123; conn = JdbcUtils.getConnection(); // 关闭自动提交 conn.setAutoCommit(false); String sql1 = "update users set rmb = rmb-100 where name='lolicon'"; String sql2 = "update users set rmb = rmb+100 where name='loli'"; st = conn.prepareStatement(sql1); st.executeUpdate(); st = conn.prepareStatement(sql2); st.executeUpdate(); // 提交事务 conn.commit(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JdbcUtils.release(conn, st, rs); &#125;&#125; 主要操作的是 conn 对象，首先我们要关闭它的自动提交，否则每发送一条 SQL 会立即完成，关闭后相当于开启了一个事务，无论执行多少次 SQL 只要不进行 commit ，数据库就会进行回滚操作 下面再说下手动设置事务的回滚点，并且手动回滚操作： 1234567891011121314151617181920212223242526272829private static void custPoint() throws SQLException &#123; Connection conn = null; PreparedStatement st = null; ResultSet rs = null; Savepoint sp = null; try &#123; conn = JdbcUtils.getConnection(); conn.setAutoCommit(false); String sql1 = "update users set rmb = rmb-100 where name='lolicon'"; String sql2 = "update users set rmb = rmb+100 where name='loli'"; st = conn.prepareStatement(sql1); st.executeUpdate(); // 设置回滚点 sp = conn.setSavepoint(); st = conn.prepareStatement(sql2); st.executeUpdate(); conn.commit(); &#125; catch (SQLException e) &#123; e.printStackTrace(); // 手动进行回滚，记得进行 commit if (conn != null) &#123; conn.rollback(sp); conn.commit(); &#125; &#125;finally &#123; JdbcUtils.release(conn,st,rs); &#125;&#125; 采用的方式是设置一个回滚点 Savepoint ，当然应该还有其他方式的….先说这一种 然后是手动设置隔离级别，就是一句话的事： 12345678private static void test() throws SQLException &#123; Connection conn = JdbcUtils.getConnection(); // 设置隔离级别 conn.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED); conn.setAutoCommit(false); //TODO JdbcUtils.release(conn,null,null);&#125; 补充一点，产生 conn 的方法还可以使用工厂模式来生产 Dao它能使各层之间能达到完全解耦的目的，但是相应的代码的复杂度会增加如果项目确定不会更换 Dao 层，那么其实不使用工厂模式比较好代码参考： Github 使用连接池首先，要明确数据库能创建的连接数（也就是 conn）是非常有限的，并且创建出来也很不容易，所以，每次操作数据库的时候就让数据库给创建连接效率是非常低的，于是就有了连接池的概念就是说，事先先创建好一批连接，存到一个“池”里，当需要的时候就从这个池里取，用完后不要释放而是再放回这个池里，这样会极大的提高效率思路大概就是这样了…. 下面来手写个连接池，当然是最基本的；简单说就是实现 DataSource 接口，这个接口只有两个方法，其他都是继承过来的，可以简单的认为实现了DataSource 接口的就是一个连接池 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class JdbcPool implements DataSource &#123; // 设计到频繁删改 使用链表结构效率好一点 private static LinkedList&lt;Connection&gt; mList = new LinkedList&lt;&gt;(); private static Properties prop = new Properties(); static &#123; try &#123; prop.load(JdbcUtils.class.getClassLoader().getResourceAsStream("db.properties")); Class.forName(prop.getProperty("driver")); // 获取 10 个连接 for (int i = 0; i &lt; 10; i++) &#123; mList.add(DriverManager.getConnection(prop.getProperty("url"), prop.getProperty("username"), prop.getProperty("password"))); &#125; &#125; catch (Exception e) &#123; throw new ExceptionInInitializerError(e); &#125; &#125; @Override public Connection getConnection() throws SQLException &#123; if (mList.size() &lt;= 0) &#123; throw new RuntimeException("数据库正忙....请稍后再获取"); &#125; Connection conn = mList.removeFirst(); // 进行装饰，使其 close 的时候再放回池里 MyConn myConn = new MyConn(conn); return myConn; &#125; /** * 使用装饰模式增强 close，使其关闭的时候添加回连接池中 * 有点繁琐，最好还是使用 动态代理（aop 面向切面，拦截技术） * * 装饰步骤 * 1.定义一个类，实现被增强类相同的接口 * 2.定义一个变量，记住被增强对象 * 3.定义一个构造函数，接收被增强对象 * 4.覆盖想增强的方法 * 5.对于不想增强的方法，直接调用目标对象的方法 */ class MyConn implements Connection&#123; private Connection conn; public MyConn(Connection conn) &#123; this.conn = conn; &#125; @Override public void close() throws SQLException &#123; // 进行自定义 mList.add(conn); &#125; @Override public Statement createStatement() throws SQLException &#123; return this.conn.createStatement(); &#125; @Override public PreparedStatement prepareStatement(String sql) throws SQLException &#123; return this.conn.prepareStatement(sql); &#125; ..... &#125; // 还有很多方法，没用到省略了 ..... DBCP连接池对于新手，自己写的连接池是很不放心啊，所以还是用比较成熟的开源的比较好，比如 BDCP 和 C3P0哦，对了 DBCP 是 Apache 的，下面是使用的参考代码，记得导入相关的包，没记错的话应该是需要三个包，一个是 DBCP 的还需要 logging 和 pool2 的包，有点麻烦，不过都是 Apache 家的，在 Apache 的官网都可以找到主包地址：https://commons.apache.org/proper/commons-dbcp/ 1234567891011121314151617181920212223242526272829303132333435363738394041public class JdbcUtilsDBCP &#123; private static DataSource ds = null; private static Properties prop = new Properties(); // 使用静态代码块保证只加载一次 static &#123; try &#123; prop.load(JdbcUtilsDBCP.class.getClassLoader().getResourceAsStream("dbcp.properties")); ds = BasicDataSourceFactory.createDataSource(prop); &#125; catch (Exception e) &#123; throw new ExceptionInInitializerError(e); &#125; &#125; public static Connection getConnection() throws SQLException &#123; return ds.getConnection(); &#125; public static void release(Connection conn, Statement st, ResultSet rs) &#123; if (rs != null) &#123; try &#123; rs.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (st != null) &#123; try &#123; st.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (conn != null) &#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 顺便贴下 DBCP 的配置文件： 12345678910111213141516171819202122232425########DBCP配置文件###########驱动名driverClassName=com.mysql.jdbc.Driver#urlurl=jdbc:mysql://127.0.0.1:3306/test#用户名username=Loli#密码password=123456#初试连接数initialSize=10#最大活跃数maxTotal=30#最大idle数maxIdle=10#最小idle数minIdle=5#最长等待时间(毫秒)maxWaitMillis=1000#程序中的连接不使用后是否被连接池回收(该版本要使用removeAbandonedOnMaintenance和removeAbandonedOnBorrow)#removeAbandoned=trueremoveAbandonedOnMaintenance=trueremoveAbandonedOnBorrow=true#连接在所指定的秒数内未使用才会被删除(秒)(为配合测试程序才配置为1秒)removeAbandonedTimeout=1 C3P0连接池使用步骤和 DBCP 类似，不过貌似效率更高一些官网地址：http://www.mchange.com/projects/c3p0/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class JdbcUtilsC3P0 &#123; private static ComboPooledDataSource ds = null; // 使用静态代码块保证只加载一次 static &#123; try &#123; ds = new ComboPooledDataSource(); /* // 如果采用的是配置文件，注释的内容可以不写 ds.setDriverClass("com.mysql.jdbc.Driver"); ds.setJdbcUrl("jdbc:mysql://115.152.254.541:3306/test"); ds.setUser("Loli"); ds.setPassword("123456"); // 设置初始连接池的大小！ ds.setInitialPoolSize(2); // 设置连接池的最小值！ ds.setMinPoolSize(1); // 设置连接池的最大值！ ds.setMaxPoolSize(10); // 设置连接池中的最大 Statements 数量！ ds.setMaxStatements(50); // 设置连接池的最大空闲时间！ ds.setMaxIdleTime(60); */ &#125; catch (Exception e) &#123; throw new ExceptionInInitializerError(e); &#125; &#125; public static Connection getConnection() throws SQLException &#123; return ds.getConnection(); &#125; public static void release(Connection conn, Statement st, ResultSet rs) &#123; if (rs != null) &#123; try &#123; rs.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (st != null) &#123; try &#123; st.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (conn != null) &#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 如果在 src 下有配置文件直接 new 就可以了，如果没有那就只能手动指定了，配置文件统一命名为 c3p0-config.xml ，示例： 12345678910111213141516171819202122232425262728&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;c3p0-config&gt; &lt;default-config&gt; &lt;property name="jdbcUrl"&gt;jdbc:mysql://115.159.234.122:3306/test&lt;/property&gt; &lt;property name="driverClass"&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name="user"&gt;Loli&lt;/property&gt; &lt;property name="password"&gt;123456&lt;/property&gt; &lt;!--当连接池中的连接耗尽的时候c3p0一次同时获取的连接数。Default: 3 --&gt; &lt;property name="acquireIncrement"&gt;3&lt;/property&gt; &lt;!-- 初始化数据库连接池时连接的数量 --&gt; &lt;property name="initialPoolSize"&gt;2&lt;/property&gt; &lt;!-- 数据库连接池中的最小的数据库连接数 --&gt; &lt;property name="minPoolSize"&gt;1&lt;/property&gt; &lt;!-- 数据库连接池中的最大的数据库连接数 --&gt; &lt;property name="maxPoolSize"&gt;10&lt;/property&gt; &lt;/default-config&gt; &lt;!-- This app is massive! --&gt; &lt;named-config name="intergalactoApp"&gt; &lt;property name="acquireIncrement"&gt;50&lt;/property&gt; &lt;property name="initialPoolSize"&gt;100&lt;/property&gt; &lt;property name="minPoolSize"&gt;50&lt;/property&gt; &lt;property name="maxPoolSize"&gt;1000&lt;/property&gt; &lt;!-- intergalactoApp adopts a different approach to configuring statement caching --&gt; &lt;property name="maxStatements"&gt;0&lt;/property&gt; &lt;property name="maxStatementsPerConnection"&gt;5&lt;/property&gt; &lt;/named-config&gt;&lt;/c3p0-config&gt; default 对应空构造函数，如果你还配置了其他的，比如上面的就是叫 named ，在 new ComboPooledDataSource 的时候把名字作为参数传入就好了，这样更加灵活 Tomcat自带连接池其实就是用的 DBCP，因为 DBCP 是 Apache 的，Tomcat 也是 Apache 的，都是一家子嘛~~使用自带的连接池的步骤在 Tomcat 的文档中写的也算是很清楚了，见：JNDI Resources 目录下因为使用自带的连接池需要配 Context ，Context 的配置方式一共有 5 种 (:应该是见：官方在线文档除去前面介绍 Tomcat 时说的几种，还有一种是在工程的 Web 目录下新建 META-INF 目录，然后再新建 context.xml 文件，在这个文件中进行配置其实这种方式最终会复制一份到 conf\Catalina\localhost 下，一般以应用名开头，所以清理项目的时候记得手动删除 好了，继续说，首先按照文档先要把驱动拷到服务器的 lib 文件夹，然后进行配置 Context 就可以了： 12345678910111213&lt;Context ...&gt; ... &lt;Resource name="jdbc/EmployeeDB" auth="Container" type="javax.sql.DataSource" username="dbusername" password="dbpassword" driverClassName="org.hsql.jdbcDriver" url="jdbc:HypersonicSQL:database" maxActive="8" maxIdle="4"/&gt; ...&lt;/Context&gt; 获取链接的示例代码为： 123456789// 初始化 JNDIContext initCtx = new InitialContext();// 获取 JNDIContext envCtx = (Context) initCtx.lookup("java:comp/env");// 获取连接池DataSource ds = (DataSource) envCtx.lookup("jdbc/EmployeeDB");// 获取连接Connection conn = ds.getConnection(); 文档为啥会在 JNDI 目录下呢，因为数据库池（DataSource）保存在 web 服务器的 JNDI 容器中（当然也是可以存其他东西的） 所以说，服务器传对象给 Servlet 其实又多了一种方式，那就放在 JNDI 容器中，需要的时候向 JNDI 中获取就行了 JNDI科普什么是 JNDI ？ JNDI是 Java 命名与目录接口（Java Naming and Directory Interface），在J2EE规范中是重要的规范之一. JNDI 是 Java 平台的一个标准扩展，提供了一组接口、类和关于命名空间的概念。如同其它很多 Java 技术一样，JDNI 是 provider-based 的技术，暴露了一个 API 和一个服务供应接口（SPI）。 这意味着任何基于名字的技术都能通过 JNDI 而提供服务，只要 JNDI 支持这项技术。 JNDI 目前所支持的技术包括 LDAP、CORBA Common Object Service（COS）名字服务、RMI、NDS、DNS、Windows 注册表等等。 很多 J2EE 技术，包括 EJB 都依靠 JNDI 来组织和定位实体。可以把它理解为一种将对象和名字捆绑的技术，对象工厂负责生产出对象，这些对象都和唯一的名字绑在一起，外部资源可以通过名字获得某对象的引用。 简单来说就是，原来我们写代码需要在代码中指定数据库的连接、用户名、密码等信息，现在只需要在 XML 配置文件中指定就可以了，达到了解耦的目的；现在这些问题都由 J2EE 容器来配置和管理，程序员只需要对这些配置和管理进行引用即可。J2EE 规范要求所有 J2EE 容器都要提供 JNDI 规范的实现。JNDI 是通过资源的名字来查找的，资源的名字在整个j2ee应用中（j2ee 容器中）是唯一的 JTA与EJB科普java Transaction API（Java事务 API）完整的名称应该是：Java Transaction API(Application Programming Interface) JTA Transaction 是指由 J2EE Transaction manager 去管理的事务。其最大的特点是调用 UserTransaction 接口的 begin，commit 和 rollback 方法来完成事务范围的界定，事务的提交和回滚。 JTATransaction 可以实现同一事务对应不同的数据库，但是它仍然无法实现事务的嵌套。 就是我们所说的全局事务 顺便再补充下什么是 EJB 吧： 企业级JavaBean（Enterprise JavaBean, EJB）是一个用来构筑企业级应用的服务器端可被管理组件。 用通俗话说，EJB 就是：”把你编写的软件中那些需要执行制定的任务的类，不放到客户端软件上了，而是给他打成包放到一个服务器上了”。 是的，没错！EJB 就是将那些”类”放到一个服务器上，用 C/S 形式的软件客户端对服务器上的”类”进行调用。 EJB 包含的 3 种 bean 是：session bean（会话 bean）, entity bean（实体 bean）, message bean（消息 bean） 更详细的内容可参考：http://blog.csdn.net/cuiran/article/details/40950487]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JDBC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDBC入门]]></title>
    <url>%2F2017%2F04%2F27%2FJDBC%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[学习之前先要弄明白的是：什么是 JDBC？ SUN 公司为了简化、统一对数据库的操作，定义了一套 Java 操作数据库的规范，称之为 JDBC 我们要操作数据库，需要有数据库厂商提供的驱动，但是它们的规范很可能是不同的，难道我们要把所有的数据库规范都看一遍？学习成本太高，不利于推广啊，于是 sun 公司就搞了个 JDBC (接口)，所有的驱动按照它的规范来，这样我们用的时候只要导入相应的驱动，代码统一使用 JDBC 的规范就可以了，降低了学习成本 其实 Servlet 也是一样的思想，也是接口，是一种规范；比如我使用的是 Tomcat ，那么具体实现就是 Tomcat 来完成的，这就是我们为什么要导入 Tomcat 中的相关 jar 我的机器只有 MySQL 的，所以我就只是用的它做的测试，驱动下载的官方地址：https://www.mysql.com/products/connector/使用 IDEA 的同学可以直接在 DATABASE的侧边栏 设置里自动下载哦 基本套路使用 JDBC 的基本套路可以用下面的代码来表示，一共六点需要注意的是：java.sql.* 和 javax.sql.* 都属于 JavaSE 了，所以建个普通工程就可以用 12345678910111213141516171819202122232425262728293031323334public class Main &#123; // URL 后面建议加 ?useUnicode=true&amp;characterEncoding=UTF-8 属性来确保编码 // 如果是连本机可以省略：jdbc:mysql:///test private static String url = "jdbc:mysql://115.159.200.212:3306/test"; private static String userName = "Loli"; private static String password = "xxxxx"; public static void main(String[] args) throws SQLException &#123; // 1. 加载驱动，推荐第二种 // DriverManager.registerDriver(new Driver()); Class.forName("com.mysql.jdbc.Driver"); // 2. 获取链接 Connection conn = DriverManager.getConnection(url, userName, password); // 3. 获取向数据库发sql语句的 statament 对象 Statement statement = conn.createStatement(); // 4. 向数据库发送 SQL，获取结果集 ResultSet resultSet = statement.executeQuery("SELECT * FROM users;"); // 5. 从结果集获取数据 while (resultSet.next())&#123; System.out.println(resultSet.getObject("id")); System.out.println(resultSet.getObject("name")); System.out.println(resultSet.getObject("sex")); &#125; // 6. 释放链接，顺序是反着的 resultSet.close(); statement.close(); conn.close(); &#125;&#125; 在第一步的加载驱动的时候，有两种方法：第一种不推荐，因为首先它会被加载两次（可以看源代码中的静态代码块），其次会有强依赖性，必须要 import 相关 jar 包；而第二种只需要一个字符串而已，在更换数据库的时候更方便。 不得不再感慨，IDEA 的代码提示救了我这英语白痴+健忘，连 SQL 语句和配置文件的内容都可以提示….. 防止在读取/存储时中文出现乱码，记得设置 url 的时候，结尾加上 ?useUnicode=true&amp;characterEncoding=UTF-8 ，如果报错了，反正就是提示 url 有问题，那么可以尝试转义一下试试，转义后的 url 为：?useUnicode=true&amp;amp;characterEncoding=UTF-8学过 html 的都懂哈…. Statement对象从上面的代码也可以看出，这个是用来向数据库发送 SQL 语句的；常见的几个方法有： executeQuery()执行查询语句，返回一个结果集 executeUpdate()执行增删改语句，返回影响的行数；判断是否大于 0 来判断是否执行成功 execute()执行所有的 SQL 语句，但是一般不太用，因为它返回的是一个 boolean ，代表是否执行成功比如查询，我们还需要判断、再用 getResultSet 来获取结果集，太麻烦 preparedStatement对象在实际中，其实使用 preparedStatement 的情况比较多，从名字上就可以猜出它与 Statement 一定关系不一般statement 和 preparedStatement 的区别： preparedStatement 是 statement 的孩子，也就是 preparedStatement 继承自 statement preparedStatement 可以防止 SQL 注入问题，设置的参数会被转义 preparedStatement 可以对 SQL 语句进行预编译，提高效率 1234567891011121314151617181920public void test() &#123; Connection conn = null; PreparedStatement st = null; ResultSet rs = null; try &#123; conn = JdbcUtils.getConnection(); String sql = "SELECT * FROM users WHERE id = ?"; st = conn.prepareStatement(sql); // 预编译 sql st.setInt(1, 1); // 补全设置的占位符 rs = st.executeQuery(); //TODO &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JdbcUtils.release(conn, st, rs); &#125;&#125; JdbcUtils 的写法可以参考：Github ResultSet对象查询返回的结果集，可以简单理解为一个表格，有一个游标，和 Android 一样默认是指向第一行之前 ；所以一般是先进行 next 获取它提供了一堆的 get 方法，获取各种类型的数据，当然无论那种数据都可以使用万能的 getObject所有的 get 方法都提供了两种重载，可以使用索引号或者列名获取一行的数据，注意索引从 1 开始 ，为了更直观，一般不会使用索引来获取；如果结果集只有一列，那么用索引就比较方便了 如果结果集只有一行数据，可以使用 if(resultSet.next()){} ，有多行就要用 while(resultSet.next()){} 来循环取出 其他的常用方法： 下一行：rs.next(); 上一行：rs.previous(); 移动到指定行：可以是负数！如果传入1，相当于 first(); ；如果传入 -1 ，相当于 last();滚动后是可以直接取数据rs.absolute(indexNum); 移动到最前，第一行之前rs.beforeFirst(); 移动到最后，最后一行之后rs.afterLast(); 批量操作如果想要同时执行多条 SQL 语句，那么一条一条的去执行看起来比较弱鸡，资源开销太大，效率太低，所以肯定有批量的法子啊！就是这么自信既然执行 SQL 语句有两种方式，那么类似的批量操作应该也有两种方式 : 使用 statement 和使用 preparedStatement 使用statement非常非常简单，直接上代码： 12345678910111213141516171819// 可以同时执行不同类别的语句，但是效率低private static void statementTest() &#123; Connection conn = null; Statement st = null; ResultSet rs = null; try &#123; conn = JdbcUtils.getConnection(); st = conn.createStatement(); st.addBatch("insert into temp(id,value) VALUES (100,'lolicon')"); st.addBatch("UPDATE temp SET id=101 WHERE id=1"); st.executeBatch(); st.clearBatch(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JdbcUtils.release(conn, st, rs); &#125;&#125; 就是通过 addBatch 方法添加进 List 集合，然后 executeBatch 一起执行，再用 clearBatch 清空 List就当作 createStatement 方法返回的 st 内部维护了一个 List 集合嘛~我也没看源码实现，应该是差不多的 (:雾 使用preparedStatement其实它们是比较类似的，事实上这一种用的比较多 1234567891011121314151617181920212223242526// 只能执行相同的语句，用于批量插入、更新等；效率高private static void preparedStatementTest() &#123; Connection conn = null; PreparedStatement st = null; ResultSet rs = null; try &#123; conn = JdbcUtils.getConnection(); st = conn.prepareStatement("insert into temp (id, value) values (?,?);"); // 插入 30 条数据 for (int i = 0; i &lt; 30; i++) &#123; st.setInt(1, i); st.setString(2, "test" + i); // 添加到 List 集合 st.addBatch(); // 每满 10 条发送一次 if (i % 10 == 0) &#123; st.executeBatch(); st.clearBatch(); &#125; &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; JdbcUtils.release(conn, st, rs); &#125;&#125; 和上面一种基本一致，只能预编译一条 SQL ，所以……因为 List 存在内存中，JVM 的内存也是有限的，所以如果要执行很多条 SQL 的话建议分段就向上面一样，每满多少条就执行一下，然后清空 List当然我这里设的 10 太小了，只为测试，设个 1000 应该也不多，具体多大凭感觉… 调用存储过程至于什么是存储过程我在 MySQL 的文章中说过了，简单说就是一个函数；所以也就是说如何调用函数了 1234567891011121314151617181920212223public static void main(String[] args) &#123; Connection conn = null; CallableStatement st = null; // 注意 ResultSet rs = null; try &#123; conn = JdbcUtils.getConnection(); st = conn.prepareCall("&#123;call demo(?,?)&#125;"); st.setString(1,"lolicon"); // 设置输出的类型 st.registerOutParameter(2,Types.VARCHAR); st.execute(); // 获取存储过程的输出值 String str = st.getString(2); System.out.println(str); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally &#123; JdbcUtils.release(conn,st,rs); &#125;&#125; 关键在于使用 prepareCall 来调用函数，在 MySQL 中，函数的返回值也写在参数，这是不太一样的地方，总之还是比较简单的 获取自动生成的键在插入数据的时候，如果某一列我们设置为自动增长，通常也设置为主键，这样在插入的时候就不需要管这列了，但是如果后续操作需要用到这一列，那就得再查询一次，这样肯定不爽，所以要自信的认为有个插入数据的时候会返回这列的值的方法；也就是说这东西可以返回它自动生成的值当然， 仅对于 insert 有效，因为只有插入的时候才会生成 ID 1234567891011121314151617181920private static void getID() &#123; Connection conn = null; PreparedStatement st = null; ResultSet rs = null; try &#123; conn = JdbcUtils.getConnection(); // 第二个参数是是否返回主键，mysql 是默认返回可以不写，写上最好 st = conn.prepareStatement("insert into keytemp (val) VALUES ('abcd')", Statement.RETURN_GENERATED_KEYS); st.executeUpdate(); // 获取返回的主键，如果没有就返回空的 ResultSet 对象 rs = st.getGeneratedKeys(); while (rs.next()) &#123; System.out.println(rs.getInt(1)); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally &#123; JdbcUtils.release(conn,st,rs); &#125;&#125; 官方 API 上说： 获取由于执行此 Statement 对象而创建的所有自动生成的键。如果此 Statement 对象没有生成任何键，则返回空的 ResultSet 对象。 注：如果未指定表示自动生成键的列，则 JDBC 驱动程序实现将确定最能表示自动生成键的列。 关于表设计一般来说一个对象（javabean）对应一个表，大部分也就是下面的几种情况 当两个表有关联的时候，比如一对多，对应到 bean 中就是主 bean （相当于主表）中有个 set 集合保存子 bean ，子 bean 包含一个主 bean 类型 一对多/多对一 先不要管映射关系，先设计出基本的属性 在多的一方加外键描述 但是呢，尽量不要使用一对多，能不用就不用，因为有时候做查询，返回有太多的数据会导致内存溢出，具体可以考虑显示的需求，如果“一”中不需要显示“多”，那么就不需要查询多的一方了 多对多 先不要管映射关系，先设计出基本的属性 设计一个中间表，一般两列；命名为：两个表名中间用 _ 连接 中间表要加约束，对应各自的 id , 当然也可以使用联合主键保证不重复 一对一（主从关系） 先不要管映射关系，先设计出基本的属性 从表要加外键约束、非空、不能重复、来自主表 某些情况可以把主键设置成外键所在列 自连接 先设计基本属性 设计一列，增加外键约束，约束来自本表，不要加非空 名字参考：parent_id主要是用来做无限分级（比如分类表），但是如果层次太深查询使用递归的时候就很容易导致内存溢出，有一种解决方案是使用树结构（树状节点），其实就是二叉树的先序遍历，这个以后再说吧 其他注意Connection 连接非常的宝贵，因为数据库支持的连接数是非常有限的，所以要遵循 尽量晚创建，尽量早释放 的原则；提示，释放连接的代码要写在 finally 里，来确保一定会释放，一般的套路是： 1234567891011121314151617181920212223242526272829303132333435public static void main(String[] args) throws SQLException, ClassNotFoundException &#123; Connection conn = null; Statement statement = null; ResultSet resultSet = null; try &#123; // 1. 加载驱动 // 2. 获取链接 // 3. 获取向数据库发sql语句的 statament 对象 // 4. 向数据库发送 SQL，获取结果集 // 5. 从结果集获取数据 &#125; finally &#123; // 6. 释放链接，顺序是反着的；保证绝对释放 if (resultSet != null) &#123; try &#123; resultSet.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (statement != null) &#123; try &#123; statement.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (conn != null) &#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; 关于 JDBC 中 Java 和 MySQL 对应的数据类型到这里查看：http://wiki.jikexueyuan.com/project/jdbc/data-types.html JDBC 的使用其实有很多重复代码，最好使用抽出来形成一个工具类，比如获取 conn 和 释放连接的方法，就像上面给的地址中的那样 更多的内容，比如事务等下次再写吧…]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JDBC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSP之自定义标签]]></title>
    <url>%2F2017%2F04%2F20%2FJSP%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A0%87%E7%AD%BE%2F</url>
    <content type="text"><![CDATA[接上次说，在 JSP 中一般是不允许出现 Java 代码的，但有的时候必须要用 Java 代码来做一些输出，这时候可以使用自定义标签来实现，自定义标签对应一个 Java 类，JSP 引擎解析到标签的时候会执行对应类中相应的方法 传统标签先来说说在 JSP2.0 之前的做法，虽然可能已经没人用了，但是了解一点没坏处啊，又不是很难，看框架的时候也许会用到呢 自定义标签需要实现 Tag 接口，这个接口非常简单，当然一般都是继承它的实现类 TagSupport 然后复写需要的方法API 参考：https://tomcat.apache.org/tomcat-5.5-doc/jspapi/下面是一个简单的例子： 123456789101112131415161718192021package web.tag;public class ViewIPTag extends TagSupport &#123; // 一般写在 Start 方法里 @Override public int doStartTag() throws JspException &#123; // 获取 pageContext 对象,也就得到了所有的隐式对象 HttpServletRequest request = (HttpServletRequest) this.pageContext.getRequest(); JspWriter out = this.pageContext.getOut(); // 获取 IP 并输出 String remoteAddr = request.getRemoteAddr(); try &#123; out.write(remoteAddr); &#125; catch (IOException e) &#123; e.printStackTrace(); throw new RuntimeException(e); &#125; return super.doStartTag(); &#125;&#125; 这里就简单写了个输出 IP 的例子，剩下的就是让这个类和标签关联起来，一般我们在 WEB-INF 目录下新建一个 tld 文件来指定，可以是多级目录下，但必须在 WEB-INF 下，如果用的是 IDEA 的话直接有现成的模板，实在不行可去抄 Tomcat 里的例子 1234567891011121314151617&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;&lt;taglib xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-jsptaglibrary_2_1.xsd" version="2.1"&gt; &lt;tlib-version&gt;1.0&lt;/tlib-version&gt; &lt;short-name&gt;bfchengnuo&lt;/short-name&gt; &lt;uri&gt;http://bfchengnuo.com&lt;/uri&gt; &lt;tag&gt; &lt;name&gt;ViewIP&lt;/name&gt; &lt;tag-class&gt;web.tag.ViewIPTag&lt;/tag-class&gt; &lt;body-content&gt;empty&lt;/body-content&gt; &lt;/tag&gt;&lt;/taglib&gt; 主要是配置 uri 和 tag 标签，下面在 JSP 中会用到 body-content 属性可以认为标签是否含有标签体，单标签双标签的区别，然后下面就是在 JSP 中进行引用了 12345678910&lt;%@ page contentType="text/html;charset=UTF-8" language="java" %&gt;&lt;%@ taglib uri="http://bfchengnuo.com" prefix="custTag" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;自定义标签测试&lt;/title&gt;&lt;/head&gt;&lt;body&gt; 您的 IP 是：&lt;custTag:ViewIP/&gt;&lt;/body&gt;&lt;/html&gt; prefix 一般写我们定义的那个 tld 的文件名，有利于查找 还有一点需要注意，自定义标签处理完后一般不会立即释放资源，也就是一般不会执行 release() 方法，和 Servlet 类似，也会被缓存，等待下一次访问，等 Web 服务器关闭的时候会释放资源 自定义标签扩展自定义标签除了上面的用法，还有几种比较常用的方式，比如控制 JSP 的 某一部分/全部内容 是否执行，重复输出某些内容、修改某些内容等，从 JSP 的用法上看它们都是一样的，就是把要控制的内容放进自定义标签体里。 控制是否执行JSP 文件就不多说了，把要控制的内容包进去就可以了，然后我们在 Java 代码中进行控制，至于怎么控制，那就是用 Start/End 方法的返回值了！在 API 文档中说的也比较详细了，Tag 接口中定义了几个常量 12345678910111213141516171819@Overridepublic int doStartTag() throws JspException &#123; // 判断是否有权限 if(flag) return Tag.EVAL_BODY_INCLUDE; else return Tag.SKIP_BODY;&#125;// 如果是控制整个 JSP 的内容，可以在 JSP 开始处加一个结束标签，然后...// 这样就不需要把所有的内容包进去，只是在头部加个结束标签@Overridepublic int doEndTag() throws JspException &#123; // 判断是否有权限 if(flag) return Tag.EVAL_PAGE; else return Tag.SKIP_PAGE;&#125; 对了，还有，不要忘记在 WEB-INF 文件夹下的 tld 文件里修改 body-content 为 JSP 12345&lt;tag&gt; &lt;name&gt;ViewIP&lt;/name&gt; &lt;tag-class&gt;web.tag.ViewIPTag&lt;/tag-class&gt; &lt;body-content&gt;JSP&lt;/body-content&gt;&lt;/tag&gt; 重复执行重复执行某段内容，Tag 接口就无能为力了，所以就有了 IterationTag 接口，是的，它是专门为了重复执行而设计的，在 Tag 的基础上加了一个常量和方法TagSupport 类已经实现了这个接口，所以说，用到直接复写 doAfterBody() 方法就可以了它的调用时机是在标签体执行完，doEndTag 方法执行前如果返回 EVAL_BODY_AGAIN 继续重复执行( doAfterBody 方法也会重复执行)；返回的是 SKIP_BODY 继续向下执行，但是 doStartTag 方法只会执行一次，毕竟配对的 doEndTag 不会执行嘛 123456789private int i = 5;// 循环输出 6 次，是在标签体执行完后才执行 doAfterBody@Overridepublic int doAfterBody() throws JspException &#123; if (--i &gt;= 0) return IterationTag.EVAL_BODY_AGAIN; else return IterationTag.SKIP_BODY;&#125; 修改内容如果想要修改内容的话，上面的两个接口都无能为力，只能靠 BodyTag 接口了，类似的我们一般是继承 BodyTagSupport 类然后复写需要的方法其实还是是 doStartTag() 只不过返回值不同了 1234567891011121314151617public class UpdateTag extends BodyTagSupport &#123; @Override public int doStartTag() throws JspException &#123; // 返回值为 EVAL_BODY_BUFFERED 会自动调用 setBodyContent(BodyContent b) // 然后就可以在 End 标签里获取到内容了 return BodyTag.EVAL_BODY_BUFFERED; &#125; @Override public int doEndTag() throws JspException &#123; BodyContent bodyContent = this.getBodyContent(); String content = bodyContent.getString(); // TODO... return Tag.EVAL_PAGE; &#125;&#125; 简单标签正是因为传统标签做不同的操作需要实现不同的接口太麻烦，所以在 JSP2.0+ 加入了SimpleTag 接口来统一，所有的功能实现这一个就可以了，不过我们还是习惯继承 SimpleTagSupport 类啦~ 在简单标签中，解析时会先执行 setJspBody 方法把标签体的内容存起来 (当然还会自动调用 setParent [没有就会传 null ]、setJspContext 方法)，然后执行 doTag() 方法要注意的是：在简单标签中没有 Start/End 方法了，开始结束全都是执行 doTag；从 API 文档中可以知道，如果不想执行后面的内容，抛一个 SkipPageException 异常即可，相当于是传统标签的 SKIP_PAGE 1234567891011121314151617181920public class SimpleTag extends SimpleTagSupport &#123; @Override public void doTag() throws JspException, IOException &#123; // 获取标签体 JspFragment jf = this.getJspBody(); // 执行标签体的内容，如果不手动调用就不会执行 jf.invoke(this.getJspContext().getOut()); // 上面那句等价于 jf.invoke(null); // 获取标签体内容. 将内容写入到事先准备的缓冲区内，然后获取 StringWriter sw = new StringWriter(); jf.invoke(sw); String content = sw.toString(); // 如果不执行后面的内容，抛一个 SkipPageException 异常 throw new SkipPageException(); &#125;&#125; 还要记得修改 body-content ，在 2.0+ 后要使用 scriptless，从名字也可以看得出，是不建议再在 JSP 中嵌入java 代码的 12345&lt;tag&gt; &lt;name&gt;SimpleTag&lt;/name&gt; &lt;tag-class&gt;web.tag.SimpleTag&lt;/tag-class&gt; &lt;body-content&gt;scriptless&lt;/body-content&gt;&lt;/tag&gt; 基本上就是这样了.. 自定义标签的属性都知道 HTML 标签都有属性，自定义的应该也有嘛~，想要设置自定义属性一般需要两步 在对应的 Java 文件中编写对应的 Set 方法，程序会自动调用的，注意名称一致 配置 TLD 文件 下面就举个简单栗子，循环输出指定次数的标签体: 12345678910111213141516public class SimpleAttributes extends SimpleTagSupport &#123; private int cont; // 可以自动转换八大基本数据类型 public void setCont(int cont) &#123; this.cont = cont; &#125; @Override public void doTag() throws JspException, IOException &#123; JspFragment jf = this.getJspBody(); for (int i = 0; i &lt; cont; i++) &#123; jf.invoke(null); &#125; &#125;&#125; 只设置一个 Set 方法就行了，Get 没什么必要，下面就是配置 tld 文件了: 123456789101112&lt;tag&gt; &lt;name&gt;custAttributes&lt;/name&gt; &lt;tag-class&gt;web.tag.SimpleAttributes&lt;/tag-class&gt; &lt;body-content&gt;scriptless&lt;/body-content&gt; &lt;attribute&gt; &lt;name&gt;cont&lt;/name&gt; &lt;required&gt;true&lt;/required&gt; &lt;rtexprvalue&gt;true&lt;/rtexprvalue&gt; &lt;!--&lt;type&gt;&lt;/type&gt;--&gt; &lt;/attribute&gt;&lt;/tag&gt; required ：表示是否是必须的rtexprvalue：表示是否允许使用表达式（比如 ${name} ）type：指定类型，一般没必要设置 然后就可以在 JSP 中进行使用了，可以实现 防盗链、if判断、迭代、处理转义等功能，就如 EL 表达式一样，迭代功能写起来可能复杂点，不过利用反射技术挺方便的PS：转义的实现在 \webapps\examples\WEB-INF\classes\util 下有现成的 最后，可以把这些自定义标签打成 jar 包，以后用到了直接丢进 lib 就行了，打包的时候建一个普通的 Java 工程就行，只要保证在 web 项目中没问题，拷贝到 Java 项目中后出现的错误不用管，是 J2EE 依赖的问题，然后把 tld 文件放在根目录下的 META-INF 目录就行，参考 Tomcat 中自带的 Jar 包结构。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JSP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编写简单的爬虫]]></title>
    <url>%2F2017%2F04%2F13%2F%E7%BC%96%E5%86%99%E7%AE%80%E5%8D%95%E7%9A%84%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[我用的 Python 版本是 3.X 的，现在大部分的库应该也都过渡到 3 了吧顺便说说比较实用的一个功能，虚拟化环境，使用的是 Virtualenv 爬虫的简单架构首先分为几个模块，首先还要有一个调度端，来负责启动爬虫、停止爬虫、监视爬虫的状态在爬虫模块中主要有：URL 管理器、网页下载器、网页解析器具体架构和运行流程见下图，说的非常清楚了 urllib库在 Python 3.x 里，urllib2 改名为 urllib，被分成一些子模块： urllib.request、urllib.parse 和 urllib.error。尽管函数名称大多和原来一样，但是在用新的 urllib 库时需要注意哪些函数被移动到子模块里了。 urllib 是 Python 的标准库（就是说你不用额外安装就可以运行这个例子），包含了从网络请求数据，处理 cookie，甚至改变像请求头和用户代理这些元数据的函数。这个库的说明文档在：https://docs.python.org/3/library/urllib.html 首先来看一下最基本也是最简单的使用： 123456789101112131415161718from urllib.request import urlopen# urlopen 用来打开并读取一个从网络获取的远程对象# 它可以读取 HTML 文件、图像文件、其他任何文件流html = urlopen("https://www.baidu.com")print(html.read())# 拓展一下from urllib import requesttry: response = request.urlopen(url, timeout=10) if response.getcode() != 200: return None return response.read() # Py3 中默认是 u8，在出现乱码时手动设定下 # return response.read().decode('UTF-8')except Exception as e: print('下载异常') return None 当然有些网站可能会对爬虫进行屏蔽处理，这时候我们一般进行浏览器伪装，必要的时候要使用 Cookie，我目前遇到的加个请求头就可以了，这里使用到了 Request 这个类，比较简单，等后面再扩展 123456789# 添加请求头，伪装浏览器def __init__(self): # 设置存放路径以及根 url self.homeUrl = "http://huaban.com/boards/3191393" self.h = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 ' &#125;req = request.Request(self.homeUrl, headers=self.h)request.urlopen(req).read().decode('UTF-8') Request 官方中文文档：http://docs.python-requests.org/zh_CN/latest/user/quickstart.html# BeautifulSoup简单说，这个库就是来解析 HTML 的，通过标签将 HTML 进行结构化处理、展示，一切都是为了便于从杂乱的 HTML 获得有用的信息 由于 BeautifulSoup 库不是 Python 标准库，因此需要单独安装，最新的版本为 BeautifulSoup 4 版本（也叫 BS4 ）我们可以通过 Python 的包管理 pip 命令进行安装：$pip install beautifulsoup4像我，如果用的 PyCharm 可以直接在项目设置里搜索安装，或者直接先写上导入代码(from bs4 import BeautifulSoup)，然后利用错误修正快速安装 好了，下面就来看看它的基本用法吧，也是官方提供的一段代码 1234567891011121314151617181920212223from bs4 import BeautifulSoupimport re# 1.文档字符串 2.HTML解析器 3.文档编码soup = BeautifulSoup(html_doc,'html.parser',from_encoding="utf-8")# 查找所有指定节点的名称、属性、内容print('获取所有连接')links = soup.find_all('a')for link in links: print(link.name,link['href'],link.get_text())print('查找指定属性')link_node = soup.find('a', href="http://example.com/lacie")print(link_node.name, link_node['href'], link_node.get_text())print('使用正则')link_node = soup.find('a', href=re.compile(r"ill"))print(link_node.name, link_node['href'], link_node.get_text())print('指定class名')p_node = soup.find('p', class_="title")print(p_node.name, p_node.get_text()) 常见错误在爬取网页的时候，常见的有两种错误，比如我们使用 urlopen 函数来打开一个网页，那么可能 这个网页不存在，就是说在服务器上找不到程序可能会返回 HTTP 错误，比如 404 或者 500 之类的，这是 urlopen 都会都会抛出 HTTPError 异常，我们可以使用 try...except 语句来进行捕捉 服务器不存在urlopen 会返回一个 None 对象，我们可以使用 if html is None 来进行判断下 除了上面最基本的两个，还有其他很多情况，比如获取到的网页内容并不是和我们预想的一样，这样的话，我们使用 BeautifulSoup 的时候就会出现问题 如果你想要调用的标签不存在，BeautifulSoup 就会返回 None 对象，如果再调用这个 None 对象下面的子标签，就会发生 AttributeError 错误，解决方案嘛，可以按照下面的代码来写 123456789101112131415161718192021from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom bs4 import BeautifulSoupdef getTitle(url): try: html = urlopen(url) except HTTPError as e: return None try: bsObj = BeautifulSoup(html.read()) title = bsObj.body.h1 except AttributeError as e: return None return titletitle = getTitle("http://www.pythonscraping.com/pages/page1.html")if title == None: print("Title could not be found")else: print(title) 如果服务器不存在，上面代码中 html 就是一个 None 对象，html.read() 就会抛出 AttributeError Virtualenv我们知道所有第三方的包都会被 pip 安装到 Python3 的 site-packages 目录下。 如果我们要同时开发多个应用程序，那这些应用程序都会共用一个 Python，就是安装在系统的 Python 3。如果应用 A 需要jinja 2.7，而应用 B 需要 jinja 2.6 怎么办？ 这种情况下，每个应用可能需要各自拥有一套“独立”的 Python 运行环境。virtualenv 就是用来为一个应用创建一套“隔离”的 Python 运行环境。也可以说是一个虚拟化环境 使用首先，我们用 pip 进行安装 1$ pip install virtualenv 然后创建一个目录，在这个目录进行初始化 123$ cd Test$ virtualenv venv# $ virtualenv --no-site-packages venv virtualenv venv 将会在当前的目录中创建一个文件夹(此例中是 venv)，包含了 Python 可执行文件，以及 pip 库的一份拷贝，这样就能安装其他包了。虚拟环境的名字（此例中是 venv ）可以是任意的；若省略名字将会把文件均放在当前目录。 有时也会加上 --no-site-packages 参数，这样已经安装到系统 Python 环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的 Python 运行环境。 还可以使用 -p 参数指定 Python 解释器，我这里只有 Py3 在 venv 环境下，用 pip 安装的包都被安装到 venv 这个环境下，系统 Python 环境不受任何影响。也就是说，venv 环境是专门针对 Test 这个应用创建的。 环境差不多已经配置完毕，下面就可以进入这个环境进行操作了，进入后当前虚拟环境的名字会显示在提示符左侧，然后就可以使用 pip 安装库，或者使用 Python 进入Py 的环境 1234$ source venv/bin/activate# 如果是 Windows 系统，使用：# venv\Scripts\activate$ pip list 完成工作后，可以使用下面的命令来停用、退出 1$ deactivate 这将会回到系统默认的 Python 解释器，包括已安装的库也会回到默认的。要删除一个虚拟环境，只需删除它的文件夹。（要这么做请执行 rm -rf venv ） virtualenv 是如何创建“独立”的 Python 运行环境的呢？原理很简单，就是把系统 Python 复制一份到 virtualenv 的环境，用命令 source venv/bin/activate 进入一个 virtualenv 环境时，virtualenv 会修改相关环境变量，让命令 python 和 pip 均指向当前的 virtualenv 环境。 重定位某些特殊需求下,可能没有网络, 我们期望直接打包一个 ENV, 可以解压后直接使用, 这时候可以使用 virtualenv --relocatable ./ 指令将 ENV 修改为可更改位置的 ENV 一般情况下，隔离环境都绑定在某个特定路径下。这也就意味着不能通过仅仅是移动或拷贝目录到另一台计算机上而迁移隔离环境。 这时可以使用 –relocatable 来重定位隔离环境 $ virtualenv –relocatable ENV 不过这个命令在 Windows 下不能使用….也是实验性的一个命令 参考可以看看我学习时写的花瓣网和百科的简单爬虫：Github关于 BeautifulSoup 这里有份不错的中文文档：https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet基础知识(HTTP相关)]]></title>
    <url>%2F2017%2F04%2F11%2FServlet%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[这篇主要讲的是 Request 和 Response 的一些基本使用还有就是 Cookie 和 Session 的使用关于 Servlet API 的介绍，可移步到这里：JavaEE 复习计划 Request和Response请求和响应，使用频率应该是非常高的，我目前也是初学者，对这两大对象了解的也不多，当然会尽量让内容丰满起来滴 Request它就是请求，带着用户的一些请求信息而来，常用的方法有： getParamenterNames(key) 获取用户提交过来的参数的名字，返回一个枚举，GET 和 POST 都可以使用，然后可以根据名字来获得值 getParameter() 很显然，这是获取指定提交参数名字(key)的值 getParameterMap() 获取一个 Map 集合类型的参数集合，可以用一些库通过反射技术直接拷贝到 javabean 里面去 返回值是 Map&lt;String, String[]&gt; getHeader(key) 可能会用到，请求头也有些有用的信息，返回的是全部的请求头信息 setAttribute( key,val ) 设置自定义参数，除了获取值还可以设置值，可以通过转发等方式继续向下传递 setCharacterEncoding() 设置 request 的编码，解决中文乱码问题，提交中文数据的时候是按当前网页的码表进行提交，但是 request 的默认码表未必是你设置的码表，所以就会有乱码，所以要先设置一下 当然改服务器的配置也是可以的，但是不推荐 但是这个方法只对 POST 请求方式有效，GET 方式出现中文要进行URL编码，内容都在 URL 中，还有其实各个浏览器默认编码也不统一，对于 GET 方式的处理方式： 12345// setCharacterEncoding 方法和 getCharacterEncoding 方法只对请求体起作用String name = request.getParameter("name");// 第一个参数是 Tomcat 编码，第二个是浏览器编码name = new String(name.getBytes("iso8859-1"),"utf-8");System.out.println(name); 至于为什么要使用 getBytes(&quot;iso-8859-1&quot;)，是因为在你浏览器用某种编码后，Servlet 容器自作多情给你用 iso-8859-1 解码了一下，所以….只能原路返回 在 Tomcat8 + 的版本，官方已使用 UTF-8 编码，不存在这个问题了 getRequestDispatcher(“path”).forward(request,response)这就是我们经常写的请求转发一个请求只能往客户机写出一次，当 clos 以后就不能再写了，所以转发后别关呀同样转发也是写数据，所以只能转发一次，为了避免报错，所以转发后最好写个 return转发时会把以前写的数据（Response中）清空 getRequestDispatcher(“path”).include(request,response);包含界面，也很常用，需要注意的是，被包含页面不要写全部的 html 标签，因为是整个文件包含进来，所以只写主体部分就可以了 getInputStream()获取流，一般上传文件的时候会用到 Response相比 Request 来说 Response 的使用就比较简单了，当然说的是常规使用通常我们会设置：setContentType(&quot;text/html;charset=utf-8&quot;);它其实相当于两句，设置响应头和 response 对象的默认编码 1234// 设置resp使用什么码表，往里写字符流的时候，建议还是写上，清晰点resp.setCharacterEncoding("UTF-8");// 设置响应头resp.setHeader("content-type","text/html;charset=utf-8"); 然后就是使用 sendRedirect() 进行重定向至于 getOutputStream 和 getWriter 方法我就不多说了，就是一个字节流一个字符流，使用字符流要注意编码，他们两个不能同时使用 如果是下载文件的需求或者存在中文记得要进行 URL编码 后再设置到 header。再说缓存问题，通过设置头信息可以禁止浏览器缓存： 12345//不允许浏览器端或缓存服务器缓存当前页面信息。response.setHeader("Pragma", "No-cache");//浏览器和缓存服务器都不应该缓存页面信息response.setHeader("Cache-Control", "no-cache");response.setDateHeader("Expires", 0); 但是需history要注意的是浏览器的后退按钮直接走的 history 缓存，默认是不刷新页面的。 Cookie和Session至于它们是什么，什么用，在以前的文章 HTTP笔记中 已经说的很清楚了，不多说，一句话概况就是用来管理会话的 Cookie一段代码说明问题，简单的使用 123456789101112131415161718// 获取用户的cookieCookie[] cookies = request.getCookies();for (int i = 0; cookies != null &amp;&amp; i &lt; cookies.length; i++) &#123; if (cookies[i].getName().equals("time"))&#123; long time = Long.parseLong(cookies[i].getValue()); Date date = new Date(time); writer.println(date.toLocaleString()); &#125;&#125;// 设置新的 cookieCookie cookie = new Cookie("time",System.currentTimeMillis()+"");// 设置有效期 单位：秒 如果设为 0 表示清除，负数表示关闭浏览器失效，path一定要一致cookie.setMaxAge(24*3600);// 如果不设置，默认是当前页面有效cookie.setPath("/webapp");response.addCookie(cookie); 重点是 path 的默认值，通过看源码就很明白了： 1234567891011121314151617181920//...............其他代码................for (String headerValue : responseHeaders.get(headerKey)) &#123; try &#123; List&lt;HttpCookie&gt; cookies = HttpCookie.parse(headerValue); for (HttpCookie cookie : cookies) &#123; if (cookie.getPath() == null) &#123; // If no path is specified, then by default // the path is the directory of the page/doc String path = uri.getPath(); if (!path.endsWith("/")) &#123; int i = path.lastIndexOf("/"); if (i &gt; 0) &#123; path = path.substring(0, i + 1); &#125; else &#123; path = "/"; &#125; &#125; cookie.setPath(path); &#125; // ...............其他代码................ 可以总结为： 当 cookie 的 path 设置了值不为 null 的时候，以设置的值为准。 当 cookie 的 path 为 null 时候，获取请求的 URI 的 path 值。 当 URI 的 path 值是以 / 结尾的时候，直接设置为 cookie 的 path 值 当 URI 的 path 值不是以 / 结尾的时候，查看 path 里面是否有“/”如果有“/”的话，直接截取到最后一个“/”，然后设置为 cookie 的 path 值。如果没有“/”的话，将 cookie 的 path 设置为 / 然后就是不要忘记父域可以访问子域的 cookie。 Session同样是一段代码就可以说明问题，简单使用，Session 还是会用到 Cookie的，默认情况下它会以一个固定的名字（Jsessionid）将ID存储到 Cookie，然后，以后的请求就会带有这个 Cookie，也就 SessionID，SessionID 也可以理解为是 Cookie 中特殊的一个值，它默认不设置有效期，也就是浏览器关闭就会失效如果需要自定义有效期有手动进行覆盖(其实就是设置此 Cookie 的有效期)，记得设 path一般情况下都要对这个 ID 进行加密处理的，同时，一个浏览器独占一个 Session 1234567891011121314151617HttpSession session = request.getSession();// 还可以接受一个参数，用来控制如果没有的话是否创建// request.getSession(false);// session 域传递参数，数据存在服务器session.setAttribute("key","data");session.setAttribute("token","base64-md5");/* 当浏览器禁用 Cookie 的时候，使用 URL 进行传输 Session ID 第一次请求会返回一个带 SessionID 的URL， 当检测到下一次请求带有Cookie的时候，表示Cookie没有被禁用，以后就不再更改 URL 然后把此 url 设置给 a 标签之类的就可以了 */String strURL = response.encodeURL("/JavaWeb/xx");// session.invalidate(); 手动销毁 如果仅仅是简单使用的话，其实只用第一行代码就足够了想要手动设置 Session 的期限的话，需要造一个 Cookie 然后把那个 SessionID 写进去，相当于覆盖了原先的，然后设个时间；这样是为了用户关闭浏览器后再打开的时候还可以保持以前的状态，当然这个时间一般最多也就是半小时，因为默认半小时后 SessionID 就被服务器给回收了，带过去也没用 1234567HttpSession session = request.getSession();String id = session.getId();// 设置 Cookie 相同的 key 进行覆盖Cookie cookie = new Cookie("JSESSIONID", id);cookie.setPath("/JavaWeb"); // 不要忘记cookie.setMaxAge(30 * 60);response.addCookie(cookie); 不过有些浏览器的版本（IE）是按进程来区分的，就是说你同时打开两个浏览器的窗口 Session 是共享的选项卡直接是共享的，这个任何浏览器都没问题 在 web.xml 中可以控制 Session 的回收时间、默认名称等，在 &lt;session-config&gt; 标签设置如果启用了 HTTPS 那么 SessionID 完全可以使用 SSLSessionID ，毕竟在握手的时候就创建了一个密钥了 作用Cookie 可以用来实现 记住密码 的功能，当然要进行加密处理，也可以保存一下其他的不是很重要的信息，因为毕竟它保存在客户端，其实就是一个文本文件，可以人为的进行修改，不安全 Session 一般用来记录用户的登陆状态，这个放在服务器端比较好，不管是从安全方面还是设计方面还可以用来防止表单的重复提交，给表单加一个隐藏域来保存 id 参数，同时也存一份在 Session 中，然后提交的时候先判断带过来的 id 是否合法，就是与 Session 中的进行比较，如果合法就删除掉，然后处理请求，如果不合法直接 return 哼 关于生成的唯一 id，或者叫令牌 Token，一般最后要进行摘要算法进行处理（比如用 MD5），变成固定长度的数据，但是这个数据是二进制的，不能直接搞成字符串，所以再进行 Base64 进行编码，这样一般就没问题了PS：了解 Base64 是什么看左边菜单的科普 补充对于四大域（ServletContext 、Request 、Session、PageContext）的使用：数据显示后就没用了，使用 Request 域，比如 Servlet 传给 JSP 进行显示数据显示后还要用，使用 Session 域数据显示后还要用，并且还要给别人用，那就使用 context 域 关于地址的写法，遵循：如果地址是给服务器用的 /代表 Web 应用如果是给浏览器用的 / 代表网站（就是 webapps），网站下有多个应用]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSP技术]]></title>
    <url>%2F2017%2F04%2F03%2FJSP%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[JSP（全称JavaServer Pages）是由 Sun Microsystems 公司倡导和许多公司参与共同建立的一种使软件开发者可以响应客户端请求，而动态生成HTML、XML或其他格式文档的Web网页的技术标准。最终还是要由 JSP 编译器翻译成 Servlet 执行 JSP运行原理服务器会先将 JSP 翻译为 Servlet ，最后会生成在 Tomcat 的 Work 文件夹，可以去扒一下源码，其实就是 Servlet 访问一个网站的流程一般是：浏览器访问 –&gt; Servlet —(通过转发)–&gt; JSP注意，转发使用的是一个请求如果有参数，使用 request 域将参数带过去 通过看 JSP 翻译后的 Servlet 文件，就可以发现 JSP 中的内容被翻译到了方法中，在这个方法中默认就定义了九大对象，我们称为是九大隐式对象 12345678910111213141516171819202122232425262728293031323334353637383940public void _jspService(final javax.servlet.http.HttpServletRequest request, final javax.servlet.http.HttpServletResponse response) throws java.io.IOException, javax.servlet.ServletException &#123; final javax.servlet.jsp.PageContext pageContext; javax.servlet.http.HttpSession session = null; final javax.servlet.ServletContext application; final javax.servlet.ServletConfig config; javax.servlet.jsp.JspWriter out = null; final java.lang.Object page = this; javax.servlet.jsp.JspWriter _jspx_out = null; javax.servlet.jsp.PageContext _jspx_page_context = null; try &#123; response.setContentType("text/html;charset=UTF-8"); pageContext = _jspxFactory.getPageContext(this, request, response, null, true, 8192, true); _jspx_page_context = pageContext; application = pageContext.getServletContext(); config = pageContext.getServletConfig(); session = pageContext.getSession(); out = pageContext.getOut(); _jspx_out = out; out.write('\r'); out.write('\n'); String path = request.getContextPath(); String basePath = request.getScheme() + "://" + request.getServerName() + ":" + request.getServerPort() + path + "/"; out.write("\r\n"); out.write("\r\n"); out.write("&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"&gt;\r\n"); out.write("&lt;html&gt;\r\n"); out.write("&lt;head&gt;\r\n"); out.write("&lt;base href=\""); out.print(basePath); ........ 我们看到，jsp 页面的内容其实就是用 out 进行了输出，如果嵌入了 java 代码（&lt;% code %&gt;），就会原封不动的翻译到这个方法里，需要注意一点是，可以看到上面翻译后的 JSP 默认调用了 getSession() 方法，也就是默认会给每一个会话产生一个 Session ，这个对于传值来说还是很方便的（比如常用的： request.getSession().setAttribute()），但是相应的也会耗费一些资源，毕竟每开一个窗口就会创建一个 Session，并且还是存在内存里在 JSP 页面可以通过 session 属性来控制是否默认产生 Session 1&lt;%@ page contentType="text/html;charset=UTF-8" language="java" session="false" %&gt; 总结一下，这九大对象是： 123456789101112RequestResponseSessionApplicationConfigOut * （有缓冲）Exception// 当前 JSP 的实例, object类型// 它代表JSP被编译成Servlet,可以使用它来调用Servlet类中所定义的方法// 注意 Page 和 pageContext 的区别PagepageContext * 其中有两个比较特殊的对象，已经使用 * 标出了通过上面的代码我们知道 jsp 中的 html，默认也是用 out 输出，它是有缓冲区，当缓冲区满了或者页面关闭时才会刷到Response 去所以，如果 代码中有 getWrite 的输出，那么html的内容会在它的后面，所以最好只使用 out 进行输出 当第一次访问 JSP 的时候会被翻译成 Servlet，而不是启动服务器的时候翻译，所以第一次访问 JSP 页面的时候比第二次慢得多，只要翻译过了，以后就会直接用 然后我们再来看看：pageContext可以认为它是 JSP 的运行环境，它在内部封装了其他八大隐式对象的引用（多用于自定义标签）所以，当需要传数据的时候可以直接传一个 pageContext 有了它，就等于啥也有了 同时它自身是一个域对象，可以用来保存数据，使用它的 setAttribute 方法 然后再来看看它的生命周期：就是 JSP 这个页面 打开/关闭，所以还是蛮短的 来看个非常好用的方法：findAttribute() 查找所有域中的数据，顺序为：pageContext —&gt; request —&gt; session —&gt; application 在 JSP 中，使用的 ${data} 其实就是调用的它在 Jsp 中我们写链接的时候，当然不能把链接的路径给写死，一般都是使用 ${pageContext.request.contextPath} 获得当前 web 应用的路径，然后再补全具体的地址还有两个也比较使用，省了不少事：forward（转发） 和 include 方法，不再多说 JSP页面中有一个内置的 exception 对象，这个 exception 对象是 Throwable 的实例。当在 JSP 中发生错误或异常时，就会将捕获的 java.lang.Throwable t 赋值给内置对象 exception 异常对象。由此可见，exception 对象仅在异常处理页面中才有效，是异常处理页面！整个 _jspService 方法会被 try 起来，当异常被捕获到后会交给 _jspx_page_context 处理，首先判断抛出异常的当前 JSP 页面 page 指令是否指定了 errorPage 属性，如果指定了，则将请求 forward 到 errorPage 属性指定的页面；否则就使用系统页面来输出异常信息。 123456789&lt;!-- page1 --&gt;&lt;%@ page errorPage="/page2.jsp" %&gt;&lt;%int c = 1 / 0; // 抛出异常%&gt;&lt;!-- 下面的是 page2 --&gt;&lt;!-- page2 中输出异常信息，因为都是 JSP 自带 exception 对象 --&gt;&lt;%=exception.toString() %&gt; 参考：http://www.jellythink.com/archives/1353 JSP指令/标签首先要明确的是：JSP 是用于输出的，格式良好的JSP 不允许出现(尽量少的出现) Java 代码 专用的脚本表达式输出某个变量/属性：&lt;%=name %&gt; ；还看到有这样用的 &lt;%=request.getContextPath()%&gt; ，感觉还是用 $ 比较好 JSP 的声明一般有两种： 123456&lt;%-- 这是注释，不会输出到 html 中 --%&gt;&lt;%-- 下面标签中如果写java代码，会被翻译到 _jspService 方法中 --%&gt;&lt;% code %&gt;&lt;%-- 下面标签中如果写java代码，会被翻译到 _jspService 方法外中，所以可以定义方法 --%&gt;&lt;%! code %&gt; JSP 指令是给解析引擎看的，这里也就是 Tomcat： 12345&lt;%-- 比如导包、设定是否创建 session 对象、错误页面，但是不要太大，&lt;1K --%&gt;&lt;%@ page %&gt;&lt;%-- 包含页面，是静态包含 --%&gt;&lt;%@ include file="" %&gt; 关于 include ：包含就是把整个页面搞进来，所以要 include 的页面最好不要写头标签 head 之类的，直接写重点即可静态包含就是把这些页面拼合在一个 Servlet 中而动态包含就是：翻译成多个 Servlet ，等访问时再合成，所以还是尽量使用静态的吧例子：request.getRequestDispatcher(&quot;path&quot;).include(request,response); 下面来说说 JSP 的标签，主要的就是这三类： &lt;jsp:forward page=&quot;&quot;&gt;就是转发啦，直接写地址就可以了 &lt;jsp:param name=&quot;xx&quot; value=&quot;dd&quot;&gt; 传递参数用的，常套在 forward 标签里使用 &lt;jsp:include page=&quot;&quot;&gt;就是动态包含了，不过不推荐 更多的标签可以去 Wiki 看下：https://zh.wikipedia.org/wiki/JSP 还是很详细的 EL表达式与JSTL更新于：2017-4-21 EL 表达式关于 EL 现简单说一个，以后再补充：${data} —–&gt; pageContext.findAttribute(&quot;data&quot;)其实就是简化了代码，最大的一个好处是：如果没有找到就返回空字符串，不会影响显示 EL 表达式的作用嘛，基本上可以概况为四点： 获取数据 执行运算 获取 web 开发常用对象 调用 java 方法 获取数据这个在之前写的就是了，就是分割线之前的内容，就是来获取数据的，其他的还有获取 Javabean 的，很简单，其实是获取的属性，只要有对应的 get/set 方法就行，写 EL 的时候省略 get；以及还有一些常用的：${name} JSP 引擎会自动拿着 name 当key，去四个域查找，上面我们也说过了 12345// 获取 List 中的数据// 和 JS 类似，data.key == data['key']$&#123;data['1'].name&#125;// 获取 Map 中的数据$&#123;data.key.name&#125; 执行运算语法：${运算表达式}其实就是在 {} 内做一些逻辑判断之类的，用的比较多的是下面几个，注意是花括号不是小括号 1234&lt;%-- 检测是否为 null 或者 空 --%&gt;$&#123;empty(data)&#125;&lt;%-- 二元表达式的支持 --%&gt;&lt;input type="radio" name="sex" $&#123;user == 'nv'?'checked':''&#125;&gt; 获取 web 开发常用对象这个用的比较频繁，前面其实也用到了，就是获取当前应用名的时候EL 表达式定义了 11 种隐式对象解析的时候先要判断传入的是否是隐式对象，语法 ${隐式对象名} pageContext pageScope返回的是 page 域中的 Map 集合；也就是从指定域 (page) 找 requestScope返回的是 request 域中找 ，下同 sessionScope applicationScope param返回请求参数的 Map 集合 paramValues返回的是数组，对应多个请求参数的情况 header返回的是请求头的 Map 集合 headerValues cookie返回的是保存了 Cookie 的 Map 集合,注意：每一个 key 获取到的是一个 Cookie 对象比如这样用：${cookie.JSESSIONID.value/name} initParam返回的是web 应用的初始化参数的 Map 集合，就是 web.xml 文件中的 &lt;content-param&gt; 中的内容 执行 Java 代码只能是 静态方法！并且相应的方法需要在 tld 文件中进行描述只能执行与 web 开发无关的代码，一些工具类啊，也能想得通，毕竟静态；所以它不能取代自定义标签在 JSP 中的使用和自定义标签一样，tld 文件的定义名称、含包名的路径、方法的定义，类似： 12345&lt;function&gt; &lt;name&gt;test&lt;/name&gt; &lt;function-class&gt;utils.WebUtils&lt;/function-class&gt; &lt;function-signature&gt;java.lang.String test(java.lang.String)&lt;/function-signature&gt;&lt;/function&gt; 然后在 JSP 中这样用：${c:test(&quot;str&quot;)} ；c 是你导入时候定义的名称不得不说 IDEA 的代码提示真是爽！ 补充其实还可以看出，EL 表达式是不支持字符串的连接的，还有一些其他需求的话自带的是满足不了我们的，所以就会有自定义 EL 函数了一般习惯于命名为 MyEL${} 中是可以套 el 函数的，比如下面的写法是完全可行的${user == null?my:test(str):&#39;&#39;} 还有一点需要注意的是：$ 中不能再嵌套 $， 写法上写一个就行了，效果还是有的 EL 表达式取出的类型会自动推断，不需要为类型（转换）而操心。 JSTL至于 JSTL ，说白了就是用来做一些逻辑判断的，其实感觉就是一些自定义标签 ，毕竟它就叫 JSP 标准标签库嘛。需要导入相应的 jar 包，jstl.jar 、 standerd.jar，放在 lib 目录下即可；导入标签库：在 JSP 的开始声明：&lt;%@ taglib url=&quot;http://java.sun.com/jsp/jstl/core&quot; prifix=&quot;c&quot; %&gt;url 可以在导入的 jar 的 c.tld 文件中找到，然后给定义个名称，一般使用 c ，和文件名统一（文件名就叫 c）。类似这样的使用，temp 可以是个 bean： 12345678&lt;c:foreach var="temp" items="$&#123;list&#125;"&gt; $&#123;temp.name&#125;&lt;/c:foreach&gt;&lt;c:if test="$&#123;name == null&#125;" /&gt;&lt;!-- 字符串等比较除了 == 可以使用 eq --&gt;&lt;c:if test="$&#123;name eq 'abc'&#125;" /&gt; 核心标签库 out&lt;c:out value=&#39;${data}&#39; default=&#39;addd&#39; escapeXml=&#39;true&#39;&gt;&lt;/c:out&gt;用来显示一个表达式的结果，与 &lt;%= %&gt; 作用相似escapeXml 控制是否进行转义，如果值为空就输出 default 的值 set&lt;c:set var=&#39;key&#39; value=&#39;data&#39; scope=&#39;page&#39;&gt;&lt;/c:set&gt;可以设置 域、bean、map( key 为 property) 的数据; 比如上面的例子是存到 page 域 remove用来删除域中的数据，使用参考上面的 set catch&lt;c:catch var=&#39;key&#39;&gt;&lt;/c:catch&gt;用来处理产生错误的异常状况，并且将错误信息储存起来默认将异常存到 page 域，需要指定一个 key if&lt;c:if test=&#39;逻辑&#39;/&gt; choose相当于 if…else，或者说 switch 分支语句，就是只选择一个，第一个通过后就不会向下执行了 1234&lt;c:choose&gt; &lt;c:when test=''&gt;if true&lt;/c:when&gt; &lt;c:otherwise&gt;else&lt;/c:otherwise&gt;&lt;/c:choose&gt; forEach 迭代，varStatus 对象表示当前选择的是那个; 还可以做分页，step 是步长 12345678&lt;c:forEach var='key' varStatus='status' items="$&#123;list&#125;" &gt; &lt;tr class="$&#123;status.conut%2==0?'even':'odd'&#125;"&gt;&lt;/tr&gt;&lt;/c:forEach&gt;// 输出 1-7&lt;c:forEach var='key' begin='1' end='7' step='1'&gt; $&#123;key&#125;&lt;/c:forEach&gt; url 主要用于 url 的重写！ 会自动构建 url 的 Session 地址，自动加入当前应用的名字，不需要手动获取了，如果不写 var 属性就会默认输出到页面上，否则就存到 var 指定的变量中 1234&lt;c:url var='key' value='url'&gt; // 设置 url 的 get 参数，如果是中文会自动进行编码 &lt;c:param name='key' value='测试'&gt;&lt;/c:param&gt;&lt;/c:url&gt; redirect 实现重定向 forTokens 用 delims 定义的值来分割 items 里的数据，然后迭代 &lt;c:forTokens var=&#39;key&#39; items=&#39;${data}&#39; delims=&#39;,&#39;&gt;&lt;/c:forTokens&gt; 例如，items 中存的是字符串 “a,b,c,d” ；那么第一次迭代 key 就是 a，第二次是 b ….. JSTL 除了核心标签库还有其他的几个库，更详细的说明可参考：http://www.runoob.com/jsp/jsp-jstl.html其中有个格式化标签需要特别注意下：&lt;%@ taglib prefix=&quot;fmt&quot; uri=&quot;http://java.sun.com/jsp/jstl/fmt&quot; %&gt; , 非常的好用 JSTL 中的 EL 函数库JSTL 也就是官方给的一个库，里面还包含了些 EL 的函数库，一般是用来操作字符串的 ，如果没有，就只能去自定义了使用之前记得导入：&lt;%@ taglib url=&quot;http://java.sun.com/jsp/jstl/functions&quot; prifix=&quot;fn&quot; %&gt; ；可以看出是在 fn.tld 的文件中描述的通过名字基本上也能看出是什么作用来，常见的有： 123456789101112131415161718192021222324252627$&#123;fn:toLowerCase(str)&#125;$&#123;fn:trim(str)&#125;$&#123;fn:length(str)&#125;// 利用 length 的迭代方式&lt;c:forEach var='i' begin='0' end='$&#123;fn:length(list)&#125;' &gt; $&#123;list[i]&#125;&lt;/c:forEach&gt;$&#123;fn:split(str,',')&#125;// 连接；1.要连接的字符串数组 2.链接符$&#123;fn:join(str,'.')&#125;// 查找；返回的是 int 索引值$&#123;fn:indexOf(str,'key')&#125;// 包含；str 中是否包含 key，返回布尔类型，大小写敏感$&#123;fn:contains(str,'key')&#125;// 是否以指定的 key 开头$&#123;fn:startsWith(str,'key')&#125;// 替换；1.源字符 2.替换那个字符 3.替换成什么$&#123;fn:startsWith(str,'key','')&#125;// 截取；从 1 截取到 3，在 java 中最后一个表示的是截取长度$&#123;fn:substring(str,1,3)&#125;// 截取之前、之后$&#123;fn:substringAfter(str,1)&#125;$&#123;fn:substringBefore(str,1)&#125;// 转义 HTML$&#123;fn:escapeXml(str)&#125; 关于乱码中文的一大特色就是乱码咯，乱码产生的原因： Java 的内核和 class 文件是基于 unicode 的，这使 Java 程序具有良好的跨平台性，但也带来了一些中文乱码问题的麻烦。首先 Java（包括 JSP）源文件中很可能包含有中文，而 Java 和 JSP 源文件的保存方式是基于字节流的，如果 Java 和 JSP 编译成 class 文件过程中，使用的编码方式与源文件的编码不一致，就会出现乱码。 对于 JSP，在文件头加上 &lt;%@ page contentType=&quot;text/html;charset=utf-8&quot;%&gt; ，不设置默认会解析为 iso8859-1，这样基本上就能解决这类乱码问题.注意，配置的 &lt;%@ page language=&quot;java&quot; pageEncoding=&quot;utf-8&quot;%&gt; 意思为设置 jsp 文件的存储格式。保险起见，还可以对请求进行软编码：&lt;%request.seCharacterEncoding(&quot;utf-8&quot;);%&gt; 对于 Servlet 的乱码，可以直接修改 Tomcat 服务器，conf/server.xml 中的 Connector 节点加入属性：useBodyEncodingForURL=&quot;true&quot; 。如果不能修改服务器，那么就只能使用硬编码转换了，因为事先知道默认编码，所以：new String(str.getBytes(&quot;ISO-8859-1&quot;),&quot;utf-8&quot;);无论那种，都不要忘了设置软编码： 123request.setCharacterEncoding("UTF-8");response.setCharacterEncoding("UTF-8");response.setContentType("text/html; charset=UTF-8"); 最好还是用过滤器处理，这样基本就不会出现乱码了。 注意！！！从 Tomcat8 开始，默认编码已更换为 UTF-8，get 请求含有中文也不会乱码了（需要软编码）！如果再进行硬编码处理，反而会乱码 其他如果需要在 web.xml 里面配置相关的映射，因为 JSP 本质还是一个 Servlet，所以按 Servlet 配置就好，只是路径换成 jsp 页面所在的位置即可 javaBean相关首先，一个良好的 JavaBean 一般都是要有一个空构造函数的 &lt;jsp:useBean id=&quot;loli&quot; class=&quot;com.bf.dd&quot; scope=&quot;page&quot;/&gt;从 page 域寻找 id 为 loli 的 bean，如果找到就返回，找不到就创建如果是个双标签，并且里面如果有内容，只有在创建的时候才执行里面的代码 &lt;jsp:setProperty name=&quot;id&quot; property=&quot;name&quot; value=&quot;xxxx&quot;/&gt;设置 bean 里面的属性，id 指定那个 bean &lt;jsp:setProperty name=&quot;id&quot; property=&quot;name&quot; param=&quot;xxxx&quot;/&gt;将属性设置为URL请求的参数，?name=xxx &lt;jsp:setProperty name=&quot;id&quot; property=&quot;*&quot;/&gt;一次性设置所有的属性，从 url 参数获取，名字要对应 &lt;jsp:getProperty name=&quot;id&quot; property=&quot;name&quot; /&gt;获取属性 MVC与三层架构关于这个我在我的公众号确实发过，当然说的也不是太深，简单理解还是够的，这里就只简单说下在 Java 中的体现它们基本可以应用于任何语言的开发，不过思想都是一样的有人说 三层架构是属于架构设计，MVC 是属于设计模式，当然也有人说 MVC 也是一种架构，这个不表，我现在还没整明白架构、框架是啥 MVCM ( Mode ) —-&gt; 其实就是 javabean ；负责数据相关V ( View ) —- &gt; 指的就是 jsp；主要是来负责显示C ( Controller ) —-&gt; 这个指的就是 servlet 了；用来处理请求，然后转发给 jsp 显示给用户看 三层架构Web层 —-&gt; 基本指的就是 Servlet、jsp业务逻辑层( Service ) —-&gt; 指的是 service、javabean，用来处理请求、数据的数据访问层 ( Dao ) —-&gt; 指的是 dao、javabean，和数据库打交道 JDBC各层之间使用接口相联系，上层调用接口，下层实现接口，这样以后如果下层的实现换了以后，上层一行代码都不需要改 在 MVC 或者三层架构下，通常 jsp 的数据是 servlet 带过来的，所以用户不能直接访问 JSP，一般放在 web-inf 文件夹中保护起来当然，首页的 JSP 肯定是放在外面的 包管理关于分包，有个模板，不知道现在还用不用了….前面的包名省略了 domain —-&gt; 一般放和数据库相关的 javabean 实体 dao —-&gt; 放数据库相关的接口 dao.impl —-&gt; 与上面的接口对应，是接口的实现 service service.impl web.controller web.ui web.listener web.filter utils …… 开发顺序一般是从下往上，也就是说一般先设计 javabean，然后编写 dao 层，然后是 service 层先写 impl 层，然后使用 IDE 的抽取接口功能就好了]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JSP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AJAX入门]]></title>
    <url>%2F2017%2F03%2F29%2FAJAX%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[AJAX = Asynchronous JavaScript and XML（异步的 JavaScript 和 XML）。AJAX 不是新的编程语言，而是一种使用现有标准的新方法。AJAX 是与服务器交换数据并更新部分网页的艺术，在不重新加载整个页面的情况下。其实简单点说，AJAX 就是 JS 中的一个对象而已，就是 XMLHttpRequest对象 同步与异步AJAX 是一种异步加载的技术多使用在表单提交、（滚动）加载更多、输入提示等方面 先来了解下同步请求是什么，比如提交一个表单，流程可以是：点击提交后，客户端向服务器发起请求，服务器进行处理，然后返回结果给客户机，然后客户端刷新页面显示结果；在客户端发起请求后，是一直处于等待服务器响应的状态的，没法做别的事，这是同步的一种体现 还是上面的例子，如果是异步请求，那就是在你输入某一个信息后，就会携带某个信息立即向服务器发起请求，服务器返回结果，从而调用 js/css 来显示给用户输入是否正确(比如用户名是否重复)在这个过程用户完全可以继续填其他的内容，也不会刷新页面，这就是异步的一种体现 向服务器发送请求前面我们说过，AJAX 的核心就是 XMLHttpRequest 对象，所有现代浏览器均支持 XMLHttpRequest 对象（IE5 和 IE6 使用 ActiveXObject）。 XMLHttpRequest 用于在后台与服务器交换数据。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。 1234567891011var xmlhttp=new XMLHttpRequest();// 参数为：请求方式、请求地址、是否异步(默认为异步)xmlhttp.open("GET","test1.action",true);// 发送请求xmlhttp.send();xmlhttp.open("POST","ajax_test.action",true);// 设置 HTTP 头，表明这是个表单数据// 使用post方式必须设置，在open和send之间xmlhttp.setRequestHeader("Content-type","application/x-www-form-urlencoded");xmlhttp.send("fname=Bill&amp;lname=Gates"); 注意 send 方法，如果是 GET 请求，因为参数都在 URL 中，所以不需要填（填了也没用），如果是 POST 请求，那么就必须把参数传入 send 方法了 服务器响应对于对象来说，响应就很简单了，ajax 支持三种类型的数据： String XML JSON 其中 String 和 JSON 的 MIME 类型都是 text/plain 都是使用 responseText 来接收；而 XML 类型的数据使用 responseXML 来接收，MIME 是 text/xml 在服务端设置的时候需要注意。 1234567891011121314// 获得字符串形式的响应数据。xmlhttp.responseText// 获得 XML 形式的响应数据。xmlhttp.responseXML// 可以直接这样获取，和html类似，但是不能用 innerHTML 方法// ajax.responseXML.getElementsByTagName('root').firstChild.nodeValue;// 获取数字或文本形式的 HTTP 状态码xmlhttp.statusxmlhttp.statusText// 获取所有的响应报头xmlhttp.getAllResponseHeader()// 查询响应中的某个字段的值xmlhttp.getResponseHeader() 我们可以通过监听 readyState 属性来实现： readyState 属性的变化代表服务器响应的变化 0：请求未初始化，open还没有调用 1：服务器连接已建立，open已经调用了 2：请求已接收，也就是接收到头信息了 3：请求处理中，也就是接收到了响应主体 4：请求已完成，且响应已就绪，也就是响应完成了 123456789var request = new XMLHttpRequest();request.open("GET","get.php",true);request.send();// 设置监听，每当 readyState 改变时，就会触发 onreadystatechange 事件request.onreadystatechange=function()&#123; if(request.readState===4 &amp;&amp; request.status===200)&#123; //做一些事情 request.responseText; &#125;&#125; 需要注意的是，不同的浏览器对这几种状态码的支持是不一样的，也就是说有的浏览器没有 0，有的没有 1；但是肯定都支持 4；并且只有在状态码发生变化后才会触发这个函数，所以说如果状态码一直是 4 也不会触发这个函数 解析XML如果是 XML 的数据，那么解析的例子： 123456789101112131415161718192021function callback2() &#123; if(ajax.readyState == 4)&#123; if(ajax.status == 200)&#123; var div = document.getElementById('show'); var ruslt = ajax.responseXML; var str = ["&lt;table style='width: 600px;margin:0 auto;' border='1px'&gt;&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;姓名&lt;/th&gt;&lt;th&gt;邮箱&lt;/th&gt;&lt;th&gt;手机&lt;/th&gt;&lt;/tr&gt;"]; var stus = ruslt.getElementsByTagName('student'); var len = stus.length; for (var i = 0; i &lt; len; i++) &#123; str.push("&lt;tr&gt;&lt;td&gt;" + stus[i].getAttribute('id') + "&lt;/td&gt;"); str.push("&lt;td&gt;" + stus[i].getAttribute('name') + "&lt;/td&gt;"); // stus[i].getElementsByTagName('email')[0].firstChild.nodeValue str.push("&lt;td&gt;" + stus[i].childNodes[0].textContent + "&lt;/td&gt;"); str.push("&lt;td&gt;" + stus[i].childNodes[1].textContent + "&lt;/td&gt;&lt;/tr&gt;"); &#125; str.push("&lt;/table&gt;"); div.innerHTML = str.join(""); &#125; &#125; 其中，for 循环为了避免每次都判断 length 属性浪费性能，所以单独提出来，如果对顺序没要求，可以直接采用倒序遍历的方式，就没有这个问题了。为了避免字符串拼接的效率问题，使用数组来代替字符串的拼接，主要是 push 方法增加，最后使用 join(&quot;&quot;) 转成字符串，如果直接使用 toString 方法那会输出 abc,def,aa,xx ，所以使用 join 来处理。 解析JSONJSON 类型的数据应该是最常见的，同时解析也非常的简单，因为 JSON 的语法和 JS 的对象定义基本完全一致的。一般原生 JS 解析 JSON 可以使用两种方式： JSON.parse(str) eval(&#39;(&#39; + str + &#39;)&#39;) 后面的遍历就简单了，完全按照 JS 中的对象处理来。 用jQuery实现jQuery 已经帮我们封装好了，使用起来也非常的方便 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 发送数据$(document).ready(function() &#123; $("#save").click(function() &#123; $.ajax(&#123; type: "post", url: "service.php", dataType: "json", data: &#123; name: $("#staffName").val(), number: $("#staffNumber").val(), sex: $("#staffSex").val(), job: $("#staffJob").val() &#125;, // 必须是一个方法，成功后回调 success: function(data) &#123; if (data.success) &#123; $("#createResult").html(data.msg) &#125; else &#123; $("#createResult").html("error: " + data.msg) &#125; &#125;, error: function(jqXHR) &#123; alert("error: " jqXHR.status); &#125; &#125;); &#125;);&#125;);// 获取数据$.ajax(&#123; type: "GET", url: "service.php?number=" + $('#keyword').val, dataType: "json", // 必须是一个方法，成功后回调，data 是 Obj 类型 success: function(data) &#123; if (data.success) &#123; $("#createResult").html(data.msg) &#125; else &#123; $("#createResult").html("error: " + data.msg) &#125; &#125;, error: function(jqXHR) &#123; alert("error: " jqXHR.status); &#125;&#125;); data ：是一个对象，连同请求发送到服务器的数据dataType：预期服务器返回的数据类型，如果不指定将根据 HTTP 包中的 MIME 信息智能判断，一般我们都用 JSON 格式 如果是 Json 类型的数据还可以使用 JavaScript 原生的 JSON.parse 方法进行转换成对象，jQuery 中使用 ajax 请求的方式除了上面的栗子有下面几种： jQuery.load( url, [data], [callback] )默认使用 GET 方式来传递的，如果 [data] 参数有传递数据进去，就会自动转换为 POST 方式的。 jQuery.get( url, [data], [callback] )相似的还有一个 $.getJSON() 方法，只是被限定为 JSON 类型 jQuery.post( url, [data], [callback], [type] ) jQuery.getScript( url, [callback] )通过 GET 方式请求载入并执行一个 JavaScript 文件 $.ajax() 是所有 ajax 方法中最底层的方法，所有其他方法都是基于 $.ajax() 方法的封装，jQuery 真的是博大精深啊callback 函数可以有三个参数，第一个就是服务器返回是数据了（对象形式，和服务器端的实体对象对应），第二个是状态码，第三个是 ajax 对象。 跨域在浏览器的规则中 JavaScript 是不被允许访问其他域下的内容的，只有在子域名和主域名都相同的情况下才不算跨域，端口号也必须相同http 与 https 之间也算是跨域 解决这个问题，一般的几种方式为： 使用代理属于后端技术，比如写个 PHP 页面专门用来转发请求，后端访问是没有问题的 JSONP （只支持GET请求）出于安全考虑，浏览器都有同源策略。即相同 domain(域) 的页面运行在一个沙箱（sandbox）中，与其他 domain 的沙箱隔离，不能跨越 domain 直接访问其他 domain 下的资源。 但HTML中有几个标签可以忽略同源限制去请求其他 domain 下的资源，比如&lt;img&gt;和&lt;script&gt;等。比如当浏览器解析到&lt;script&gt;标签，就会发起一个get请求，请求的 URL 即为 scr 所指定的 url。这就相当于跨域访问了一个资源。 JSONP 就是这样的原理，我们可以利用 Src 来跨域得到我们想要的数据，但这这样就会变成下面的样子 123&lt;script&gt; &#123;['some string 1', 'some data', 'whatever data']&#125;&lt;/script&gt; 但这样的数据解析很麻烦，所以 JSONP 做了下处理，返回的结果是：my_callback({[&#39;some string 1&#39;, &#39;some data&#39;, &#39;whatever data&#39;]});可以看到，这里的返回结果是直接执行了一个函数 my_callback(...), 实参就是我们需要的数据；那么只要在代码里实现 my_callback 函数，就可以做任何想做的事了 当然也可以使用 jQuery 来做，type 选择 jsonp，然后增加一个属性 jsonp，值可以任意，其实就是上面的那个函数名 然后在跨域的服务端要获取到这个名字，可以看出是在 url 传递了一个参数，所以也就只支持GET请求了，比如： 123456$.ajax(&#123;type:"GET",url:"http://127.0.0.1:8080/ajaxdemo/service.php?number"+$("#keyword").val(),dataType:"jsonp", //由"json"改为"jsonp"jsonp:"callback", //增加此项，用于后台代码编写.... 后端代码以 PHP 为例 12345$jsonp = $_GET["callback"];// 返回值我们说过应该是一个函数，函数名就是上面的jsonp变量了// 所以要改在一下，外面套一层函数$result = $jsonp.'(data..)' 简单说就是在 script 标签里我们请求 ：&lt;script src=‘http://b.com?callback=fun’ /&gt; ，为了方便后端返回把函数名当做参数传回去了，后端拿到这个参数就相当于拿到了回调函数的名称，然后拼一个函数把数据当做参数传入就行了（fun(&#39;datadatadata&#39;)），这样前台就能直接调用了，方法的定义在前台并且后端处理更灵活，如果发现没带 callback 参数那就返回正常的 JSON 数据，如果带了就返回 JSONP 的格式，一个接口适应了两种情况 XHR2HTML5 提供的 XMLHttpRequest Level2 已经实现了跨域访问以及其他的一些新功能 在服务器端加入下面两句 那个域可以访问，* 表示所有 header(‘Access-Control-Allow-Origin:*‘); 支持什么方法 header(‘Access-Control-Allow-Methods:POST,GET’); 其他补充异步请求使用一个 XMLHttpRequest 对象就足够了，发起的异步请求都会交给它来处理，它内部必然有个类似数组结构的变量来存，然后依次请求服务器（将原始请求进行包装，然后以 http 协议发送），当响应完成后（通过流的形式）再由 XMLHttpRequest 通知调用方（返回给调用方）进行处理；Ajax 其实就是起到了一个中转的作用 利用 js 将 select 元素的 options 的长度设为 1，会只保留第一项其他的全部清空，比如： selectElement.options.length = 1 ajax 技术还经常用在验证码的时候，判断输入的个数以及按键弹起事件（onkeyup）通过后，通过 ajax 从后台获取输入的是否正确，及时给予用户提示]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Servlet]]></title>
    <url>%2F2017%2F03%2F27%2F%E5%88%9D%E8%AF%86Servlet%2F</url>
    <content type="text"><![CDATA[Servlet 是sun提供的一门专门用于开发动态web资源的技术，传统的步骤分为两步： 编写一个java类，实现servlet接口不过一般是继承自 HttpServlet ，因为大多都是用于http，它默认已经实现了所有未实现的方法，需要那个覆盖那个即可 把开发好的java类部署到web服务器 听起来蛮简单的，其中的道道不少呢Servlet已经是属于J2EE的其中之一，在java SE 的API中是没有的，不过Tomcat中是自带的，因为他要解析啊 什么是Servlet引用自Wiki上的一句：Servlet（Server Applet），全称Java Servlet，未有中文译文。是用Java编写的服务器端程序。其主要功能在于交互式地浏览和修改数据，生成动态Web内容。狭义的Servlet是指Java语言实现的一个接口，广义的Servlet是指任何实现了这个Servlet接口的类，一般情况下，人们将Servlet理解为后者。 Servlet运行于支持 Java 的应用服务器中。从实现上讲，Servlet 可以响应任何类型的请求，但绝大多数情况下Servlet只用来扩展基于HTTP协议的Web服务器。 与JSP的关系Java服务器页面（JSP）是 HttpServlet 的扩展。由于 HttpServlet 大多是用来响应HTTP请求，并返回Web页面（例如HTML、XML），所以不可避免地，在编写 servlet 时会涉及大量的HTML内容，这给 servlet 的书写效率和可读性带来很大障碍，JSP便是在这个基础上产生的。 其功能是使用HTML的书写格式，在适当的地方加入 Java 代码片段，将程序员从复杂的 HTML 中解放出来，更专注于 servlet 本身的内容。 JSP在首次被访问的时候被应用服务器转换为servlet ，在以后的运行中，容器直接调用这个 servlet，而不再访问 JSP 页面。JSP的实质仍然是 servlet。 生命周期在 Servlet 中有了生命周期的概念，也就有了相应的方法，比如 init() 和destroy() 当用户第一次访问的时候创建，执行 init 方法完成初始化，此后会一直存在于容器等待客户机的第二次访问web服务器关闭的时候 Servlet 才会销毁，也就是说 一个Class文件只会存在一个Servlet对象 但是每一次请求，服务器都会创建一个新的 request 和 response 对象（同时也会创建一个新线程），注意是每一次请求，一个用户就可以发起多次请求，好在它们的生命周期很短，随请求的结束就销毁了，因为一次请求的时间本来就很短每一次请求调用 Servlet 一次，但是对象只有一个 因为每一次请求都会有各自的线程进行处理，虽然解决了一个对象可以同时被访问（也就是说不同的请求可以同时cao执行一个对象[servlet]里的方法），但是存在线程安全问题，当然如果变量是在类里定义的，那对象只有一个，变量也就只有一个 编写Servlet我们写一个最简单的 Servlet ，国际惯例使用 Hello World 测试，本质就是一个Java类 12345678910111213141516171819202122@WebServlet("/HelloWorld")public class HelloWorld extends HttpServlet &#123; private String msg; @Override public void init() throws ServletException &#123; msg = "is Hello World java"; &#125; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; resp.setContentType("text/html"); PrintWriter writer = resp.getWriter(); writer.print(msg); &#125; @Override public void destroy() &#123; super.destroy(); &#125;&#125; 这样就会输入 msg 的消息，我是用 IDEA 写的，在类名的上面加上标注(注解) @WebServlet(&quot;/HelloWorld&quot;) 意思是设置 servlet 对应的url地址 Servlet3.0 之后提供了注解(annotation)，使得不再需要在 web.xml 文件中进行 Servlet 的部署描述，简化开发流程 JDK1. 5版本之后， JAVA提供了一种叫做 Annotation 的新数据类型，中文译为注解或标注，它的出现为铺天盖地的XML配置文件提供了一个完美的解决方案，让 JAVA EE 开发更加方便快速，也更加干净了 如果不想使用注解，还有另一种方式，就是在 web.xml 文件中进行手动配置 123456789&lt;servlet&gt; &lt;servlet-name&gt;HelloWorld&lt;/servlet-name&gt; &lt;servlet-class&gt;com.bfchengnuo.test.HelloWorld&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;HelloWorld&lt;/servlet-name&gt; &lt;url-pattern&gt;/hw&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; servlet 标签配置名字以及具体的 class，servlet-mapping 是设置映射关系，将那个名字的 Servlet 映射到什么虚拟目录(浏览器访问时输入)上面的一段其实和 @WebServlet(name=&quot;HelloWorld&quot;,value=&quot;/hw&quot;) 是完全相同的当标注(注解)与 web.xml 同时配置时，标注无效。使用标注：由于是在对应的类中配置的信息，因而则可以不用在标注中配置class了。对于 web.xml 中的配置，在标注中通通都有配置在 web.xml 中一个 servlet 可以配置多个 servlet-mapping, 只要在其中指定相同的 servlet-name 即可。标注也可以指定多个的，但不再用 value，而是用urlPatterns 数组。@WebServlet(name=&quot;HelloWorld&quot;,urlPatterns={&quot;/HelloWroldServlet&quot;,&quot;/HelloWorld&quot;}) 同时如果需要随服务器加载，可以在 servlet 标签下设置 &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; 数字越小越优先 关于映射映射如果写的是类似 /1.html 这种就是伪静态，因为实际访问的是动态的web资源 1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;HelloWorld&lt;/servlet-name&gt; &lt;url-pattern&gt;/hw.html&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 虽然你在浏览器输入的确实html，但是其实请求的是一个 Servlet映射当然也是支持通配符的，比如 /* 就是映射到任何页面但是只能有两种固定的格式： *. 开头 + 扩展名 [ 固定扩展名(*.jsp) ] / 开头并且以 /* 结尾 [固定路径] 像 /abc/*.html 是错误的,有歧义嘛~ ; 服务器不知道它是路径映射还是扩展映射 这样也就不可避免的会有存在匹配冲突的情况下，谁匹配更多(也就是更精确)就是那个，和正则类似，*的匹配优先级最低，所以*开头的都很低，一般是最后匹配缺省的 Servlet 的映射为： / ，当找不到 Servlet 的时候找它其实任何请求都是由 Servlet 进行处理的，当访问静态资源的时候其实就是因为找不到相关的 Servlet 映射，就交给系统缺省的 Servlet 映射负责处理，来找到那些静态的资源，如果静态资源也没用，那就只能映射到 404 页面了如果覆盖了缺省的 Servlet ，那静态资源就访问不到了，只会访问自定义的缺省的 Servlet ，所以不要把 url-pattern 设置为 / ServletContextWeb容器在启动的时候，会为每一个 web 应用都创建一个 ServletContext 对象，它代表当前的 web 应用，注意是当前的web应用不是 ServletServletContext 在服务器启动时创建，关闭时销毁，既然它代表web应用，那么就有很多相应的方法，比如获取各种配置等，面向对象思想嘛~上一篇 了解Tomcat 中已经说的很详细了ServletConfig 对象中维护了 ServletContext 对象的引用，所以可以在 Servlet 中通过 this.ServletConfig.getServletContext() 获得 ServletContext 对象当然其实也是直接可以通过 getServletContext 方法获取到的，因为继承的是 HttpServlet 在爷爷辈的 init 方法中已经进行了保存 1234567public void init(ServletConfig config) throws ServletException &#123; this.config = config; this.init();&#125;public ServletContext getServletContext() &#123; return this.getServletConfig().getServletContext();&#125; ServletContext 对象通常称为 context 域 转发&amp;重定向这个用的次数是非常之频繁的，先说明下它和重定向的主要区别： 重定向：我没有，我给你个地址，让你去找别人要 转发：我没有，我帮你找别人获取，然后再把你需要的资源给你 转发对于客户端来说只发一次请求，网址也不变，客户端甚至不会察觉，也不知道这个资源到底是谁的( 嗯，有点像反向代理呢 ，在我的公众号已经写过正向代理和反向代理是什么了)用代码来表达就是： 1234RequestDispatcher rd = getServletContext().getRequestDispatcher("/index.jsp")rd.forward(request,response);// 一般的写法为request.getRequestDispatcher("path").forward(request,response); 至于这两者的区别，最大的区别就是 ServletContext 转发时路径必须绝对路径，也就是 / 开头使用 request 进行转发既可以使用绝对路径也可以使用相对路径（相对于 web 应用也就是比起上面可以不写 /），一般这种方式用的比较多，大多数是使用绝对路径的 当然如果需要携带参数需要 Request 域来实现，使用 setAttribute 设置自定义参数，下次再说，当然也可以使用 getParameter 方法来获得 URL 携带的数据 重定向就是用 sendRedirect 方法设置下路径就可以了，路径就是浏览器中的地址了，所以如果 / 开头就是指的此网站，后面多半需要加 web 应用的名，或者通过 request.getContextPath() 来获取此 web 应用的地址，然后再加 “/xxx.jsp”这样。 重定向（redirect）后：确认了要跳转的页面的 url，继续执行 redirect 下面的代码；执行完后，断开当前的与用户所发出的请求连接，即断开 request 的引用指向，因此 request 里存放的信息也会丢失。转发（forward）后：确认了要转发的页面的地址，现在停止继续执行后面的代码，而是先执行转发后的那个 servlet 里的代码，执行完后再回来继续执行后面的代码；在这期间 check 和 success 共享一个 request 和 response 对象。 读取/存储问题我们可以把一些信息存在 web.xml 文件中，方便修改和获取，在 web.xml 中也有两种形式，一种是属于 Servlet 的，一种是整个web应用的 12345678910111213&lt;context-param&gt; &lt;param-name&gt;conData&lt;/param-name&gt; &lt;param-value&gt;Lolicon&lt;/param-value&gt;&lt;/context-param&gt;&lt;servlet&gt; &lt;servlet-name&gt;HelloWorld&lt;/servlet-name&gt; &lt;servlet-class&gt;com.bfchengnuo.test.HelloWorld&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;data&lt;/param-name&gt; &lt;param-value&gt;is test data&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt; 获取方式基本也相同，当然也可以进行设置： 1234// 获取 Servlet 中的配置的数据String str = this.getServletConfig().getInitParameter("data");// 获取 web 应用的配置的数据String str2 = this.getServletContext().getInitParameter("conData"); 设置初始化参数使用注解也是可以的，类似这样@WebServlet(name=&quot;HelloWorld&quot;,urlPatterns={&quot;/HelloWorld&quot;},initParams={@WebInitParam(name=&quot;id&quot;,value=&quot;1&quot;),@WebInitParam(name=&quot;name&quot;,value=&quot;Loli&quot;)}) properties对于无逻辑性的数据，一般使用：.properties 来进行存储，内容格式非常简单一般就是一行一个 key=val 的形式，详细的介绍见 维基百科Java中提供了专门的类来处理这种文件 12345InputStream resourceAsStream = this.getServletContext().getResourceAsStream("/WEB-INF/classes/a.properties");Properties prope = new Properties();prope.load(resourceAsStream);String str = prope.getProperty("key"); 如果读取资源文件的程序不是 Servlet 类，就只能通过类装载器去读，文件不能太大，因为会被加载进内容，太大会内存溢出ParamTest.class.getClassLoader().getResourceAsStream(path)类装载器的 path 相对于的是 src 目录，也就是包名的开始目录由于类装载器只装载一次，在 JVM 没有重启的情况下，修改文件无效解决方案是用传统方式读取，首先通过类装载器获取到文件的绝对路径，再 FileInputStream 读取FileInputStream 的相对路径是相对的 JVM 的启动目录，所以最好传绝对路径，如果用 ServletContext 去读，相应的 / 就是 web 应用的目录，用类装载器的话相对的目录就是 src 下开始获取路径的方法为：HelloWorld.class.getClassLoader().getResourceAsStream(&quot;com/bfchengnuo/xx&quot;).getPath() 关于@WebServlet的补充@WebServlet 主要属性列表： 属性名 类型 描述 name String 指定 Servlet 的 name 属性，等价于 &lt;servlet-name&gt;。如果没有显式指定，则该 Servlet 的取值即为类的全限定名。 value String[] 该属性等价于 urlPatterns 属性。两个属性不能同时使用。 urlPatterns String[] 指定一组 Servlet 的 URL 匹配模式。等价于 &lt;url-pattern&gt;标签。 loadOnStartup int 指定 Servlet 的加载顺序，等价于 &lt;load-on-startup&gt; 标签。 initParams WebInitParam[] 指定一组 Servlet 初始化参数，等价于 &lt;init-param&gt;标签。 asyncSupported boolean 声明 Servlet 是否支持异步操作模式，等价于 &lt;async-supported&gt;标签。 description String 该 Servlet 的描述信息，等价于 &lt;description&gt; 标签。 displayName String 该 Servlet 的显示名，通常配合工具使用，等价于 &lt;display-name&gt;标签。 例如初始化参数： 1234567@WebServlet(name="HelloWorld",urlPatterns=&#123;"/HelloWroldServlet","/HelloWorld"&#125;,initParams=&#123;@WebInitParam(name="id",value="yeh"),@WebInitParam(name="name",value="Loli")&#125;)class Test extends HttpServlet&#123;&#125;// 分开配置@WebServlet("hello")@WebInitParam(name="id",value="yeh")Public class Test extends HttpServlet&#123;&#125; 注意格式就好，第二种低版本的 Tomcat 好像不识别 参考http://blog.csdn.net/zw_2011/article/details/7432839]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解Tomcat]]></title>
    <url>%2F2017%2F03%2F26%2F%E4%BA%86%E8%A7%A3Tomcat%2F</url>
    <content type="text"><![CDATA[Tomcat的大名就不用多说了，要学习 java web 肯定会接触到它，可以说是学习 Java web 的前提，所以为了弄明白 Tomcat 的目录结构层次以及它的体系结构，就有了这篇文章，以便让我能更好的理解整个 java web 网站的处理流程，能更快的理解学习吧，当然还有一些配置相关的东西。在使用Tomcat之前，首先要认识两个环境变量： JAVA_HOME：必须先配置 JAVA_HOME，因为 Tomcat 启动需要使用 JDK CATALANA_HOME：如果是安装版，那么还需要配置这个变量，这个变量用来指定 Tomcat 的安装路径如果是绿色版，并且配置了这个环境变量，无论你从那个文件夹执行的启动脚本，最终执行的就是这个环境变量下的 Tomcat，所以最好不要配 目录结构层次 bin二进制文件以及执行脚本文件(比如启动和关闭)的存放目录。常见的几个重要文件： catalina.sh ：用于启动和关闭 tomcat 服务器 configtest.sh ：用于检查配置文件 startup.sh ：启动 Tomcat 脚本 shutdown.sh ：关闭 Tomcat 脚本 conf 配置文件存放目录。 lib Tomcat 的库文件夹，用于存放 Tomcat 所依赖的以及第三方扩展的 jar 文件。 logs 日志文件存放目录 localhost_access_log ：访问日志 localhost.log ：错误和其它日志 manager.log ：管理日志 catalina.log： Tomcat 启动或关闭日志文件 temp 临时文件存放目录 webapps 主要 Web 发布目录（存放我们自己的 JSP, SERVLET,类） work 存放 Tomcat 编译 JSP 对应的 Java 字节码类文件。 conf目录server.xml ：Tomcat 的全局配置文件web.xml ：为不同的 Tomcat 配置的 web 应用设置缺省值的文件tomcat-users.xml ：Tomcat 用户认证的配置文件 把 Web 应用交给服务器管理的过程称为：虚拟目录映射，比如在 server 配置文件中，我们可以配置一个 web 应用： 123&lt;Host&gt; &lt;Context path="/home" docBase="c:\news" /&gt;&lt;/Host&gt; Context 标签类似于代表一个 web 应用，path 就是对外路径(虚拟路径)，如果把 path 设置为空那就成为默认的 web 应用了，说白了虚拟路径就是在输入网址时候的路径，是虚拟的，本地并不存在，后面的 docBase 指定的是本地硬盘的 web 应用的路径，这就是一个映射过程注：这样搞重启才能生效，所以官方是不建议这样用的，上面只是为演示通常可以在conf\Catalina\localhost目录下建一个 XML 文件，文件名为虚拟路径(多级目录可以用 # 区分)，如果文件名是 ROOT 那就是默认的 web 应用了(并非是默认网页)，然后可以在文件内配置 Context，自然 path 就不需要写了，这样会被自动加载，不需要重启 当然最简单的方法是直接往 webapps 目录一扔就 OK 了，文件名就是虚拟目录名 web.xml文件是部署描述符文件，这个文件中注册了很多 MIME 类型，即文档类型。这些 MIME 类型是客户端与服务器之间说明文档类型的，如用户请求一个 html 网页，那么服务器还会告诉客户端浏览器响应的文档是 text/html 类型的，这就是一个 MIME 类型。客户端浏览器通过这个 MIME 类型就知道如何处理它了。当然是在浏览器中显示这个 html 文件了。但如果服务器响应的是一个 exe 文件，那么浏览器就不可能显示它，而是应该弹出下载窗口才对。MIME 就是用来说明文档的内容是什么类型的！ tomcat-users 文件，就是存储 tomcat 用户的文件，这里保存的是 tomcat 的用户名及密码，以及用户的角色信息。可以按着该文件中的注释信息添加 tomcat 用户，然后就可以在 Tomcat 主页中进入 Tomcat Manager 页面了 Webapp目录这个目录就是用来放 web 应用的，也就是网站的一个个的功能，将开发好的 web 应用(文件夹)仍在这里面就好，然后就是对于一个个的 web 应用也是有一定的目录结构的，如果是静态资源 ( HTML/CSS/JS等 ) 就放在根目录即可，JSP 文件同样也是如果存在 ROOT 目录那就是默认的应用目录web 应用程序下还要有一个 WEB-INF 文件夹，注意要大写，里面主要就是放一些 java 相关的东西了： WEB-INF/web.xml 这是一个 Web 应用程序的描述文件。这个文件是一个 XML 文件，描述了 Servlet 和这个 Web 应用程序的其他组件信息，此外还包括一些初始化信息和安全约束等等。例如，web 应用的默认网页就是在这里配置的 WEB-INF/classes/ 这个目录及其下的子目录应该包括这个 Web 应用程序的所有 JavaBean 及 Servlet 等编译好的 Java 类文件（*.class）文件，以及没有被压缩打入 JAR 包的其他 class 文件和相关资源。注意：在这个目录下的Java类应该按照其所属的包层次组织目录 WEB-INF/lib/ 这个目录是来存放各种 jar 包的除了各种依赖 jar，通常 Web-INF/classes/ 这个目录下的类文件也可以打包成 JAR 文件，放在该目录下。如将 classes 目录下的各个*.class文件打包成 WebMis.jar 文件 注意：WEB-INF 目录中包含应用软件所使用的资源，但是 WEB-INF 却不在公共文档根目录之中。在这个目录中所包含的文件都不能被客户机所访问。如果一个类出现在 JAR 文件中同时也出现在类的目录中，类加载器会加载位于类目录中的那一个。 如果我们不知道web.xml 应该怎么写，最简单的就是去其他项目或者自带的例子中拷(抄)一份过来，然后改一下就行了，一般就是要它的头和尾 体系结构一个 Tomcat 只会启动一个 JVM，所有 webapps 公用一个 JVM 进程Tomcat 体系结构中的六个主要概念： ServerServer 代表整个容器(container)。它可以包含一个或多个 Service，还可以包含一个 GlobalNamingResources。A Server element represents the entire Catalina servlet container. (Singleton) Service它由一个或者多个 Connector (连接器)组成，以及一个 Engine (引擎)，负责处理所有 Connector 所获得的客户请求。 Engine Engine 下可以配置多个虚拟主机 Virtual Host，每个虚拟主机都有一个域名 当 Engine 获得一个请求时，它把该请求匹配到某个Host上，然后把该请求交给该 Host 来处理 Engine 有一个默认虚拟主机，当请求无法匹配到任何一个 Host 上的时候，将交给该默认 Host 来处理 Host 代表一个 Virtual Host，虚拟主机，每个虚拟主机和某个网络域名 (Domain Name) 相匹配 每个虚拟主机下都可以部署 (deploy)一个或者多个 Web App，每个 Web App 对应于一个 Context，有一个 Context path 当 Host 获得一个请求时，将把该请求匹配到某个 Context 上，然后把该请求交给该 Context 来处理 匹配的方法是“最长匹配”，所以一个 path==&quot;&quot; 的 Context 将成为该 Host 的默认 Context 所有无法和其它 Context 的路径名匹配的请求都将最终和该默认 Context 匹配 ConnectorTomcat 有两个典型的 Connector，一个直接侦听来自浏览器的 http 请求，一个侦听来自其它 WebServer 的请求当然现在 https 也越来越普遍了 Context 一个 Context 对应于一个 Web Application，一个 Web Application 由一个或者多个 Servlet 组成 Context 在创建的时候将根据配置文件 $CATALINA_HOME$/conf/web.xml和$WEBAPP_HOME$/WEB-INF/web.xml 载入 Servlet 类 当 Context 获得请求时，将在自己的映射表 (mapping table) 中寻找相匹配的 Servlet 类 如果找到，则执行该类，获得请求的回应，并返回 CATALINA_HOME 是 Tomcat 的安装目录，CATALINA_BASE 是 Tomcat 的工作目录。 当我们想要运行多个 Tomcat 实例，但是不想拷贝多个 Tomcat 副本时，那么我们可以配置多个不同工作目录 (修改 catalina.sh 文件) 如何处理一个请求假设来自客户的请求为：http://localhost:8080/test/index.jsp 对照上面的图更好理解 请求被发送到本机端口 8080，被在那里侦听 http 的连接器 ( Coyote HTTP/1.1 Connector ) 获得 Connector 把该请求交给它所在的 Service 的 Engine 来处理，并等待来自 Engine 的回应 Engine 获得请求 localhost/test/index.jsp，匹配它所拥有的所有虚拟主机 ( Virtual Host ) Engine 匹配到名为 localhost 的 Host（即使匹配不到也把请求交给该 Host 处理，因为该 Host 被定义为该 Engine 的默认主机） localhost Host 获得请求/test/index.jsp，匹配它所拥有的所有 Context Host 匹配到路径为 /test 的 Context（如果匹配不到就把该请求交给路径名为&quot;&quot;的默认 Context 去处理） path=&quot;/test&quot; 的 Context 获得请求 /index.jsp，在它的 mapping table 中寻找对应的 servlet Context 匹配到 URL PATTERN 为 *.jsp 的 servlet，对应于 JspServlet 类 构造 HttpServletRequest 对象和 HttpServletResponse 对象，作为参数调用 JspServlet 的 doGet 或 doPost 方法 Context 把执行完了之后的 HttpServletResponse 对象返回给 Host Host 把 HttpServletResponse 对象返回给 Engine Engine 把 HttpServletResponse 对象返回给 Connector Connector把 HttpServletResponse 对象返回给客户浏览器 配置相关用到了就来补充，现在应该很乱，等数量够了再集中整理一下 context相关前面提到过：Context 标签类似于代表一个 web 应用path 属性就是对外路径(虚拟路径)，如果设为path=&quot;&quot; 就代表默认应用docBase 属性为映射的物理路径reloadable：是否自动加载，如果设置为 true 就是自动加载，开发调试时小的应用可以开，大的应用就不要开了，可能会内存溢出 配置用户用户相关的配置在 tomcat-users 文件下，默认也有几个，但是被注释了，如果想进入管理页面还需要手动加一个管理员的权限 123456&lt;role rolename="tomcat"/&gt;&lt;role rolename="role1"/&gt;&lt;role rolename="manager"/&gt;&lt;user username="tomcat" password="tomcat" roles="tomcat,manager"/&gt;&lt;user username="both" password="tomcat" roles="tomcat,role1"/&gt;&lt;user username="role1" password="tomcat" roles="role1"/&gt; 然后就可以在 Tomcat 主页中进入Tomcat Manager 页面了，就是 http://localhost:8080/manager/html 单向加密连接器当然在实际的环境下，都是双向加密，这里只是稍微提一下，以后再补充我们可以用java自带的 keytool 工具生成一个密钥库:keytool -genkey -alias mytest -keyalg RSA上面的命令在生成密钥库的时候指定了一个别名叫 mytest，以及指定了算法是 RSA，当然还可以指定有效期，还有很多参数如加 -keystore 后面跟指定保存的路径，其他的就不多说了，只说下常用的，然后输入域名和密码即可，其他的可忽略弄好以后会生成一个 .keystore 文件，然后可以把它放到 Tomcat 的目录里去，比如放到 conf 下，并且配置它的全局配置文件 server.xml 打开 HTTPS 的连接器，设置密钥库的路径( keystoreFile )和密码( keystorePass )属性。当然，要想让浏览器信任，需要 CA 的签名的证书上面只是给服务器生成了证书，要做到双向加密还需要给客户端（浏览器）也生成证书，并且要让服务器信任客户端的证书，客户端也需要信任服务器的证书，这个过程比较复杂，但是也不难，可以自行Google资料 补充当我们的web应用开发好以后要部署在服务器上，通常当然要先打一个包，然后再上传到服务器，这样既能保证数据的完整，还能节省点流量，java有打包工具，包就是我们常说的jar包，但是一般会命名为 .war 后缀，因为这种后缀Tomcat会自动进行解压 参考http://qiita.com/CoffeeDog/items/7082c1db5ab84c8df60bhttp://blog.csdn.net/clementad/article/details/46842309]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础知识复习]]></title>
    <url>%2F2017%2F03%2F25%2FJava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一切都是对象每种编程语言都有自己的操作内存中元素的方式，Java中，我们认为一切皆对象，我们通过引用来操纵对象，引用可以比作遥控器，对象就是电视机，就算没有电视机，遥控器也可以独立存在。例如：String s这里我们只是创建了一个引用，并不是对象，如果我们现在向s发送一个信息(调用)就会报一个错误，因为它并没有和任何事物关联为了简化对象的生命周期，在创建对象时把它放进堆里，通过引用来进行访问，需要注意的是，基本类型是个特例，它一般很小、很简单，还放在堆里不是很有效，所以对于这种类型一般就直接放在堆桟中，同时为了跨平台，大小一般是固定的Java中数组也是对象，当创建一个数组对象时，就创建了一个引用数组，每个引用会自动进行初始化为某个特定值，该值拥有自己的关键字null，一旦Java看到null就知道这个引用还没有指向一个对象。 关于默认值当变量作为类的成员使用时，Java才确保给定其默认值，以确保那些基本类型的成员变量得到初始化，防止出现错误。然而初始化并不适用于局部变量(即并非某个类中的字段)，比如某个方法中的变量是不会被默认初始化的。如果变量没有被初始化，编译器会报一个错误而不像C报一个警告 关于Static静态静态最大的特点就是：单独分配存储空间，并且在内存中只有一份，多用于数据共享；不依赖于对象，也就是不与任何对象实例关联(不持有引用)。使用类名是引用static变量的首选方式，如：ClassName.varName它不仅强调了变量的static结构，还在某些情况下为编译器的优化提供了更好的机会。对于静态方法也是如此，可以直接调用而不需要创建实例 静态变量存储于内存中的方法区(也有的叫共享区/数据区)的静态区什么时候用static呢？ 需要访问对象中的特有数据 如果对象中没有成员变量，只有方法，那么就定义成静态 静态代码块中如果有异常不能直接抛，只能先抓，然后在catch里可以new一个异常再抛 参数传递我们先看一段经典代码： 12345678910public class Test1 &#123; public static void main(String[] args) &#123; int n = 3; changeData(n); System.out.println(n); &#125; public static void changeData(int data) &#123; data = 10; &#125;&#125; 打印结果当然还是3.可以理解为传入方法处理的时候将n拷贝了一份赋给变量data，data存在于changeData方法中，生命周期也就伴随着这个方法，当方法执行完毕这个变量也就销毁了。基本类型作为参数传递时，是传递值的拷贝，无论你怎么改变这个拷贝，原值是不会改变的 特别的，如果传入的是引用类型(对象)，那么就是传递的引用，如果改变了这个对象，那就是真的改变了，比如下面的例子，数组也是个对象： 1234567891011public class Demo &#123; public static void main(String[] args) &#123; int[] ls = &#123;1,2&#125;; updateData(ls); System.out.println(ls[0]); &#125; public static void updateData(int[] data)&#123; data[0] = 6; &#125;&#125; 入坑开始我测试的其实是字符串，毕竟String也是对象嘛~~ 1234567891011public class Demo &#123; public static void main(String[] args) &#123; String ls = "abc"; updateData(ls); System.out.println(ls[0]); &#125; public static void updateData(String data)&#123; data += "d"; &#125;&#125; 如上，结果还是abc，不变，让我懵逼….随后才想到，忘记了一个知识点，String其实可以说一旦声明后就是是不可变的(学Py的话应该也会了解到)，data += &quot;d&quot;;这一句意思就是将data这个对象赋值一份然后追加上d拼成一个新的对象，再把这个对象的引用赋给data，于是最后data随updateData方法的生命周期销毁了…. 所以字符串操作应该是尽量少用的，它会在堆中产生大量垃圾，会引起频繁的GC进行回收，卡顿，尽量使用 StringBuffer 和StringBuilder 吧 字符串拼接的效率问题：使用 + 、使用 StringBuilder 、使用 StringBuffer在 1.5+ 的 JDK 版本：StringBuilder = + &gt; StringBuffer在 1.5- 的 JDK 版本：StringBuilder &gt; StringBuffer &gt; +原因就是 Java 也意识到了使用 + 的问题，所以在编译的时候会自动转换成 StringBuilder ，所以在 1.5+ 的版本它们的效率是一样的，因为要保证多线程同步所以势必会慢一些，但是还是建议使用 StringBuilder 需要说明的是，这里的转换是在 str1 + str2 这种情况下的，当使用 str += &quot;asjdkla&quot;; 这种形式的时候实际上是转换为：str = new StringBuilder().append(str).append(&quot;asjdkla&quot;).toString();一眼就能看出创建了太多的 StringBuilder 对象，而且在每次循环过后 str 越来越大，导致每次申请的内存空间越来越大，很多人喜欢把它放到 for 里循环做对比测试。还有一点就是使用 stringbuilder 的时候，默认它会创建一个长度 16 的容器，当不够了的时候就再 +16，然后把内容拷过去，所以，当大量拼接时，可以根据估计长度来设置这个值：new StringBuilder(24)PS：此方案不适用于 List ，反而会增加损耗。使用 String.valueOf（） 方法转换字符串能避免 toString 空指针问题；两个字符串拼接直接调用 String.concat() 性能最好格式化输出可以使用 String.format() 【使用 %1$2s 等占位】或者 Message.format() 【使用 {} 占位】 如果了解字符串在堆中的存储结构应该会很好的理解推荐这篇文章：https://segmentfault.com/a/1190000007099818 自增运算问题Java简化了运算，用++可以实现自增，但是如果这样写 12345678public static void main(String[] args)&#123; int a = 3,b = 3; a = ++a; b = b++; System.out.println(a); System.out.println(b);&#125; 先说结果a为4，b为3；对于a没什么好说的，主要是b，刚开始我所想的是b赋给b，然后b再自增应该是4，但是呢，这里确实不太好理解，我感觉正确的理解应该是这样的：我们都知道=运算先执行右边，但是b++要等到整句执行完才自增才对，程序是不能回头的，咋办？于是java就搞出了个临时变量，先把b(就是等号右边的b)存到临时变量中，执行自增运算，然后进行了=运算把这个临时变量赋给了b，所以，最后b是3，在C中也是如此 123t=b; //存到临时变量b=b+1; //执行自增，右边运算结束b=t; //用临时变量t开始左边的“=”运算 直接常量我们使用“直接常量”的时候，有时候是模棱两可的，如果出现这样的情况要对编译器进行适当的“指导”，比如： 123456int i1 = 0X2f;int i2 = 0X2F;int i3 = 0177;long n1 = 200L;float f1 = 12F;double d1 = 2D; 十六进制数适用于所有的整数数据类型，前缀0x或0X，是数字0，再有，如果是long的类型，在后面最好用大写的L，小写的l和数字1很像 内部类关于创建的问题，静态内部类可以直接被创建new A.B();，如果内部类不是静态那就只能这样创建: 12A a = new A();A.B b = a.new B(); 静态内部类的创建并不依赖于外围类，也就是不含外围类的引用，毕竟不能用外围类的方法嘛~广泛意义上的内部类一般来说包括这四种：成员内部类、局部内部类、匿名内部类和静态内部类。关于这个以前的某篇文章貌似也是说过的，哎~写的太乱，分类也乱，我也很无奈啊，先这样吧 重写问题父类的静态方法不能被子类重写子类继承父类后，用相同的静态方法和非静态方法，这时非静态方法覆盖父类中的方法（即方法重写），父类的该静态方法被隐藏（如果对象是父类则调用该隐藏的方法），另外子类可继承父类的静态与非静态方法，至于方法重载我觉得它其中一要素就是在同一类中，不能说父类中的什么方法与子类里的什么方法是方法重载的体现隐藏和覆盖的主要区别是：如果是覆盖，当子类转换成父类时不能调用父类原本的方法，因为以及被覆盖了如果是隐藏，当子类变为父类对象时，是可以执行原本父类的方法，而不会去调用子类的 关于权限Java中有4种权限，至于各个的作用，通过一张表就能明白了，很有规律 公共(public) 保护(protected) 默认(default) 私有(private) 同一类中 √ √ √ √ 同一包中 √ √ √ 子类中 √ √ - - 不同的包中 √ - - - 字段和属性一个类里定义的变量叫做字段，当提供了get/set方法，就称为属性，比如常见的javabean里面的变量都称为属性属性的多少与变量无关，只与get/set方法的数量有关这应该是比较正规的叫法 注意，在定义属性的时候命名不要用 mXxx 的这种形式，规范是一方面，还有就是当你生成 get/set 方法时就变成了 getMXxxx ，是不是很恶心，IDE 基本会自动帮你出去开头的 M，所以就变成了 getXxxx，但是返回的还是 mXxxx，这样就更恶心了，所以…..要规范！不要在 java bean 里乱定义名字 instanceof运算符instanceof是Java的一个二元操作符，和==，&gt;，&lt;是同一类东东。由于它是由字母组成的，所以也是Java的保留关键字。它的作用是测试它左边的对象是否是它右边的类的实例，返回boolean类型的数据。 12String s = "I AM an Object!";boolean isObject = s instanceof Object; String类当然是继承自Object，所以当然返回是True了instanceof运算符 只被用于对象引用变量，检查左边的被测试对象 是不是 右边类或接口的 实例化。如果被测对象是null值，则测试结果总是false判断实例和类的方法常用的就是下面的三种，第一种就是上面我们所说： instanceOf关键字，用来判断对象是否是类的实例 isAssignableFrom，用来判断类型间是否存在派生关系 isInstance方法，用来判断对象是否属于某个类型的实例 注意，后两个方法是class类中的，一般是这样用 12345ArrayList.class.isAssignableFrom(Object.class); //false Object.class.isAssignableFrom(ArrayList.class); //trueString s=new String("javaisland"); System.out.println(String.class.isInstance(s)); //true 其他遇到精度问题，例如经典的 1.0 - 0.9 ，原因就不说了，都知道，就算使用 BigDecimal 依然有精度问题，需要说明的是使用 BigDecimal 的时候不要使用 new 来构造，使用 BigDecimal.valueOf() 来初始化值，运算一律使用方法进行（除法运算除不尽时可能会抛异常） 遇到 if 连续嵌套太深（无 else ，也推荐不要写 else），为了可读性可以考虑反向条件分解成多个独立的 if，这样会比较好阅读 存档系列把那些复习 Java 时做的笔记大部分转移到了 Github，不占博客的空间了，随着熟练程度的增加那些也没啥用了。。。。 这里提供下索引，集中到这篇笔记中。 Java快速捡起基础知识概念，包含：重载、构造函数、继承、多态、封装现在看着写的还算不错 Java复习之内存JVM 内存规划的基础基础，后面看了 深入理解 JVM 表示实写的实在是太简单了 java中的集合框架同样是早期学习的产物，现在看看写的太浅了，深入一点的可以参考:Java基础复习计划二.md) 抽象类和接口早期学习产物，基本的对比而已，当时竟然还单独写了一篇，看来当时这是个难点 继承，覆盖，抽象类，和多态早期学习产物，概念的对比，当时自学时被这些概念困扰了好久啊 o(￣▽￣)ゞ))￣▽￣)o]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Centos搭建自己的邮件服务器]]></title>
    <url>%2F2017%2F03%2F21%2F%E7%94%A8Centos%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E9%82%AE%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[今天主要是闲的无聊，买的VPS不能白白的浪费掉啊，就搭个邮件服务来玩，使用的是Postfix和Dovecot来搭建，用Foxmail客户端登陆测试收信、发信均没问题先了解几个email有关的名词： MUA：用户代理端，即用户使用的写信、收信客户端软件 MTA：邮件传送端，即常说的邮件服务器，用于转发、收取用户邮件。 MDA：邮件代理端，相当于MUA和MTA的中间人，可用于过滤垃圾邮件。 POP：邮局协议，用于MUA连接服务器收取用户邮件，通信端口110。 IMOP：互联网应用协议，功能较POP多，通信端口143。 SMTP：简单邮件传送协议，MUA连接MTA或MTA连接MTA发送邮件使用此协议，通信端口25。 postfix正是提供MTA功能的开源软件，是用来收发邮件的，它没有web页面，所以要配合本地的MUA（类似于foxmail，outlook之类的软件）来进行可视化的邮件管理操作。Dovecot 是一个开源的IMAP 和POP3 邮件服务器，支持Linux/Unix 系统。作为IMAP和POP3服务器，Dovecot为邮件用户代理(MUA)提供了一种访问服务器上存储的邮件的方法(简单理解为检测用户的合法性)。但是，Dovecot并不负责从其他邮件服务器接收邮件。Dovecot只是将已经存储在邮件服务器上的邮件通过MUA显示出来。 好了下面就开始干吧 安装Postfix和Dovecot在安装之前，首先我们先卸载默认的sendmail（如果有的话），因为它是系统默认的MTA程序 1yum remove sendmail 然后就可以安装了： 12yum install postfixyum install cyrus* cyrus函数库为postfix的stmp提供安全的验证支持修改MTA（默认邮件传输代理） 1alternatives --config mta 检查一下是不是已经设置成功了。 1alternatives --display mta 第一行可以看到mta的状态。 例如：mat - status is manual.安装Dovecot： 1yum install dovecot 设置域名解析需要添加两条记录，如果要设置过滤垃圾邮件的规则的话，还要再加一条，关键字：SPF和TXT记录类型设置后一段时间后才会生效，10几分钟吧 主机记录 记录类型 记录值 MX优先级 @(不填即可，默认会给这个) MX mail.bfchengnuo.com 10 mail A 服务器的公网IP - 下面再说说这两条记录的作用 MX 记录这是为邮件服务器专门设计的，用于指定负责处理发往收件人域名的邮件服务器，简单邮件传输协议（SMTP）会根据 MX 记录的值来决定邮件的路由过程。比如用 Gmail 往 163 发邮件，因为不是一个域的，在“转发”的过程中需要根据收信人地址(@163.com)查找 DNS 以确定对方域的 IP ，这样才能正确的投递 A 记录这个是为了免登陆的，正常情况下，我们发邮件需要登陆自己的账户，那么 Gmail 往 163 进行“转发”也是在发邮件，这个过程显然不可能也要进行登陆，这就用到了 A 记录Gmail 转发邮件时通过 ehlo 打招呼的时候表明自己的邮件服务器域，也就是类似 mail.gmail.com 这样的，就是指向发邮件的服务器；对方(163)收到后进行确认，拿着这个地址查 DNS，找出对应的 IP，然后核对发送人是不是这个 IP，这样就可以确认发送人是服务器而不是个人用户，就不需要登陆了 大型的邮件系统收信服务器和发信服务器不会是在一台物理服务器上，比如基本都是 imap.gmail.com 收信服务器；smtp.gmail.com 发信服务器；对应着协议，以及现在用的比较少的 POP3 POP3 协议允许电子邮件客户端下载服务器上的邮件，但是在客户端的操作（如移动邮件、标记已读等），不会反馈到服务器上，比如通过客户端收取了邮箱中的3封邮件并移动到其他文件夹，邮箱服务器上的这些邮件是没有同时被移动的 。 而 IMAP 提供 webmail 与电子邮件客户端之间的双向通信，客户端的操作都会反馈到服务器上，对邮件进行的操作，服务器上的邮件也会做相应的动作。 http://help.163.com/10/0203/13/5UJONJ4I00753VB8.html 配置Postfix1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950vi /etc/postfix/main.cf#大约在75行，postfix主机名，修改成你的域名 此项需要添加A记录并指向postfix所在主机公网IPmyhostname = mail.bfchengnuo.com#大约在83行，后面为主机域名mydomain = bfchengnuo.com#大约在100行，设置postfix邮箱的域名后缀为$mydomainmyorigin = $mydomain#大约在117行#指定postfix系统监听的网络接口#若注释或填入公网ip 服务器的25端口将对公网开放#默认值为all 即监听所有网络接口#此项指定localhost后 本机postfix就只能发邮件不能接收邮件inet_interfaces = all#大约在120行,指定网络协议inet_protocols = ipv4#大约在165行#指定postfix接收邮件时收件人的域名，换句话说，也就是你的postfix系统要接收什么样的邮件。#此项配置中$myhostname表示postfix接受@$myhostname为后缀的邮箱的邮件 逗号分割支持指多项#此项默认值使用myhostnamemydestination = $myhostname, localhost.$mydomain, localhost#大约在266行#指定你所在的网络的网络地址#这里我填的依次是公网IP、内网IP、本地IP#请依据实际情况修改mynetworks = 40.120.xxx.xxx, 10.200.xx.xxx, 127.0.0.1#大约在571行#指定MUA通过smtp连接postfix时返回的header头信息#原始配置附带有postfix版本号 去掉即可，此项酌情处理smtpd_banner = $myhostname ESMTP#SMTP Config ,将下面的内容添加到文件尾部即可# 规定邮件最大尺寸为10Mmessage_size_limit = 10485760# 规定收件箱最大容量为1Gmailbox_size_limit = 1073741824# SMTP认证smtpd_sasl_type = dovecotsmtpd_sasl_path = private/authsmtpd_sasl_auth_enable = yessmtpd_sasl_security_options = noanonymoussmtpd_sasl_local_domain = $myhostnamesmtpd_recipient_restrictions = permit_mynetworks,permit_auth_destination,permit_sasl_authenticated,reject 修改好了之后使用/etc/rc.d/init.d/postfix start开启postfix使用chkconfig postfix on将postfix开机启动。 其中，myhostname 还有说是运行 hostname 命令返回的结果，如果不成功，可尝试 配置Dovecot还是修改配置文件，注意要修改的文件比较多，跟着改即可 1234567891011121314151617181920212223242526vi /etc/dovecot/dovecot.conf# 26行: 如果不使用IPv6，请修改为*listen = *vi /etc/dovecot/conf.d/10-auth.conf# 9行: 取消注释并修改# 是否允许在沒有 SSL/TLS 下以明码登录disable_plaintext_auth = no# 97行: 添加auth_mechanisms = plain loginvi /etc/dovecot/conf.d/10-mail.conf# 30行: 取消注释并添加mail_location = maildir:~/Maildirvi /etc/dovecot/conf.d/10-master.conf# 88-90行: 取消注释并添加 Postfix smtp 验证unix_listener /var/spool/postfix/private/auth &#123; mode = 0666 user = postfix group = postfix&#125;# 开启服务和加入开机启动/etc/rc.d/init.d/dovecot startchkconfig dovecot on 使用一切都弄好以后，就可以使用Foxmail等第三方软件来收发邮件了。在这里需要说一下，系统用户就是邮件的用户，例如root，就是一个邮箱用户，邮箱是root@bfchengnuo.com，密码就是root的密码，所以需要创建用户，只要使用useradd创建用户，再使用passwd设置密码。不建议使用root用户来测试，还是useradd一个admin用户比较好 如果使用的是Foxmail，输入E-mail地址和密码后，选择接收服务器类型是IMAP，邮件账号是你的用户名（不是邮箱地址）下面的IMAP和SMTP服务器保持默认的mail.bfchengnuo.com即可 还有就是：一定别忘了开启服务器的相应的端口！比如基本的143和25端口，对应上面的两个服务就酱！ 相关问题 启动 postfix 服务提示找不到？试试 service postfix start 或者 systemctl start postfix ；顺便可使用 service postfix reload 刷一下配置 出现意外情况？查看日志查看 /var/log/maillog 下的日志或许会有些帮助，然后放到 Google 上 PS：后来换阿里云的测试….谁能想阿里云防垃圾邮件默认封杀 25 端口，需要申请解封，心累… 参考Dovecot是什么http://lomu.me/post/linux-email-server]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>邮件系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你所应该知道的JS特性]]></title>
    <url>%2F2017%2F03%2F16%2F%E4%BD%A0%E6%89%80%E5%BA%94%E8%AF%A5%E7%9F%A5%E9%81%93%E7%9A%84JS%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[这算是我知道的唯一一种既能写前端又能写后端的语言，这么腻害的语言不会太简单，并且还是脚本语言更是有各种不可描述的用法，这只是一小部分….. 包装对象当基本类型以对象的方式去使用时，JavaScript会自动转换成对应的包装类型，相当于new了一个对象，内容和基本类型的内容一样，但是如果执行完毕后，这个临时对象会被销毁，所以如果再访问时候就是undefined比如string类型是基本数据类型，但它可以调用length属性，也可以赋予属性，但是如果访问之前赋予的属性就会提示undefined 1234var str = "lalala";str.length; // 6str.t=3; //成功str.t; //undefined 作用域js中是没有块级({})作用域的，但是函数作用域是有的，如果单独定义代码块({})，定义在里面的变量在代码块外照样可以使用，定义在里面和外面并无区别，比如 12345678for (var i = 0; i &lt; 10; i++)&#123; //TODO&#125;//和下面的写法是完全一样的var i = 0;for (; i &lt; 10; i++)&#123; //TODO&#125; 但是啊，在ES6面试之后有了let，即开始有了块级作用域…..还有，当我们在函数内定义局部变量的时候，有时会用var a=b=1这样来建立2个变量，如果这样写，a确实是局部变量，但b是全局变量，在函数外也是可以拿到的，所以，一定要分开写 语句创建函数我们一般使用两种形式，直接定义fun或者赋给一个变量，直接定义的会被预处理，在定义之前调用也是可以的 1234show();function show() &#123; console.log('lalala')&#125; 使用for...in的时候要注意，它遍历的顺序是不确定的，具体与引擎的实现有关使用with会使引擎的优化变得困难，所以不建议使用，可以用变量来代替，甚至在严格模式下，禁用了with如果想要在严格模式下执行，只需要在函数第一句或者js文件的第一句写上&quot;use strict&quot;，如果老版本浏览器会被当成字符串忽略，做到了很好的向下兼容 对象对象中定义的属性是无序的，并且每一个属性都有一个字符串key和对应的value，就算你输入的不是字符串类型，js也会帮你转换成字符串类型，这也就能解释的通为什么等价于a.s的a[&#39;s&#39;]里面是字符串了我们也稍微了解到过，对象一般都是有原型的，就是prototype，当我们通过new创建一个对象的时候，prototype就指向了函数定义的原型，当然函数原型中也有它的原型就是Object，一层一层Object的原型就是null了，就好比很多属性都有toString方法，这个就是从Object继承而来的，感觉有点说白明白，举个栗子 1234567891011function foo() &#123;&#125;// 在原型上定义一个属性foo.prototype.x = 3;var obj = new foo();obj.a = 1;obj.b = 2;console.log(obj.a); // 1console.log(obj.b); // 2console.log(obj.x); // 3 这就叫原型链，我怎么感觉和继承重写似得…..需要注意的一点是使用in来判断某属性是否存在时，是会顺着原型链查找的创建对象还可以使用var a = Object.create(null);这样的意思其实就是a的原型指向null，不过一般是传入的一个对象，也就是指定原型的指向吧(继承 :雾) null与undefined说来也比较奇怪，js中竟然有两个表示“无”的值，一切都是历史遗留问题undefined和null在if语句中，都会被自动转为false，相等运算符（==）甚至直接报告两者相等。平常的使用中，它们基本没啥区别，也没啥问题 1995年JavaScript诞生时，最初像Java一样，只设置了null作为表示”无”的值。 根据C语言的传统，null被设计成可以自动转为0。 null像Java里的一样，被当成一个对象，是的，null是一个对象，如果你用typeof(null)来查看返回的是Object所以嘛，有人(Brendan Eich)就认为这样是不合理的，所以又设计出了undefinedJavaScript的最初版本是这样区分的：null是一个表示”无”的对象，转为数值时为0；undefined是一个表示”无”的原始值，转为数值时为NaN。但是呢，在实践中证明这样是不可行的，所以后来也就…..目前普遍认为的是：null表示”没有对象”，即该处不应该有值。典型用法是： 作为函数的参数，表示该函数的参数不是对象。 作为对象原型链的终点。 undefined表示”缺少值”，就是此处应该有一个值，但是还没有定义。典型用法是： 变量被声明了，但没有赋值时，就等于undefined。 调用函数时，应该提供的参数没有提供，该参数等于undefined。 对象没有赋值的属性，该属性的值为undefined。 函数没有返回值时，默认返回undefined。 undefined ：表示未知的事物，啥也没有，无法想象。null ：有这么个概念，但是没东西 ，但是 null并非object，虽然 typeof null 的结果是 object 全局变量一般来说，定义全局变量有三种方式第一种就是直接var一个，不过不要在函数内，否则就是局部变量了(哦，对JS中是没有class概念的)另一种是直接给标识符赋值，这样会隐式的声明了全局变量test。即使该语句是在一个function内，当该function被执行后test变成了全局变量。有人说这才是最正统的定义全局变量的方式最后一种就是不使用var使用window，类似：window.x = 4查看定义的全局变量可以使用for in window 123456// 判断有没有a1 a2 a3变量for(a in window)&#123; if(a=='a1'||a=='a2'||a=='a3')&#123; alert(a) &#125;&#125; 需要注意一点的是：通过var声明的变量无法删除(delete a1)，所有浏览器表现一致。这在犀牛书上也有提到。 NaN类型全局属性NaN表示 Not-A-Number 的值，所以说它是一个全局对象的属性，NaN属性的初始值就是NaN，和Number.NaN 的值一样。也就是说 NaN 是一种特殊的 Number 类型值 无穷大除以无穷大、给任意负数做开方运算 或者 算数运算符与不是数字或无法转换为数字的操作数一起使用时都将返回 NaN。 如果 JavaScript 期望使用一个数字，它把给定的值将转换为数字（如果转换结果无意义的话将返回 NaN）。 举个栗子就是：Number(&#39;a&#39;); 首先全局的 isNaN() 函数不能严格判断输入值是否为 NaN。严格判断使用typeof value === &#39;number&#39; &amp;&amp; isNaN(value);NaN 和任何对象做运算都会返回NaN，好玩的是 NaN != NaN 确实是这样，它谁都不等，包括自己 window对象JavaScript由三部分组成：EMCAScript、DOM、BOM。DOM是一个使程序和脚本有能力动态地访问和更新文档的内容、结构以及样式的平台和语言中立的接口。,而BOM定义了JavaScript可以进行操作的浏览器的各个功能部件的接口。首先应该清楚的是两者皆为接口定义。 DOM是W3C的标准（所有浏览器公共遵守的标准） BOM是各个浏览器厂商根据DOM在各自浏览器上的实现 window是BOM对象，而非JavaScript对象，不过恰好为EMCAScript中所定义的Global对象 window是BOM对象，而非JavaScript对象，不过恰好为EMCAScript中所定义的Global对象由于window包含了document，因此JavaScript可以直接通过使用window的document对象来访问、检索、修改文档内容与结构。因为document对象又是DOM的根节点，所以可以理解为BOM包含了DOM。即浏览器提供出来给予访问的是BOM对象，而BOM对象再访问到DOM对象，从而js可以操作浏览器以及浏览器读取到的文档。 原文：https://www.zhihu.com/question/29917511 字符串的拼接在很多高级语言中，加号(+)在字符串的操作中被赋予了更多的意义：作为字符串拼接的操作符。不过在Java和C#中，我们也知道如何频繁进行字符串拼接的操作，使用加号(+)就会产生效率问题，因此在这种情况下就会推荐使用StringBuilder。我们知道 java 中 String 是引用类型，使用 += 进行字符串拼接将会频繁地分配新地址，指向新的地址块，这无疑白白地消耗了系统的性能。javascript中的字符串类型同java的String类似，如果我们大量使用+=进行字符串拼接的话，将会使界面失去响应(卡死状态)解决方案可以使用数组来代替 StringBuilder： 123456789101112var strArr = new Array();strArr.push("aaaa");strArr.push("bbbb");strArr.push("ccccc");alert(strArr.join(' '));// 或者var sb = [];for(var i = 0; i &lt;=21; i++) &#123; sb.push(i);&#125;document.write(sb.join('')); 在其他语言中也是类似，所以频繁拼接的尽量避免 += 方式，比如 python 中可以使用 &#39;&#39;.join 来进行拼接涉及到中文的字符记得使用 js 中的 encodeURL 方法进行编码. 箭头函数ES6标准新增了一种新的函数：Arrow Function（箭头函数），它相当于是匿名函数，简化了函数的定义，有点 lambda 的味道~ 123456789101112131415161718192021222324252627282930313233x =&gt; x * x// 等价于function (x) &#123; return x * x;&#125;// 包含多条语句的x =&gt; &#123; if (x &gt; 0) &#123; return x * x; &#125; else &#123; return - x * x; &#125;&#125;// 两个参数:(x, y) =&gt; x * x + y * y// 无参数:() =&gt; 3.14// 可变参数:(x, y, ...rest) =&gt; &#123; var i, sum = x + y; for (i=0; i&lt;rest.length; i++) &#123; sum += rest[i]; &#125; return sum;&#125;// 返回一个对象，为了避免冲突使用括号x =&gt; (&#123; foo: x &#125;) 箭头函数看上去是匿名函数的一种简写，但实际上，箭头函数和匿名函数有个明显的区别：箭头函数内部的this是词法作用域，由上下文确定。回顾前面的例子，由于JavaScript函数对this绑定的错误处理，下面的例子无法得到预期结果： 123456789101112131415161718192021var obj = &#123; birth: 1990, getAge: function () &#123; var b = this.birth; // 1990 var fn = function () &#123; return new Date().getFullYear() - this.birth; // this指向window或undefined &#125;; return fn(); &#125;&#125;;// 箭头函数方式var obj = &#123; birth: 1990, getAge: function () &#123; var b = this.birth; // 1990 var fn = () =&gt; new Date().getFullYear() - this.birth; // this指向obj对象 return fn(); &#125;&#125;;obj.getAge(); // 25 由于this在箭头函数中已经按照词法作用域绑定了，所以，用call()或者apply()调用箭头函数时，无法对this进行绑定，即传入的第一个参数被忽略。 其他补充定义正则用 /str/ ,如果是全文匹配就是 /str/g ，全文匹配表示匹配到一个后并不会停下来，它不需要转义，但是使用 new RegExp(&quot;\\d+&quot;) 这样的原始形式是需要转义的。 isNaN() 函数通常用于检测 parseFloat() 和 parseInt() 的结果，以判断它们表示的是否是合法的数字。 禁止事件冒泡： e.preventDefault()，IE 则是使用 e.returnValue = false; ，e 为事件 event，一般在函数中传递。javascript 的 return false 只会阻止默认行为，而是用 jQuery 的话则既阻止默认行为又防止对象冒泡。 参考http://www.ruanyifeng.com/blog/2014/03/undefined-vs-null.html]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP笔记]]></title>
    <url>%2F2017%2F03%2F13%2FHTTP%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[发现写的好乱，内容基本是图解HTTP里的，但感觉讲的确实比较浅，以前我确实也是特意搜过这些，抓包的时候会用到吧~啊哈哈，真的是又乱又杂….就先这样吧，比如SSL/TLS这种我在公众号写过了，这里就直接不多解释了，解释起来内容也挺多了….. 简单的HTTP协议如果查看数据包，前两行应该是类似这样的： 12OPTIONS * HTTP/1.1Host:bfchengnuo.com OPTIONS方法主要用来查询针对请求URL指定资源支持的方法，*代表的就是不是对特定资源，而是对服务器发起的请求 CONNECT方法要求在与代理服务器通讯时建立隧道，实现用隧道协议进行TCP通信，主要使用SSL/TLS协议把通信内容进行加密然后经过网络隧道传输：CONNECT 代理服务器名:端口号 HTTP版本，后面也就一样了跟Host之类，如果响应为200，那么之后就进入网络隧道了其实我们常用的也就是GET和POST方法，GET常用于获取，POST用于发送，明显的区别就是GET会把参数放在URL中传送，所以不安全内容大小也有限，但是速度快，POST相反，内容放在请求头中，相对安全 我们知道HTTP是无连接的协议，也就是每进行一次HTTP请求就要断开一次TCP连接，如果访问一个很多图片的网站，除了本身的获取HTML文件需要建立连接、发送请求/响应、断开连接，每一个图片的获取也要都来一遍，这就非常的浪费流量，并且还增加了服务器负载所以在HTTP/1.1中有了持久连接，就是说，只要任意一端没有明确提出断开连接，则保持TCP连接状态，在1.1中默认都是持久连接，当然是需要客户端(比如浏览器)支持的支持管线化后可以同时发送多个HTTP请求，使网页加载更快但毕竟是无连接的协议，在网页跳转之后服务器就不认识你了，在需要记录登陆状态的网站，一般都需要使用Cookie来实现，当发送HTTP请求的时候携带Cookie，服务器再进行对比分析(通过Session)，从而确认是谁发起的请求具体的步骤是：客户端请求—-&gt;服务器生成Cookie，并且通过响应返回给客户端—-客户端在请求中添加Cookie—-&gt;服务器知道是这家伙发送的请求Cookie最大也只能是4KB~存在于请求头中 HTTP报文HTTP报文有两部分组成，报文首部和报文主体，中间用空行(CR+LF)分割，以访问百度为例请求报文： 123456789101112GET / HTTP/1.1Host: www.baidu.comConnection: keep-aliveCache-Control: max-age=0Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8DNT: 1Accept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.6Cookie: BAIDUID=241D3FBE4A65352E2607D08762736012:FG=1;省略....太长了空行 响应报文： 1234567891011121314151617HTTP/1.1 200 OKServer: bfe/1.0.8.18Date: Sat, 11 Mar 2017 07:30:01 GMTContent-Type: text/html;charset=utf-8Transfer-Encoding: chunkedConnection: keep-aliveCache-Control: privateExpires: Sat, 11 Mar 2017 07:30:01 GMTContent-Encoding: gzipX-UA-Compatible: IE=Edge,chrome=1Strict-Transport-Security: max-age=172800BDPAGETYPE: 2BDQID: 0x8bb8b8d40000c99aBDUSERID: 273454118Set-Cookie: BDSVRTM=111; path=/空行html网页内容（报文主体） 为了节省流量，服务器返回的主体可能会进行压缩，比如gzip, deflate方式等，还有一个是identity是不进行编码 分割发送因为浏览器必须等到主体后才能加载，当一个网页很大时，那么加载就需要一定时间，页面一片空白，这时可以把主体部分进行分割发送，让浏览器先加载一部分分块时每一块都会用16进制来标记块的大小，主体的最后一块使用0(CR+LF)来标记 范围请求也就是所说的断点续传，就是可以获取部分内容的请求，比如要获取5001-10000字节内的资源在请求头中有：Range:bytes=5001-10000与之对应的响应：Content-Range:bytes 5001-10000/10000如果是获取从5000以后的全部可以直接5000-这样写，以及可以多重范围：从开始到200以及300以后的就是：-200,300-如果服务器支持会返回206 Partial Content的状态码，不支持就返回200 OK并且会返回全部的内容 内容协商就是根据你的环境返回最适合的内容，比如使用Accept-Language指明语言为中文，返回的就是中文网页 状态码 1XX: 信息 2XX: 成功 3XX:重定向301：永久重定向302：临时重定向301、302、303返回时，几乎所有浏览器都会把POST改为GET，并删除报文中的主体部分，然后再次发送虽然301、302的标准是进制改为GET的 4XX客户端错误400：请求是不是写错了？服务器不理解401：需要认证并且认证失败403：不允许访问404：没找到 5XX服务器错误503：服务器正忙 详细参考：http://www.w3school.com.cn/tags/html_ref_httpmessages.asp Web服务器的协作一台Web服务器是运行部署多个域名的，虽然会被解析到同一台服务器，但是因为Host首部指定了唯一的域名地址，所以是可以正常解析的 代理其实就相当于在服务器与客户端之间的中间人，负责转发数据包 网关网关是转发其他服务器通信数据的服务器，接收到客户端发来的请求时，像自己拥有资源一样进行处理，客户端甚至感觉不到通信目标是个网关网关顾名思义就是连接两个网络的设备，能在网络间转递数据包，主机则不能利用网关也可以把HTTP请求转换为其他通信协议 隧道简单说就是：相隔较远的服务器和客户端之间进行中转，并保持双方通信连接的程序，隧道本身是透明的使用SSL等加密手段确保客户端和服务器进行安全的通信，隧道本身不会去解析HTTP请求，保持原样转发隧道协议：将另一个不同的网络协议，封装在负载部分。使用隧道的原因是在不兼容的网络上传输数据，或在不安全网络上提供一个安全路径。比如SSH隧道提供一个绕过防火墙，从而连到某些被禁止的互联网服务的的方法。被封装的数据包在互联网上传递时所经过的逻辑路径被称为”隧道”。隧道可在网络的任一层实现，最常用的是两层：数据链路层和网络层。 缓存缓存都会有一个期限，以浏览器为例：当超过后浏览器会向服务器确认是否过期，如果确实失效，然后再重新获取 HTTP首部主要是HTTP/1.1版本，每隔字段下面其实还细分很多参数，太多了就不贴了，用到的时候去找文档贴下各首部主要是干什么的 通用首部字段 首部字段名 说明 Cache-Control 控制缓存的行为(no-cache可以缓存，但需要向服务器确认) Connection 逐跳首部，连接的管理(Keep-Alive持久连接，close关闭连接) Date 创建报文的日期时间 Pragna 报文指令 Trailer 报文末端的首部一览 Transfer-Encoding 指定报文主体的传输编码方式 Upgrade 升级为其他协议 Via 代理服务器的相关信息 Warning 错误通知 请求首部字段需要说明的是，除了下面的这些 HTTP 头是可以自定义的，在做 RESTful 的时候可能会用到；在 HTTP 协议本身是没有限制 HTTP 头的大小。尽管如此，很多Web服务器、客户端和代理软件都对HTTP头的大小进行限制。如 Apache2.3 中，每个头的大小最多是8160个字节，一个请求里面最多包含100个头。 首部字段名 说明 Accept 用户代理可处理的媒体类型（xx;q=0.6, xxx表示权重） Accept—Charset 优先的字符集 Accept-Encoding 优先的内容编码 Accept-Language 优先的语言（自然语言） Authorization Web认证信息(证书信息，会返回401) Expect 期待服务器的指定行为 From 用户的电子邮箱地址 Host 请求资源所在服务器 if-Match 比较实体标记（ETag） if-Modified-Since 比较资源的更新时间 if-None-Match 比较实体标记（与if-Match相反） if-Range 资源为更新时发送实体Byte的范围请求 if-Unmodified-Since 比较资源的更新时间（与if-Modified-Since相反） Max-Forwards 最大传输逐跳数 Proxy-Authorization 代理服务器要求客户端的认证信息 Range 实体字节范围请求 Referer 对请求中的URL的原始获取方法（防盗链） TE 传输编码的优先级 User-Agent HTTP客户端程序的信息 更多可参考：http://dafeizizhu.github.io/2013/07/12/http-header/ 响应首部字段 首部字段名 说明 Accept-Ranges 是否接受字节范围请求 Age 推算资源创建经过时间 ETag 资源的匹配信息 Location 令客户端重定向至指定的URL Proxy-Authenticate 代理服务器对客户端的认证信息 Rety-After 对再次发起请求的时机要求 Server HTTP服务器的安装信息 Vary 代理服务器缓存的管理信息 WWW-Authenticate 服务器对客户端的认证信息 实体首部字段 首部字段名 说明 Allow 资源科支持的HTTP方法 Content-Encoding 实体主体适用的编码方式 Content-Language 实体主体的自然语言 Content-Length 实体主体的大小（单位：字节） Content-Location 替代对资源的URL Content-MD5 实体主体的报文摘要 Content-Range 实体主体的位置范围 Content-Type 实体主体的媒体类型 Expires 实体主体过期的日期时间 Last-Modified 资源的最后修改日期时间 为Cookie服务的首部字段 首部字段名 说明 首部类型 Set-Cookie 开始状态管理所有的Cookie信息 响应首部字段 Cookie 服务器接收到的Cookie信息 请求首部字段 Set—Cookie字段的属性 属性 说明 NAME=VALUE 赋予Cookie的名称和其值 expires=DATE Cookie的有效期（若不mingque指定则默认为浏览器关闭前为止） path=PATH 将服务器上的文件目录作为Cookie的适用对象（若不指定则默认为文档所在的目录） domain=域名 作为Cookie适用对象的域名（若不指定则默认为创建Cookie的服务器的域名，更安全） Scure 仅在HTTPS安全通信时才会发送Cookie HttpOnly 加以限制，使Cookie不能被JavaScript脚本访问 参考：https://ttop5.gitbooks.io/illustration-http/content/chapter6.html Session与Cookie关于这个在PHP(二)中提到过，这里再补充下cookie保存在客户端，session保存在服务器端，session 可以放在 文件(默认)、数据库、或内存中都可以。它们大多用来保持会话状态，流程类似：客户端发送认证信息—-&gt;服务器认证后生成一个Session然后把ID存到Cookie里返回给客户端客户端再次请求时携带Cookie—–&gt;服务器验证后确认是真实用户用户的多数信息可以放在Session中，一方面是因为安全问题，Cookie在客户端可以随意更改；另一方面Cookie只能保存4KB大小，Session随意了，因为一般不会放在内存中 我们知道HTTP是无连接的，当我们从A页面跳到B页面时，HTTP请求肯定是不同的，不同的HTTP请求之间肯定是无法共享数据的，但是Cookie在整个网站的域是可以随意访问的，所以可以利用Cookie来进行保持会话 Session生命周期Session在用户第一次访问服务器的时候自动创建。需要注意只有访问JSP、Servlet、PHP等后端程序时才会创建Session，只访问HTML、IMAGE等静态资源并不会创建Session。Session生成后，只要用户继续访问，服务器就会更新Session的最后访问时间，并维护该Session。用户每访问服务器一次，无论是否读写Session，服务器都认为该用户的Session“活跃（active）”了一次。服务器会把长时间内没有活跃的Session从内存/硬盘删除。这个时间就是Session的超时时间。如果超过了超时时间没访问过服务器，Session就自动失效了。不过有人说当浏览器关闭后(非标签)，Session就会销毁，这种是通过什么实现的我没找到….不过我知道服务器为保存SessionID而设置的Cookie是没有有效期的： Cookie如果没有设置有效期，它的maxAge属性一般为–1，表示仅当前浏览器内有效，关闭浏览器就会失效。 服务器回写的 Cookie 名默认叫 JSESSIONID，注意是有path的，默认为当前应用 所以，当关掉浏览器这个Cookie就失效了，ID也就丢失了，服务器判断一段时间 Session 没有活跃，所以就删除了 更多内容参考：http://www.admin10000.com/document/7097.html 追加协议HTTP/1.1也有很多的缺点： 一条连接只能发送一个请求 请求只能从客户端开始，客户端不能接收响应外的指令 发送冗长的首部，每次发相同的首部也是浪费流量 非强制压缩，有可能未经压缩就发送 如果是一个对实时更新要求比较高的网站，服务器端更新了在没有请求的情况下不能发送给客户端，只能客户端不断的去请求，发送大量重复的首部，每次返回的都是全部的内容 Ajax核心技术是XMLHttpRequest的API，通过JS脚本就可以和服务器进行HTTP通信，从而实现在加载完的页面上发送请求，并且进行局部更新 Comet通常，服务器收到请求，处理完毕后会立即返回响应，但是Comet会将响应置于挂起状态，一旦服务器更新了再返回响应，这样也就是说，一次连接的时间边长了 但是以上两种并没有解决HTTP本身的问题，还是会发送大量的重复首部，后来有了Google的SPDYSPDY工作在会话层，也就是TCP(SSL)和HTTP之间，还是采用HTTP建立连接，考虑到安全性规定使用SSL，它做到了： 单路复用在单一的TCP连接上可以处理多个HTTP请求 赋予请求优先级 压缩HTTP首部 推送功能服务器可以直接发送数据给客户端，不用等待客户端的请求 服务器提示主体提示客户端请求的所需资源，在客户端请求资源前就知道了资源的存在，减少了请求 WebSocket协议也是为了解决HTTP协议的瓶颈，它支持发送任意类型的数据，推送功能，会一直保持连接状态所以也就可以进行全双工通信，可以直接向客户端发送数据 建立WebSocket还是用的HTPP，在首部加入Upgrade:websocket;Connection:Upgrade，服务器收到后返回101 Switching Protocols然后双方就开始使用WebSocket进行通信了]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git基本操作]]></title>
    <url>%2F2017%2F03%2F12%2FGit%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[知道及接触Git也有一段时间了，但是很惭愧的只会5条左右的命令….(:捂脸)，就是添加、提交、推送等，还有一些功能比如回退这是很常用的，这次搞个总结，以后忘记了来巴拉巴拉 准备工作下载就不说了，然后是要配置Git，Git的全局配置一般会写到用户目录下的.gitconfig文件中，所以之间修改这个文件也是可以的，当然用命令也是可以的，首先就是配置用户名和邮箱啦 12$ git config --global user.name "John Doe"$ git config --global user.email johndoe@example.com 加上global就是全局的意思，如果想单独设置可以去掉global，会保存在.git/config下修改差异比较工具：$ git config --global merge.tool vimdiff查看配置：git config --list 基本操作首先我们要得到一个仓库才行啊，有两种方式，1.本地创建一个 2.从别处克隆(下载)一个 1234$ git init# or# 默认克隆到当前目录，也可以后面跟个指定的目录名$ git clone git://github.com/schacon/grit.git 然后就可以操作这可库了，比如通过add跟踪一个文件，commit提交，status查看文件状态 1234# 查看文件状态$ git status# 开始跟踪一个新文件$ git add README 文件被跟踪后会进入暂存区，文件一单有改动使用status就可以看到基本变动信息，并且需要重新把这个变更的文件重新添加到暂存区，也就是说，你的改动并不会影响暂存区的文件，暂存区的文件相当于一份拷贝，你必须重新add后暂存区的文件才会更新成现在你最新更新的文件，所以说add方法是个多功能的命令至于意义，因为当你commit提交的时候只会提交暂存区的文件，无论你本地修改了多少内容，只要不add，就是无效 Git还可以自动忽略某些文件，在仓库目录新建一个.gitignore的文件，在这里面规定就可，可以使用通配符，比如 1234567891011121314151617# 忽略所有以 .o 或 .a 结尾的文件。*.[oa]# 忽略所有以~结尾的文件*~# 此为注释 – 将被 Git 忽略# 忽略所有 .a 结尾的文件*.a# 但 lib.a 除外!lib.a# 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO/TODO# 忽略 build/ 目录下的所有文件build/# 会忽略 doc/notes.txt 但不包括 doc/server/arch.txtdoc/*.txt# ignore all .txt files in the doc/ directorydoc/**/*.txt 文件 .gitignore 的格式规范如下： 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 要查看尚未暂存的文件更新了哪些部分，不加参数直接输入 git diff若要看已经暂存起来的文件和上次提交时的快照之间的差异，可以用 git diff --cached 或者git diff --staged命令 好了，最后就是提交了，一般使用类似$ git commit -m &quot;Story 182: Fix benchmarks for speed&quot;的命令，后面跟提交说明如果给 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤 撤销操作有时候我们提交完了才发现漏掉了几个文件没有加，或者提交信息写错了。想要撤消刚才的提交操作，可以使用 --amend 选项重新提交：$ git commit --amend，就是说这条命令有两个作用： 提交当前暂存区，并合并到上一次commit。常用于提交后发现漏了几个文件，又不想再提交一次的情况； 可以修改上一次commit的描述。 如果我们不小心add了某个文件，怎么将这个文件从暂存区删除呢，可以使用git reset HEAD &lt;file&gt;...，注意的是文件的修改并不会被抹掉，只是从暂存区删除如果我们感觉某个文件完全没修改的必要，想要回退到未修改的状态，也就是与版本库一致，那么可以使用：$ git checkout -- benchmarks.rb强制回退到某个版本，可以使用git reset --hard 02c4b5b31后面的是ID，保证唯一即可，注意加了hard所有的修改会被抹掉，还有一些常用的： 12345$ git reset HEAD^ #将当前分支往回退一步,不会抹掉修改，出现很多未暂存文件HEAD^ # HEAD之前的commitHEAD^^ # HEAD回退两步的commitmaster~5 # master指针回退5步的commit 你已经执行了 git push, 把你的修改发送到了 GitHub，现在你意识到这些 commit 的其中一个是有问题的，你需要撤销那一个 commit ，这时可以使用 git revert &lt;SHA&gt; :git revert 会产生一个新的 commit，它和指定 SHA 对应的 commit 是相反的（或者说是反转的），也就是说这次反转也会被记录在 commit 历史里，这样更安全，但是由于一些原因不希望记录回退的记录那么可以……还有一个大杀招： 1234# 取消当前版本之前的两次提交git reset --hard HEAD~2# 强制提交到远程版本库，从而删除之前的两次提交数据git push origin HEAD --force 远程仓库在推送之前首先得和远程仓库建立联系，如果使用的clone命令创建的仓库，那么就会自动关联了，如果是init创建的，就需要手动的关联：git remote add origin &lt;server&gt;然后就可以使用git push origin master推送了，master可以换成你想要推送的任何分支当然也可以添加多个远程仓库，名字不用重即可，比如上面那个就是叫origin的，通过克隆的会自动归于origin下删除远程库：$ git remote rm paul查看所关联的仓库：$ git remote -v获取远程仓库的最新内容可以使用：git pull，他会拉取并自动合并只是拉取的话也可以使用：git fetch origin 分支/标签管理查看本地所有分支（如果至查看远程就加 -r ，查看全部 -a）：git branch创建一个叫做“feature_x”的分支，并切换过去：git checkout -b feature_x切换回主分支：git checkout master再把新建的分支删掉：git branch -d feature_x除非你将分支推送到远端仓库，不然该分支就是 不为他人所见的：git push origin &lt;branch&gt;要合并其他分支到你的当前分支（例如 master），执行：git merge &lt;branch&gt;删除远程分支：git push origin --delete [branch-name] 查看所有 tag：git tag删除本地分支：git tag -d [tag]删除远程分支：git push origin :refs/tags/[tagName]查看 tag 信息：git show [tag]提交指定/全部 tag：git push [remote] [tag]git push [remote] --tags新建一个分支，指向某个 tag：git checkout -b [branch] [tag]可以执行如下命令创建一个叫做 1.0.0 的标签：git tag 1.0.0 1b2e1d63ff1b2e1d63ff 是你想要标记的提交 ID 的前 10 位字符。当然不一定是10位，只要保证唯一性就可以，可以使用log命令获得，默认是当前最近的一次 commit 其他Git默认是忽略文件夹的大小写的，如果想要它对大小写敏感使用：git config core.ignorecase false在需要的仓库下执行即可，如果想全局生效加--global参数 待补充…]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery-动画]]></title>
    <url>%2F2017%2F03%2F11%2FjQuery-%E5%8A%A8%E7%94%BB%2F</url>
    <content type="text"><![CDATA[隐藏和显示让页面上的元素不可见，一般可以通过设置css的display为none属性。jQuery中可以直接调用.hide()来隐藏，这个方法可以接受一个参数，用来实现动画化，并且有两个快捷参数fast 和slow 分别代表200和600毫秒的延时，就是元素会执行200/600毫秒的动画后再隐藏 jQuery在做hide操作的时候，是会保存本身的元素的原始属性值，再之后通过对应的方法还原的时候还是初始值。 比如一个元素的display属性值为inline，那么隐藏再显示时，这个元素将再次显示inline。一旦透明度达到0，display样式属性将被设置为none，这个元素将不再在页面中影响布局 123456$("#a2").hide(&#123; duration: 3000, complete: function() &#123; alert('执行3000ms动画完毕') &#125;&#125;) 既然有隐藏，当然也就有显示，使用方法类似，举个栗子：$(&#39;elem&#39;).hide(3000).show(3000)但是需要注意的是：如果让show与hide成为一个动画，那么默认执行动画会改变元素的高度，高度，透明度 show与hide是一对互斥的方法。需要对元素进行显示隐藏的互斥切换，通常情况是需要先判断元素的display状态，然后调用其对应的处理方法。对于这样的操作行为，jQuery提供了一个便捷方法toggle用于切换显示或隐藏匹配元素，其实调用的就是上面两个方法，所以使用方法都是一样的 上卷下拉效果上面的显示和隐藏方法会将元素的宽度，高度，以及不透明度，同时进行动画操作。这样看起来不是很爽下拉动画：.slideDown()：用滑动动画显示一个匹配元素.slideDown()方法将给匹配元素的高度的动画，这会导致页面的下面部分滑下去，弥补了显示的方式常见的操作，提供一个动画是时间，然后传递一个回调，用于知道动画是什么时候结束：.slideDown( [duration ][, complete ] )持续时间（duration）是以毫秒为单位的，数值越大，动画越慢，不是越快。字符串 ‘fast’ 和 ‘slow’ 分别代表200和600毫秒的延时。如果提供任何其他字符串，或者这个duration参数被省略，那么默认使用400 毫秒的延时。具体使用： 123$("ele").slideDown(1000, function() &#123; //等待动画执行1秒后(也就是动画完成后),执行别的动作....&#125;); 那么上卷函数就是slideUp了，使用方法一样 因为动画是异步的，所以要在动画之后执行某些操作就必须要写到回调函数里面，这里要特别注意 和隐藏方法类似，也有上卷下拉切换的方法slideToggle 淡入淡出效果fadeOut()函数用于隐藏所有匹配的元素，并带有淡出的过渡动画效果，使用方法和上面的几个函数是一样的有out当然就有in，所以有相应的fadeIn()方法相应的也有一个切换的方法：fadeToggle()如果隐藏就显示，如果显示就隐藏既然是透明度，那么就不止0和1，还有0.5这样的半透明状态，所以jQ还提供了一个方法：fadeTo()来过渡到指定的透明度，用法都是类似，不多说 自定义动画自定义主要使用的是animate方法 .animate()方法允许我们在任意的数值的CSS属性上创建动画。2种语法使用，几乎差不多了，唯一必要的属性就是一组CSS属性键值对。这组属性和用于设置.css()方法的属性键值对类似，除了属性范围做了更多限制。第二个参数开始可以单独传递多个实参也可以合并成一个对象传递了 使用方法可以参考下面的栗子 12345678910111213141516171819202122232425var $aaron = $("#aaron");if (v == "1") &#123; // 数值的单位默认是px $aaron.animate(&#123; width :300, height :300 &#125;);&#125; else if (v == "2") &#123; // 在现有高度的基础上增加100px $aaron.animate(&#123; width : "+=100px", height : "+=100px" &#125;);&#125; else if (v == "3") &#123; $aaron.animate(&#123; fontSize: "5em" &#125;, 2000, function() &#123; alert("动画 fontSize执行完毕!"); &#125;);&#125; else if (v == "4") &#123; //通过toggle参数切换高度 $aaron.animate(&#123; width: "toggle" &#125;);&#125; 关于动画的时间也是提供’fast’ 和 ‘slow’字符串，分别表示持续时间为200 和 600毫秒。如果多个元素执行动画，回调将在每个匹配的元素上执行一次，不是作为整个动画执行一次除了上面的创建方式，其实还有一种：.animate( properties, options )options参数 duration - 设置动画执行的时间 easing - 规定要使用的 easing 函数，过渡使用哪种缓动函数 step：规定每个动画的每一步完成之后要执行的函数 progress：每一次动画调用的时候会执行这个回调，就是一个进度的概念 complete：动画完成回调 1234567891011if (v == "1") &#123; $aaron.animate(&#123; height: '50' &#125;, &#123; duration :2000, //每一个动画都会调用 step: function(now, fx) &#123; $aaron.text('高度的改变值:'+now) &#125; &#125;)&#125; 还有就是可以手动停止动画，方法就是stop了，它最多可以接受2个参数，具体怎么用： 123456789$("#aaron").animate(&#123; height: 300&#125;, 5000)$("#aaron").animate(&#123; width: 300&#125;, 5000)$("#aaron").animate(&#123; opacity: 0.6&#125;, 2000) stop()：只会停止第一个动画，第二个第三个继续stop(true)：停止第一个、第二个和第三个动画stop(true ture)：停止动画，直接跳到第一个动画的最终状态 其他补充jQuery中有个很重要的核心方法each，大部分jQuery方法在内部都会调用each，其主要的原因的就是jQuery的实例是一个元素合集jQuery的大部分方法都是针元素合集的操作，所以jQuery会提供$(selector).each()来遍历jQuery对象 12345$.each(["Aaron", "慕课网"], function(index, value) &#123; //index是索引,也就是数组的索引 //value就是数组中的值了 return false; //停止迭代&#125;); each就是for循环方法的一个包装，内部就是通过for遍历数组与对象，通过回调函数返回内部迭代的一些参数 jQuery.inArray()函数用于在数组中搜索指定的值，并返回其索引值。如果数组中不存在该值，则返回 -1。$.inArray(5,[1,2,3,4,5,6,7]) 返回对应的索引：4，在ECMAScript5已经有数据的indexOf方法支持了 JQ同样也支持trim方法去除字符串两段的空格 如果想要获得指定的DOM对象，比如：第二个a元素的查找： $(a).get(1)，索引从0开始嘛~但是它是支持从后往前找的，比如找最后一个get(-1)，index方法就是相反了，根据DOM元素来获取索引]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中使用正则]]></title>
    <url>%2F2017%2F03%2F06%2FJava%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%AD%A3%E5%88%99%2F</url>
    <content type="text"><![CDATA[这应该是比较常用的吧….可惜我忘了，还有字符串相关的方法使用频率也很高Java 提供了 java.util.regex 包来与正则表达式进行模式匹配。注意使用的就是两个类，Pattern 和Matcher 其他语言也挺类似的，使用方法也比较一致对了字符串这个对象吧….其实是“固定的”（不可变），每次改变都会重新开辟内存，所以还是尽量少频繁的使用String+=… java.util.regex包主要包含了下面的三个类： Pattern 类:一个 Pattern 对象是正则表达式编译表示。 Pattern 类没有提供公共的构造函数。要创建一个 Pattern 对象，你必须首先调用他的公用静态编译方法来获得 Pattern 对象。这些方法的第一个参数是正则表达式。 Matcher 类:一个 Matcher 对象是用来解释模式和执行与输入字符串相匹配的操作。和 Pattern 类一样 Matcher 类也是没有构造方法的，你需要通过调用 Pattern 对象的 matcher 方法来获得 Matcher 对象。 PatternSyntaxException: 一个 PatternSyntaxException 对象是一个不被检查的异常，来指示正则表达式中的语法错误。 捕获组的概念捕获组可以通过从左到右计算其开括号来编号，编号是从1 开始的。例如，在正则表达式 ( (A) (B (C) ) )中，存在四个这样的组: 1—–((A)(B(C))) 2—–(A) 3—–( B (C) ) 4—–(C) 其实还有一个0组，始终代表整个表达式。至于为什么要说这个捕获组呢，因为很快就会用到了 非捕获组既然有捕获组那就有非捕获组 建议看完其他内容后最后再看这个 非捕获组(non-capturing)： (?:X) 、(?=X)、 (?&lt;=X)、 (?!X)、 (?&lt;!X)这里就只是简单的说下吧(?:X):我们知道了分组用group(index)可以获得指定组的匹配结果，那么(?:X)的作用就是只分组不捕获，就是说匹配的时候还是管用的，但是如果它是在第二组，使用group(2)的时候并不会获得它，会跳过，获得的是第三组的结果(?=X)：举个例子[0-9a-z]{2}(?=aa)意思就是是两位字符（数字，或字母），且后面紧跟着两个a。但是使用while循环匹配group()获取的时候虽然匹配aa但是不会显示在匹配结果中。这里需要注意的是当第二次搜索的时候是从第一次匹配的aa(?&lt;=)：和上面差不多，不一样的是它是匹配前面的，也就是要放在前面(?!)和(?&lt;!)：不多说了举两个栗子吧：[0-9a-z]{2}(?!aa) 意思是：匹配两个字符，且后面紧跟着的不是aa(?&lt;=aa)[0-9a-z]{2} 意思是：匹配两个字符，且前面紧跟着的不是aa Pattern前面说过这个类可以认为是负责编译正则的，如果书写的正则表达式有错误那么它就会报错然后就说下常用的几个方法 分割字符串123Pattern p=Pattern.compile("\\d+"); String[] str=p.split("我的QQ是:456456我的电话是:0532214我的邮箱是:aaa@aaa.com"); // 结果:str[0]="我的QQ是:" str[1]="我的电话是:" str[2]="我的邮箱是:aaa@aaa.com" 说白了就是用p去分割字符串 快速匹配有一个静态方法是：Pattern.matcher(String regex,CharSequence input) 可以快速让我们知道某个字符串是否符合某个规则 12Pattern.matches("\\d+","2223");//返回truePattern.matches("\\d+","2223aa");//返回false,需要匹配到所有字符串才能返回true,这里aa不能匹配到 Matcher好了，重头戏来了，前面说过获取Mather必须由Pattern来，也就是Pattern.matcher方法Pattern类只能做一些简单的匹配操作,要想得到更强更便捷的正则匹配操作,那就需要将Pattern与Matcher一起合作.Matcher类提供了对正则表达式的分组支持,以及对正则表达式的多次匹配支持.下面是官方API里的例子，典型调用顺序 1234567Pattern p = Pattern.compile("a*b"); //编译Matcher m = p.matcher("aaaaab"); //获取结果boolean b = m.matches(); //使用结果//上面三句可以写成一句//但是对于重复的匹配而言它效率不高，因为它不允许重用已编译的模式。boolean b = Pattern.matches("a*b", "aaaaab"); 如果写成一句，那就是编译加匹配，编译还是蛮耗时的，所以上面一行代码的适合不频繁使用的情况 匹配操作的三个方法首先说明三个方法均返回boolean类型,当匹配到时返回true,没匹配到则返回false Matcher.matches()对整个字符串进行匹配,只有整个字符串都匹配了才返回true 12345Pattern p=Pattern.compile("\\d+"); Matcher m=p.matcher("22bb23"); m.matches();//返回false,因为bb不能被\d+匹配,导致整个字符串匹配未成功. Matcher m2=p.matcher("2223"); m2.matches();//返回true,因为\d+匹配到了整个字符串 Matcher.lookingAt() lookingAt()对前面的字符串进行匹配,只有匹配到的字符串在最前面才返回true 12345Pattern p=Pattern.compile("\\d+"); Matcher m=p.matcher("22bb23"); m.lookingAt();//返回true,因为\d+匹配到了前面的22 Matcher m2=p.matcher("aa2223"); m2.lookingAt();//返回false,因为\d+不能匹配前面的aa Matcher.find() find()对字符串进行匹配,匹配到的字符串可以在任何位置，不再多说 此方法从匹配器区域的开头开始，如果该方法的前一次调用成功了并且从那时开始匹配器没有被重置，则从以前匹配操作没有匹配的第一个字符开始。 获得信息这里同样也有常用的三个方法 start() 返回匹配到的子字符串在字符串中的索引位置. end() 返回匹配到的子字符串的最后一个字符在字符串中的索引位置. group() 返回匹配到的子字符串准确的说是：返回由以前匹配操作所匹配的输入子序列。 1234567Pattern p=Pattern.compile("\\d+"); Matcher m=p.matcher("aaa2223bb"); m.find();//匹配2223m.start();//返回3 m.end();//返回7,返回的是2223后的索引号 m.group();//返回2223 如果没有匹配到的话返回的就是0、字符串长度、原字符串 分组的使用利用上面find的特性可以使用while循环全匹配 12345678910Pattern p=Pattern.compile("\\d+"); Matcher m=p.matcher("我的QQ是:456456 我的电话是:0532214 我的邮箱是:aaa123@aaa.com"); while(m.find()) &#123; System.out.println(m.group()); &#125;// 输出//456456//0532214//123 使用分组的话，类似这样 123456789101112131415Pattern r = Pattern.compile("(\\d+)(test.*)");Matcher m = r.matcher("abc23333test123");if (m.find()) &#123; // 第一个是原字符串哦 System.out.println("Found value: " + m.group(0)); System.out.println("Found value: " + m.group(1)); System.out.println("Found value: " + m.group(2));&#125; else &#123; System.out.println("NO MATCH");&#125;//结果：//Found value: 23333test123//Found value: 23333//Found value: test123 贪婪与非贪婪这里还是再说下吧，贪婪就是尽可能多的匹配，默认就是这种模式，这种也很好理解，不多说重要的是非贪婪匹配，一般带有量词以及?的都是非贪婪吧….就是尽可能少的匹配下面是常见的几种非贪婪匹配模式： *?重复任意次，但尽可能少重复 +? 重复1次或更多次，但尽可能少重复 ?? 重复0次或1次，但尽可能少重复 {n,m}? 重复n到m次，但尽可能少重复 {n,}? 重复n次以上，但尽可能少重复 参考http://www.cnblogs.com/ggjucheng/p/3423731.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>正则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware的三种网络模式]]></title>
    <url>%2F2017%2F03%2F06%2FVMware%E7%9A%84%E4%B8%89%E7%A7%8D%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一般都是默认不动，能上网就行了嘛，不过特殊情况要手动配置网络的时候就完全不知道怎么搞了VM给了三种网络模式，名字都挺高大上的，也看不懂，从网上找了些资料，了解这几种模式的区别VM给的三种模式分别为： bridged (桥接模式) //对应网卡vment0 NAT (网络地址转换模式) //对应网卡vment8 host-only (主机模式) //对应网卡vment1 就网络环境来说，我们常见的家庭网络拓扑结构有下面两种： 主机通过拨号直接连接Internet 主机处于局域网环境中，通过路由器拨号连接Internet 如果你是属于第一种网络环境，由于是ISP分配你的公网IP（假设只有一个地址），则不能使用桥接模式，因为桥接模式需要你拥有属于你机器相同网段内的另一个IP地址。这种情况下可以使用NAT和Host-only。而如果是属于第二种网络环境，则三种模式可以任意选用。 桥接模式桥接模式是三种模式中最简单的一种，VMware安装的时候默认就使用这种配置方式。在这种模式下，虚拟机相当于局域网中的一台独立机器，和主机处于同一个网段，公用同一个网关。桥接模式使用的是虚拟机的VMnet0网卡，一般情况下，在虚拟机中将网络设置成自动获取IP就能直接联网。 在桥接模式下，虚拟机和主机可以互相ping通，虚拟机可以访问Internet，虚拟机上的服务也可以通过虚拟机IP地址在本机直接访问，如果ping不通，那就要检测下本机连接属性里面是否勾选了VMware Bridge Protocol和防火墙设置 NAT模式上面也说了，如果你不在局域网内，只有一个IP，那么NAT模式正适合你。当然如果你在局域网内，NAT模式也未尝不可，不过使用NAT模式后，主机就变成了双网卡：本身的网卡连接Internet或连接拨号的路由器，另一个虚拟网卡VMnet8连接由虚拟机组成的一个虚拟网络。从外部网络来看，无法直接访问这个虚拟网络。虚拟网络则通过本机上的NAT虚拟服务器进行转发访问Internet。主机和虚拟机直接是可以进行互相访问的，如果局域网的其他机器想访问虚拟机的资源，可以配置下VM的NAT端口转发 NAT模式是让虚拟机实现访问Internet最快的方式，几乎不用任何配置，只要主机能上网，那么虚拟机也就肯定能上网。如果又问题，那就检查下VM的服务是否已开启 Host-only模式Host-only模式和NAT一样，也相当于主机双网卡，网络拓扑和NAT也是一样，只是主机不提供NAT功能了，所以虚拟网络只能和主机访问，不能访问Internet。如果需要一个完全隔离的网络环境，则Host-only最合适不过。Host-only相当于使用双绞线直接连接虚拟机和主机，这是最原始的网络结构，当然也是最灵活的。这种情况下虚拟机就不能访问Internet了吗？局域网下的其他机器就不能访问虚拟机上的服务了吗？当然不是。如果我们自己在主机上搭建起我们自己的NAT服务和DHCP服务，那么Host-only其实和NAT是一样的。从下面的示意图也可以看出，Host-only和NAT的唯一区别就在于，主机上少了NAT这个部分。 类似于NAT，具体的配置这里略过。下面通过Windows上的ICS服务（Internet Connection Sharing，就是Internet连接共享）来实现Host-only模式的虚拟机访问Internet。ICS是Windows上的一种共享网络访问的服务，类似于mini版NAT，提供了NAT地址转换和DHCP的功能，但不支持端口转发（Port Forwarding）。首先在网络连接里找到当前正在使用的连接，选择属性 -&gt; 共享，选中“允许其他网络用户通过此计算机的Internet连接来连接”，然后在网络连接下拉框中选择Host-only对应的虚拟网卡（这里是VMnet1），如下图 在确定的时候，可能会弹出对话框提示错误：“Internet连接共享访问被启用时，出现了一个错误（null）”，这时去服务中找到Windows Firewall，启动即可。 ICS配置好之后，Host-only就和NAT一样了，在虚拟机中设置自动获取IP或手工设置IP，保证和VMnet1处于同一个网段内，如果一切顺利，就可以在虚拟机中访问Internet了。 关于桥接的vment0VM0网卡是桥接到本地网卡的，所以在网络适配器里面是看不到的，在VM的网络配置中可以看到，但若本地有多个网卡，问题就出现了。现在笔记本一般有三块网卡，一块无线、一块有线、一块蓝牙，你看的没错，蓝牙也算一块网卡，另外，还可能有微软虚拟的路由器。VMnet0默认是自动桥接到物理网卡（4个都被桥接），所以用以太网通讯时，就找不见真正的有线网卡，若要正常使用，还需要手动指定VMnet0桥接到电脑的有线网卡上。在VM虚拟机的编辑(E)下拉菜单中点击“虚拟网络编辑器”然后选择下面的更改设置可以知道桥接到那块网卡 参考http://www.aneasystone.com/archives/2015/04/three-network-modes-of-vmware-in-action.html#comment-613]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android中SQLite的使用]]></title>
    <url>%2F2017%2F03%2F05%2FAndroid%E4%B8%ADSQLite%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[SQLite是一个软件库，实现了自给自足的、无服务器的、零配置的、事务性的 SQL 数据库引擎。SQLite是一个增长最快的数据库引擎，这是在普及方面的增长，与它的尺寸大小无关。SQLite 源代码不受版权限制。 SQLite是什么SQLite是一个进程内的库，实现了自给自足的、无服务器的、零配置的、事务性的 SQL 数据库引擎。它是一个零配置的数据库，这意味着与其他数据库一样，您不需要在系统中配置。就像其他数据库，SQLite 引擎不是一个独立的进程，可以按应用程序需求进行静态或动态连接。SQLite 直接访问其存储文件。SQLite 是非常小的，是轻量级的，完全配置时小于 400KiB，省略可选功能配置时小于250KiB（但它竟然可以支持高达2TB大小的数据库）与许多其它数据库管理系统不同，SQLite不是一个客户端/服务器结构的数据库引擎，而是被集成在用户程序中。 SQLite采用动态数据类型，当某个值插入到数据库时，SQLite将会检查它的类型，如果该类型与关联的列不匹配，SQLite则会尝试将该值转换成该列的类型，如果不能转换，则该值将作为本身的类型存储，SQLite称这为“弱类型”。但有一个特例，如果是INTEGER PRIMARY KEY，则其他类型不会被转换，会报一个“datatype missmatch”的错误。所以，还是尽量按规定的类型来存 在Android中主要涉及的就是SQLiteDatabase和SQLiteOpenHelper这两个类 SQLiteDatabase这个类提供了一些管理SQLite数据库的方法，比如创建、删除、执行SQL命令，和执行其他常见的数据库管理任务的方法。所以说这个类是比较核心的常用的方法有： db.execSQL(String sql) //执行任何的SQL语句 db.insert(String table,String nullColumnHack,ContentValues values) //插入记录 db.delete(String table,String whereClause,String[] whereArgs)//删除记录 db.update(String table,ContentValues values,String whereClause,String[] whereArgs)//更新记录 db.query(String table,String[] columns,String selection,String[] selectionArgs,String groupBy,String having,String orderBy)//查询记录 db.rawQuery(String sql,String[] selectionArgs)//通过sql语句查询记录 然后看一段在Activity具体使用的代码： 123456789101112131415161718192021222324252627282930313233public class MainActivity extends AppCompatActivity &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); //通过openOrCreateDatabase来打开或创建一个数据库,返回SQLiteDatabase对象 // 第二个参数是权限，关于更详细见 Android安全开发 SQLiteDatabase db = openOrCreateDatabase("user.db",MODE_PRIVATE,null); //创建一个表 db.execSQL("create table if not exists userTb (" + "_id integer primary key," + "name text not null,age integer not null," + "sex text not null)"); //向表中插入记录 db.execSQL("insert into userTb (name,age,sex) values ('张三',18,'女')"); //Cursor为查询结果对象，类似于JDBC中的ResultSet，rawQuery是使用sql语句自己封装 Cursor queryResult = db.rawQuery("select * from userTb", null); // 可以使用占位符：db.rawQuery("select * from ?", new String[]&#123;"userTb"&#125;); if (queryResult != null) &#123; while (queryResult.moveToNext()) &#123; Log.i("info", "id: " + queryResult.getInt(queryResult.getColumnIndex("_id")) + " 姓名: " + queryResult.getString(queryResult.getColumnIndex("name")) + " 年龄: " + queryResult.getInt(queryResult.getColumnIndex("age")) + " 性别: " + queryResult.getString(queryResult.getColumnIndex("sex"))); &#125; //关闭游标对象 queryResult.close(); &#125; //关闭数据库 db.close(); &#125;&#125; db 查询方法得到的 cursor 是指向第一条记录之前的因此查询得到 cursor 后第一次调用 moveToFirst（实际要向后移一个位置）或 moveToNext（从指向第一条记录之前向后移动一个位置变为刚好指向第一条记录位置）都可以将 cursor 移动到第一条记录上。 SQLiteOpenHelper这个类为SQLiteDatabase的帮助类，主要用于管理数据库的创建与版本更新。SQLiteHelper是一个抽象类，一般通过创建一个继承自它的子类并重写onCreat()和onUpgrade()这两个回调方法进行使用。 onCreat(SQLiteDatabase db) 首次创建数据库时调用，一般用于建表等操作。当检测到数据库已经存在后就不再执行了，只能用升级的方法更新 onUpgrade(SQLiteDatabase db,int oldVersion,int newVersion) 当升级数据库版本时调用 1234567891011121314151617181920212223242526public class SQLiteHelper extends SQLiteOpenHelper &#123; /** * 这里以2个参数的构造函数为例，所有构造函数都要调用super的4个参数的构造 * context:上下文对象 * name:数据库名 */ public SQLiteHelper(Context context, String name) &#123; super(context, name, null, 1); &#125; //首次创建数据库的时候调用，一般进行建表或某些初始化的操作 @Override public void onCreate(SQLiteDatabase db) &#123; //建表 db.execSQL("create table if not exists userTb (" + "_id integer primary key," + "name text not null,age integer not null," + "sex text not null)"); &#125; //当数据库版本升级时自动调用 @Override public void onUpgrade(SQLiteDatabase db, int oldVersion, int newVersion) &#123; &#125;&#125; 有时候还会用到一个回调函数：onOpen，就是数据库打开的时候会回调创建好了辅助类就可以在Activity中进行使用了 1234567891011121314151617181920212223242526272829303132public class MainActivity2 extends AppCompatActivity &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main2); //创建一个SQLiteHelper对象 SQLiteHelper helper = new SQLiteHelper(MainActivity2.this,"stu.db"); //使用getWritableDatabase()或getReadableDatabase()方法获得SQLiteDatabase对象 //它们默认都是打开的可读写的数据库，如果没有就新建再打开 //区别就是：当磁盘已满的时候，getReadableDatabase会打开只读的数据库 SQLiteDatabase db = helper.getWritableDatabase(); //插入记录 db.execSQL("insert into userTb (name,age,sex) values ('张三',18,'女')"); db.execSQL("insert into userTb (name,age,sex) values ('李四',19,'男')"); //获取游标对象 Cursor queryResult = db.rawQuery("select * from userTb", null); if (queryResult != null) &#123; //打印所有记录 while (queryResult.moveToNext()) &#123; Log.i("info", "id: " + queryResult.getInt(queryResult.getColumnIndex("_id")) + " 姓名: " + queryResult.getString(queryResult.getColumnIndex("name")) + " 年龄: " + queryResult.getInt(queryResult.getColumnIndex("age")) + " 性别: " + queryResult.getString(queryResult.getColumnIndex("sex"))); &#125; //关闭游标对象 queryResult.close(); &#125; //关闭数据库 db.close(); &#125;&#125; 更多内容待补充… 关于事务写数据库的时候也许会用到事务，SQLite是支持事务操作的，很简单的几句代码 12345678// 事务开始db.beginTransaction();//....一顿写之类的//设置事务处理成功，不设置会自动回滚不提交。db.setTransactionSuccessful();//在setTransactionSuccessful和endTransaction之间不进行任何数据库操作db.endTransaction(); //处理完成 参考http://www.cnblogs.com/caobotao/p/5118463.html]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>SQLite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery-事件]]></title>
    <url>%2F2017%2F03%2F04%2FjQuery-%E4%BA%8B%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[越到后面才越看出jQuery的强大，这些封装确实方便了不少，纯js实现的话真是太可怕啦…虽然有人说jQuery已经跟不上潮流了，但是依然很多很多在用前端更新真是……py2与py32333333理论知识够了，动手能力还很弱鸡 鼠标事件用交互操作中，最简单直接的操作就是点击操作。jQuery提供了两个方法一个是click方法用于监听用户单击操作，另一个方法是dbclick方法用于监听用户双击操作。这两个方法的用法是类似的 12345678910&lt;div id="test"&gt;点击触发&lt;div&gt;$("#test").click(function() &#123; //this指向 div元素&#125;);$("#test").click(11111,function(e) &#123; //this指向 div元素 //e.date =&gt; 11111 传递数据,可选参数&#125;); 注意：在同一元素上同时绑定 click 和 dblclick 事件是不可取的。各个浏览器事件触发的顺序是不同的，一些浏览器在dblclick之前接受两个 click 事件 ，而一些浏览器只接受一个 click 事件 判断鼠标点击其实就是判断鼠标按下、抬起的这个过程，都有相应的方法mousedown和mouseup，他们的用法和click基本一致，其中注意的是：用event 对象的which区别按键，敲击鼠标左键which的值是1，敲击鼠标中键which的值是2，敲击鼠标右键which的值是3PS:键盘也一样有keydown和keyup，还可以用KeyPress来获取输入，不过不能获取特殊按键，会有1个字符的延迟 123$('.target1').keypress(function(e) &#123; $("em").text(e.target.value)&#125;); 还有一个使用率比较高的就是鼠标移动事件，也就是mousemove方法，用法和上面也是一致的，这个不要做太复杂的运算，因为像素点只要变化就会触发 在学JS的时候，有两个方法叫移入移出事件，就是onmouseover()与onmouseout()事件~jQuery当中同样提供了这样的事件来监听用户的移入移出操作，mouseover()与mouseout()事件，两者用法类似，使用方法还是同上 用交互操作中，经常需要知道用户操作鼠标是否有移到元素内部或是元素外部，因此jQuery提供了一个mouseenter和mouseleave的快捷方法可以监听用户移动到内部的操作，感觉和mouseover没啥区别啊，但是不可能同一个事件蛋疼的写出两种调用，区别它们的关键点就是：冒泡的方式处理问题，就是说：mouseenter事件只会在绑定它的元素上被调用，而不会在后代节点上被触发 123&lt;div class="aaron2"&gt; &lt;p&gt;鼠标离开此区域触发mouseleave事件&lt;/p&gt;&lt;/div&gt; 如果p、div元素它们都设置了mouseover事件，p触发后会传递给div，也就是它们的mouseover事件都被触发了，而如果是mouseenter就不会所以为了避免冒泡问题，一般会用mouseenter处理，并且一般我们使用都是成对出现的，所以jQuery还给了一个简便的方法：$(selector).hover(handlerIn, handlerOut) 还有一个事件就是聚焦事件，使用focusin方法，失去焦点就是focusout了，用法和上面一致 表单事件鼠标事件说过focusin事件与focusout事件，同样用于处理表单焦点的事件还有blur与focus事件，它们的本质区别就是：是否支持冒泡，blur与focus是不会冒泡的既然是表单那就应该有输入（改变），也就有相应的事件，就是change了，用法都懂，一般就是传个函数就可以了选择事件select，这个可以传数据进去，和上面鼠标事件是一样的，只是浏览器的默认行为也会调用它 表单提交是个很重要的事件，常常在这进行校验，错误则返回false就是不提交，减轻服务器压力，使用的是submit方法，使用上也差不多，需要注意的只有： 123$("#target").submit(11111,function(data) &#123; //绑定提交表单触发 //data =&gt; 1111 //传递的data数据&#125;); 具体能触发submit事件的行为： &lt;input type=&quot;submit&quot;&gt; &lt;input type=&quot;image&quot;&gt; &lt;button type=&quot;submit&quot;&gt; 当某些表单元素获取焦点时，敲击Enter（回车键） form元素是有默认提交表单的行为，如果通过submit处理的话，需要禁止浏览器的这个默认行为 传统的方式是调用事件对象 e.preventDefault() 来处理， jQuery中可以直接在函数中最后结尾return false即可 事件绑定jQuery on()方法是官方推荐的绑定事件的一个方法。翻开源码其实可以看到，所有的快捷事件在底层的处理都是通过一个”on”方法来实现的。基本用法：.on( events ,[ selector ] ,[ data ] ) 12345678910111213141516171819$("#elem").on('click',function()&#123;&#125;) //on方式// 多个事件绑定同一个函数$("#elem").on("mouseover mouseout",function()&#123; &#125;);// 多个事件绑定不同函数$("#elem").on(&#123; mouseover:function()&#123;&#125;, mouseout:function()&#123;&#125;,&#125;);// 将数据传递到处理程序function greet( event ) &#123; alert( "Hello " + event.data.name ); //Hello 慕课网&#125;// 使用三个参数$( "button" ).on( "click", &#123; name: "慕课网"&#125;, greet ); on的高级用法-委托机制：.on( events ,[ selector ] ,[ data ], handler(eventObject) ) 123456789&lt;div class="left"&gt; &lt;p class="aaron"&gt; &lt;a&gt;目标节点&lt;/a&gt; &lt;/p&gt;&lt;/div&gt;&lt;script&gt; $("div").on("click","p",fn)&lt;/script&gt; 事件绑定在最上层div元素上，当用户触发在a元素上，事件将往上冒泡，一直会冒泡在div元素上。如果提供了第二参数，那么事件在往上冒泡的过程中遇到了选择器匹配的元素，将会触发事件回调函数 既然有绑定也就有卸载，绑定用on卸载就是off 123456789//绑定2个事件$("elem").on("mousedown mouseup",fn)//删除一个事件$("elem").off("mousedown")//删除所有事件$("elem").off("mousedown mouseup")$("elem").off() //或者 事件对象事件中会涉及到很多方法，有的方法参数中出现了Event这个词，它就是事件对象了 事件对象是用来记录一些事件发生时的相关信息的对象。事件对象只有事件发生时才会产生，并且只能是事件处理函数内部访问，在所有事件处理函数运行结束后，事件对象就被销毁 event.target代表当前触发事件的元素，可以通过当前元素对象的一系列属性来判断是不是我们想要的元素，通常用于比较 event.target 和 this 来确定事件是不是由于冒泡而触发的。event.type：获取事件的类型event.pageX 和event.pageY：获取鼠标当前相对于页面的坐标，通过这2个属性，可以确定元素在当前页面的坐标值，坐标系从左上角开始的，不受滚动条的影响event.preventDefault()方法：阻止默认行为，可以用 event.isDefaultPrevented() 来确定这个方法是否(在那个事件对象上)被调用过了event.stopPropagation() 方法：阻止事件冒泡event.which：获取在鼠标单击时，单击的是鼠标的哪个键event.currentTarget : 在事件冒泡过程中的当前DOM元素，相当于this js中事件是会冒泡的，所以this是可以变化的，但event.target不会变化，它永远是直接接受事件的目标DOM元素； .this 和 event.target都是dom对象 如果要使用jquey中的方法可以将他们转换为jquery对象。比如this和$(this)的使用、event.target和$(event.target)的使用； 自定义事件原生事件比如click都是浏览器提供，并且需要和用户交互的，如果使用jQuery可以进行人为的干预，比如调用$(&#39;#elem&#39;).trigger(&#39;click&#39;); 就可以触发绑定在该元素的click事件此外还可以进行自定义： 1234$('#elem').trigger('Aaron',['参数1','参数2'])$('#elem').on('Aaron', function(event,arg1,arg2) &#123; alert("自触自定义事件") &#125;); 自定义事件对象，是jQuery模拟原生实现的，并且可以传递参数trigger事件还有一个特性：会在DOM树上冒泡，所以如果要阻止冒泡就需要在事件处理程序中返回false或调用事件对象中的.stopPropagation() 方法可以使事件停止冒泡如果是自定义的事件对象，那么就有一个不可避免的问题：event无法完美的实现，毕竟不是原生的自定义事件也是通过改造现有的实现的，所以使用时由于冒泡机制，可能会触发其他事件，若要触发通过 jQuery 绑定的事件处理函数，而不触发原生的事件，使用.triggerHandler() 来代替triggerHandler与trigger的用法是一样的，重点看不同之处： triggerHandler不会触发浏览器的默认行为，.triggerHandler( “submit” )将不会调用表单上的.submit() .trigger() 会影响所有与 jQuery 对象相匹配的元素，而 .triggerHandler() 仅影响第一个匹配到的元素 使用 .triggerHandler() 触发的事件，并不会在 DOM 树中向上冒泡。 如果它们不是由目标元素直接触发的，那么它就不会进行任何处理 与普通的方法返回 jQuery 对象(这样就能够使用链式用法)相反，.triggerHandler() 返回最后一个处理的事件的返回值。如果没有触发任何事件，会返回 undefined]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux中使用screen命令]]></title>
    <url>%2F2017%2F03%2F03%2FLinux%E4%B8%AD%E4%BD%BF%E7%94%A8screen%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux下经常使用putty或者secureCRT等通过ssh远程登录服务器，但如果在执行程序的过程中关闭远程终端窗口，则原先运行的程序会被立即杀死。这对于一些花费时间较长的程序非常不利，这时就要用screen了相当于是后台执行吧这个命令其实挺实用的，对于用shell连接服务器来说原文：http://noalgo.info/1000.html 简介screen是一款由GNU计划开发的用于命令行终端切换的自由软件，实现全屏窗口管理的功能，能够混合多个工作到一个终端上。一般linux系统中自带有screen命令，如果没有那就要手动安装下 简单使用通过putty等远程服务器后，直接在命令行下运行以下命令新建一个screen会话： 1screen 也可以指定会话的名称，以下即创建以noalgo为名称的会话： 1screen -S loli 也可以在新建会话时指定要运行的程序，以下运行了vi编辑器，注意此时退出vi编辑器即表示退出了screen会话。 1screen -S loli vi helloworld.c 新建会话后即进入了screen的世界，在这里做的事情和在普通的shell中的事情没有什么区别，只是此时的会话是可以进行恢复的，即使发生网络中断，也可以通过再次运行screen命令回到刚才的会话中，而且，再次回来时屏幕上显示的是刚才的画面，而如果程序动态运行时，此时显示最新的结果。如果有事需要离开，而服务器上的程序需要同时在运行，此时可以通过命令d分离会话。在screen会话中进行的操作都是以ctrl+a开始，所以分离时需要先按下ctrl+a，然后再按d 此时会回到原先的putty窗口，就可以随意关掉putty去干其他事情了。当要回去的时候可以先通过putty进行登录，然后运行以下命令查看系统中已有的screen会话：sscreen -list(ls)得到的结果类似为: 1234[Loli@LoliconServer ~]$ screen -listThere is a screen on: 8530.loli (Detached)1 Socket in /var/run/screen/S-Loli. 然后可以通过screen -r 8530回到会话中，也可以输入名字：screen -r loli另外，通过screen -x命令可以实现会话共享，此时多个用户登录到同一个会话中，如果他们同时处于同一个窗口下时，彼此的操作会同步给每一个用户，即达到共享桌面的效果。 1234# 创建一个名称为“BENET”的共享屏幕会话screen –S BENET# 连接到共享屏幕，在另一个终端上screen -x BENET PS：有时候遇到无法恢复的情况，可以尝试使用 screen -D -r name 来恢复，它的意思就是先踢出原有用户，然后再恢复。 多窗口在普通的shell环境中，如果要同时执行多个程序，可以通过ctrl+z，以及fg和bg等命令交替执行，但screen提供了多窗口的功能同样可以达到这个目的。通过screen命令进入了screen会话默认的一个窗口，通过Ctrl + a + c命令可以新建一个窗口并进入新的窗口，在不同的窗口间切换可以通过下面两个命令进行，分别是进入下一个和前一个窗口： 12Ctrl + a + nCtrl + a + p 使用以下命令可以查看当前共有几个窗口，标注*号的为当前所在的窗口： 1Ctrl + a + w 使用以下命令强行关闭一个窗口，如果当前只剩下最后一个窗口，则终止当前的会话： 1Ctrl + a + k 使用exit命令也可以达到同样的效果，当使用多个窗口时，可以通过将屏幕分割成几个区域来提高效率。使用以下命令进行分屏，分别是水平分割和垂直分割： 12Ctrl + a + SCtrl + a + | 拥有多个屏幕时，使用以下命令进行切换： 1Ctrl + a + Tab 使用以下命令关闭某个分屏， 1Ctrl + a + X 或者关闭处当前区域的所有其他区域： 1Ctrl + a + Q Screen详细参数以上是通过简单的例子介绍screen的常见用法，下面对其参数进行详细介绍。screen的命令语法为： screen [-AmRvx -ls -wipe][-d ][-h &lt;line&gt;][-r ][-s ][-S ] 其中的参数意义如下： -A：将所有的视窗都调整为目前终端机的大小。 -d：分离指定的screen会话。 -h：指定视窗的缓冲区行数。 -m：即使目前已在会话中的screen会话，仍强制建立新的screen会话。 -r：恢复分离的screen会话。 -R：先试图恢复离线的会话。若找不到离线的会话，即建立新的screen会话。 -s：指定建立新视窗时，所要执行的shell。 -S：指定screen会话的名称。 -v：显示版本信息。 -x：恢复之前离线的screen会话。 -ls：显示目前所有的screen会话。 -list：显示目前所有的screen会话。 -wipe：检查目前所有的screen会话，并删除已经无法使用的screen会话。 在每个screen会话中，可以使用的命令如下。注意，screen的命令都是以ctrl+a(C-a)开始的，以下省略C-a而直接以后面的按键替代： ?：Help，显示按键绑定情况。 c：Create，创建新的窗口。 n：Next，切换到下个窗口。 p：Previous，切换到前一个窗口。 M：查看活动状态。 x：锁住当前的窗口，需用用户密码解锁。 d：Detach，暂时离开当前会话，此后可以恢复。 z：把当前会话放到后台执行，可以使用shell的fg命令回去。 w：Windows，列出已创建的窗口。 t：Time，显示当前时间。 K：Kill，强行关闭当前的窗口。 [0..9]：切换到第 0..9个窗口。 [Space]：由窗口0顺序切换到窗口9。 C-a：在两个最近使用的窗口间切换。 S：水平分屏。 |：垂直分屏。 X：关闭当前分屏。 Q：关闭除当前分屏的所有分屏。 [Tab]：在分屏中切换。 [：Copy,进入拷贝模式，此时可以回滚、搜索、复制，就像用使用vi一样。 ]：Paste，粘贴刚刚在拷贝模式选定的内容。 其中在拷贝模式下可以使用的命令包括 C-b：Backward，PageUp。 C-f：Forward，PageDown。 H：High，将光标移至左上角。 L：Low，将光标移至左下角。 0：移到行首。 $：移到行末。 w：forward one word，前移一个字。 b：backward one word，后移一个字。 Space：第一次按标记起点，第二次按标记终点。 Esc：结束copy mode。 这里列的也不是全部的参数，需要更详细的内容，可以直接通过以下命令进行获取：man screen]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux解压缩、VIM、Bash快捷键]]></title>
    <url>%2F2017%2F03%2F02%2FLinux%E8%A7%A3%E5%8E%8B%E7%BC%A9%E3%80%81VIM%E3%80%81Bash%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[好丢人，完了这么久连个解压命令都记不住，只知道个unzip….（捂脸还有一些bash shell的快捷键也是很好用的，VIM成神之路….. 压缩相关命令原文：http://www.cnblogs.com/eoiioe/archive/2008/09/20/1294681.html .tar 打包解包：tar -xvf FileName.tar打包：tar -cvf FileName.tar DirName（注：tar是打包，不是压缩！） .gz解压1：gunzip FileName.gz解压2：gzip -d FileName.gz压缩：gzip FileName .tar.gz 和 .tgz解压：tar -zxvf FileName.tar.gz压缩：tar -zcvf FileName.tar.gz DirName .bz2解压1：bzip2 -d FileName.bz2解压2：bunzip2 FileName.bz2压缩： bzip2 -z FileName .tar.bz2解压：tar -jxvf FileName.tar.bz2压缩：tar -jcvf FileName.tar.bz2 DirName .bz解压1：bzip2 -d FileName.bz解压2：bunzip2 FileName.bz压缩：未知 .tar.bz解压：tar -jxvf FileName.tar.bz压缩：未知 .Z解压：uncompress FileName.Z压缩：compress FileName .tar.Z解压：tar -Zxvf FileName.tar.Z压缩：tar -Zcvf FileName.tar.Z DirName .zip解压：unzip FileName.zip压缩：zip FileName.zip DirName .rar解压：rar x FileName.rar压缩：rar a FileName.rar DirName .rpm和.deb解包：rpm2cpio FileName.rpm | cpio -div解包：ar p FileName.deb data.tar.gz | tar zxf - Bash Shell常用快捷键原文在Github ，点击我跳转 移动光标 ctrl+b: 前移一个字符(backward) ctrl+f: 后移一个字符(forward) alt+b: 前移一个单词 alt+f: 后移一个单词 ctrl+a: 移到行首（a是首字母） ctrl+e: 移到行尾（end） ctrl+x: 行首到当前光标替换 编辑命令 alt+.: 粘帖最后一次命令最后的参数（通常用于mkdir long-long-dir后, cd配合着alt+.） alt+d: 删除当前光标到临近右边单词开始(delete) ctrl+w: 删除当前光标到临近左边单词结束(word) ctrl+h: 删除光标前一个字符（相当于backspace） ctrl+d: 删除光标后一个字符（相当于delete） ctrl+u: 删除光标左边所有 ctrl+k: 删除光标右边所有 ctrl+l: 清屏 ctrl+shift+c: 复制（相当于鼠标左键拖拽） ctrl+shift+v: 粘贴（相当于鼠标中键） 其它 ctrl+n: 下一条命令 ctrl+p: 上一条命令 alt+n: 下一条命令（例如输入ls, 然后按’alt+n’, 就会找到历史记录下的ls命令） alt+p: 上一条命令（跟alt+n相似） shift+PageUp: 向上翻页 shift+PageDown: 向下翻页 ctrl+r: 进入历史查找命令记录， 输入关键字。 多次按返回下一个匹配项 VIM快捷键VIM太神，这远远不够展示它的强大，但这是基础，成神之路原文地址：http://blog.csdn.net/leexide/article/details/17269013 进入输入模式 i：在当前光标所在处前插入文本； I：将光标移动到当前行的行首，并在行首前插入文本； a：在当前光标所在处之后插入文本； A：将光标移动到当前行的行末，并在行末之后插入文本； o：在光标所在行的下面新插入一行，并将光标移动到新行的行首插入文本； O：在光标所在行的上面新插入一行，并将光标移动到新行的行首插入文本； 光标移动 h或者左方向键:将光标向左移动一格； j或者下方向键：将光标向下移动一格； k或者上方向键：将光标向上移动一格； l或者右方向键：将光标向右移动一格； $：移动光标到当前行的行末；数字0：移动光标到当前行的行首； w:移动光标到下个字的开头； e：移动光标到下个字的字尾； b：移动光标回上个字的开头； nl：在当前行中往右移动n个字符，如：2l、34l； crtl+b:屏幕往上翻一页； crtl+f:屏幕往下翻一页； crtl+u:屏幕往上翻半页； crtl+d:屏幕往下翻半页； lG：移动光标到文件的第一行； G：移动光标到文件的最后一行。 gg：移动光标到第一行 文本编辑 r：替换光标所在处的字符； R：替换光标所到之处的字符，直到按ESC键为止； J：把光标所在行的下一行内容接到当前行的行末； x：删除光标所在位置的字符； nx：删除光标所在位置开始的n个字符，如3x删除3个字符； X：删除光标所在位置的前一个字符； nX：删除光标所在位置的前n个字符； dw：删除光标所处位置的单词； ndw：删除由光标所处位置之前的n个单词； db：删除光标所处位置之前的一个单词； ndb：删除光标所处位置之前的n个单词； dd：删除光标所在的行； ndd：删除光标所在行开始的n行； d0：删除由光标所在行的第一个字符到光标所在位置的前一个字符之间的内容； d$：删除由光标所在位置到光标所在行的最后一个字符之间的内容； dlG：删除由文件第一行到光标所在行之间的内容； dG：删除由光标所在行到文件最后一行之间的内容； u：撤销更改的内容； ctrl+u：撤销在输入模式下输入的内容。 复制粘贴 yw：复制光标所在位置到单词末尾之间的字符； nyw：复制光标所在位置之后的n个单词； yy：复制光标所在行； nyy：复制由光标所在行开始的n行； p：将复制的内容粘贴到光标所在位置。 查找与替换 /str：从光标位置开始往文件末尾查找str，按n查找下一个，按N返回上一个； ?str：从光标位置开始往文件开头查找str，按n查找下一个，按N返回上一个； :s/p1/p2/g：将光标所处行中所有p1均用p2替代； :n1,n2s/p1/p2/g：将第n1到n2行中所有p1均用p2替代； :g/p1/s//p2/g：将文件中所有p1均用p2替换 末行模式命令 w：保存当前文件； w!：强制保存； w file：将当前编辑的内容写到文件file中； q：退出vi； q！：不保存文件退出vi； e file：打开并编辑文件file，如果文件不存在则创建一个新文件； r file：把文件file的内容添加到当前编辑的文件中； n：移动光标到第n行； ！command：执行Shell命令command r！command：将命令command的输出结果添加到当前行。 ZZ：保存修改并退出vi 其它 ：set nu指设置行号； ：set nonu指取消行号； ：n指移动光标到第n行； ：n1，n2d指删除指定范围的行； VI的配置文件VI配置文件的位置：~/.vimrc（默认为空）在编辑VI时我们常常需要打开行号的功能，如果要打开VI文件就显示行号，可以在~/.vimrc中加入set nu这句话就可以了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos5/6安装GCC4.8+]]></title>
    <url>%2F2017%2F03%2F02%2FCentos5-6%E5%AE%89%E8%A3%85GCC4-8%2F</url>
    <content type="text"><![CDATA[一般来说Centos5/6自带的GCC已经很低了，一般是4.1/4.4左右，但如果安装一些软件需求高版本的gcc就比较麻烦了。除了下源码编译安装外，一直想找一种YUM源来直接安装的方法，源码编译太慢了…..腾讯云cenos7版本没有32位的只好选个6版本的玩啦…果然Linux中的编译是一件很麻烦的事啊而且还可能出现各种错误 最简单的方案看到的时候说是centos5的方案，但是测试了下对6版本同样适用，几条命令执行一下即可 1234567# 下载源wget https://people.centos.org/tru/devtools-2/devtools-2.repo -O /etc/yum.repos.d/devtools-2.repoyum install devtoolset-2-gcc devtoolset-2-binutils devtoolset-2-gcc-c++ln -s /opt/rh/devtoolset-2/root/usr/bin/* /usr/local/bin/hash -rgcc --version 可以顺便装下libxml2，编译PHP会用到 12yum install libxml2yum install libxml2-devel yum安装全方案GCC 4.8和上面的其实基本一样啦~~ 123456789101112131415wget http://people.centos.org/tru/devtools-2/devtools-2.repo -O /etc/yum.repos.d/devtools-2.repoyum install devtoolset-2-gcc devtoolset-2-binutils devtoolset-2-gcc-c++ -y# 临时编译前使用export CC=/opt/rh/devtoolset-2/root/usr/bin/gccexport CPP=/opt/rh/devtoolset-2/root/usr/bin/cppexport CXX=/opt/rh/devtoolset-2/root/usr/bin/c++# 以下为替换系统GCC，不建议这样操作ln -s /opt/rh/devtoolset-2/root/usr/bin/* /usr/local/bin/hash -rgcc --version GCC 4.9 123456789wget https://copr.fedoraproject.org/coprs/rhscl/devtoolset-3/repo/epel-6/rhscl-devtoolset-3-epel-6.repo -O /etc/yum.repos.d/devtools-3.repoyum install devtoolset-3-gcc devtoolset-3-binutils devtoolset-3-gcc-c++ -y# 临时编译前使用export CC=/opt/rh/devtoolset-3/root/usr/bin/gccexport CPP=/opt/rh/devtoolset-3/root/usr/bin/cppexport CXX=/opt/rh/devtoolset-3/root/usr/bin/c++ GCC 5.2 123456789wget https://copr.fedoraproject.org/coprs/hhorak/devtoolset-4-rebuild-bootstrap/repo/epel-6/hhorak-devtoolset-4-rebuild-bootstrap-epel-6.repo -O /etc/yum.repos.d/devtools-4.repoyum install devtoolset-4-gcc devtoolset-4-binutils devtoolset-4-gcc-c++ -y# 临时编译前使用export CC=/opt/rh/devtoolset-4/root/usr/bin/gccexport CPP=/opt/rh/devtoolset-4/root/usr/bin/cppexport CXX=/opt/rh/devtoolset-4/root/usr/bin/c++ 源码安装试过一次，编译很慢，最后还失败了…..可能是我的姿势不对…先存档，以后有机会再试在编译安装 GCC 之前，系统里必须先要通过 yum 安装老版本的 GCC 和依赖库。如果是在 x86_64 系统下编译的话，还需要安装 libgcc.i686、glibc-devel.i686 才行。编译安装 GCC 内存不小于 1GB，Swap 不小于 1GB，硬盘最低不小于 10GB，否则极有可能会中途报错退出。编译安装完后，目录 gcc-4.8.5 将会有 5GB 之多。最前面的下载解压就不说了，去官网下就行了，然后就是解压后，下载编译所依赖的文件 1234567891011121314151617# 安装前提条件yum install -y gcc texinfo-tex flex zip libgcc.i686 glibc-devel.i686cd gcc-4.8.1./contrib/download_prerequisitescd ..# 新建目录用于存放编译结果：mkdir gcc-build-4.8.1# 进入新目录，并执行configure命令，产生makefile：cd gcc-build-4.8.1../gcc-4.8.1/configure --enable-checking=release --enable-languages=c,c++ --disable-multilib# 编译，j4是四个线程，如果你是四核的话，如果配置低直接make吧...就像我make -j4sudo make install 补充-换yum源我尝试换了阿里的yum源，但是还是4.4的版本，听说163的是4.8+的，这个没测试不过换成国内的源应该还是比较好的，可以选择阿里的 12# 备份mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 下载对应版本repo文件, 放入/etc/yum.repos.d/(操作前请做好相应备份)，以下为下载链接:http://mirrors.163.com/.help/CentOS6-Base-163.repo 然后就是生成缓存、安装了 123yum clean allyum makecacheyum -y install gcc gcc-g++ CentOS 5：wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-5.repoCentOS 6：wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo 参考http://lok.me/a/2045.htmlhttps://www.zhangfangzhou.cn/centos6-devtoolset-gcc.htmlhttps://my.oschina.net/vaero/blog/210485https://teddysun.com/432.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>GCC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery-DOM节点操作]]></title>
    <url>%2F2017%2F03%2F01%2FjQuery-DOM%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[我为什么要看jQuery呢！我也不知道…..强迫症吧，看着学习进度不是100%就难受也许是想放松下，看点简单的jQuery还是很强大的！ 创建节点使用Jquery创建节点变的非常容易，直接书写HTML就可以了 12var div = $("&lt;div class='right'&gt;&lt;div class='aaron'&gt;动态创建DIV元素节点&lt;/div&gt;&lt;/div&gt;")$body.append(div) 插入节点内部插入常用的几种方法：append：这个操作与对指定的元素执行原生的appendChild方法，将它们添加到文档中的情况类似。appendTo：实际上，使用这个方法是颠倒了常规的$(A).append(B)的操作，即不是把B追加到A中，而是把A追加到B中。 append() ：前面是被插入的对象，后面是要在对象内插入的元素内容 appendTo() ：前面是要插入的元素内容，而后面是被插入的对象 在元素内部进行操作的方法，除了在被选元素的结尾（仍然在内部）通过append与appendTo插入指定内容外，相应的还可以在被选元素之前插入，jQuery提供的方法是prepend与prependTo，和上面的两个其实类似，只是是插入到前面 append() ：向每个匹配的元素内部追加内容 prepend() ：向每个匹配的元素内部前置内容 appendTo() ：把所有匹配的元素追加到另一个指定元素的集合中 prependTo() ：把所有匹配的元素前置到另一个指定的元素集合中 123456789101112131415//参数是将要插入的内容。在内部后面追加$(".content").append('&lt;div class="append"&gt;通过append方法添加的元素&lt;/div&gt;')//.appendTo()刚好相反，内容在方法前面，//无论是一个选择器表达式 或创建作为标记上的标记//它都将被插入到目标容器的末尾。$('&lt;div class="appendTo"&gt;通过appendTo方法添加的元素&lt;/div&gt;').appendTo($(".content"))//找到class="aaron1"的div节点//然后通过prepend在内部的首位置添加一个新的p节点$('.aaron1').prepend('&lt;p&gt;prepend增加的p元素&lt;/p&gt;')//找到class="aaron2"的div节点//然后通过prependTo内部的首位置添加一个新的p节点$('&lt;p&gt;prependTo增加的p元素&lt;/p&gt;').prependTo($('.aaron2')) 外部插入的几种方法：before(向前面)与after(向后面)都是用来对相对选中元素外部增加相邻的兄弟节点两个方法都是都可以接收HTML字符串，DOM 元素，元素数组，或者jQuery对象，用来插入到集合中每个匹配元素的前面或者后面，都支持多个参数传递after(div1,div2,....)jQuery由于内容目标的位置不同，然增加了2个新的方法insertAfter与insertBefore.before()和.insertBefore()实现同样的功能。主要的区别是语法——内容和目标的位置。 对于before()选择表达式在函数前面，内容作为参数，而insertBefore()刚好相反，内容在方法前面，它将被放在参数里元素的前面.after()和.insertAfter() 实现同样的功能。主要的不同是语法——特别是（插入）内容和目标的位置。 对于after()选择表达式在函数的前面，参数是将要插入的内容。对于 insertAfter(), 刚好相反，内容在方法前面，它将被放在参数里元素的后面before、after与insertBefore。insertAfter的除了目标与位置的不同外，后面的不支持多参数处理 12345678910111213//在匹配test1元素集合中的每个元素前面插入p元素$(".test1").before('&lt;p style="color:red"&gt;before,在匹配元素之前增加&lt;/p&gt;', '&lt;p style="color:red"&gt;多参数&lt;/p&gt;')//在匹配test1元素集合中的每个元素后面插入p元素$(".test2").after('&lt;p style="color:blue"&gt;after,在匹配元素之后增加&lt;/p&gt;', '&lt;p style="color:blue"&gt;多参数&lt;/p&gt;')//在test1元素前后插入集合中每个匹配的元素//不支持多参数，“多参数” 内容无效$('&lt;p style="color:red"&gt;测试insertBefore方法增加&lt;/p&gt;', '&lt;p style="color:red"&gt;多参数&lt;/p&gt;').insertBefore($(".test1"))//在test2元素前后插入集合中每个匹配的元素//不支持多参数$('&lt;p style="color:red"&gt;测试insertAfter方法增加&lt;/p&gt;').insertAfter($(".test2")) 移除节点使用empty()方法会移除指定元素中的所有子节点，但是调用的节点还是存在的remove()与empty一样，都是移除元素的方法，但是remove会将元素自身移除，同时也会移除元素内部的一切，包括绑定的事件及与该元素相关的jQuery数据 123456$(".test1").remove()//找到所有p元素中，包含了3的元素//这个也是一个过滤器的处理$("p").remove(":contains('3')")//也可以这样：$("p").filter(":contains('3')").remove() 如果我们希望临时删除页面上的节点，但是又不希望节点上的数据与事件丢失，并且能在下一个时间段让这个删除的节点显示到页面，这时候就可以使用detach方法来处理，也就是它只会从显示上移除但是：detach方法是JQuery特有的，所以它只能处理通过JQuery的方法绑定的事件或者数据 123456//通过detach方法删除元素//只是页面不可见，但是这个节点还是保存在内存中//数据与事件都不会丢失p = $("p").detach()//还原操作$("body").append(p); 复制与替换复制也就是克隆，使用.clone()方法深度复制所有匹配的元素集合，包括所有匹配元素、匹配元素的下级元素、文字节点。clone方法比较简单就是克隆节点，但是需要注意，如果节点有事件或者数据之类的其他处理，我们需要通过clone(ture)传递一个布尔值ture用来指定，这样不仅仅只是克隆单纯的节点结构，还要把附带的事件与数据给一并克隆了使用时需要注意的一些细节： clone()方法时，在将它插入到文档之前，我们可以修改克隆后的元素或者元素内容，如可以使用 $(this).clone().css(&#39;color&#39;,&#39;red&#39;)增加了一个颜色 通过传递true，将所有绑定在原始元素上的事件处理函数复制到克隆元素上 clone()方法是jQuery扩展的，只能处理通过jQuery绑定的事件与数据 元素数据（data）内对象和数组不会被复制，将继续被克隆元素和原始元素共享。深复制的所有数据，需要手动复制每一个 至于替换，使用的是replaceWith方法，用$()选择节点A，调用replaceWith方法，传入一个新的内容B（HTML字符串，DOM元素，或者jQuery对象）用来替换选中的节点AreplaceAll()和replaceWith()功能类似，但是目标和源相反 replaceAll()和replaceWith()功能类似，主要是目标和源的位置区别 replaceWith()与replaceAll() 方法会删除与节点相关联的所有数据和事件处理程序 replaceWith()方法，和大部分其他jQuery方法一样，返回jQuery对象，所以可以和其他方法链接使用 replaceWith()方法返回的jQuery对象引用的是替换前的节点，而不是通过replaceWith/replaceAll方法替换后的节点 如果要将元素用其他元素包裹起来，也就是给它增加一个父元素，针对这样的处理，JQuery提供了一个wrap方法比如给p元素增加一个div包裹$(&#39;p&#39;).wrap(&#39;&lt;div&gt;&lt;/div&gt;&#39;)也可以这样用，使用一个函数： 1234// 与上面是一个效果$('p').wrap(function() &#123; return '&lt;div&gt;&lt;/div&gt;';&#125;) jQuery还提供了一个unwarp()方法 ，作用与wrap方法是相反的。将匹配元素集合的父级元素删除，保留自身（和兄弟元素，如果存在）在原来的位置。上面说的是包裹一个元素的，相应的也就有包裹多个的方法：wrapAll( wrappingElement)给集合中匹配的元素增加一个外面包裹HTML结构，不过这里需要注意的是，如果是通过回调的方式可以单独处理每一个元素 123456789101112131415161718192021222324&lt;p&gt;p元素&lt;/p&gt;&lt;p&gt;p元素&lt;/p&gt;&lt;script&gt;$('p').wrapAll('&lt;div&gt;&lt;/div&gt;')&lt;/script&gt;&lt;div&gt; &lt;p&gt;p元素&lt;/p&gt; &lt;p&gt;p元素&lt;/p&gt;&lt;/div&gt;-------------------------------------------------------&lt;script&gt;$('p').wrapAll(function() &#123; return '&lt;div&gt;&lt;div/&gt;'; &#125;)&lt;/script&gt;&lt;div&gt; &lt;p&gt;p元素&lt;/p&gt;&lt;/div&gt;&lt;div&gt; &lt;p&gt;p元素&lt;/p&gt;&lt;/div&gt; 如果要将合集中的元素内部所有的子元素用其他元素包裹起来，并当作指定元素的子元素，针对这样的处理，JQuery提供了一个wrapInner方法，用法和上面的是一样的 遍历jQuery是一个合集对象，如果想快速查找合集里面的第一级子元素，此时可以用children()方法。这里需要注意：.children(selector)方法是返回匹配元素集合中每个元素的所有子元素（仅儿子辈，这里可以理解为就是父亲-儿子的关系）意思就是可以不传入参数返回第一级的子元素，也可以传入表达式进行筛选如果想快速查找DOM树中的这些元素的后代元素，此时可以用find()方法，这也是开发使用频率很高的方法。这里要注意 children与find方法的区别，children是父子关系查找，find是后代关系（包含父子关系）使用find方法需要注意的： find是遍历当前元素集合中每个元素的后代。只要符合，不管是儿子辈，孙子辈都可以。 与其他的树遍历方法不同，选择器表达式对于.find()是必需的参数。如果我们需要实现对所有后代元素的取回，可以传递通配选择器 ‘*’。 find只在后代中遍历，不包括自己。 选择器 context 是由 .find() 方法实现的；因此，$(&#39;.item-ii&#39;).find(&#39;li&#39;) 等价于 $(&#39;li&#39;, &#39;.item-ii&#39;) (找到类名为item-ii的标签下的li标签)。 类似的就有找父亲的方法，parent()这个方法只会向上查找一级，相应的parents()方法就可以查找到所有的祖辈元素$( &quot;html&quot; ).parent()方法返回一个包含document的集合，而$( &quot;html&quot; ).parents()返回一个空集合。 Jquery还提供了一个closest()方法接受一个匹配元素的选择器字符串从元素本身开始，在DOM 树上逐级向上级元素匹配，并返回最先匹配的祖先元素，至于和parents的区别 起始位置不同：closest开始于当前元素 ；parents开始于父元素 遍历的目标不同：closest要找到指定的目标，parents遍历到文档根元素，closest向上查找，直到找到一个匹配的就停止查找，parents一直查找到根元素，并将匹配的元素加入集合 结果不同：closest返回的是包含零个或一个元素的jquery对象，parents返回的是包含零个或一个或多个元素的jquery对象 其他的还有next方法和prev方法，他们应该是一对，一前一后，用法和上面一样，看名字也能知道是干什么的siblings方法是查找兄弟节点，使用add添加节点，它的参数可以几乎接受任何的$()，包括一个jQuery选择器表达式，DOM元素，或HTML片段引用： 123$('li').add('p')$('li').add(document.getElementsByTagName('p')[0])$('li').add('&lt;p&gt;新的p元素&lt;/p&gt;').appendTo(目标位置) .each()方法就是一个for循环的迭代器，它会迭代jQuery对象合集中的每一个DOM元素。每次回调函数执行时，会传递当前循环次数作为参数(从0开始计数) 123456789101112&lt;ul&gt; &lt;li&gt;慕课网&lt;/li&gt; &lt;li&gt;Aaron&lt;/li&gt;&lt;/ul&gt;&lt;script&gt; $("li").each(function(index, element) &#123; // index 索引 0,1 // element是对应的li节点 // this 指向的是li &#125;)&lt;/script&gt; 对象转换jQuery 对象和 JavaScript 的对象是不同的，简单说 jQuery 把 js 原生的对象外面加了一层包装，相当于套了一层数组，然后能使用 jq 中定义的各种事件和方法，他们之间互相独立的，方法不能混用，但是对象之间可以进行转换。js 对象转 jq：$(obj) 直接套一下就 ok；jq 对象转 js：obj[0] 或者 obj.get(0) 毕竟外面套的是数组。为了便于区分，jq 对象一般的命名是以 $ 开头。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP学习笔记(二)]]></title>
    <url>%2F2017%2F02%2F27%2FPHP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[基本的语法与编写两篇总结应该就差不多了，PHP自带各种强大函数基本都封装好了，学起来也不是很难这篇记录 cookie、session以及IO和异常等还有点操作数据库的一些库的使用没整理，框架级开发就先不看了 cookie相关PHP通过setcookie函数进行Cookie的设置，任何从浏览器发回的Cookie，PHP都会自动的将他存储在$_COOKIE的全局变量之中，因此我们可以通过$_COOKIE[&#39;key&#39;]的形式来读取某个Cookie值。 设置cookiePHP设置Cookie最常用的方法就是使用setcookie函数，setcookie具有7个可选参数，我们常用到的为前5个： name（ Cookie名）可以通过$_COOKIE[&#39;name&#39;]进行访问 value（Cookie的值） expire（过期时间）Unix时间戳格式，默认为0，表示浏览器关闭即失效 path（有效路径）如果路径设置为’/‘，则整个网站都有效 domain（有效域）默认整个域名都有效，如果设置了’www.imooc.com’,则只在www子域中有效 因为Cookie是通过HTTP标头进行设置的，所以也可以直接使用header方法进行设置。header(&quot;Set-Cookie:cookie_name=value&quot;);；在values后面可加分号追加有效期删除cookie没有专门的函数，也是为了不让请求过于复杂，可以通过设置有效时间来达到删除目的：setcookie(&#39;test&#39;, &#39;&#39;, time()-1); session相比cookie，session是将用户的会话数据存储在服务端，没有大小限制，不像cookie只能是4kb，通过一个session_id进行用户识别，PHP默认情况下session id是通过cookie来保存的，因此从某种程度上来说，seesion依赖于cookie。但这不是绝对的，session id也可以通过参数来实现，只要能将session id传递到服务端进行识别的机制都可以使用session。 123456789101112131415161718192021&lt;?php//开始使用sessionsession_start();//设置一个session$_SESSION['test'] = time();//显示当前的session_idecho "session_id:".session_id();echo "&lt;br&gt;";//读取session值echo $_SESSION['test'];//销毁一个sessionunset($_SESSION['test']);//删除所有数据，但是session_id仍然存在。session_destroy();echo "&lt;br&gt;";//此函数显示关于一个或多个表达式的结构信息，包括表达式的类型与值。//数组将递归展开值，通过缩进显示其结构。var_dump($_SESSION); session会自动的对要设置的值进行encode与decode，因此session可以支持任意数据类型，包括数据与对象等。默认情况下，session是以文件形式存储在服务器上的，因此当一个页面开启了session之后，会独占这个session文件，这样会导致当前用户的其他并发访问无法执行而等待。可以采用缓存或者数据库的形式存储来解决这个问题. 关于删除，值得注意的是，session_destroy并不会立即的销毁全局变量$_SESSION中的值，只有当下次再访问的时候，$_SESSION才为空，因此如果需要立即销毁$_SESSION，可以使用unset函数。如果需要同时销毁cookie中的session_id，通常在用户退出的时候可能会用到，则还需要显式的调用setcookie方法删除cookie中的session_id值。 文件读取PHP具有丰富的文件操作函数，最简单的读取文件的函数为file_get_contents($src)，当然也可以是url，可以将整个文件全部读取到一个字符串中.一般情况下在对文件进行操作的时候需要先判断文件是否存在，PHP中常用来判断文件存在的函数有两个is_file与file_exists.如果只是判断文件存在，使用file_exists就行，file_exists不仅可以判断文件是否存在，同时也可以判断目录是否存在而is_file是确切的判断给定的路径是否是一个文件。更加精确的可以使用is_readable与is_writeable在文件是否存在的基础上，判断文件是否可读与可写。通过filesize函数可以取得文件的大小，文件大小是以字节数表示的。如果需要转换要自己写函数，并且不支持目录级的计算，要用到递归实现 拓展： fileowner：获得文件的所有者 filectime：获取文件的创建时间 filemtime：获取文件的修改时间 fileatime：获取文件的访问时间 当然在PHP也可以使用文件指针的方式进行读写文件 123456789&lt;?php//die() 函数输出一条消息，并退出当前脚本,该函数是exit()函数的别名。$myfile = fopen("webdictionary.txt", "r") or die("Unable to open file!");// 输出单字符直到 end-of-filewhile(!feof($myfile)) &#123; //从文件中读取单个字符,文件指针也会下移 echo fgetc($myfile);&#125;fclose($myfile); 时间和日期PHP提供了内置函数time() 来取得服务器当前时间的时间戳。date()函数，来取得当前的日期；date(时间戳的格式, 规定时间戳【默认是当前的日期和时间，可选】)返回值是：函数日期和时间比如：date(&quot;Y-m-d&quot;,&#39;1396193923&#39;);结果就是2014-03-30；默认第二个参数是当前的时间戳，也就是按照格式打印当前时间还可以使用strtotime(&#39;2014-04-29 00:00:01&#39;);类似的获取指定日期的时间戳strtotime函数预期接受一个包含美国英语日期格式的字符串并尝试将其解析为 Unix 时间戳。参数是要解析的时间字符串, 当然也可以不填，默认是当前的时间 echo strtotime(&quot;now&quot;);//相当于将英文单词now直接等于现在的日期和时间，并把这个日期时间转化为unix时间戳。这个效果跟echo time();一样。 echo strtotime(&quot;+1 seconds&quot;);//相当于将现在的日期和时间加上了1秒，并把这个日期时间转化为unix时间戳。这个效果跟echo time()+1;一样。 echo strtotime(&quot;+1 day&quot;);//相当于将现在的日期和时间加上了1天。 echo strtotime(&quot;+1 week&quot;);//相当于将现在的日期和时间加上了1周。 echo strtotime(&quot;+1 week 3 days 7 hours 5 seconds&quot;);//相当于将现在的日期和时间加上了1周3天7小时5秒。 gmdate(&#39;Y-m-d H:i:s&#39;, time())函数能格式化一个GMT的日期和时间，返回的是格林威治标准时（GMT），我们是在GMT+8区，所以时间会比现在慢八小时设置时区可以使用date_default_timezone_set(&quot;Asia/Shanghai&quot;);函数完成 图形图像操作GD指的是Graphic Device，PHP的GD库是用来处理图形的扩展库，通过GD库提供的一系列API，可以对图像进行处理或者直接生成新的图片。PHP除了能进行文本处理以外，通过GD库，可以对JPG、PNG、GIF、SWF等图片进行处理。GD库常用在图片加水印，验证码生成等方面。 PHP默认已经集成了GD库，只需要在安装的时候开启就行。 12345678910111213header("content-type: image/png");$img=imagecreatetruecolor(100, 100); //创建一个真彩色的空白图片：$red=imagecolorallocate($img, 0xFF, 0x00, 0x00); //进行分配画笔颜色imageline($img,0,0,100,100,$red); //进行线条的绘制，通过指定起点跟终点来最终得到线条。//imagefill($img, 0, 0, $red); 区域填充，相邻点都会被填充，填充背景//imagesetpixel (resource $image ,int $x ,int $y ,int $color)绘制点imagepng($img); //得到一个图片文件，输出到网页//如果想保存成文件,使用imagejpeg将图片保存成jpeg格式，imagegif将图片保存成gif格式//需要说明的是，imagejpeg会对图片进行压缩，因此还可以设置一个质量参数。//imagejpeg($img, $filename, 80);//imagepng($img, 'img.png');imagedestroy($img); //销毁图片 然后使用imagestring函数来进行文字的绘制，这个函数的参数很多：imagestring ( resource $image , int $font , int $x , int $y , string $s , int $col )，可以通过$font来设置字体的大小，x,y设置文字显示的位置，$s是要绘制的文字,$col是文字的颜色。例子：imagestring($img, 5, 0, 0, &quot;Hello world&quot;, $red); 通过imagecreatefromjpeg可以直接从图片文件创建图像。 1$im = imagecreatefromjpeg($filename); 创建图像对象以后，我们就可以通过前面的GD函数，绘制字符串到图像上。如果要加的水印是一个logo图片，那么就需要再建立一个图像对象，然后通过GD函数imagecopy将logo的图像复制到源图像中。 123456$logo = imagecreatefrompng($filename);//将im图像中坐标从0，0开始，宽度为width，高度为height的一部分拷贝到logo图像中坐标为15和15的位置上。imagecopy($im, $logo, 15, 15, 0, 0, $width, $height);$size = getimagesize('logo.png');//$size[0]是宽度，1是高度，2是格式 当将logo图片复制到原图片上以后，将加水印后的图片输出保存就完成了加水印处理。 异常Exception具有几个基本属性与方法，其中包括： message —- 异常消息内容 code —- 异常代码 file —- 抛出异常的文件名 line —- 抛出异常在该文件的行数 其中常用的方法有： getTrace —- 获取异常追踪信息 getTraceAsString —- 获取异常追踪信息的字符串 getMessage —- 获取出错信息 当然也是可以进行自定义异常 12345678910111213141516171819202122232425&lt;?phpclass MyException extends Exception &#123; function getInfo() &#123; return '自定义错误信息'; &#125;&#125;try &#123; throw new MyException('error');&#125; catch(Exception $e) &#123; echo $e-&gt;getInfo();// echo $e-&gt;getMessage();&#125;//一般处理方案try &#123; throw new Exception('wrong'); //可能出现问题的代码&#125; catch(Exception $ex) &#123; $msg = 'Error:'.$ex-&gt;getMessage()."\n"; $msg.= $ex-&gt;getTraceAsString()."\n"; $msg.= '异常行号：'.$ex-&gt;getLine()."\n"; $msg.= '所在文件：'.$ex-&gt;getFile()."\n"; //将异常信息记录到日志中 file_put_contents('error.log', $msg);&#125;]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP学习笔记(一)]]></title>
    <url>%2F2017%2F02%2F21%2FPHP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[平时还是能时常看到PHP的代码的，然而每次都看不懂，抽几天了解下基本语法，但求看得懂就行….不做深入考虑毕竟是世界上最好的语言，还是要了解一下的2333看了这几门语言现在感觉有点乱了…..额….也差不多全了，我想了解的，Google的Go语言…..再说 基础知识打印是用echo和shell类似~~当然使用print_r也是可以的在php中字符串连接符是用点.来表示的，这一点比较特殊，其它多数语言中是用加号+来表示的php声明变量、调用都需要在前面加$在PHP中，支持8种原始类型，其中包括四种标量类型、两种复合类型和两种特殊类型。PHP会自动把变量转换为自动的数据类型boolean类型不区分大小写，true输出为1，否则什么也没有空为null，不区分大小写逻辑运算有and、or、xor(逻辑异或；有且仅有一个为true的情况)、!、&amp;&amp;、||(and与||相同但是有优先级问题) 可以用 define() 函数来定义常量，在 PHP 5.3.0 以后，可以使用 const 关键字在类定义之外定义常量。一个常量一旦被定义，就不能再改变或者取消定义。 常用系统常量： （1）__FILE__ :php程序文件名。它可以帮助我们获取当前文件在服务器的物理位置。 （2）__LINE__ :PHP程序文件行数。它可以告诉我们，当前代码在第几行。 （3）PHP_VERSION:当前解析器的版本号。它可以告诉我们当前PHP解析器的版本号，我们可以提前知道我们的PHP代码是否可被该PHP解析器解析。 （4）PHP_OS：执行当前PHP版本的操作系统名称。它可以告诉我们服务器所用的操作系统名称，我们可以根据该操作系统优化我们的代码。 获取常量还可以使用constant()函数，接受一个str可以是常量名或者一个变量，返回这个常量的值;使用bool defined(string constants_name)判断是否存在常量 foreach特点，常用的两种形式： 只取值，不要下标数组为例：foreach (数组 as 值){}汉字替换成变量即可 取下标和值数组为例：foreach (数组 as 下标 =&gt; 值){} 可以将@放置在一个PHP表达式之前，该表达式可能产生的任何错误信息都被忽略掉 数组PHP有两种数组：索引数组、关联数组。索引和关联两个词都是针对数组的键而言的。通俗将：索引数组就是一般的我们使用的用下标的方式，关联数组就是用key-value的形式，类似java的map 可以使用$arr = array();来创建一个空数组，也可用$fruit = array(&quot;苹果&quot;,&quot;香蕉&quot;,&quot;菠萝&quot;); 的方式来创建索引数组。可以使用print_r($fruit);打印查看对于数组的赋值，除了常规的方式，PHP还多了一种：用array()创建一个空数组，使用=&gt;符号来分隔键和值，左侧表示键，右侧表示值。当然，索引数组中，键一定是整数。比如:array(&#39;0&#39;=&gt;&#39;苹果&#39;);循环取值除了常规的for循环也可以使用foreach，不过就是有点不习惯….两种数组都适用 12345&lt;?php$fruit=array('苹果','香蕉','菠萝');foreach($fruit as $key=&gt;$value)&#123; echo '&lt;br&gt;第'.$key.'值是：'.$value;&#125; 函数和js类似，使用关键字function来定义函数，PHP中也不允许函数返回多个值，只能是一个这里有个在我看来很神奇的东西–可变函数 12345function name() &#123; echo 'jobs';&#125;$func = 'name';$func(); //调用可变函数 神奇就在于把一个变量赋予一个函数名的字串，竟然就可以通过这个变量调用函数了！至于到底是怎么回事我就不深究了，毕竟只是了解的心态，知道有这样的用法就行了，便于动态调用我倒感觉这样写比较容易理解$func = name; 多好个人认为最强大的还是内置函数，灰常多，灰常强大，常用的有当我们创建了自定义函数，并且了解了可变函数的用法，为了确保程序调用的函数是存在的，经常会先使用function_exists(&#39;name&#39;)判断一下函数是否存在。同样的method_exists(&#39;name&#39;)可以用来检测类的方法是否存在。类是否定义可以使用class_exists;文件是否存在file_exists等 类和对象和其他语言类似，也是有类与对象的定义，同样是用class作为关键字，举个栗子 12345678910111213&lt;?php//定义一个类class Car &#123; var $name = '汽车'; function getName() &#123; return $this-&gt;name; &#125;&#125;//实例化一个car对象$car = new Car();$car-&gt;name = '奥迪A6'; //设置对象的属性值echo $car-&gt;getName(); //调用对象的方法 输出对象的名字 比较让我在意的是：相对java来说，因为PHP中的.相当于java中的+；于是用-&gt;表示java中的.：java：this.name在php写的话就是:$this-&gt;name；嗯我乱说的…..23333 在类的定义的时候，属性、方法是支持使用修饰符的，比如方法默认就是public啊，还有protected、private、static；对于静态属性则使用::双冒号进行访问不要用-&gt;；静态方法中，$this伪变量不允许使用。可以使用self，parent，static在内部调用静态方法与属性。 类属性必须定义为公有、受保护、私有之一。为兼容PHP5以前的版本，如果采用 var 定义，则被视为公有。 方法就是在类中的function，很多时候我们分不清方法与函数有什么差别，在面向过程的程序设计中function叫做函数，在面向对象中function则被称之为方法。 构造函数与析构函数PHP5可以在类中使用__construct()定义一个构造函数，具有构造函数的类，会在每次对象创建的时候调用该函数，因此常用来在对象创建的时候进行一些初始化工作。function __construct(){}和java比的话这里有点区别，在子类中如果定义了__construct则不会调用父类的__construct，如果需要同时调用父类的构造函数，需要使用parent::__construct() 显式的调用。PHP5支持析构函数，使用__destruct()进行定义，析构函数指的是当某个对象的所有引用被删除，或者对象被显式的销毁时会执行的函数。可以使用unset($className);回收对象进行测试，一般不需要手动调用 类似的，如果把构造函数私有化，那就不允许进行初始化了，比如单例模式 继承与重载继承使用关键字extends，方法同可以进行覆盖至于重载，就有些不同了 PHP中的重载指的是动态的创建属性与方法，是通过魔术方法来实现的。 属性的重载通过__set，__get，__isset，__unset来分别实现对不存在属性的赋值、读取、判断属性是否设置、销毁属性 方法的重载通过__call来实现，当调用不存在的方法的时候，将会转为参数调用__call方法，当调用不存在的静态方法时会使用__callStatic重载。 12345678910111213&lt;?phpclass Car &#123; public $speed = 10; //在这里使用重载实现speedDown方法,参数为方法名和调用时传入的参数 public function __call($name,$args)&#123; if ($name == 'speedDown')&#123; $this-&gt;speed -= 10; &#125; &#125;&#125;$car = new Car();$car-&gt;speedDown(); //调用不存在的speedDown方法echo $car-&gt;speed; 其他特性对象复制，在一些特殊情况下，可以通过关键字clone来复制一个对象，这时__clone方法会被调用，通过这个魔术方法来设置属性的值。 123456789// 不需要返回值public function __clone() &#123; $obj = new Car(); $obj-&gt;name = $this-&gt;name; &#125;// -----f g x-------$b = clone $a;if ($a == $b) echo '=='; //trueif ($a === $b) echo '==='; //false 对象序列化，可以通过serialize方法将对象序列化为字符串，用于存储或者传递数据，然后在需要的时候通过unserialize将字符串反序列化成对象进行使用。 字符串除了常规的定义方法，PHP中还可以这样定义：heredoc语法结构定义的字符串： 123$hello = &lt;&lt;&lt;TAGhello worldTAG; 为什么感觉似曾相识，想不起是那种语言了，罢了关于定义字符串单引号和双引号的区别：PHP允许我们在双引号串中直接包含字串变量。而单引号串中的内容总被认为是普通字符。 去除字符串的空格： trim去除一个字符串两端空格。 rtrim是去除一个字符串右部空格，其中的r是right的缩写。 ltrim是去除一个字符串左部空格，其中的l是left的缩写。 计算长度与替换： strlen() 最好用来计算英文字串的长度 mb_strlen() 可以用来计算中文，同时可以传入一个编码格式的参数 替换：str_replace(要查找的字符串, 要替换的字符串, 被搜索的字符串, 替换进行计数[可选]) 字符串截取与查找： 英文：substr(字符串变量,开始截取的位置，截取个数） 中文：mb_substr(字符串变量,开始截取的位置，截取个数, 网页编码） 查找：strpos(要处理的字符串, 要定位的字符串, 定位的起始位置[可选]) 返回的是索引，从0开始 格式化字串，没有数据类型就是好： sprintf(格式, 要转化的字符串) sprintf(‘%01.2f’,$str) %06.2f—-&gt;整个字串至少需要6个字符占位，如果不足就用0填充，小数点后保留2位 合并与分割、转义： 合并：implode(分隔符[可选], 数组) 分割，返回数组：explode(分隔符[可选], 字符串) 转义：addslashes() 返回一个转义后的字符串，如’i’m’–&gt;’i\’m’ 正则PHP中使用PCRE库函数进行正则匹配，preg_match(path,str)用于执行一个正则匹配，常用来判断一类字符模式是否存在。标识一个字串是正则可以用分隔符斜线：&#39;/表达式/&#39;；分隔符可以是非数字、非反斜线、非空格的任意字符。经常使用的分隔符是正斜线(/)、hash符号(#) 以及取反符号(~)分隔符后面可以使用模式修饰符，模式修饰符包括：i, m, s, x等，例如使用i修饰符可以忽略大小写匹配：&#39;//i&#39;可以传入三个参数：preg_match($pattern, $subject, $matches);，匹配结果会写入到matches中，以数组的方式，第一个一般是完整的匹配，第二个就是第一个子组的所有匹配到的不确定字符 (: 大雾\-==&gt;_ 嗯…..上面的方法只能匹配一次，想要匹配多次可以使用preg_match_all方法，用法一样 1234$string = 'April 15, 2014';$pattern = '/(\w+) (\d+), (\d+)/i';$replacement = '$3, $&#123;1&#125; $2';echo preg_replace($pattern, $replacement, $string); //结果为：2014, April 15 其中${1}与$1的写法是等效的，表示第一个匹配的字串，$2代表第二个匹配的。分组技术~~~]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-正则与数据库]]></title>
    <url>%2F2017%2F02%2F19%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%AD%A3%E5%88%99%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[学习路线方面应该差不多就先到这吧，毕竟也开学啦，后面的内容感觉挺深的，对web开发目前没啥兴趣，比较在意的就是多线程了…..开始着重看下爬虫和数据挖掘方面吧Py的语法真的好优雅，一袋能顶两袋撒~~23333 正则-re模块Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\转义，所以要特别注意\\表示\因此强烈建议使用Python的r前缀，就不用考虑转义的问题了,r代表没有转义，还可以跟一个参数如使用re.I表示不区分大小写 1234567891011import repa = re.compile(r'^\d+$')# pa = re.compile(r'^Abcd$',re.I)ma = pa.math(str)# 上面的两句可以合为一句# ma = re.math(r'abc',str)if ma: print(ma)else: print("无匹配") match()方法判断是否匹配(从头开始匹配)，如果匹配成功，返回一个Match对象，否则返回None如果正则表达式中定义了组，通俗说就是使用了小括号，那么就可以在Match对象上用group()方法提取出子串来。需要注意的是group(0)永远是原始字符串 123456789101112131415import repa = re.compile(r'^(\d&#123;2&#125;)-(\d&#123;3,4&#125;)$')ma = pa.match('43-1234')if ma: print(ma.group(0)) print(ma.group(1)) print(ma.groups())else: print("无匹配")# 结果：# 43-1234# 43# ('43', '1234') 贪婪匹配与非贪婪匹配最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0： 12&gt;&gt;&gt; re.match(r'^(\d+)(0*)$', '102300').groups()('102300', '') 由于\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。必须让\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\d+采用非贪婪匹配： 12&gt;&gt;&gt; re.match(r'^(\d+?)(0*)$', '102300').groups()('1023', '00') 非贪婪匹配，尽可能的少匹配 xx*? 匹配零次 因为可以是0次或任意次，*最少那就是0咯 xx+? 匹配一次 +可以是1次或任意次，最少就是1次咯 拓展之split与编译如果我们用传统的split切割是没办法切割多个空格的，因为这是不确定事件，但是如果用正则就很简单了re.split(r&#39;\s+&#39;, &#39;a b c&#39;) ，无论多少个空格都切给你看 编译器处理正则的过程是首先把正则式子进行编译，如果不符合规范会报错，然后再进行比较记得上面说过两种方式，可以直接用math或者拆成2步先compile以下，这里的compile就相当于编译了如果你的正则要复用，还是拆开比较好，效率会高一点 操作MySQLpython3.5以下的使用MySQLdb，官方下载地址：https://www.python.org/dev/peps/pep-0249/，安装无难度python 3.5以上的需要把MySQLdb换成pymysql，使用pip直接安装即可pip install PyMySQL 这里主要使用到了2个对象，connect和cursor；conn相当于在客户端和服务器直接修了一条路，cur是运输的货车，当然它也是有可能出现异常的这个模块默认是关闭自动commit，避免一句sql当作一个事务，所以增删改需要commit哦，目前我的环境是3.5 1234567891011121314151617import pymysqlconn = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='123', db='test', charset='UTF8')# 关闭自动commit# conn.autocommit(False)cur = conn.cursor()try: cur.execute("insert into temp(name) values('测试')") conn.commit() print("执行完毕！")except Exception as e: print(e) # 出现异常回滚操作 conn.rollback()cur.close()conn.close() 然后是关于一些简单的查询 123456789101112131415161718192021222324import pymysqlconn = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='123', db='test', charset='UTF8')cur = conn.cursor()cur.execute("select * from temp")# 返回的数据是元组的元组，可以进行遍历# for i in cur:# print("id:%d 姓名：%s" %i)# 返回多少行(增删改中是影响的行数)print(cur.rowcount)# 返回一行数据，将指针移动一行rs = cur.fetchone()print(rs)# 返回指定的行数 对于当前指针位置rs = cur.fetchmany(3)print(rs)# 返回全部，当前指针位置到最后rs = cur.fetchall()print(rs)cur.close()conn.close() 关于sqliteSQLite是一种嵌入式数据库，它的数据库就是一个文件。由于SQLite本身是C写的，而且体积很小，所以，经常被集成到各种应用程序中，甚至在iOS和Android的App中都可以集成。Python就内置了SQLite3，所以，在Python中使用SQLite，不需要安装任何东西，直接使用。 12345678910111213141516171819# 导入SQLite驱动:&gt;&gt;&gt; import sqlite3# 连接到SQLite数据库# 数据库文件是test.db# 如果文件不存在，会自动在当前目录创建:&gt;&gt;&gt; conn = sqlite3.connect('test.db')&gt;&gt;&gt; cursor = conn.cursor()&gt;&gt;&gt; cursor.execute('create table user (id varchar(20) primary key, name varchar(20))')&lt;sqlite3.Cursor object at 0x10f8aa260&gt;&gt;&gt;&gt; cursor.execute('insert into user (id, name) values (\'1\', \'Michael\')')&lt;sqlite3.Cursor object at 0x10f8aa260&gt;# 通过rowcount获得插入的行数:&gt;&gt;&gt; cursor.rowcount1&gt;&gt;&gt; cursor.close()&gt;&gt;&gt; conn.commit()&gt;&gt;&gt; conn.close() mysql的占位符是%s，sqlite用? (‘select from %s’ %(‘temp’,)) (‘select from ?’,(‘temp’,)) 好吧，我被坑了 关于对象的补充对象、类这一块不可能上次写的那么一点就能概况，这里提下我比较有疑惑的问题：关于继承中的super super(SubClass, self).method() 的意思是，根据self（站在self的角度）去找SubClass的‘父亲’，然后调用这个‘父亲’的method() 角度问题很重要，不像Java，Python是支持多继承的，那么就不可避免有这个问题首先要明确的是：当一个类从多个类继承的时候，按照从左到右的顺序继承。比如class test(A,B)，编译器理解为test的父亲是A，A的父亲是B….这样说可能不太严谨，但是我不知道怎么形容了在test的角度A的父亲就是B，即便A可能还继承了C；当切换到A的角度，A的父亲就是C了理解了这些就能理解super的这种奇怪的用法了 在python3 中做了个简化，如果你在类定义的语句块内写一个不带参数的super()，则相当于写了 super(本类名，self)；因为这样的用法比较多嘛~~ 推荐2篇文章 Python和java中的super python 的 super，一次性整明白]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>正则</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JetBrains系列IDE正确使用姿势]]></title>
    <url>%2F2017%2F02%2F15%2FJetBrains%E7%B3%BB%E5%88%97IDE%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8%E5%A7%BF%E5%8A%BF%2F</url>
    <content type="text"><![CDATA[只有有了一个好的环境才能心情愉悦的码代码~~有那些神奇的插件为啥不用？？最终决定还是在这里发下吧，虽然篇幅确实有点短，当时最先用的是 AndroidStudio，现在因为主力 Java，已经转移到 IDEA 中，但是配置都差不多，JetBrains 系列的 IDE 基本都可以通用。本来是发在了公众号，但是现在我不能在文章中加连接，所以也就做不成目录，搜索功能还搜不到，所以还是在这里再发一下吧 主题修改经常看白色是不是看烦了，换一下感觉也是挺好的，黑色主题是不是会让人感觉很厉害的样子呢？选择菜单栏“File–settings–apperance–theme”，主题选择Darcula：当然系统提供的两种主题可能都不太好看，我们可以进入网站http://color-themes.com/来获取第三方主题，比如说Sublime主题感觉还是不错的呢下载下来之后，是一个jar包，回到Android Studio，选择菜单栏“ File–Import Settings”，将下载好的jar包导入即可。 字体修改选一款合适的字体是很有必要的，对于猿来说，能准确的分清各个符号字母是最关键的，选择菜单栏“File–settings–Editor–Colors&amp;Fonts–Font”：同样也可以修改控制台的字体，就是在console font选项卡中修改完之后发现AS的一些默认字体如侧边栏的工程目录的字体并没有发生变化，如果想改的话 代码自动补全新版本中AS是默认开启的，在：AS默认的代码提示是大小写敏感的，我是喜欢不敏感的，这样设置： 自动导包虽然手动的话也是有快捷键的，但是还是自动导比较爽呢，这样设置： 关闭自动打开工程默认AS启动的时候会自动打开上一次的工程，这个我感觉非常不爽，修改下设置就可以进入欢迎页面了： 修改新建文件文件头每次建新类的话，对下面这段注释肯定很熟悉吧？ 123/*** Created by XXXX on 2015/5/7.*/ 其实是可以设置的，改成我们的个性样式，哈 设置代理基于国内的特殊情况，还是挂个代理比较好，你懂得，直接搜索proxy或者http就行了 插件推荐这里推荐几个，还有更多的神级插件需要自己去学习使用咯~至于安装直接在AS设置中选择plugins搜索即可 CodeGlance可用于快速定位代码，类似于Sublime编辑器右侧定位视图。看着就是舒服 Android ButterKnife Zelezny非常方便的初始化工具，它的功能远远不止这个，很强大 （图片挂了:( GsonFormat根据json数据快速生成相应的java bean，好用！ （图片挂了:( ECTranslation如果你英语也想我一样很烂，这个翻译工具是必不可少的，记得设置下翻译的快捷键，划词翻译~~ FindViewByMe如果你不想用上面的那个初始化工具，写findviewbyid都写吐了吧，可以试试这个小工具，自动根据xml文件生成相应的代码，只需复制，然后到相应位置粘贴即可 ADB Idea通过该插件可以轻松完成以下操作而不用手动输入ADB命令： 卸载应用 杀掉应用进程 启动应用 重启应用 清除应用数据 清楚应用数据并且重启应用 使用方法： Ctrl + Alt + Shift + A快速调出菜单，选择相应的操作，回车执行。 其他可以尝试下阿里的 Java 开发规范插件检查不规范的代码，直接搜 alibaba 就有了。使用 Grep Console 插件可以自定义控制台输出的颜色。iBATIS/MyBatis plugin ：轻松通过快捷键找到MyBatis中对应的Mapper和XML，CTRL+ALT+B；也可尝试 MyBatisCodeHelper。Stack Overflow：控制台的错误可以直接进行在 Stack Overflow 搜索。Background Image Plus：设置代码区的背景图片，面向对象编程Lombok：忘记 getter/setter 方法吧。emacsIDEAs：跳转神器。keyPromoter：记不住快捷键？用它吧。 其他的 MarkdownSupport、Maven Helper 就不多介绍了 修改快捷键有一些快捷键我个人感觉还是改一下比较好，比如代码提示默认是Ctrl+空格，这个快捷键在中文的系统上被输入法给占了，换成Ctrl+,感觉比较好，搜索basic即可找到，remove掉重设即可 还有一个就是Ctrl+D，我们常用来复制一行，但如果选中多行（部分，不是完整选择）复制的话不能完整的将多行复制，这个快捷键默认给的是Duplicate Line or Selection，我们设给Duplicate Entire Line就可以了，会有冲突提示，选Leave即可，因为这个快捷键还有其他功能 修改注释紧随文字我们按注释的快捷键的时候默认会把两条斜线加在一行的开始位置，我认为这样是非常的别扭的，习惯改为紧贴文字在设置中搜索Java，定位到Code Style里，选择最后的Code Generation： 当然也可以加两个空格在前面，就是上面的Add a …选项 杂项显示行数： 使用鼠标进行缩放字体：只需要在设置搜索mouse，相信你已经看到了！ 配置 Git 的忽略文件列表 如果使用 Git，这个一般都要配，避免上传不必要的东西，在项目的目录下创建一个 .gitignore 文件，文件内容模板为 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950*.iml.gradle/local.properties/.idea.DS_Store/build/captures### Android template# Built application files*.apk*.ap_# Files for the ART/Dalvik VM*.dex# Java class files*.class# Generated filesbin/gen/out/# Gradle files.gradle/build/# Local configuration file (sdk path, etc)local.properties# Proguard folder generated by Eclipseproguard/# Log Files*.log# Android Studio Navigation editor temp files.navigation/# Android Studio captures foldercaptures/# Intellij.idea/workspace.xml# Keystore files*.jks 更多待补充…. 参考http://www.cnblogs.com/smyhvae/p/4390905.htmlhttp://www.jianshu.com/p/6f5f818afe4b?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql基础知识储备]]></title>
    <url>%2F2017%2F02%2F13%2FMySql%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%82%A8%E5%A4%87%2F</url>
    <content type="text"><![CDATA[最基本的增删改查SQL命令就略过了，记录下我认为有点难度又容易忘的东西MySQL应该还是挺好用的，对于数据库的知识还是比较匮乏的 数据库引擎的知识没写，需要时再Google吧…. 入门稍微一提，啊哈哈 安装服务：mysqld -install开启服务：net start mysql停止服务：net stop mysql登陆mysql：mysql -u用户名 -p密码 -P端口 -h服务器地址例如：mysql -uroot -p&lt;br&gt;，然后输入密码即可win下cls清屏如果是对于Linux关于启动不太一样，其他倒是类似启动：1、使用 service 启动：service mysqld start2、使用 mysqld 脚本启动：/etc/inint.d/mysqld start3、使用 safe_mysqld 启动：safe_mysqld&amp;停止：1、使用 service 启动：service mysqld stop2、使用 mysqld 脚本启动：/etc/inint.d/mysqld stop3、mysqladmin shutdown重启：1、使用 service 启动：service mysqld restart2、使用 mysqld 脚本启动：/etc/inint.d/mysqld restart默认是3306端口，可用netstat -ano 查看 显示所有数据库：SHOW DATABASES;显示某个数据库的细节：show create database name;打开/进入数据库：USE Name;查看当前数据库：SELECT DATABASE();清空数据表：truncate table tabName;过滤重复的数据（查询结果只显示不同的结果）：select distinct cloName from tabName;查看当前选择的数据库的所有表：SHOW TABLES;查看指定数据库中的所有表：SHOW TABLES FROM TEST;查看数据表结构：SHOW COLUMNS FROM tbl_name查看表结构的另一种：DESC tabName;显示创建表的语句：SHOW CREATE TABLE table_name;查看表是否有索引：SHOW INDEXS FROM table_name;以网格查看表是否有索引：SHOW INDEXS FROM table_name\GSQL语句中表名后面加as name，可以给表起别名以网格查看的话就是 反斜线G 不要加分号为了避免输入和数据库的关键字冲突，可以把值用 [` ] 包起来 下面的是一些我经常忘记的语法，是修改表相关的增加一列：alter table tabName add newName char(2);修改表的列类型：alter table tabName modify columnName varchar(20);删除某列：alter table tabName drop sex;改表名：alter table tabName to newName;改列名：alter table tabName change colum newName; 使用 \s 可以查看系统信息，可以用于查看当前在那个数据库然后再说下备份数据库，其实就是生成数据的 SQL 语句win: mysqldump -uname -p databaseName &gt; file.sql恢复数据库(恢复库中的数据，但是不能恢复库，如果库被删要手动创建)：win：mysql -uroot -p databaseName &lt; file.sqlmysql：Source file.sql; ；当然最好先 use 进入目标数据库 用户管理创建用户：CREATE USER &#39;username&#39;@&#39;host&#39; IDENTIFIED BY &#39;password&#39;;参数解释： username：你将创建的用户名 host：指定该用户在哪个主机上可以登陆，如果是本地用户可用localhost，如果想让该用户可以从任意远程主机登陆，可以使用通配符% password：该用户的登陆密码，密码可以为空，如果为空则该用户可以不需要密码登陆服务器 授权：GRANT privileges ON databasename.tablename TO &#39;username&#39;@&#39;host&#39;显示授权信息可以直接执行 SHOW GRANTS;参数解释： privileges：用户的操作权限，如SELECT，INSERT，UPDATE等，如果要授予所的权限则使用ALL比如：GRANT ALL ON *.* TO &#39;pig&#39;@&#39;%&#39;; databasename：数据库名 tablename：表名，如果要授予该用户对所有数据库和表的相应操作权限则可用*表示，如*.* 对应的，删除授权就是：REVOKE privilege ON databasename.tablename FROM &#39;username&#39;@&#39;host&#39;;删除用户：DROP USER &#39;username&#39;@&#39;host&#39;;然后还有一个就是查看用户，最简单的可以直接查 mysql 这个数据库中的 user 表.引用：https://www.jianshu.com/p/d7b9c468f20d MySQL5.7+ 后 mysql.user 表没有 password 字段改为 authentication_string； 分组和排序前：花括号必须选择，中括号可以忽略，ASC升序，DESC降序标准：[GROUP BY {col_name |position [ASC|DESC],...}关于使用 GROUP 语句，在 select 指定的字段要么就要包含在 Group By 语句的后面，作为分组的依据；要么就要被包含在聚合函数中，如果 Select 后有字段，GROUP 中没有，那么就会报错。having 语句分组，用在group by后面追加条件，判断式中的字段是必须出现在前面select中的 或者是可以包含没有出现在前面查询中的字段的一个聚合函数举个例子：SELECT sex, age FROM users GROUP BY 1 HAVING age &gt; 35;SELECT sex FROM users GROUP BY 1 HAVING count(id) &gt;= 2; 关于排序，标准：[ORDER BY {col_name | expr | position} [ASC|DESC],...]当然是可以按照多个字段排序的，如果第一个就已经达到了要求（不会出现相同的值）那么会忽略后面的，反正则在第一字段的前提下再对相同值按照第二个字段进行排序 可以使用LIMIT进行限制返回的数目，加在 sql 语句的最后，比如返回前两条 LIMIT 2；第2-4条：LIMIT 1,3 子查询与连接子查询是指出现在 其他SQL语句内 的 SELECT 子句子查询指嵌套在 查询内部，且必须始终 出现在圆括号内。子查询可以包含多个关键字或者条件，如DISTINCT,GROUP BY,ORDER BY,LIMIT,函数等子查询的外层查询可以是：SELECT,INSERT,UPDATE,SET或DO子查询可以返回值：标量、一行、一列或者子查询 对于select中的子查询，通过上面的定义基本能猜到大部分的子查询是用在了where判断中，还有一个问题是，如果子查询返回的是多个值怎么办？所以有了下面的3个关键字，含义都在表中了 运算符\关键字 ANY SOME ALL >、&gt;= 最小值 最小值 最大值 &lt;、&lt;= 最大值 最大值 最小值 = 任意值 任意值 &lt;&gt;、!= 任意值 举个例子就是：SELECT * FROM test where age &gt; ANY (select age from test2 where sex=&#39;女&#39;);除了上表所说的，还有两个关键词：IN和NOT IN其实也很简单，可以理解为简写：IN相当于=any；NOT IN 相当于 !=all；可以运用到上面的那个例子上就是：SELECT * FROM test where age IN (select age from test2 where sex=&#39;女&#39;);你也完全可以把IN换成=any；一样的 将查询结果写入数据表看SQL语句就知道了，比如：INSERT test（username） SELECT username FROM users WHERE age &gt;=30;以上就是把users表中的年龄大于30的姓名写入了test表；需要注意的是表名后面的列不能省略 多表更新简单说就是A表参照B表的内容进行更新，比如：update A inner join B on a_name=b_name set a_cate=b_id; 解释：A：想要更改的表名inner join： 内连接B：关联的附表a_name=b_name： 两个表对应列的关系(要修改的列名) = (映射的列名) INNER JOIN,内连接 ​ 在MySQL中，JOIN, CROSS JOIN 和 INNER JOIN 是等价的。 LEFT [OUTER] JOIN ,左外连接 RIGHT [OUTER] JOIN,右外连接 连接上面简单提到了3钟连接，这里进一步解释下使用 ON 关键字来设定连接条件，也可以使用 WHERE 来代替。但通常使用 ON 关键字来设定连接条件；使用 WHERE 关键字进行结果集记录的过滤内连接：返回左表及右表符合连接条件的记录（即两表的交集部分）例子：SELECT * FROM tabA JOIN tabB ON tabA.name = tabB.name;左外连接(LEFT JOIN)：显示左表全部和左右符合连接条件的记录右外连接(RIGHT JOIN)：显示左右符合连接条件的记录和右表全部记录若某字段只存在某一表，则另一表的里字段返回null 当然是可以连接多个表的，直接在后面追加连接即可 还有一种比较特殊的情况，就是”自连接”，其实和上面的几种没多少区别，可以想象成有两张完全相同的表来进行连接，当然这就必须要起别名了，要不然分不清啊…. 多表删除假设一个表中有重复内容，我们利用多表删除去重复，其实是一个表，你可以看做两个表嘛，第二个表就是你用子查询查出来的重复内容的那个表了delete t1 from test as t1 left join(select id,name from test group by name having count(name)&gt;=2) as t2 on t1.name=t2.name where t1.id&gt;t2.id;将test看做t1与子查询所得到的表进行左连接，然后选出id较大的数据，进行删除最简单的是：从数据表t1中把那些id值在数据表t2里有匹配的记录全删除掉：DELETE t1 FROM t1,t2 WHERE t1.id=t2.id我主要是强调t1是不可忽略的…..在多表删除中更多关于多表删除 其他当然还有create….select等语句，可以变得更简洁 级联删除和更新说的是当两个表用外键连接起来后，如果主表被删或者更新，从表应该如何处理MySQL 支持外键的存储引擎只有 InnoDB比如下面这条定义的外键约束，当主表被删或者更新，从表也跟着删除或者更新，换句话说就是：当外键指向的那个表删除或更新，保存外键的那个表的动作 12FOREIGN KEY (`rootid`) REFERENCES roottb(`id`) ON DELETE CASCADE;FOREIGN KEY (`rootid`) REFERENCES roottb(`id`) ON UPDATE CASCADE; 常用的选项是： CASCADE表示父表在更新或者删除时，更新或者删除子表对应记录； SET NULL表示父表在更新或者删除的时候，子表的对应字段被 SET NULL。 RESTRICT 和 NO ACTION 相同是指在子表有关联记录的情况下父表不能更新； 自定义函数以例子来解释： 123456789101112131415161718192021mysql&gt; DELIMITER $$mysql&gt; CREATE FUNCTION hello(num1 VARCHAR(255),num2 VARCHAR(255)) -&gt; RETURNS VARCHAR(255) -&gt; BEGIN -&gt; select count(*) into total from test where num1 like num2; -&gt; RETURN 'Hello world,i am mysql'; -&gt; END $$Query OK, 0 rows affected (0.11 sec)mysql&gt; DELIMITER ;mysql&gt; SELECT hello('abc','a');+-------------------------+| hello() |+-------------------------+| Hello world,i am mysql |+-------------------------+1 row in set (0.00 sec)mysql&gt; DROP FUNCTION hello; DELIMITER 是用来修改分隔符的，因为当我们写的函数体有多条语句的时候要用;分割，但是它正好也是语句的结束标志，所以就会导致还没写完就执行了，所以我么先把它修改成别的，写完后再改回来RETURNS 表明了返回值的类型RETURN 是要返回的值SELECT INTO 的用法就是说将查询出的内容插入到另一个表中，就上面而言，是把查询出的 count 技术保存到了 total 表中当函数体内需要执行的是多条语句时，要使用BEGIN...END语句执行函数使用 SELECT 语句，其实前面其实我们已经用到过了删除函数使用DROP FUNCTION使用SHOW CREATE FUNCTION name;可以查看函数的定义上面我故意用了变量，这里补充下： 1.用户变量：以”@”开始，形式为”@变量名”用户变量跟mysql客户端是绑定的，设置的变量，只对当前用户使用的客户端生效 2.全局变量：定义时，以如下两种形式出现，set GLOBAL 变量名 或者set @@global.变量名对所有客户端生效。只有具有super权限才可以设置全局变量 SELECT @nums; 这样就可以认为是定义了一个变量 也可以这样声明用户自定义变量：set @t1=1; set语句可用于向系统变量或用户变量赋值; 也可使用select语句来定义 对于SET，可以使用=或:=来赋值，对于SELECT只能使用:=来赋值。 存储过程 我们常用的操作数据库语言SQL语句在执行的时候需要要先编译，然后交给存储引擎执行，而存储过程（Stored Procedure）是一组为了完成特定功能的SQL语句集，经编译后存储在数据库中，用户通过指定存储过程的名字并给定参数（如果该存储过程带有参数）来调用执行它。 一个存储过程是一个可编程的函数，它在数据库中创建并保存。它可以有SQL语句和一些特殊的控制结构组成。当希望在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟。它允许控制数据的访问方式。 存储过程通常有以下优点： 存储过程增强了SQL语言的功能和灵活性。存储过程可以用流控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算。 存储过程允许标准组件是编程。存储过程被创建后，可以在程序中被多次调用，而不必重新编写该存储过程的SQL语句。而且数据库专业人员可以随时对存储过程进行修改，对应用程序源代码毫无影响。 存储过程能实现较快的执行速度。如果某一操作包含大量的Transaction-SQL代码或分别被多次执行，那么存储过程要比批处理的执行速度快很多。因为存储过程是预编译的。在首次运行一个存储过程时查询，优化器对其进行分析优化，并且给出最终被存储在系统表中的执行计划。而批处理的Transaction-SQL语句在每次运行时都要进行编译和优化，速度相对要慢一些。 存储过程能过减少网络流量。针对同一个数据库对象的操作（如查询、修改），如果这一操作所涉及的Transaction-SQL语句被组织程存储过程，那么当在客户计算机上调用该存储过程时，网络中传送的只是该调用语句，从而大大增加了网络流量并降低了网络负载。 存储过程可被作为一种安全机制来充分利用。系统管理员通过执行某一存储过程的权限进行限制，能够实现对相应的数据的访问权限的限制，避免了非授权用户对数据的访问，保证了数据的安全。 然后我们可以创建一个最简单的存储过程：CREATE PROCEDURE sp1() SELECT VERSION();这个存储过程就是sp1了，它不带参数，查询版本语句是它的函数体，调用我们用CALL sp_name()，关键字就是CALL啦，带参数的存储过程的调用必须有()，无参数的可以省略下面我们看个比较一般的： 123456789DELIMITER //CREATE PROCEDURE test(IN showID INT UNSIGNED,OUT showName INT UNSIGNED)BEGINDELETE FROM user WHERE id = showID;SELECT count(ID) FROM user INTO showName;END//CALL test(27, @nums);SELECT @nums; 感觉和定义函数还是差不多的，注意到参数中有IN/OUT这些词，它们的作用也差不多可以猜出来 IN表示输入参数；表示该形参的值必须在调用存储过程时指定并传递给存储过程，在存储过程中修改该参数的值不能被返回，为原来值。 OUT表示输出参数； 该值可在存储过程内部被改变，并更新调用外面的变量。 INOUT表示既可以是输入，也可以是输出； INTO就是把结果写入到后面跟的变量啦 事务相关MySQL 是支持事务的，并且支持还非常好，简单说下在 MySQL 中使用事务，主要就是三条命令 开启事务start transaction; 回滚事务Rollback; 提交事务Commit; 还有就是 MySQL 是支持 4 种隔离级别的，顺便说下这四种： Serializable ：可避免上面的全部 （串行化） Repeatable read ：可避免 脏读、不可重复读 （可重复读） [mysql 默认] Read committed ：可避免脏读 （读已提交） [oracle 默认] Read uncommitted : 最低级别，均无法保证 （读未提交） 设置隔离级别（仅当前窗口有效）set transaction isolation level Read uncommitted; 查询当前隔离级别select @@tx_isolation; 别管其他窗口（连接）的隔离级别，你设置的最低你就有所有的问题！ 数据库中的事务 锁表这里指的一般是手动操作，关于表锁行锁，可以移步 这里 进行查阅。 lock tables 命令是为当前线程锁定表。这里有 2 种类型的锁定，一种是读锁定，用命令 lock tables tablename read；另外一种是写锁定，用命令 lock tables tablename write；下边分别介绍 如果一个线程获得在一个表上的 read 锁，那么该线程和所有其他线程只能从表中读数据，不能进行任何写操作（等待）。注意：表必须为 Myisam 表，如果表为 innodb 表，它是事务型的，–single-transaction 是一个更好的选项，因为它不根本需要锁定表。 如果一个线程在一个表上得到一个 WRITE 锁，那么只有拥有这个锁的线程可以从表中读取和写表，其它的线程被阻塞。 mysql 的 表锁 lock tables 感觉就像一个 封闭的空间；mysql 发现 lock tables 命令的时候，会将带有锁标记的表带入封闭空间，直到出现 unlock tables 命令或线程结束，才关闭封闭空间。进入封闭空间时 , 仅仅只有锁标记的表可以在里面使用，其他表无法使用。 原文参考：https://www.cnblogs.com/youxin/p/3584370.html 集群从5.6+的版本开始吧，MySQL 官方就支持集群功能了，为的是解决读库压力过大，使用读写分离也就是说只有一个主库负责写，其他的从库负责分担读的需求，这样的话就会有两种实现，一类是利用应用层的判断来确定是操作那个数据库（Spring 的 AOP 已经很好的支持），一类就是使用中间件，但是目前并没有太好的中间件。 主从复制原理：master 将数据改变记录到二进制日志(binary log)中,也即是配置文件 log-bin 指定的文件(这些记录叫做二进制日志事件，binary log events)；slave 将 master 的 binary log events 拷贝到它的中继日志(relay log)；最后 slave 重做中继日志中的事件,将改变反映它自己的数据(数据重演) 需要注意的问题1.、主DB server和从DB server数据库的版本一致2、主DB server和从DB server数据库数据一致[ 这里就会可以把主的备份在从上还原，也可以直接将主的数据目录拷贝到从的相应数据目录]3、主DB server开启二进制日志,主 DB server 和从 DB server 的 server_id 都必须唯一 下面就具体的操作，首先在主 MySQL 的 ini 配置文件中配置： 123456#开启主从复制，主库的配置log-bin = mysql3306-bin#指定主库serveridserver-id=101#指定同步的数据库，如果不指定则同步全部数据库binlog-do-db=mybatis 可以使用 SHOW MASTER STATUS 来查询状态，记录 Position 值，在从库里会用到并且最好是单独创建一个同步的用户： 12grant replication slave on *.* to &apos;slave01&apos;@&apos;127.0.0.1&apos; identified by &apos;123456&apos;;flush privileges; 从库的 ini 没什么可配置的，如果一台机子有多台 MySQL 除了设置端口不同还要设置 server-id 不同，就是上面设置的那个然后执行下面的 SQL： 12345678910111213CHANGE MASTER TO master_host='127.0.0.1', master_user='slave01', master_password='123456', master_port=3306, master_log_file='mysql3306-bin.000006', master_log_pos=1120;#启动slave同步START SLAVE;#查看同步状态SHOW SLAVE STATUS\G; 当看到两个 yes 就证明是配置成功了。如果是在一台机器做测试，记得 UUID 也不能相同，在 data/data/auto.cnf 目录下 PS：这样的架构只能解决读库压力大的情况，如果是写库压力大可以尝试的方案有： 使用缓存（不推荐，同步问题不好解决） 多表、多库存储（查询效率会拖慢） 使用队列（推荐） 补充插入数据的时候可以一句插入多条：insert test(id) values(&#39;1&#39;),(&#39;2&#39;);，类似这样，注意没有into了哦 所有的聚合函数都会忽略 Null 值，并且使用 count(1) 比使用 count(*) 效率高很多（HQL 就不支持） 有时会见到 SQL 语句中有 comment 之类的关键字，比如：name varchar(50) DEFAULT NULL COMMENT &#39;资源名称&#39;, ；其中的 comment 后面跟的是注释说明 Key 是索引约束，对表中字段进行约束索引的（KEY cid (&#39;cid&#39;)） 索引能够提高 SELECT 查询和 WHERE 子句的速度，但是却降低了包含 UPDATE 语句或 INSERT 语句的数据输入过程的速度。索引的创建与删除不会对表中的数据产生影响。对需要排序的字段或者根据其进行搜索的字段创建索引是比较合适的 更多待补充]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql修改密码与密码重置]]></title>
    <url>%2F2017%2F02%2F13%2FMySql%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%E4%B8%8E%E5%AF%86%E7%A0%81%E9%87%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[忘密码这种事，常有的事~~o(￣▽￣)ゞ))￣▽￣)o不过我至少设置的密码复杂性还可以~以至于我每次都不可能猜对….. 修改密码MySQL 的“root”用户默认状态是没有密码的，目前稍微新一点的版本首次启动都是将密码打印在日志文件中，可以使用 grep &#39;temporary password&#39; /var/log/mysqld.log 查看。 如果您想为 MySQL 中的“root”用户设置密码，请在控制台中使用“mysqladmin”命令。例如： mysqladmin.exe -u root password 123456 另外，如果是先前有密码，则修改命令为： mysqladmin.exe -u root -p password 123456 回车后提示你输入当前密码，确认后会被修改为新密码 Linux 可以尝试在 ls /usr/bin/mysql* 下寻找 重置密码因为我用的是 centOS，所以我使用第一种方法成功重置了，其他的没测试 安全模式重置法基本的思路是，以安全模式启动mysql，这样不需要密码可以直接以root身份登录，然后重设密码。首先，我们停掉MySQL服务：service mysql stop 上面的命令适用于Ubuntu和Debian。CentOS、Fedora和RHEL下使用mysqld替换mysql。 以安全模式启动MySQL：mysqld_safe --skip-grant-tables --skip-networking &amp; 注意我们加了--skip-networking，避免远程无密码登录 MySQL。这样我们就可以直接用root登录，无需密码：mysql -u root接着重设密码： 123mysql&gt; use mysql;mysql&gt; update user set password=PASSWORD("mynewpassword") where User='root';mysql&gt; flush privileges; 注意，命令后需要加分号。重设完毕后，我们退出(quit 或者 exit)，然后启动 MySQL 服务：重启服务：service mysql restart 同样，以上命令适用于Ubuntu和Debian，Centos、Fedora和RHEL需要用mysqld替换mysql。 现在可以尝试用新密码登录了：mysql -u root -pmynewpassword 注意，-p 和密码间不能有空格。 推荐使用mysql -u root -p然后回车再输密码 高版本重置我测试 MySQL8 版本中，mysqld_safe 这个命令已经不存在了，从 stackoverflow 找到了一种可行方案： 1234567891011121314151617181920212223242526272829303132# 1. Stop mysql:systemctl stop mysqld# 2. Set the mySQL environment option systemctl set-environment MYSQLD_OPTS="--skip-grant-tables"# 3. Start mysql usig the options you just setsystemctl start mysqld# 4. Login as rootmysql -u root# 5. Update the root user password with these mysql commandsmysql&gt; UPDATE mysql.user SET authentication_string = PASSWORD('MyNewPassword') -&gt; WHERE User = 'root' AND Host = 'localhost';mysql&gt; FLUSH PRIVILEGES;mysql&gt; quit# As mentioned my shokulei in the comments, for 5.7.6 and later, you should use mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass';# 6. Stop mysqlsystemctl stop mysqld# 7. Unset the mySQL envitroment option so it starts normally next timesystemctl unset-environment MYSQLD_OPTS# 8. Start mysql normally:systemctl start mysqld# 9. Try to login using your new password:mysql -u root -p 我还遇到的其他问题： 密码过于简单？新版加密规则 Nav 不认？外网用户无法连接？ 12345678910# 显示密码验证参数SHOW VARIABLES LIKE 'validate_password%';# 设置密码强度检查等级，0/LOW、1/MEDIUM、2/STRONGSET GLOBAL validate_password.policy = 0;# 允许外网用户连接update user set host='%' where user ='root';# 使用旧版的密码加密规则ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '新密码'; 安装过程参考另一篇。 win下安全模式重置首先也是先停掉服务：net stop mysql然后cd进入mysql的安装目录下的bin目录执行：mysqld.exe --skip-grant-tables然后再打开一个命令行，登陆mysql，这时候密码是空了接着重设密码： 12345mysql&gt; use mysql;mysql&gt; update user set password=PASSWORD("mynewpassword") where User='root';# 在mysql5.7以上可能没有password这个字段了新名字叫 authentication_string；所以是下面的语句# update user set authentication_string=PASSWORD("mynewpassword") where User='root';mysql&gt; flush privileges; 然后进任务管理器结束所有mysql的服务进程，再打开下mysql服务即可 其他方案在Ubuntu和Debian系统上，有一个debian-sys-maint用户，Debian类系统下一些系统脚本对mysql的操作是通过这个用户完成的。所以我们可以通过这个用户来修改 root 密码。该用户的密码可以在/etc/mysql/debian.cnf下找到登陆后执行sudo mysql -u debian-sys-maint -p重置即可]]></content>
      <categories>
        <category>我是修电脑的</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-异常与IO]]></title>
    <url>%2F2017%2F02%2F12%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%BC%82%E5%B8%B8%E4%B8%8EIO%2F</url>
    <content type="text"><![CDATA[感觉已经差不多了，必须要实战下了，当时还是为了爬虫学的，现在发现搞数据挖掘也挺好/火然而….还是等熟练了再说吧最近好浮躁啊啊啊！！ 错误/异常处理有一类错误是完全无法在程序运行过程中预测的，比如写入文件的时候，磁盘满了，写不进去了，或者从网络抓取数据，网络突然断掉了。这类错误也称为异常，在程序中通常是必须处理的，否则，程序会因为各种问题终止并退出。Py使用try...except...finally...的错误处理机制，一个例子： 123456789try: print('try...') r = 10 / 0 print('result:', r)except ZeroDivisionError as e: print('except:', e)finally: print('finally...')print('END') 当我们认为某些代码可能会出错时，就可以用try来运行这段代码，如果执行出错，则后续代码不会继续执行，而是直接跳转至错误处理代码，即except语句块，执行完except后，如果有finally语句块，则执行finally语句块，至此，执行完毕。错误有时不会只有一个，所以可以使用多个except进行处理；所有的错误类型都继承自BaseException这个类继承关系：https://docs.python.org/3/library/exceptions.html#exception-hierarchy如果错误没有被捕获，它就会一直往上抛，最后被Python解释器捕获，打印一个错误信息，然后程序退出。不过报错信息貌似是从下往上看，有点和其他不一样….（调用XX出错，在X行，原因是第X行 :雾）另外一种形式： 12345678910try: passexcept XXXError,e: passexcept XXXError,e: passelse: passfinally: pass 使用try except else 语句，当有异常是会执行except的语句，如果没有异常，则会执行else的语句或者可以进行简写 12with xxx as f: pass xxx其实就是一个类，返回的值存在f中，可以是多个用元组就可以了，比如：open(xx),返回的是file对象，会自动进行关闭操作(无论是否发生了异常)，不需要手动关闭又叫做上下文管理器，执行时调用__enter__方法，返回值存在f；退出时执行__exit__方法(发生异常也执行) 记录错误Python内置的logging模块可以非常容易地记录错误信息，主要代码： 123456import loggingtry: bar('0')except Exception as e: logging.exception(e)print('END') 同样是出错，但程序打印完错误信息后会继续执行，并正常退出。通过配置，logging还可以把错误记录到日志文件里，方便事后排查。 抛出异常raise 主动抛出异常；格式：raise TypeError,&quot;描述&quot;，或者抛一个异常对象： 1234567891011# 自定义异常class FooError(ValueError): passdef foo(s): n = int(s) if n==0: raise FooError('invalid value: %s' % s) # 异常描述 return 10 / nfoo('0') raise语句如果不带参数，就会把当前异常原样抛出。此外，在except中raise一个Error，还可以把一种类型的异常转化成另一种类型 调试assert 断言语句，一个简单的使用例子：assert 0==1 &quot;描述&quot;会抛出一个assert异常，也就是说，如果后面的判断不成立就会抛出异常启动Python解释器时可以用-O参数来关闭assert：$ python3 -O err.py；关闭后，你可以把所有的assert语句当成pass来看。 使用logging和assert比，logging不会抛出错误，而且可以输出到文件；它允许你指定记录信息的级别，有debug，info，warning，error等几个级别，当我们指定level=INFO时，logging.debug就不起作用了。同理，指定level=WARNING后，debug和info就不起作用了。是不是和某Log比较相似… 12import logginglogging.basicConfig(level=logging.INFO) logging的另一个好处是通过简单的配置，一条语句可以同时输出到不同的地方，比如console和文件。 pdb单步调试其实就是单步调试，最好还是使用IDE手动的话就是：$ python3 -m pdb err.py输入命令l来查看代码；输入命令n可以单步执行代码；任何时候都可以输入命令p 变量名来查看变量；输入命令q结束调试，退出程序 文件读写读写文件是最常见的IO操作。Python内置了读写文件的函数，用法和C是兼容的。 读写文件前，我们先必须了解一下，在磁盘上读写文件的功能都是由操作系统提供的，现代操作系统不允许普通的程序直接操作磁盘，所以，读写文件就是请求操作系统打开一个文件对象（通常称为文件描述符），然后，通过操作系统提供的接口从这个文件对象中读取数据（读文件），或者把数据写入这个文件对象（写文件）。 读文件打开一个文件对象就是open(&quot;name&quot;,&quot;role&quot;)，关于权限，常用的有这几种： 权限 描述 r 只读，文件必须存在，否则会抛异常 w 只写，文件不存在时创建文件，存在则清空文件内容 a 追加，文件不存在时创建文件 r+ 打开文件会保持原文件内容不变，同样可以同时对文件进行读写,覆盖追加 w+ 打开文件会将原文件内容删除，可以同时对文件进行读写 a+ 追加和读写方式 rb,wb,ab,rb+,wb+,ab+ 二进制方式打开(读取图片等) 拿到文件对象后就可以进行读取内容了，可以使用read([size])读取内容，也可以使用readline([size]) 读取一行;使用readlines([size]) 读取完缓冲区左右（io.DEFAULT_BUFFER_SIZE），返回每一行组成的列表.最后一步是调用close()方法关闭文件。文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的，但是有可能没有关闭之前出现了异常，所以最好放在try ... finally里可以直接使用for in来读取file对象的每行的数据 file-like Object像open()函数返回的这种有个read()方法的对象，在Python中统称为file-like Object。除了file外，还可以是内存的字节流，网络流，自定义流等等。file-like Object不要求从特定类继承，只要写个read()方法就行。 StringIO就是在内存中创建的file-like Object，常用作临时缓冲。 字符编码要读取非UTF-8编码的文本文件，需要给open()函数传入encoding参数，例如:open(&#39;./gbk.txt&#39;, &#39;r&#39;, encoding=&#39;gbk&#39;)；如果遇到非法编码的字符，open()函数还接收一个errors参数，表示如果遇到编码错误后如何处理。最简单的方式是直接忽略:open(&#39;./gbk.txt&#39;, &#39;r&#39;, encoding=&#39;gbk&#39;, errors=&#39;ignore&#39;) 写入到文件还是用的open这个函数，但是权限不同了，上面的表已经写的很明白了你可以反复调用write()来写入文件，但是务必要调用f.close()来关闭文件。当我们写文件时，操作系统往往不会立刻把数据写入磁盘，而是放到内存缓存起来，空闲的时候再慢慢写入。只有调用close()方法时，操作系统才保证把没有写入的数据全部写入磁盘。忘记调用close()的后果是数据可能只写了一部分到磁盘，剩下的丢失了。当然也可以使用flush进行刷新；所以，还是用with语句来得保险： 12with open('/Users/michael/test.txt', 'w') as f: f.write('Hello, world!') 要写入特定编码的文本文件，请给open()函数传入encoding参数，将字符串自动转换成指定编码。 使用with语句操作文件IO是个好习惯。 文件指针seek()函数接受2个参数：1.偏移量 2.相对偏移位置 file.tell() 返回文件的偏移os.SEEK_SET：相对文件起始位置os.SEEK_CUR：当前位置os.SEEK_END：末尾位置 例子：f.seek(0,os.SEEK_SET)：使文件指针回到开始位置；f.seek(-5,os.SEEK_CUR)：从后向前移动5个字节f.seek(0,os.SEEK_END)：将文件指针移到最后 文件属性 file.fileno() 文件描述符 .mode 文件打开权限 .closed 是否正确关闭 .enconding 编码方式 文件标准输入：sys.stdin； raw_input()该函数是从命令行接受输入，回车终止，可以传一个提示参数str 文件标准输出：sys.stdout； 文件标准错误：sys.stderr; sys模块提供sys.argv属性,通过该属性可以得到命令行参数; sys.argv:字符串组成的列表，第一个参数多为文件名 codecs模块支持open指定编码 StringIO和BytesIO很多时候，数据读写不一定是文件，也可以在内存中读写。 StringIOStringIO顾名思义就是在内存中读写str。下面是个例子： 12345678910&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO()&gt;&gt;&gt; f.write('hello')5&gt;&gt;&gt; f.write(' ')1&gt;&gt;&gt; f.write('world!')6&gt;&gt;&gt; print(f.getvalue())hello world! getvalue()方法用于获得写入后的str。要读取StringIO，可以用一个str初始化StringIO，然后，像读文件一样读取： 1234567891011&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO('Hello!\nHi!\nGoodbye!')&gt;&gt;&gt; while True:... s = f.readline()... if s == '':... break... print(s.strip())...Hello!Hi!Goodbye! BytesIOStringIO操作的只能是str，如果要操作二进制数据，就需要使用BytesIO。 123456&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO()&gt;&gt;&gt; f.write('中文'.encode('utf-8'))6&gt;&gt;&gt; print(f.getvalue())b'\xe4\xb8\xad\xe6\x96\x87' 请注意，写入的不是str，而是经过UTF-8编码的bytes。和StringIO类似，可以用一个bytes初始化BytesIO，然后，像读文件一样读取. OS模块我们也是可以使用OS模块打开文件的（更贴近底层以及为了跨平台），有更多的方法，先把常用的方法列一下 os.open(filename, flag [,mode]):打开文件 flag打开方式： os.O_CREAT:创建文件 os.O_RDONLY:只读方式打开 os.O_WRONLY:只写方式打开 os.O_RDWR:读写方式打开 os.read(fd, buffersize):读取文件 os.write(fd, string):写入文件 os.lseek(fd, pos, how): 文件指针操作 os.close(fd):关闭文件 os.access(path, mode)： F_OK, R_OK ,W_OK, X_OK 判断是否有权限 os.listdir(path)： 返回当path路径下所有文件名组成的列表 os.remove(path)：删除文件 os.rename(old, new)：修改文件或者目录名 os.mkdir(path[, mode])：创建目录 os.makedirs(path[, mode])：创建多级目录 os.removedirs(path)：删除多级目录 os.rmdir(path)：删除目录(目录必须空目录) os.path.exists(path)：当前路径是否存在 | 也可以判断是否有该文件 os.path.isdir(s)：是否是一个目录 os.path.isfile(path)：是否是一个文件 os.path.getsize(filename)：返回文件大小 | 返回目录文件大小 os.path.dirname(p)：返回路径的目录 os.path.basename(p)：返回路径的文件名 os.path.split()：拆分出前面的路径和最好一级目录或文件 os.path.splitext()：拆分文件后缀名 os.name()：系统类型 posix/nt os.uname()：查看系统信息 os.environ：环境变量（可以指定key比如：os.environ.get(‘PATH’)） 序列化Python提供了pickle模块来实现序列化。使用前注意Py的版本 12345678&gt;&gt;&gt; import pickle&gt;&gt;&gt; d = dict(name='Bob', age=20, score=88)&gt;&gt;&gt; pickle.dumps(d)b'\x80\x03&#125;q\x00(X\x03\x00\x00\x00ageq\x01K\x14X\x05\x00\x00\x00scoreq\x02KXX\x04\x00\x00\x00nameq\x03X\x03\x00\x00\x00Bobq\x04u.'&gt;&gt;&gt; f = open('dump.txt', 'wb')&gt;&gt;&gt; pickle.dump(d, f)&gt;&gt;&gt; f.close() pickle.dumps()方法把任意对象序列化成一个bytes，然后，就可以把这个bytes写入文件。或者用另一个方法pickle.dump()直接把对象序列化后写入一个file-like Object.当我们要把对象从磁盘读到内存时，可以先把内容读到一个bytes，然后用pickle.loads()方法反序列化出对象，也可以直接用pickle.load()方法从一个file-like Object中直接反序列化出对象。 为了方便在不同语言间传递，我们一般序列化为XML或者JSON格式，JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面中读取，非常方便。 JSON表示的对象就是标准的JavaScript语言的对象，JSON和Python内置的数据类型对应如下： JSON类型 Python类型 {} dict [] list “string” str 1234.56 int或float true/false True/False null None Python内置的json模块提供了非常完善的Python对象到JSON格式的转换。 参考关于with详细解释http://www.liaoxuefeng.com]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片藏文件的实现及原理]]></title>
    <url>%2F2017%2F02%2F07%2F%E5%9B%BE%E7%89%87%E8%97%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[无意中看到了这个，虽然很早之前就已经用烂了，但是我也只是略懂怎么回事而已，今天就好好深入挖掘下吧也许…老司机发车必备吧…. 尾部追加法这是最简单的一种方法，当然藏个小文本、种子之类的还是挺好的，不要想藏个病毒了，稍微有点电脑基础就能分辨出来； 先来说个例子：现在有两个文件，1.jpg和2.zip，现在要把2.zip文件藏在1.jpg这张图里，并且要可以查看，只需要进入cmd进入到当前目录执行：copy/b 1.jpg+2.zip 3.jpg这样就会得到3.jpg这个图片文件，可以正常查看，大小是两个文件的之和，如果用压缩软件打开这个文件(改个后缀也可以，用rar直接打开也可以)就会看到2.zip里的内容。至此，老司机就可以去发车了。最好使用“最大压缩”的选项。 部分测试结果： 把ZIP和RAR文件添加到JPG/BMP/GIF/PNG，没有问题，全部可以实现。 把TXT添加到图像格式里，用记事本打开，在乱码的最后能隐约看到原来文件的样子，英文都能看到，中文全是乱码。 把ZIP/RAR添加到PDF里，也能通过。 把 ZIP/RAR添加到MP3/WMA/MID里，添加完后的音频不受影响，但ZIP/RAR打不开了，没有意义。 别的格式没有多试。 下面说原理 copy/b的作用是把两个文件首尾接起来，串个串。一般情况下，这样会破坏2个文件，造成无法读取，其实，能不能读全看程序怎么处理文件和文件本身的格式。有人说是因为图像格式的文件大小定义在头部，而RAR格式的文件大小定义在尾部，所以2个文件共存才没有冲突。不知道对不对，google一下，查了BMP的文件格式，BMP文件简单来说分4个部分，文件头+图像头+颜色表+数据区，具体格式在这里可以看到。文件头部分有bfSize是定义整个文件大小，就是从文件头开始到数据区结束的总大小，也就是说，超出这个偏移的数据对文件来说没有意义，读图的程序也不会去理会，程序只读取bfSize里定义的那么多数据，别的一概不管。而RAR格式呢，由于是私有格式，我只能查到一点点信息，这里解释了一部分格式的定义，由此我的理解是RAR文件内部以区块为单位，数据以区块存储，区块数量和大小不定，但必须包含几个特定区块，用来保存基本信息，所有数据分段的保存在大量区块中，类似IP数据包，而每个区块有独立的大小定义以及类似链表的关联定义，每个区块的大小都可知且独立。这样推测WinRAR读取文件的过程是先查找，再验证，再读取，也就是说找到区块，读取大小和类型信息后把整个区块的数据读出来，而区块外，区块间的数据对WinRAR来说无意义。这样就好解释之前的方法了，图像程序对于头部定义的大小之外的数据不管，WinRAR对于区块外的数据，也就是图像的数据不管，2个互相不管，各读各的，当然可以共存。推而广之，2种格式类似以上情况的都可以共存。 内容覆盖法通常来说，图片文件都有包含2部分：文件头和数据区。而“内容覆盖法”，就是把要隐藏的文件，直接覆盖到图片文件的数据区的尾部。比方说，某图片有 100KB，其中文件头占 1KB，那么，数据区就是 99KB。也就是说，最多只能隐藏 99KB 的文件。切记：覆盖的时候，千万不可破坏文件头。文件头一旦破坏，这个图片文件就不再是一个合法的图片文件了。使用这种方法，对图片文件的格式，是有讲究的——最好用24位色的 BMP 格式。为啥捏？一来，BMP 格式本身比较简单，数据区随便覆盖，问题不大；二来，24位色的 BMP 相对其它的格式 BMP，文件尺寸更大，可以隐藏更多内容。 处理步骤这种方法当然是不能和上面那样一句命令就能完成的，需要一些专用工具，或者可以写个Py脚本，很简单，有点编程基础的就可以看懂.如下代码没有严格计算 BMP 的文件头尺寸，俺只是大致预留了 1024 字节，感觉应该够了。 12345678910111213141516171819202122import sysdef embed(container_file, data_file, output_file) : container = open(container_file, "rb").read() data = open(data_file, "rb").read() if len(data)+1024 &gt;= len(container) : print("Not enough space to save " + data_file) else : f = open(output_file, "wb") f.write(container[ : len(container)-len(data)]) f.write(data) f.close()if "__main__" == __name__ : try : if len(sys.argv) == 4 : embed(sys.argv[1], sys.argv[2], sys.argv[3]) else : print("Usage:\n%s container data output" % sys.argv[0]) except Exception as err : print(err) 提取方法如果是藏的压缩文件，直接用rar打开即可，文件大小还不会发生变化哦不过由于隐藏的文件覆盖了数据区，因此，图片在显示的时候，会有一块区域变成灰蒙蒙的（如果遭遇“肉眼审查”，可能会引起怀疑） 隐写术这是一种比较高级的方法了，搞信息安全的可能比较熟此方法会涉及较深奥的技术领域，本人实在能力有限。通俗地说：如果把图片的某个像素的颜色，进行微小的调整，肉眼是看不出来的；因此，专门的软件，利用某些高深的算法，就可以在变化的像素中隐藏信息。有兴趣的同学，可以看“这里”的介绍； 处理方法这种方法一般也只能用专门的软件了；使用这种方法，你需要用专门的工具来进行信息的隐藏和提取。在进行隐藏时，你除了指定图片文件和被隐藏的文件，还需要设置一个密码。隐写工具会把你的隐藏文件先加密，然后再进行隐写；提取的时候，需要用同一款隐写工具进行提取，并输入同样的密码，才能提取出来。假如图片文件落入攻击者手中，他必须同时知道2个信息（你用哪款隐写工具，你隐写时设置的密码），才有可能破解出隐含的信息。因此，安全性相当高。 名称 类型 界面 Silent Eye 开源 图形界面 Steg Hide 开源 命令行界面 Ultima Steganography 商业 图形界面 这种方法虽然隐蔽性和安全性都很高，但是只能隐藏较少的信息（此方法能隐藏的信息量，和图片面积有关，和图片格式无关。比如一张 1600*1200 尺寸的，无论哪种格式，大约只能隐藏 几KB 的数据） 参考http://blog.fulin.org/2006/11/simple_hide_file/https://program-think.blogspot.com/2011/06/use-image-hide-information.htmlhttps://zh.wikipedia.org/wiki/%E9%9A%90%E5%86%99%E6%9C%AF]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-面向对象]]></title>
    <url>%2F2017%2F01%2F22%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[wiki上说Py的启发语言有C/C++、java等，果然语言都是想通的，定义形式上差不多~~不过后面的一些特性确实厉害，不过….就是看不太懂 = =,要达到熟练使用的地步…(摊手)我感觉出了：将Py比作是坦克很形象！ 类和实例在Python中，定义类也是通过class关键字，类名通常是大写开头的单词，经典的student例子： 12class Student(object): pass 括号中的object是表示该类是从哪个类继承下来的，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。然后就是创建实例，Py中不需要new关键字： 12345678910&gt;&gt;&gt; bart = Student()&gt;&gt;&gt; bart&lt;__main__.Student object at 0x10a67a590&gt;&gt;&gt;&gt; Student&lt;class '__main__.Student'&gt;# 绑定属性&gt;&gt;&gt; bart.name = 'Bart Simpson'&gt;&gt;&gt; bart.name'Bart Simpson' 还可以自定义初始化： 123456class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print('%s: %s' % (self.name, self.score)) 注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。有了__init__方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去，总的来说挺想构造函数的注意：相同名称的实例属性将屏蔽掉类属性 访问限制在Python中，变量名类似__xxx__的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是private变量 有些时候，你会看到以一个下划线开头的实例变量名，比如_name，这样的实例变量外部是可以访问的，但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。 双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问__name是因为Python解释器对外把__name变量改成了_Student__name，所以，仍然可以通过xxx._Student__name来访问__name变量，但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把__name改成不同的变量名。 如果你确实使用了bart.__name = xxx这样的语句，确实不会报错，它真的会给bart增加一个__name 属性，但它不是内部的name变量，内部的name变量名叫_xxx__name 动态语言和静态语言中的多态对于静态语言（例如Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用run()方法。对于Python这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个可以调用的方法就可以了 动态语言的“鸭子类型”，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。 进阶-slots相关给实例绑定属性很简单，直接点就行了，还可以给实例绑定方法，当然这些都是在这一个实例中有效而已 12345678&gt;&gt;&gt; def set_age(self, age): # 定义一个函数作为实例方法... self.age = age...&gt;&gt;&gt; from types import MethodType&gt;&gt;&gt; s.set_age = MethodType(set_age, s) # 给实例绑定一个方法&gt;&gt;&gt; s.set_age(25) # 调用实例方法&gt;&gt;&gt; s.age # 测试结果25 但是，如果我们想要限制实例的属性怎么办？比如，只允许对Student实例添加name和age属性。为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的__slots__变量，来限制该class实例能添加的属性： 12class Student(object): __slots__ = ('name', 'age') # 用tuple定义允许绑定的属性名称 使用__slots__要注意，__slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的 进阶-使用@property在绑定属性时，如果我们直接把属性暴露出去，虽然写起来很简单，但是，没办法检查参数，如果使用get/set的方法实现感觉又太麻烦，记得前面学过装饰器，Python内置的@property装饰器就是负责把一个方法变成属性@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值 1234567891011121314151617class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError('score must be an integer!') # 抛出异常 if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = value &gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.score = 60 # OK，实际转化为s.set_score(60)&gt;&gt;&gt; s.score # OK，实际转化为s.get_score()60 进阶-多继承与定制类对，是的，Py是支持多继承的，用逗号分开即可，多继承的这种设计通常称之为MixIn。我们知道以双下划线开头的都是特殊的，比如 __str__()返回用户看到的字符串，__repr__()返回程序开发者看到的字符串 如果一个类想被用于for ... in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的__next__()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。 如果要像list那样按照下标取出元素，就需要实现__getitem__()方法 当调用不存在的属性时(也就是找不到的时候)，Python解释器会试图调用__getattr__(self, &#39;XXX&#39;)来尝试获得属性，这个方法也是可以返回函数的，不过调用的时候要加()执行 任何类，只需要定义一个__call__()方法，就可以直接对实例进行调用。(name())通过callable()函数，我们就可以判断一个对象是否是“可调用”对象。 进阶-枚举类与元类貌似3.5版本以后才支持呢，定义一个枚举类类似： 12from enum import EnumMonth = Enum('Month', ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，上面的定义等价于： 12345class Month(Enum): Jan = 1 Feb = 2 Mar = 3 Apr = 4 当然还有些高级用法，我没怎么看，因为感觉智商有点不够用 Py作为解释性语言，在执行的时候才会编译，所以动态生成类就有了可能，type()函数可以查看一个类型或变量的类型，同时也可以产生新的类，比如： 1234&gt;&gt;&gt; def fn(self, name='world'): # 先定义函数... print('Hello, %s.' % name)...&gt;&gt;&gt; Hello = type('Hello', (object,), dict(hello=fn)) # 创建Hello class 要创建一个class对象，type()函数依次传入3个参数： class的名称； 继承的父类集合，注意Python支持多重继承，如果只有一个父类，别忘了tuple的单元素写法； class的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。 通过type()函数创建的类和直接写class是完全一样的，因为Python解释器遇到class定义时，仅仅是扫描一下class定义的语法，然后调用type()函数创建出class。 除了使用type()动态创建类以外，要控制类的创建行为，还可以使用metaclass。 metaclass，直译为元类，简单的解释就是： 当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。 但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。 连接起来就是：先定义metaclass，就可以创建类，最后创建实例。 所以，metaclass允许你创建类或者修改类。换句话说，你可以把类看成是metaclass创建出来的“实例”。 metaclass是Python面向对象里最难理解，也是最难使用的魔术代码。正常情况下，你不会碰到需要使用metaclass的情况，所以，以下内容看不懂也没关系，因为基本上你不会用到。 http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014319106919344c4ef8b1e04c48778bb45796e0335839000]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记(二)]]></title>
    <url>%2F2017%2F01%2F13%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[上一个实在是太多了，然而这篇就有点少….还剩下一丢丢看完马上进入shell编程啦~~ 进程管理进程管理主要就是查看和杀死吧，哈哈….检测系统是否健康很重要 查看进程-ps查看进程用的ps命令，一般使用ps aux或者ps -le可以看到所有进程信息，前一种是没有-的，当然就算加上也不影响执行，会有行报错，原因是沿用了以前的BSD系列ps列出来的是当时一瞬间的进程列表，不会时时刷新，然后是说下几个标识： VSZ：使用的虚拟内存 RSS：使用的物理内存 TTY：在那个终端机运作，显示？的一般是系统启动的，不是由终端启动的，ty1-tty6 是本机上面的登入者程序，若为 pts/0 等等的就是远程登陆了 STAT：进程的状态 START：进程启动的日期 TIME：进程使用CPU的时间 然后再详细说下状态这一栏，它是用英文字母作为标志，常用的各个的表示意思为： R：正在运行 D：不可中断 S：睡眠状态，可被某些信号唤醒 T：该程序目前正在侦测或者是停止了 Z：僵尸进程，该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 X：死掉的进程（一般不会看到） +：位于后台的进程组 l：多线程 s：包含子进程 &lt;：高优先级 N：低优先级 L：有些页被锁进内存 了解了上面的这些基本就能看懂了。 然后可以使用pstree来查看进程的树结构，加个-p可以展开来看，就是会比较多。 另外补充一个命令：lsof（list open files），这是一个列出当前系统打开文件的工具，在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件；感觉这个会比较有用！例如你可以查看占用指定端口的程序：lsof -i:8080 查看进程-toptop命令是个好东西，默认3秒刷新一次，当然是可以自己设定的，但是这个工具比较吃资源，所以3秒就可以了.top命令前面会列出系统的状态信息，主要就是看这些啦第一行：当前时间，系统已运行时间，登陆用户数，以及1分钟、5分钟、15分钟的负载情况。需要注意的是，系统负载最好不要超过你CPU的核心数，这当然也是个经验值，还要视具体情况第二行：进程的一些信息，都看得懂第三行：CPU的相关状态，us就是用户占用的资源，sy就是系统占用的资源，关键是看id空闲百分比，一般这个不要低于20%第四行：内存的一些信息，总的、使用的、空闲的、缓存的第五行：交换分区的信息，同内存差不多，最后一个是缓冲的 第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。 纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 关于缓存和缓冲区，其实都是在内存，缓存简单说就是把那些频繁从硬盘读的数据先存在内存中，以加快读取的速度；而缓冲相反，把需要往硬盘写的数据先存起来，最后一起写入硬盘，这样避免在硬盘频繁读写的时候加重其的负载所以，如果真的要算可用内存，应该是free的加上缓冲和缓存区的大小 还有，top命令执行后是处于交换状态的，比如按q可以退出，c是显示完整的命令，M根据内存排序，P根据cpu进行排序，T就是根据时间排序 杀死进程杀进程可用3个命令：kill / killall / pkillkill -l可以查看它的信号，比如9是强制结束，1是平滑重启（不会中断用户的连接），15是默认的正常结束举个例子，比如平滑重启某个进程：kill -HUP id或者kill -1 id都可以至于killall是杀死一类进程，按照进程名，kill只能杀死单个进程pkill可以按照终端号进行“踢人”，pkill -9 -t xx 关于优先级其实一般也用不到，了解下NI一般就是指优先级 范围是：-20到19 普通用户只能0-19配合上面的进程管理命令，有个公式：PRI(最终值) = PRI(原始值) + NI至于设置优先级，可以使用nice -n [-5] cmd 但是不能修改已经存在的进程如果非要修改存在的进程，那么使用renice [-10] id不过优先级无论怎么设，我们一般是感觉不出来的，CPU的速度太快了…. 后台管理查看后台列表：jobs [-l]把进程放入后台的方式： 命令后面加&amp;，这种方式会在后台执行 命令执行后按ctrl + z，这种方式放入后台会暂停 需要说下的是，只要是要交互的比如vi、top无论你用那种方式放入后台，都是暂停状态 恢复(工作号在jobs查看)：fg (%)工作号 恢复到前台执行，%可省略bg (%) 工作号 后台暂停状态恢复到后台执行如果工作号省略那就是执行最后一个加入的，带有+标识的，-是倒数第二个这种方式后台会随终端的关闭而关闭，如果想不让他关闭那就需要加个nohup命令，类似：nohup cmd &amp; 系统资源查看其实用top足够了，不过下面的几个资源占用可能小点vmstat 1 3 每隔1秒刷新，共3次硬件状态：dmesg查看内存情况：free -m查看CPU信息：cat /proc/cpuinfo以及top的精简版？：uptime查看内核：uname -a可通过查看系统命令来判断系统是多少位的：file cmd文件被谁调用：lsof | more [-c：进程被谁调用；-u被那个用户调用] 搜索命令find命令find是最常见和最强大的查找命令，你可以用它找到任何你想找的文件。也就是会遍历系统文件，如果是很大的范围比如/那就非常的消耗系统资源了，是完全匹配，常用的通配符有*、？、[] $ find &lt;指定目录&gt; &lt;指定条件&gt; &lt;指定动作&gt; &lt;指定目录&gt;： 所要搜索的目录及其所有子目录。默认为当前目录。 &lt;指定条件&gt;： 所要搜索的文件的特征。 &lt;指定动作&gt;： 对搜索结果进行特定的处理。 举个栗子 123456789101112131415161718# 搜索当前目录中，所有文件名以my开头的文件，并显示它们的详细信息。$ find . -name 'my*' -ls# 搜索当前目录中，所有过去10分钟中更新过的普通文件。如果不加-type f参数，则搜索普通文件+特殊文件+目录。# atime 文件访问时间# ctime 改变文件属性# mtime 修改文件内容$ find . -type f -mmin -10# 查找没有所有者的文件$ find /root -nouser# 按照所有者去搜索$ find /root -user root# 当前目录下,大于25k的文件 M 默认单位是扇区$ find . -size +25k# -a 表示and ；-o 表示or ，搜索大于25k小于10m的文件# -exec 和 &#123;&#125;\; 是固定写法，前面的搜出来的交给ls -lh执行，显示详细信息$ find . -size +25k -a -size -10M -exec ls -lh &#123;&#125; \; locate命令locate命令其实是find -name的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库（/var/lib/locatedb），这个数据库中含有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库。比如：$ locate -i ~/m 就是搜索用户主目录下，所有以m开头的文件，并且忽略大小写 whereiswhereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。如：$ whereis grep whichwhich命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。如：$ which grep grep文本搜索Linux 系统中 grep 命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep 全称是 Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。grep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。grep 可用于 shell 脚本，因为 grep 通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回2。我们利用这些返回值就可进行一些自动化的文本处理工作。 前面已经说过它可以用于管道符过滤，类似这样：cat test.txt | grep &quot;lock&quot;还有一个很爽的作用是从多个文件中查找关键词：grep -rn &quot;hello,world!&quot; *下面就说一下参数的作用： * ： 表示当前目录所有文件，也可以是某个文件名 -r： 是递归查找 -n：是显示行号，在显示符合样式的那一行之前，标示出该行的列数编号 -R：查找所有文件包含子目录 -i：忽略大小写 -s：不显示错误信息 -c ：计算找到 ‘搜寻字符串’ 的次数（列数） -v：显示不包含匹配文本的所有行 -l：列出文件内容符合指定的样式的文件名称 -L：列出文件内容不符合指定的样式的文件名称 -F：将样式视为固定字符串的列表 -G：将样式视为普通的表示法来使用 文件上传一般来说，linux 服务器大多是通过 ssh 客户端来进行远程的登陆和管理的，使用 ssh 登陆 linux 主机以后，如何能够快速的和本地机器进行文件的交互呢，也就是上传和下载文件到服务器和本地。使用之前记得安装 lrzsz 包：yum -y install lrzsz常用的命令有两个： sz：将选定的文件发送（send）到本地机器 rz：运行该命令会弹出一个文件选择窗口，从本地选择文件上传到服务器(receive) rz，sz 是便是 Linux/Unix 同 Windows 进行 ZModem 文件传输的命令行工具(不支持文件夹)，当然需要客户端支持，Xshell 和 SecureCRT 都是支持的。注意：单独用 rz 会有两个问题：上传中断、上传文件变化（md5不同），解决办法是上传是用 rz -be，并且去掉弹出的对话框中“Upload files as ASCII”前的勾选。 详情参考：http://blog.csdn.net/k346k346/article/details/71515740 其他补充个命令w，可以查看当前连接的终端，配合pkill使用很爽 /bin/sync 把内存的数据向硬盘转移 设置环境变量：编辑 /etc/profile 输入 G 跳转到最后一行，加上环境变量就行了最后使用 source /etc/profile 让其生效 通过 id name 可以查看用户的基本信息，组 id 之类的]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记(一)]]></title>
    <url>%2F2017%2F01%2F11%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[主要侧重下日常常用的命令，然后是基本的使用，并不涉及完整的功能，用到高级功能还是要去搜索啊….有些命令是redhat系列专有的…写的有点混乱 用户管理增加一个用户：useradd 用户名设置当前用户的密码（后面可跟用户名设置指定用户密码）：passwd 密码删除用户（加-r参数 删除用户目录）：userdel 用户创建用户组：groupadd 组名修改用户名：usermod -l 新用户名 旧用户名修改用户的组：usermod -g 要加入的组名 用户名添加附属组：gpasswd -a userName groupName锁定用户（ -u 解锁账户 -d 无密码登陆）：passwd -l 用户名切换用户，root可省略用户名：su userName 显示当前登陆用户名：whoami显示用户信息：id name显示组信息：groups name 用户的密码都加密存在/etc/passwd,格式类似：root:x:0:0:root:/root:/bin/bash[用户名]:密码:UID:GID:身份描述:主目录:登录shell 网络操作查看网络状态 ：netstat [-tuln 监听 -an全部]登出：logout查看路由：netstat -rnDNS查询：nslookup端口探测（远程登陆不安全）：telnet ip 端口路由跟踪：traceroute addrs 下载：wget 下载文件URL抓包：tcpdump -i eth0 -nnX port 21 （[nn：ip端口显示；X：16进制拆分数据包；port：只查看21端口的） 启动ftp服务：service vsftpd start 下载文件(远程机用户名@IP:路径 本地路径)scp [-r] name@ip:src locaSrc上传文件scp [-r] file name@ip:src setup命令可以配置一些常见配置，如ip，图形化界面，redhead系列特有手动配置IP看这里 包管理已安装的软件都在/var/lib/rpm/中的数据库中安装命令：rpm -ivh 包全名 (参数：安装、详细信息、显示进度)其他参数：-U 升级跟包全名-e 卸载-q 查询包信息-qa 全部的已安装包-qi 查询详细信息-qip 未安装包-ql 安装的文件-qf 文件属于那个包-V 校验 上面的是手动安装，碰到依赖很多的包是很烦人的，所以就有了yum：yum listyum search 关键字yum -y install nameyum -y update [xxname]yum -y remove nameyum clean (清缓存) 源码包一般放在 /usr/local/src/name安装在 /usr/local/name 权限管理u-用户 g-组 o-其他权限（读写执行）对应：r=4 、w=2(不包括删除) 、x=1权限是对下一层的描述，文件的权限就是对文件中数据的描述，目录权限是对文件的描述常用：最高权限：777，一般文件：644，可执行文件：755目录最高权限：w 文件最高权限：x目录有效权限：0、5、7命令：chmod [-R]chmod u+x,g-x,o=rw xxx [a=all] 修改所有者：chown user file修改用户组 ：chgrp group file chown命令也可以：chown user:group 除了用:，, 、 .都可以 umask 用来查看默认权限 第一个是特殊权限（写在后面）文件默认权限是 666 目录是 777；在/etc/profile中定义 PS：配合下面的 sudo 权限，使用：chmod u+s xxName 命令可以让某个可执行文件在运行时拥有 root 权限，在运行完毕后撤销 root 权限 ACL权限用来解决用户身份不够的问题，可以针对单一用户或组来设置特定的权限，递归时会导致权限溢出，尽量少用查看分区ACL权限是否开启(挂载了ACL)：dumpe2fs -h /dev/sda显示：Default mount options: user_xattr acl即为可用 dumpe2fs命令是查询指定分区详细文件系统信息的命令 -h：仅显示超级块中信息，而不显示磁盘块组的详细信息 临时开启分区ACL权限：重新挂载根分区，并挂在加入ACL权限：mount -o remount,acl /永久开启ACL权限：修改文件/etc/fstab在默认文件系统后加上acl即可 UUID=24f28fc6-717e-4bcd-a5f7-32b959024e26 / ext4 defaults,acl 0 1 重新挂载：mount -o remount /查看(带+的是有ACL权限的)：getfacl fileName设定：setfacal -m u:name:rx [-R] file [m:设定，u:给用户设定，g:给组设定]添加默认ACL权限：setfacal -m d:u:name:rx [-R] filemask 最大有效权限，设定的权限和它做逻辑与运算得到的结果是实际权限修改mask：-m m:rx fileName-x 删除-b 删除所有 sudo权限root把只能超级用户执行的权限赋予普通用户执行sudo的操作对象是系统命令设置sudo，权限：visudo实际修改的是 /etc/sudoers 格式：root ALL=(ALL) ALLroot：指定个那个用户赋予sudo，前面加%就是给用户组设置第一个ALL：被管理的电脑IP(可以在那台电脑运行sudo命令)第二个ALL：可切换的身份，ALL就是可切换为任意用户，可省略第三个ALL：可以执行的命令，绝对路径，如：/sbin/；写的越详细越安全，参数也限制，可使用通配符!之类，多个条件以,分隔sudo -l 查看可以执行的sudo命令 文件特殊权限不要自己去设置，系统会用到 SUID/SetUID 必须是可执行文件 拥有X权限SUID权限是第一个，代号s，数字表示4，其他操作和其他权限(基本权限rwx)类似执行此程序会变成文件的所有者 SGID/SetGID 对于文件，要求和SUID一样 权限显示S是无限的权限，有效权限是小s=S+x 执行文件时，组身份变成文件的所属组 赋予：g+s或者2755 对于目录，必须拥有r和x，在目录中创建文件所属组都是上级目录的组，而不是登陆用户的组 SBIT/sticky BIT 只能是目录，其他人需要有最高权限 赋予：o+t 或者1755，普通用户只能删自己的文件 chattr(不可改变位权限) chattr +i (insert) 被锁定，不能任何修改(对于目录就是不能新建和删除)。root也不可以 chattr +a (append) 不能删，只能增加。增加只能用输出重定向 &gt;&gt; 不能用vi 查看权限：lsattr -a 查看所有文件和目录 -d 查看目录权限 计划任务crontab服务是否启动 service crond status查看当前用户任务列表： crontab -l服务大部分后面会加个d，如果没有安装，安装命令：yum install vixie-cronyum install crontabs添加任务（当前用户）：crontab -e [-e -u：指定给用户添加]/[-l -u：查看某个用户任务]系统级计划任务在/etc/crontab；用户的配置文件在/var/spool/cron/root或者/var/spool/cron/tabs/root执行crontab -e实际上是修改/var/spool/cron/root下面对应当前账号的文件。系统服务crond会每分钟从配置文件刷新定时任务、执行它还存在白名单和黑名单(都是对用户来说)：crontab .allow 白名单（优先，默认不存在）crontab .deny 黑名单（默认存在） 格式：* * * * * cmd分钟 小时 日期 月份 星期 命令0-59 0-23 1-31 1-12 0-7 *就是每分钟、小时、、、多个使用逗号分割 ;范围使用-*/2 每隔 2 分钟；1-59/2 从 1 开始每隔 2 分钟(奇数)* 就是所有时间都匹配 除了使用crontab -e进行添加，还可以将脚本复制到/etc/cron.{daily.weekly...}中的任意一个文件夹下，分别每天、周、月执行【推荐】这种方式其实是使用的anacron，它会检测/etc/cron.{daily.weekly...}和/etc/crontab中的文件是否在系统关机的过程中错过的定时任务,通过命令添加的是不会进行检测的。 /var/spool/anacron/con.{daily.weekly…}内存着最后一次crontab的时间，如果当前时间到记录的时间差大于指定的差值，证明有命令漏执行，就会被anacron进行随机延迟执行，配置文件在/etc/anacrontab可以设置延迟的最大时间，所以这种方式有个缺点就是不知道什么时候执行 所有定时任务的日志保存在/var/log/cron 定时任务at总体和crontab差不多，只不过它是只执行一次的，首先也是看服务有没有运行，服务名是atd同样存在白名单和黑名单：at.allow 白名单（优先，默认不存在）at.deny 黑名单 （默认存在） at now +5 minutes然后输入执行的命令就行了，ctrl + d 退出atq 查询at -c id 查看具体内容atrm 删除 服务管理runlevel 是系统运行级别init X 是设置系统级别 查看系统自启服务 RPM包服务 (cent6)：chkconfig --list CentOS7之后 ，chkconfig命令已经被systemctl命令取代 其他命令：ps aux/netstat -tuln -an既然能查看就能修改：chkconfig (--level 2345) httpd on/off括号内的可省略添加自启动另一种方法：修改/etc/rc.d/rc.local文件【推荐】以及图形化界面ntsysv，只能修改5等级 启动服务 server xx start/stop/status/restart(/etc/init.d/xxx start) 对于源码包，启动服务的server (redhat系列)、chkconfig都是没效果的，想要支持server命令需要创建一个软连接：ln -s xxx /etc/init.d/，也就是说server读取的就是/etc/init.d/目录下的文件(实际目录是/etc/rc.d/init.d/)想要被 chkconfig 识别：修改 init.d 文件中的软连接的脚本，增加： 第一行定义运行级别、启动顺序、关闭顺序，顺序号只要不冲突即可 第二行是描述信息 # chkconfig:345 86 76 # description:source package apache 然后执行chkconfig --add xxx即可PS:/etc/rc3.d/下面的文件是运行级别3时 系统开启与关闭分别要执行的服务文件，启动顺序与关闭顺序不能和现有的冲突，k开头的是关闭，s开头是开启 centos系列可以使用图形化界面管理，命令setup；然后再补充下系统运行级别： 0 停机，关机 1 单用户，无网络连接，不运行守护进程，不允许非超级用户登录 2 多用户，无网络连接，不运行守护进程 3 多用户，正常启动系统 4 用户自定义 5 多用户，带图形界面 6 重启 一些常用操作修改计算机名查看计算机名：hostname至于修改关键是/etc/hosts文件和/etc/sysconfig/network文件然后重启reboot或shutdown -r now(-c取消) 设置默认语言临时设置：export LANG=&quot;zh_CN.UTF-8&quot;永久生效：修改/etc/sysconfig/i18n英文是：en_US.UTF-8 设置提示忽略大小写编辑~/.inputrc（没有的话，就新建一个），在最后加一行:set completion-ignore-case on 管道符;连接多个命令，无逻辑 如果报错也还是会继续执行下个命令1&amp;&amp;2 1正确执行才会执行21||2 1如果不正确执行2；如果1正确2不会执行1|2 1的执行结果是2的参数 目录解释 / 根目录 /bin 命令保存目录（普通用户就可以读取的命令） /boot 启动目录，启动相关文件 /dev 设备文件保存目录 /etc 配置文件保存目录 /home 普通用户的家目录 /lib 系统库保存目录 /mnt 系统挂载目录 /media 挂载目录 /root 超级用户的家目录 /tmp 临时目录 /sbin 命令保存目录（超级用户才能使用的目录） /proc 直接写入内存 /sys /usr 系统软件资源目录 /usr/bin/ 系统命令（普通用户 /usr/sbin/ 系统命令（超级用户） /var 系统相关文档内容 配置文件一般都在 etc下用户名 杂项shell连接：ssh name@ip创建文件：touch创建文件夹：mkdir统计字符：wc [-c 字节 ；-w 单词； -l 行]分屏显示：more查询字符串：grep 可配合管道符|筛选使用[-i：忽略大小写；-w：搜索整个词汇]，如$grep -i hAL /etc/passwd不断显示文件的最后几行：tail -f fileName ；比如：tail -3 temp是查看最后三行查看文件内容：cat查看硬盘信息：df创建一个软连接：ln -s [原文件] [目标文件]]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-函数式编程]]></title>
    <url>%2F2017%2F01%2F01%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[还没有到IO啊…..什么时候才能写的出爬虫….不过还是希望能够比较系统的进行学习….继续吧，这次是函数式编程 简介函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。 函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！Python对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。函数式编程也可以归结到面向过程的程序设计。 函数是Python内建支持的一种封装，我们通过把大段代码拆成函数，通过一层一层的函数调用，就可以把复杂任务分解成简单的任务，这种分解可以称之为面向过程的程序设计。函数就是面向过程的程序设计的基本单元。 高阶函数什么是高阶函数呢，比如举几个栗子： 变量可以指向函数这点类似js，比如abs()这个函数，abs是函数本身，加括号就是调用 123&gt;&gt;&gt; f = abs&gt;&gt;&gt; f(-10)10 函数名也是变量 还是用abs这个栗子，abs就是一个变量，你甚至可以给他赋值，他只是保存了计算绝对值这个函数的引用，也就是说他指向具体实现的地方 注：由于abs函数实际上是定义在import builtins模块中的 传入函数 既然变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。 返回函数 有时候我们并不需要立即执行函数，可以传入需要的参数然后让其返回一个函数，需要执行的时候再执行，后面会有详细介绍 12def add(x, y, f): return f(x) + f(y) f应该传入一个函数，比如abs() map和reducePython内建了map()和reduce()函数。先来看第一个map函数：它接收两个参数，一个是函数，一个是Iterable(可迭代对象)，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator(迭代器)返回。 123# 将list转换成字符串&gt;&gt;&gt; list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9]))['1', '2', '3', '4', '5', '6', '7', '8', '9'] 再看reduce的用法。reduce把一个函数作用在一个序列[x1, x2, x3, ...]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是：reduce(f, [x1, x2, x3, x4])等价于f(f(f(x1, x2), x3), x4)比方说对一个序列求和，就可以用reduce实现： 123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def add(x, y):... return x + y...&gt;&gt;&gt; reduce(add, [1, 3, 5, 7, 9])25 当然求和运算可以直接用Python内建函数sum()，没必要动用reduce。 filter函数和map()类似，filter()也接收一个函数和一个序列。和map()不同的是：filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。比如过滤空字符的： 1234def not_empty(s): return s and s.strip()list(filter(not_empty, ['A', '', 'B', None, 'C', ' '])) 注意到filter()函数返回的是一个迭代器，也就是一个惰性序列，所以要强迫filter()完成计算结果，需要用list()函数获得所有结果并返回list。 sorted排序函数它接受一个list对其进行排序，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21], key=abs)[5, 9, -12, -21, 36] key函数作用于每一项，并根据key函数返回的结果进行排序。如果需要反向排序可以传入第三个参数reverse=True 返回函数这部分刚开始简单的一些还好，后面的高级应用感觉是比较难的(比如..装饰器的时候)，一个简单的例子说明一切： 12345678910111213141516def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax + n return ax return sum&gt;&gt;&gt; f1 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f2 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function lazy_sum.&lt;locals&gt;.sum at 0x101c6ed90&gt;&gt;&gt;&gt; f1()25&gt;&gt;&gt; f1 == f2False 可以看出内部函数sum可以引用外部函数的参数和局部变量，当lazy_sum返回函数sum时，相关参数和变量都保存在返回的函数中，这种称为“闭包（Closure）”的程序结构拥有极大的威力。每次调用都互不影响，比如上面f1和f2是不同的 闭包想要返回函数时，内部函数还能引用外部函数的变量实现起来是不容易的返回闭包时牢记的一点就是：返回函数不要引用任何循环变量，或者后续会发生变化的变量返回的所有函数会引用变量最后的值如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变： 123456789def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fs 匿名函数关键字lambda表示匿名函数，比如：lambda x: x * x冒号前面的x表示函数参数，写成一般的函数就是： 12345def f(x): return x * x# 一个例子&gt;&gt;&gt; list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9]))[1, 4, 9, 16, 25, 36, 49, 64, 81] 匿名函数有个限制，就是只能有一个表达式，当然return也是不可以用的，同时你也可以把匿名函数赋给一个变量，可以通过这个变量来进行调用，或者用于返回函数 装饰器函数对象有一个__name__属性，可以拿到函数的名字，比如 12&gt;&gt;&gt; str.__name__'str' 至于什么是装饰器，比如我们定义了个函数，后来写某个功能的时候又想在原来的基础上增加一些功能，我们最好还是不要在原来的函数上进行改动，这种在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。本质上，decorator就是一个返回函数的高阶函数，它接受一个函数，经过装饰后，返回一个新函数比如写一个在执行函数前先打印下函数名的装饰器： 12345def log(func): def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 装饰器写好了然后就是使用了，我们要借助Python的@语法，把decorator置于函数的定义处 1234567@logdef now(): print('2015-3-25')# 调用now函数时，除了执行其本身还打印了其函数名&gt;&gt;&gt; now()call now():2015-3-25 其实把@log放到now()函数的定义处，相当于执行了语句:now = log(now)但是原来的now函数还是存在的，只不过新的now指向了由装饰器构造的函数wrapper()函数的参数定义是(*args, **kw)，因此，wrapper()函数可以接受任意参数的调用。 如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，需要三层嵌套 1234567891011121314151617def log(text): def decorator(func): def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator#使用@log('execute')def now(): print('2015-3-25') # 执行&gt;&gt;&gt; now()execute now():2015-3-25 如果把@语法还原就是这样：now = log(&#39;execute&#39;)(now)到这里还有最后一个问题，前面我们说了装饰后的函数它指向了新的函数，也就是wrapper函数，我们并没有修改它的__name__ ，它有自己的name等属性，如果去看经过decorator装饰之后的函数，它们的name已经从原来的’now’变成了’wrapper’想要解决这个问题不需要编写wrapper.__name__ = func.__name__这样的代码，Python内置的functools.wraps就是干这个事的，所以，一个完整的decorator的写法如下 123456789101112131415161718192021import functools #导入模块def log(func): # 注意写在函数的上面 @functools.wraps(func) def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper# 或者那个三层嵌套的import functoolsdef log(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 在面向对象（OOP）的设计模式中，decorator被称为装饰模式。OOP的装饰模式需要通过继承和组合来实现，而Python除了能支持OOP的decorator外，直接从语法层次支持decorator。Python的decorator可以用函数实现，也可以用类实现。 decorator可以增强函数的功能，定义起来虽然有点复杂，但使用起来非常灵活和方便。 偏函数Python的functools模块提供了很多有用的功能，其中一个就是偏函数（Partial function）。在介绍函数参数的时候，我们讲到，通过设定参数的默认值，可以降低函数调用的难度。而偏函数也可以做到这一点。functools.partial就是用来帮助我们创建一个偏函数的，它的作用就是：把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。比如设置一个新函数来用于2进制的转换 12345678&gt;&gt;&gt; import functools&gt;&gt;&gt; int2 = functools.partial(int, base=2)&gt;&gt;&gt; int2('1000000') # == int('1000000',base=2)64# 其实它只是设置了一个默认值，也可以这样调用&gt;&gt;&gt; int2('1000000', base=10)1000000 创建偏函数时，实际上可以接收函数对象、*args和**kw这3个参数，上面的新函数每次调用的时候都传了一个默认的参数就是 12345678910kw = &#123; 'base': 2 &#125;int('10010', **kw)# 如果是max2 = functools.partial(max, 10)# 实际上会把10作为*args的一部分自动加到左边max2(5, 6, 7)#等价于args = (10, 5, 6, 7)max(*args) 所以：当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。 模块模块和java中的分类、分包基本一致，不过Py有个很大的特点是：每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。__init__.py可以是空文件，也可以有Python代码，因为__init__.py本身就是一个模块，而它的模块名就是顶层目录的名。模块的使用以内建的sys模块为例，编写一个hello的模块： 1234567891011121314151617181920#!/usr/bin/env python3# -*- coding: utf-8 -*-' a test module '__author__ = 'Michael Liao'import sysdef test(): args = sys.argv if len(args)==1: print('Hello, world!') elif len(args)==2: print('Hello, %s!' % args[1]) else: print('Too many arguments!')if __name__=='__main__': test() 第4行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释；第六行就是声明下作者当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。 作用域在一个模块中，我们可能会定义很多函数和变量，但有的函数和变量我们希望给别人使用，有的函数和变量我们希望仅仅在模块内部使用。在Python中，是通过_前缀来实现的类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如上面的__author__，__name__就是特殊变量，hello模块定义的文档注释也可以用特殊变量__doc__访问，我们自己的变量一般不要用这种变量名；类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用，注意是不能直接被引用而不是不能被使用，不过我们一般是不会去引用的 第三方模块安装在Python中，安装第三方模块，是通过包管理工具pip完成的。比如：pip install Pillow 当我们试图加载一个模块时，Python会在指定的路径下搜索对应的.py文件，如果找不到，就会报错，默认情况下Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中 临时追加： &gt;&gt;&gt; import sys &gt;&gt;&gt; sys.path.append(&#39;/Users/michael/my_py_scripts&#39;) 永久追加： 设置环境变量PYTHONPATH，Python自己本身的搜索路径不受影响。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS定位属性Position]]></title>
    <url>%2F2016%2F12%2F27%2FCSS%E5%AE%9A%E4%BD%8D%E5%B1%9E%E6%80%A7Position%2F</url>
    <content type="text"><![CDATA[事实证明，有些东西你一段时间不用就会遗忘….于是有了这篇笔记虽然我认为web前端是个坑，我应该是不会入的，不过这也说不定，这篇文来证明我曾经学过… 总览定位一般指的就是position属性了，一共有五种取值： relative(相对定位) absolute(绝对定位) fixed(固定定位) static(静态定位，无定位) inherit(继承父元素position属性) 元素在页面中的布局遵守一套文档流的方式，默认的定位属性值为static。它其实是未被设置定位的。元素如果被定位了，那么它的top,left,bottom,right值就会生效，一般常用的定位属性是relative,absolute和fixed需要注意的另一点是被定位的元素层次(z-index)会得到提高。inherit一般是很少用到的，这里不提了 文档流概念 将窗体自上而下分成一行行， 并在每行中按从左至右的顺序排放元素，即为文档流。 每个非浮动 块级元素都独占一行， 浮动元素则按规定浮在行的一端。 若当前行容不下， 则另起新行再浮动。 内联元素也不会独占一行。 几乎所有元素(包括块级，内联和列表元素）均可生成子行， 用于摆放子元素。 有三种情况将使得元素脱离文档流而存在，分别是 浮动，绝对定位， 固定定位。 但是在IE6中浮动元素也存在于文档流中。 relative相对定位相对于其正常(原来)位置进行定位。它的特点是不会脱离文档流，一张图就能说明一切问题：它是唯一并没有对周围的元素有任何影响的定位属性！！它依然存在于文档流中。它的位移是根据它在文档流中的原始位置发生的！！ absolute绝对定位绝对定位是一个非常牛逼的属性，牛逼到，你不知道会发生什么。注意，它的解释是什么——“生成绝对定位的元素，相对于 static 定位以外的第一个父元素进行定位。”也就是说，它可以相对于各种各样的东西进行定位。除了 static 其他都可以！！！注意！注意！注意！ 是 除了 ！如果所有层次的父元素是static，也就是无效的，它默认会以文档为定位对象，文档对象(document)不等于窗口对象(window)一般我们给坐标对象(必须是父级元素)加一个relative属性就可以了，如果不加left、top等属性相对定位没有任何效果，但是可以起到绝对定位参照的作用 fixed固定定位它是相对于浏览器窗口来说的，同时也是脱离文档流的，不会随滚动条的滚动而发生变化同样通过top,left,bottom,right来控制位置 总结 position: relative;不会脱离文档流，position: fixed;position: absolute;会脱离文档流 position: relative; 相对于自己在文档流中的初始位置偏移定位。 position: fixed; 相对于浏览器窗口定位。 position: absolute; 是相对于父级非position:static 浏览器定位。如果没有任何一个父级元素是非position:static属性，则会相对于文档定位。这里它的父级元素是包含爷爷级元素、祖爷爷级元素、祖宗十八代级元素的。任意一级都可以。如果它的父级元素和爷爷级元素都是非position:static 属性，则，它会选择距离最近的父元素。 拓展之display属性每一个元素都有默认的display属性，使用最多的是block, inline和inline-block，不常用的是table-cell。根据display属性，我们可以将元素分为块级元素(block)和内联级元素(inline)。它们最大区别是:block元素可以设置宽度，独占一行。inline元素宽度由内容决定，与其他元素并列在一行。常见的block属性元素有：div, h1-h6, ul, li, ol, dl, dd, dt。常见的inline属性元素有: span, a, em。 block宽高可以自行设置，默认宽度由父容器决定，默认高度有内容决定。自己独占一行。 inline宽度和高度都有内容决定，与其他元素共占一行。 inline-block宽度可以自行设置，类似block，但是与其他元素共占一行，类似inline。长用于设置垂直居中。 table-cell此属性指让标签元素以表格单元格的形式呈现，单元格有一些比较特殊的属性，可以设置元素的垂直居中等。 参考http://blog.csdn.net/fungleo/article/details/50056111https://leohxj.gitbooks.io/front-end-database/content/html-and-css-basic/css-position.html]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ASP.NET笔记]]></title>
    <url>%2F2016%2F12%2F23%2FASP-NET%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[学校开的课，没怎么深入，跟着学了点常用的，整理下以备用其实都差不多…. 母版页母版页也就是模板页 大雾:)，就是一个模板，网站的很多页面有很多地方都是相同的，尤其是后台管理系统，我们不可能每次都copy一份代码，所以就有了母版页，采用母版页的页面保证只有在母版页允许自定义的地方才能写前台代码，母版页中会包含下面这样的一个标签： 1234&lt;asp:ContentPlaceHolder id="ContentPlaceHolder1" runat="server"&gt; ...&lt;/asp:ContentPlaceHolder&gt;&lt;!--上面的标签内是可以添加前台代码的地方--&gt; Session全局变量网页间的传值是个问题，ASP中不能和winform那样传值了，一般是用Session，我也就会这一种了….创建方法右键–添加新项–全局应用程序类，默认是个名为Global.asax的文件 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;%@ Application Language=&quot;C#&quot; %&gt;&lt;script runat=&quot;server&quot;&gt; void Application_Start(object sender, EventArgs e) &#123; // 在应用程序启动时运行的代码 //存在于服务器，只有一个 Application.Lock(); Application[&quot;online&quot;] = 0; Application.UnLock(); &#125; void Application_End(object sender, EventArgs e) &#123; // 在应用程序关闭时运行的代码 &#125; void Application_Error(object sender, EventArgs e) &#123; // 在出现未处理的错误时运行的代码 &#125; void Session_Start(object sender, EventArgs e) &#123; // 在新会话启动时运行的代码 //每一个用户拥有自己的一个Session，用户连接的时候分配，用户离开站点(连接断开)即被释放 //存在于服务端，用户无法直接获取 Session[&quot;uname&quot;] = &quot;loli&quot;; Application.Lock(); Application[&quot;online&quot;] = (int)Application[&quot;online&quot;] + 1; Application.UnLock(); &#125; void Session_End(object sender, EventArgs e) &#123; // 在会话结束时运行的代码。 // 注意: 只有在 Web.config 文件中的 sessionstate 模式设置为 // InProc 时，才会引发 Session_End 事件。如果会话模式设置为 StateServer // 或 SQLServer，则不引发该事件。 Application.Lock(); Application[&quot;online&quot;] = (int)Application[&quot;online&quot;] - 1; Application.UnLock(); &#125;&lt;/script&gt; 在代码中可以直接用Session[&quot;uname&quot;]来进行获取值 页面跳转以及IsPostBack页面跳转没什么特别可说的，就一行代码： 1Response.Redirect("./Admin/Default.aspx"); 可以使用相对路径也可以使用绝对路径然后就是ASP中一个比较大的坑，我们可以看到ASP官方的控件中都有个属性叫runat=&quot;server&quot;，可以理解为当触发某个条件时比如按钮的点击事件，就会和服务器进行一次交流，不说性能方面的问题，还有一个很大的问题就是会刷新整个页面，也就是说会把后台代码再执行一遍，很多时候就会把用户设置、输入的内容给重置了….所以我们一般都会写这样的代码： 12345678protected void Page_Load(object sender, EventArgs e)&#123; //是不是第一次加载，只有第一次加载才进行数据绑定 if (!IsPostBack) &#123; bindData(); &#125;&#125; 连接数据库稍微学过点C#的这里应该比较熟悉了，都差不多 12345678910111213141516private static SqlConnection getConn()&#123; string connString = ConfigurationManager.ConnectionStrings["conn"].ConnectionString; return new SqlConnection(connString);&#125;public static DataSet GetDataSet(string strSQL)&#123; SqlConnection conn = getConn(); conn.Open(); SqlDataAdapter da = new SqlDataAdapter(strSQL, conn); DataSet ds = new DataSet(); da.Fill(ds); conn.Close(); return ds;&#125; 就是获取的Web.config中的配置，数据库连接字符串在这里面，部署到服务器后是不允许下载这个文件的 123456789101112&lt;configuration&gt; &lt;connectionStrings&gt; &lt;add name="conn" connectionString="Data Source=.;Initial Catalog=test;uid=sa;pwd=12345;" /&gt; &lt;/connectionStrings&gt; &lt;system.web&gt; &lt;compilation debug="true" targetFramework="4.5.2" /&gt; &lt;!--用户最大上传文件为40M，超时时间最大60秒，最大并发100--&gt; &lt;httpRuntime targetFramework="4.5.2" maxRequestLength="40960" executionTimeout="60" appRequestQueueLimit="100"/&gt; &lt;/system.web&gt;&lt;/configuration&gt; GridView控件对于这个控件确实是非常好用的，它可以直接用鼠标设置，也可以用代码，一般是要进行修改列名为中文的，可读性比较好，在编辑列选项中添加BoundField，记得进行和数据表中的字段进行绑定，同时可以添加一些编辑、更新、删除等操作，如果要用这些功能一定记得加ISPostBack判断如果要自定义加按钮什么的，可以添加个 TemplateField里面再套个ItemTemplate然后把控件装进去还有一点，如果加了单选框之类的控件默认改变是不会自动上传服务器的，需要手动加个AutoPostBack=&quot;true&quot;属性，但是我们又不希望每点一个就和服务器通讯次，为了减轻压力以及更好的用户体验，可以考虑把GridView外面套一个updatepanel控件，这样应该会进行批量更新….updatepanel标签需要和scriptmanager配套使用，我没仔细研究过，我当时只是为了解决让它不频繁的触发刷新不要忘了在控件的属性中设置对应的事件下面的两个例子把上面的所有功能都用到了，应该…. 1234567891011121314151617181920&lt;asp:scriptmanager id="ScriptManager1" runat="server"&gt;&lt;/asp:scriptmanager&gt;&lt;asp:updatepanel runat="server" id="UpdatePanel1"&gt; &lt;ContentTemplate&gt; &lt;asp:GridView ID="GridView1" runat="server" AutoGenerateColumns="False" DataSourceID="SqlDataSource1" AllowSorting="True" OnRowDataBound="GridView1_RowDataBound"&gt; &lt;Columns&gt; &lt;!-- ReadOnly="True"可设置为不可编辑部分 --&gt; &lt;asp:BoundField DataField="name" HeaderText="姓名" /&gt; &lt;!-- 自定义选项部分 --&gt; &lt;asp:TemplateField HeaderText="性别"&gt; &lt;ItemTemplate&gt; &lt;asp:RadioButton ID="RadioButton1" runat="server" Text="男" Checked="true" GroupName="g1" AutoPostBack="true" /&gt; &lt;asp:RadioButton ID="RadioButton2" runat="server" Text="女" GroupName="g1" AutoPostBack="true" /&gt; &lt;/ItemTemplate&gt; &lt;/asp:TemplateField&gt; &lt;/Columns&gt; &lt;/asp:GridView&gt; &lt;/ContentTemplate&gt;&lt;/asp:updatepanel&gt; 然后是如果使用了编辑、删除、更新之类的，还是要记得加相应的事件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private void BindData()&#123; //设置分页显示相关，也可以在属性里设 GridView1.AllowPaging = true; GridView1.PageSize = 10; GridView1.DataSource = GetDT(); //更新数据必须要有条件，就是这个了 GridView1.DataKeyNames = new string[] &#123; "UserID" &#125;; GridView1.DataBind();&#125;//删除protected void GridView1_RowDeleting(object sender, GridViewDeleteEventArgs e)&#123; if (DeleteRows("TabName", "UserID", GridView1.DataKeys[e.RowIndex].Value.ToString())) BindData();&#125;//编辑protected void GridView1_RowEditing(object sender, GridViewEditEventArgs e)&#123; //让其(行)处于编辑状态，要刷新数据才能生效 GridView1.EditIndex = e.NewEditIndex; BindData();&#125;//更新protected void GridView1_RowUpdating(object sender, GridViewUpdateEventArgs e)&#123; //找到编辑状态那一行的第1列中的第一个控件，默认编辑状态后是TextBox //操作完了不要忘了取消编辑状态，刷新数据源 if (UpdateRows("TabName", "UserID", ((TextBox)GridView1.Rows[e.RowIndex].Cells[0].Controls[0]).Text.ToString().Trim())) &#123; GridView1.EditIndex = -1; BindData(); &#125;&#125;//取消protected void GridView1_RowCancelingEdit(object sender, GridViewCancelEditEventArgs e)&#123; GridView1.EditIndex = -1; BindData();&#125;//切换页数protected void GridView1_PageIndexChanging(object sender, GridViewPageEventArgs e)&#123; GridView1.PageIndex = e.NewPageIndex; BindData();&#125; Repeater控件默认有五种模板： ItemTemplate : 对每一个数据项进行格式设置 AlternatingItemTemplate : 对交替数据项进行格式设置 ，显示2、4、6条 SeparatorTemplate : 对分隔符进行格式设置 HeaderTemplate : 对页眉进行格式设置 FooterTemplate : 对页脚进行格式设置 ItemTemplate标签内的内容会重复，和数据表的行数一致 1234567891011121314151617181920212223242526272829&lt;asp:Repeater ID="Repeater1" runat="server" OnItemCommand="Repeater1_ItemCommand"&gt; &lt;HeaderTemplate&gt; &lt;!-- 显示头部 --&gt; &lt;table class="movies"&gt; &lt;!-- table头部声明--&gt; &lt;tr&gt; &lt;th&gt;序号&lt;/th&gt; &lt;th&gt;详细信息&lt;/th&gt; &lt;/tr&gt; &lt;/HeaderTemplate&gt; &lt;ItemTemplate&gt; &lt;!-- 数据行 --&gt; &lt;tr&gt; &lt;td&gt; &lt;%--自动编号--%&gt; &lt;asp:Label ID="lbNo" runat="server" Text="&lt;%#Container.ItemIndex+1 %&gt;"&gt;&lt;/asp:Label&gt; &lt;/td&gt; &lt;td&gt; &lt;asp:TextBox ID="TBweek" runat="server" Text='&lt;%#DataBinder.Eval(Container.DataItem,"Week") %&gt;'&gt;&lt;/asp:TextBox&gt; &lt;asp:TextBox ID="TBtime" runat="server" Text='&lt;%#DataBinder.Eval(Container.DataItem,"Time") %&gt;'&gt;&lt;/asp:TextBox&gt; ' &lt;/td&gt; &lt;/tr&gt; &lt;/ItemTemplate&gt; &lt;FooterTemplate&gt; &lt;!-- 脚注行 --&gt; &lt;/table&gt; &lt;!-- table尾 --&gt; &lt;/FooterTemplate&gt;&lt;/asp:Repeater&gt; 上传文件到服务器比如你如果想导入Excel也好还是设置头像也好，文件是必须先要上传到服务器才能操作的，这里使用FileUpload控件来上传 123456789101112131415161718192021222324252627private string Upload(FileUpload fuload)&#123; //获取选择文件的扩展名 string fileExtenSion = Path.GetExtension(fuload.FileName); //检测文件扩展名(格式) if (fileExtenSion.ToLower() != ".xls" &amp;&amp; fileExtenSion.ToLower() != ".xlsx") &#123; return null; &#125; try &#123; //GetFileName返回文件名和扩展名 string FileName = "App_Data\\" + Path.GetFileName(fuload.FileName); //判断文件是否存在，如果存在先删除，Server.MapPath返回服务器的物理路径 if (File.Exists(Server.MapPath(FileName))) &#123; File.Delete(Server.MapPath(FileName)); &#125; //上传文件到指定目录 fuload.SaveAs(Server.MapPath(FileName)); return Server.MapPath("./") + FileName; &#125; catch (Exception e) &#123; return null; &#125;&#125; 读取Excel文件我基本也是从网上找的实例代码，需要安装相应的支持库才行，区分32与64位系统，可以在项目中设置IIS以64位运行 1234567891011121314151617181920212223242526272829303132333435363738private static OleDbConnection getOleConn(string fileName)&#123; System.GC.Collect(); OleDbConnection oleConn; //HDR=Yes，这代表第一行是标题，不做为数据使用 //如果用HDR=NO，则表示第一行不是标题，做为数据来使用。系统默认的是YES string connstr2003 = "Provider=Microsoft.Jet.OLEDB.4.0;Data Source=" + fileName + ";Extended Properties='Excel 8.0;HDR=Yes;IMEX=1;'"; string connstr2007 = "Provider=Microsoft.ACE.OLEDB.12.0;Data Source=" + fileName + ";Extended Properties=\"Excel 12.0;HDR=YES\""; string fileExtenSion = fileName.Substring(fileName.LastIndexOf(".") + 1); //建立连接，根据不同的扩展名，选择不同的引擎 if (fileExtenSion.ToLower() == "xls") &#123; oleConn = new OleDbConnection(connstr2003); &#125; else &#123; oleConn = new OleDbConnection(connstr2007); &#125; return oleConn;&#125;public static void ReadExcelToDataSet(string fileName, string strSQL)&#123; OleDbConnection conn = getOleConn(fileName); conn.Open(); OleDbDataAdapter da = new OleDbDataAdapter(strSQL, conn); da.SelectCommand.CommandTimeout = 600; ds = new DataSet(); //在ds中规定表名为ExcelInfo da.Fill(ds, "ExcelInfo"); conn.Close(); conn.Dispose();&#125; Excel批量快速导入主要使用ASP中一个叫SqlBulkCopy的类，想要导入最快要保证内存中的DT和数据库的表结构相同，倒是没必要字段也相同 123456789101112131415public static void SQLBulkCopy(DataTable dt,string dtName)&#123; //using内的对象在代码块结束后会自动销毁，所以conn不用close using (SqlConnection conn = getConn()) &#123; conn.Open(); using (SqlBulkCopy bulkCopy = new SqlBulkCopy(conn)) &#123; bulkCopy.DestinationTableName = dtName; //假设数据库和Excel表的列名不同，内存列名映射到数据库的列名 //bulkCopy.ColumnMappings.Add("loct", "serve"); bulkCopy.WriteToServer(dt); &#125; &#125;&#125; 补充datatable控件是个很好用的控件，包括类，对于DT的顺序问题，是有个排序方法的，比如可以这样 12345DataTable dt = getDT();DataView dv = dt.DefaultView;dv.Sort = "week ASC,time";Repeater1.DataSource = dv.ToTable();Repeater1.DataBind(); 对于判断一个字符串是不是为空，又很多写法都可以实现，据说下面的这种方式比较高效：strTest.Length == 0]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>ASP.NET</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-高级特性]]></title>
    <url>%2F2016%2F12%2F19%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[Py笔记大部分来自于：廖雪峰的BLOG非常适合初学者的系列教程！写的太好了！简直无可挑剔，果然大牛~ 切片取一个list或tuple的部分元素是非常常见的操作，用一般的方法处理比如循环啦是非常繁琐的，因此，Python提供了切片（Slice）操作符，能大大简化这种操作。 123456789&gt;&gt;&gt; L = ['Michael', 'Sarah', 'Tracy', 'Bob', 'Jack']# 取前三个元素&gt;&gt;&gt; L[0:3]['Michael', 'Sarah', 'Tracy']# 如果从0开始可以省略&gt;&gt;&gt; L[:3]['Michael', 'Sarah', 'Tracy'] L[0:3]表示，从索引0开始取，直到索引3为止，但不包括索引3，正好是3个元素。类似的，既然Python支持L[-1]取倒数第一个元素，那么它同样支持倒数切片 1234&gt;&gt;&gt; L[-2:]['Bob', 'Jack']&gt;&gt;&gt; L[-2:-1]['Bob'] 还可以支持第三个参数，用于隔数取，如 L[0:3:2]隔两个取一个tuple也是一种list，唯一区别是tuple不可变。因此，tuple也可以用切片操作，只是操作的结果仍是tuple字符串&#39;xxx&#39;也可以看成是一种list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串，所以Python没有针对字符串的截取函数，只需要切片一个操作就可以完成。 迭代如果给定一个list或tuple，我们可以通过for循环来遍历这个list或tuple，这种遍历我们称为迭代（Iteration）。在Python中，迭代是通过for ... in来完成的，而很多语言比如C或者Java，迭代list是通过下标完成的Python中的for…in可以迭代任何可迭代的对象，无论是否具有下标如何判断是否可以迭代呢？方法是通过collections模块的Iterable类型判断： 1234567&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance('abc', Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False 如果要对list实现类似Java那样的下标循环怎么办？Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身： 123456&gt;&gt;&gt; for i, value in enumerate(['A', 'B', 'C']):... print(i, value)...0 A1 B2 C 在for里使用两个变量也是很常见的，例如 123456&gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:... print(x, y)...1 12 43 9 列表生成式举个例子，要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))但如果要生成[1x1, 2x2, 3x3, ..., 10x10]怎么做？方法一是循环： 123456&gt;&gt;&gt; L = []&gt;&gt;&gt; for x in range(1, 11):... L.append(x * x)...&gt;&gt;&gt; L[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list： 12&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 写列表生成式时，把要生成的元素x * x放到前面，后面跟for循环，就可以把list创建出来，十分有用，多写几次，很快就可以熟悉这种语法。 for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方： 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 还可以使用两层循环，可以生成全排列： 12&gt;&gt;&gt; [m + n for m in 'ABC' for n in 'XYZ']['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ'] 生成器通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： 123&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 可以通过next()函数获得generator的下一个返回值 1234&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1 generator保存的是算法，每次调用next(g)，就计算出g的下一个元素的值，直到计算到最后一个元素，没有更多的元素时，抛出StopIteration的错误。但是这种不断调用next(g)实在是太变态了，正确的方法是使用for循环，因为generator也是可迭代对象generator非常强大。如果推算的算法比较复杂，用类似列表生成式的for循环无法实现的时候，还可以用函数来实现。比如，著名的斐波拉契数列（Fibonacci），除第一个和第二个数外，任意一个数都可由前两个数相加得到： 1, 1, 2, 3, 5, 8, 13, 21, 34, … 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易： 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(b) a, b = b, a + b n = n + 1 return 'done' 注意，赋值语句：a, b = b, a + b相当于： 123t = (b, a + b) # t是一个tuplea = t[0]b = t[1] 仔细观察，可以看出，fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了： 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 return 'done' 这就是定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。需要注意的是：但是用for循环调用generator时，拿不到generator的return语句的返回值。需要捕获错误才行。 迭代器可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator。需要注意的是可迭代对象和迭代器是完全不同的两个东西，如果要把Iterable(可迭代对象)变成Iterator(迭代器)可以使用iter()函数，当然也可以用isinstance来进行检测 1234&gt;&gt;&gt; isinstance(iter([]), Iterator)True&gt;&gt;&gt; isinstance(iter('abc'), Iterator)True 你可能会问，为什么list、dict、str等数据类型不是Iterator(迭代器)？这是因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。 Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。凡是可作用于for循环的对象都是Iterable类型；凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列；Python的for循环本质上就是通过不断调用next()函数实现的； 一些方法的补充items相关方法把dict对象转换成了包含tuple的list，我们对这个list进行迭代，可以同时获得key和value 类似方法 iteritems()需要注意的是因为字典是无序的，所以用items方法返回字典的所有项，也是没有顺序的。还有它是占额外的内存的 123&gt;&gt;&gt; d = &#123;'x':'a','y':'b','z':'c'&#125;&gt;&gt;&gt; d.items()dict_items([('z', 'c'), ('x', 'a'), ('y', 'b')]) 至于iteritems方法：与items方法相比作用大致相同，只是它的返回值不是列表，而是一个迭代器。不占额外内存在很多情况下使用iteritems更高效（尤其是想要迭代结果的情况下）。 重要： stackoverflow上这样一个问题：dict.items()和dict.iteritems()有什么区别？ ，第一个答案大致的意思是这样的：“起初 items() 就是返回一个像上面那样的包含dict所有元素的list，但是由于这样太浪费内存，所以后来就加入了（注：在Python 2.2开始出现的）iteritems(), iterkeys(), itervalues()这一组函数，用于返回一个 iterator 来节省内存，但是在 3.x 里items() 本身就返回这样的 iterator，所以在 3.x 里items() 的行为和 2.x 的 iteritems() 行为一致，iteritems()这一组函数就废除了。”不过更加有意思的是，这个答案虽然被采纳，下面的评论却指出，这种说法并不准确，在 3.x 里 items() 的行为和 2.x 的 iteritems() 不一样，它实际上返回的是一个”full sequence-protocol object”，这个对象能够反映出 dict 的变化，后来在 Python 2.7 里面也加入了另外一个函数 viewitems() 和 3.x 的这种行为保持一致 viewitems和iteritems有什么区别呢，viewitems() 返回的是view object，它可以反映出 dictionary 的变化，就是说当字典变化后依然可以进行遍历 总结起来，在 2.x 里面，最初是 items() 这个方法，但是由于太浪费内存，所以加入了 iteritems() 方法，用于返回一个 iterator，在 3.x 里面将 items() 的行为修改成返回一个 view object，让它返回的对象同样也可以反映出原 dictionary 的变化，同时在 2.7 里面又加入了 viewitems() 向下兼容这个特性。所以在 3.x 里面不需要再去纠结于三者的不同之处，因为只保留了一个 items() 方法。 文：http://blog.csdn.net/revilwang/article/details/38686635]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-函数]]></title>
    <url>%2F2016%2F12%2F10%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[继续学习函数部分，为后面的高级特性打基础，Py中函数的正确打开方式是什么呢？梦想之路还很远呐~~~ 函数函数名其实就是指向一个函数对象的引用，完全可以把函数名赋给一个变量，相当于给这个函数起了一个“别名”： 123&gt;&gt;&gt; a = abs # 变量a指向abs函数&gt;&gt;&gt; a(-1) # 所以也可以通过a调用abs函数1 函数也是放在栈里，递归要避免栈溢出 定义函数Py中定义函数用def关键字，木有类似花括号的作用域，使用冒号:进行区分，缩进表示层次，Py中缩进非常严格，标准4个空格 12345def my_abs(x): if x &gt;= 0: return x else: return -x 如果没有return语句，函数执行完毕后也会返回结果，只是结果为None。return None可以简写为return。 可以在Py文件的当前目录下启动Python解释器，用from 文件名 import 函数名来导入函数，注意是文件名（不含.py扩展名） 空函数和pass定义一个空函数： 12def nop(): pass pass语句什么都不做，那有什么用？实际上pass可以用来作为占位符，比如现在还没想好怎么写函数的代码，就可以先放一个pass，让代码能运行起来。在其他地方也适用，比如if和for： 12if age &gt;= 18: pass 参数检查调用函数时，如果参数个数不对，Python解释器会自动检查出来，并抛出TypeError： 1234&gt;&gt;&gt; my_abs(1, 2)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: my_abs() takes 1 positional argument but 2 were given 但是如果参数类型不对，Python解释器就无法帮我们检查。试试my_abs和内置函数abs的差别： 123456789&gt;&gt;&gt; my_abs('A')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 2, in my_absTypeError: unorderable types: str() &gt;= int()&gt;&gt;&gt; abs('A')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: bad operand type for abs(): 'str' 当传入了不恰当的参数时，内置函数abs会检查出参数错误，而我们定义的my_abs没有参数检查，会导致if语句出错，出错信息和abs不一样。所以，这个函数定义不够完善。让我们修改一下my_abs的定义，对参数类型做检查，只允许整数和浮点数类型的参数。数据类型检查可以用内置函数isinstance()实现： 1234567def my_abs(x): if not isinstance(x, (int, float)): raise TypeError('bad operand type') if x &gt;= 0: return x else: return -x 添加了参数检查后，如果传入错误的参数类型，函数就可以抛出一个错误： 12345&gt;&gt;&gt; my_abs('A')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 3, in my_absTypeError: bad operand type 返回多个值Py中函数可以返回“多个值”比如在游戏中经常需要从一个点移动到另一个点，给出坐标、位移和角度，就可以计算出新的新的坐标： 12345import mathdef move(x, y, step, angle=0): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny import math语句表示导入math包，并允许后续代码引用math包里的sin、cos等函数。 然后，我们就可以同时获得返回值： 123&gt;&gt;&gt; x, y = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(x, y)151.96152422706632 70.0 但其实这只是一种假象，Python函数返回的仍然是单一值： 123&gt;&gt;&gt; r = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(r)(151.96152422706632, 70.0) 原来返回值是一个tuple！但是，在语法上，返回一个tuple可以省略括号，而多个变量可以同时接收一个tuple，按位置赋给对应的值，所以，Python的函数返回多值其实就是返回一个tuple，但写起来更方便。 设置默认参数这个感觉是一个很爽的功能，设置了默认参数调用的时候是可以不写的 1234567# 计算x的n次方def power(x, n=2): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 这里注意的是，默认参数只能是存在于最后，要不然就没法区别了…当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。当如果又多个默认参数的时候，可以顺序的输入，也可以不按顺序提供部分默认参数。当不按顺序提供部分默认参数时，需要把参数名写上。比如调用enroll(&#39;Adam&#39;, &#39;M&#39;, city=&#39;Tianjin&#39;)，意思是，city参数用传进去的值，其他默认参数继续使用默认值。 这里有一个坑，和java中的参数传递类似：Python函数在定义的时候，假设默认参数L的值就被计算出来了，即[]是个可变对象，因为默认参数L也是一个变量，它指向对象[]，如果在函数中改变了L所指向对象的内容，则下次调用时，默认参数的内容就变了，不再是函数定义时的[]了。 所以，定义默认参数要牢记一点：默认参数必须指向不变对象！ 可变参数12345def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 定义可变参数和定义一个list或tuple参数相比，仅仅在参数前面加了一个*号。在函数内部，参数numbers接收到的是一个tuple，因此，函数代码完全不变。但是，调用该函数时，可以传入任意个参数，包括0个参数如果已经有一个list或者tuple，要调用一个可变参数怎么办？Python允许你在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去： 123&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(*nums)14 *nums表示把nums这个list的所有元素作为可变参数传进去。这种写法相当有用，而且很常见。 关键字参数关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例： 12def person(name, age, **kw): print('name:', name, 'age:', age, 'other:', kw) 函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数： 12&gt;&gt;&gt; person('Michael', 30)name: Michael age: 30 other: &#123;&#125; 也可以传入任意个数的关键字参数： 1234&gt;&gt;&gt; person('Bob', 35, city='Beijing')name: Bob age: 35 other: &#123;'city': 'Beijing'&#125;&gt;&gt;&gt; person('Adam', 45, gender='M', job='Engineer')name: Adam age: 45 other: &#123;'gender': 'M', 'job': 'Engineer'&#125; 关键字参数有什么用？它可以扩展函数的功能。比如，在person函数里，我们保证能接收到name和age这两个参数，但是，如果调用者愿意提供更多的参数，我们也能收到。试想你正在做一个用户注册的功能，除了用户名和年龄是必填项外，其他都是可选项，利用关键字参数来定义这个函数就能满足注册的需求。如果传入的也是个dict怎么办，和上面一样了用**name来区别，反正就是拷贝一份数据，对原数据都是不会影响的，因为不存在什么引用数据类型、基本数据类型，所以都是一样的 命名关键字参数对于关键字参数，函数的调用者可以传入任意不受限制的关键字参数。如果要限制关键字参数的名字，就可以用命名关键字参数，例如，只接收city和job作为关键字参数。这种方式定义的函数如下： 12def person(name, age, *, city, job): print(name, age, city, job) 和关键字参数**kw不同，命名关键字参数需要一个特殊分隔符*，*后面的参数被视为命名关键字参数。调用方式如下： 12&gt;&gt;&gt; person('Jack', 24, city='Beijing', job='Engineer')Jack 24 Beijing Engineer 如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符*了： 12def person(name, age, *args, city, job): print(name, age, args, city, job) 命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错命名关键字参数可以有缺省值，从而简化调用： 12def person(name, age, *, city='Beijing', job): print(name, age, city, job) 由于命名关键字参数city具有默认值，调用时，可不传入city参数： 12&gt;&gt;&gt; person('Jack', 24, job='Engineer')Jack 24 Beijing Engineer 使用命名关键字参数时，要特别注意，如果没有可变参数，就必须加一个*作为特殊分隔符。否则就当作是位置参数 组合参数在Python中定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用。但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的。 递归函数所有的递归函数都可以写成循环的方式，但循环的逻辑不如递归清晰。使用递归函数需要注意防止栈溢出。在计算机中，函数调用是通过栈（stack）这种数据结构实现的，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。由于栈的大小不是无限的，所以，递归调用的次数过多，会导致栈溢出。解决递归调用栈溢出的方法是通过尾递归优化，事实上尾递归和循环的效果是一样的，所以，把循环看成是一种特殊的尾递归函数也是可以的。尾递归是指，在函数返回的时候，调用自身本身，并且，return语句不能包含表达式。这样，编译器或者解释器就可以把尾递归做优化，使递归本身无论调用多少次，都只占用一个栈帧，不会出现栈溢出的情况。例如下面一个求阶乘的例子： 1234567def fact(n): return fact_iter(n, 1)def fact_iter(num, product): if num == 1: return product return fact_iter(num - 1, num * product) 遗憾的是，大多数编程语言没有针对尾递归做优化，Python解释器也没有做优化，所以，即使把上面的fact(n)函数改成尾递归方式，也会导致栈溢出。尾递归事实上和循环是等价的，没有循环语句的编程语言只能通过尾递归实现循环。 常用函数生成序列Python提供一个range()函数，可以生成一个整数序列，再通过list()函数可以转换为list。比如range(5)生成的序列是从0开始小于5的整数(不包括5)： 12&gt;&gt;&gt; list(range(5))[0, 1, 2, 3, 4] 如果不了解某个函数的使用可以使用help()来查看帮助，如help(abs) 字符串操作isinstance(x, str) 可以判断变量 x 是否是字符串upper() 方法可以返回大写的字母 获取信息相关type()函数可来判断对象的类型对于class的继承关系来说，使用type()就很不方便。我们要判断class的类型，可以使用isinstance()函数。 123456&gt;&gt;&gt; isinstance('a', str)True&gt;&gt;&gt; isinstance(123, int)True&gt;&gt;&gt; isinstance([1, 2, 3], (list, tuple))True 如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list仅仅把属性和方法列出来是不够的，配合getattr()、setattr()以及hasattr()，我们可以直接操作一个对象的状态 12345678910111213141516&gt;&gt;&gt; hasattr(obj, 'x') # 有属性'x'吗？True&gt;&gt;&gt; obj.x9&gt;&gt;&gt; hasattr(obj, 'y') # 有属性'y'吗？False&gt;&gt;&gt; setattr(obj, 'y', 19) # 设置一个属性'y'&gt;&gt;&gt; hasattr(obj, 'y') # 有属性'y'吗？True&gt;&gt;&gt; getattr(obj, 'y') # 获取属性'y',可以再传入一个默认值19&gt;&gt;&gt; obj.y # 获取属性'y'19 关于math等abs() 求绝对值cmp(x, y) 比较函数（如果 x &lt; y，返回 -1；如果 x==y，返回 0；如果 x &gt; y，返回 1）int()/str() 转换函数sum() 可计算list的和sqrt() 平方根 更多待更新…]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-基础]]></title>
    <url>%2F2016%2F11%2F20%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[一起愉快的学Py吧，早就听说Py的大名了，感觉还并不是太难，只是一时心血来潮，一直有个梦想，写个爬虫去爬那个啥….o(￣▽￣)ゞ))￣▽￣)o 开始py作为解释性语言，能够很好的被读懂(大概)，一向是以简洁 优雅著称因为是解释执行，不会进行预编译成二进制文件，所以….代码是不能被加密的，发就发源代码了…还有一点在py中是不建议使用分号用来结尾的，都是以缩进(四个空格)来区分代码的范围 Py文件的头一般有这两句： 12#!/usr/bin/env python3# -*- coding: utf-8 -*- 第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释；第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。 数据类型和变量因为有java语言的基础，主要总结下与java不同的地方 字符串如果字符串中有太多的转义字符可以在前面加r这样里面就不需要进行转义了r&#39;字符串内容&#39;如果需要很多的换行符，为了简便可以使用&#39;&#39;&#39;...&#39;&#39;&#39;，在里面回车会被记录，不用再写转义\n，注意在输入多行内容时，提示符由&gt;&gt;&gt;变为...，提示你可以接着上一行输入。多行字符串&#39;&#39;&#39;...&#39;&#39;&#39;还可以在前面加上r使用。 123456&gt;&gt;&gt; print('''line1... line2... line3''')line1line2line3 布尔类型一个布尔值只有True、False两种值，注意第一个字母是大写的，布尔值可以用and、or和not运算(类似与或非运算吧…)。Python把0、空字符串&#39;&#39;和None看成 False，其他数值和非空字符串都看成 True 在计算 a and b 时，如果 a 是 False，则根据与运算法则，整个结果必定为 False，因此返回 a；如果 a 是 True，则整个计算结果必定取决与 b，因此返回 b。 在计算 a or b 时，如果 a 是 True，则根据或运算法则，整个计算结果必定为 True，因此返回 a；如果 a 是 False，则整个计算结果必定取决于 b，因此返回 b。 空值空值是Python里一个特殊的值，用None表示。和java中的null类似 关于除法用/进行运算的除法得到的是一个浮点数，即使两个整数相除还有一种除法是//，称为地板除，两个整数的除法仍然是整数，会舍弃小数部分 python的整数和浮点数没有大小限制！ 循环和JS中类似，Py中的for循环只有for…in的形式，比如经典问题：计算1+2+…+100： 12345# range(101)就可以生成0-100的整数序列sum = 0for x in range(101): sum = sum + xprint(sum) 还有另一种循环while，这个就变化不大了，只要条件满足，就不断循环： 123456sum = 0n = 99while n &gt; 0: sum = sum + n n = n - 2print(sum) 还有break 和 continue也是一样的 条件判断py中的if确实和java之类不太一样，简化了不少，也不需要加括号，不过要注意:的问题 12345678910111213141516age = 3if age &gt;= 18: print('adult')elif age &gt;= 6: print('teenager')else: print('kid')# input()能够获取用户的输入，接受的一个str用于提示用户# 获取的内容都会转成str类型，需要什么类型的要相对应强转下s = input('birth: ')birth = int(s)if birth &lt; 2000: print('00前')else: print('00后') list和tuple我认为py中的集合类的类型显得更加重要，很多神奇的操作都发生在这里。o(￣▽￣)ゞ))￣▽￣)o listPython内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。感觉挺像数组的，因为是动态引用，所以里面的元素没有类型的限制，并且list[-1]可以表示最后一个元素，以此类推 123456789101112131415161718# list的定义，用中括号定义a = [1,]# 通过append方法向list尾部添加元素a.append('Adam')# 把元素插入到指定位置，pos为索引a.insert(1, 'Jack')# 删除末尾元素,并且会返回这个元素# 并且它可以接受一个索引参数，删除指定索引的元素a.pop()# 替换元素可以直接进行赋值a[0] = 123# list中可以再存list，类似二维数组s = ['python', 'java', ['asp', 'php'], 'scheme'] 当索引超出了范围时，Python会报一个IndexError错误，所以，要确保索引不要越界，记得最后一个元素的索引是len(classmates) - 1。 tuple另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改它也没有append()，insert()这样的方法。其他获取元素的方法和list是一样的不可变的tuple有什么意义？因为tuple不可变，所以代码更安全。如果可能，能用tuple代替list就尽量用tuple。 1234567# 定义tuple使用小括号(),当只有一个元素时，为了避免产生歧义当作小括号进行运算，我们一般会在后面加个,t = (1,)# 定义一个“可变”的tuple# 可以看作其中的list是引用数据类型，t[2]指向的是list的地址，而list是可变的# t指向的list并没有改变所以可以认为tuple确实是不可修改的t = ('a', 'b', ['A', 'B']) dict和setPython内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。为什么dict查找速度这么快？因为dict的实现原理和查字典是一样的。假设字典包含了1万个汉字，我们要查某一个字，一个办法是把字典从第一页往后翻，直到找到我们想要的字为止，这种方法就是在list中查找元素的方法，list越大，查找越慢。第二种方法是先在字典的索引表里（比如部首表）查这个字对应的页码，然后直接翻到该页，找到这个字。无论找哪个字，这种查找速度都非常快，不会随着字典大小的增加而变慢。如上它们都是无序的，获取的时候不能使用下标的方式 dictdict就是第二种实现方式，基本的使用(使用{}花括号来定义)： 1234567891011121314151617&gt;&gt;&gt; d = &#123;'Michael': 95, 'Bob': 75, 'Tracy': 85&#125;&gt;&gt;&gt; d['Michael']95# 我们还可以通过key进行赋值，重复赋值会被覆盖&gt;&gt;&gt; d['Adam'] = 67&gt;&gt;&gt; d['Adam']67# 获取的时候如果key不存在就会报错，为了避免可以判断下&gt;&gt;&gt; 'Thomas' in dFalse# 通过dict提供的get方法，如果key不存在，可以返回None，或者自己指定的value&gt;&gt;&gt; d.get('Thomas')&gt;&gt;&gt; d.get('Thomas', -1)-1 注意：返回None的时候Python的交互式命令行不显示结果。要删除一个key，用pop(key)方法，对应的value也会从dict中删除： 1234&gt;&gt;&gt; d.pop('Bob')75&gt;&gt;&gt; d&#123;'Michael': 95, 'Tracy': 85&#125; 请务必注意，dict内部存放的顺序和key放入的顺序是没有关系的。 和list比较，dict有以下几个特点： 查找和插入的速度极快，不会随着key的增加而变慢； 需要占用大量的内存，内存浪费多。 key不能重复，同时存储的键值对是没有顺序的 作为Key的元素必须是不可变的 而list相反： 查找和插入的时间随着元素的增加而增加； 占用空间小，浪费内存很少。 所以，dict是用空间来换取时间的一种方法。 dict可以用在需要高速查找的很多地方，在Python代码中几乎无处不在，正确使用dict非常重要，需要牢记的第一条就是dict的key必须是不可变对象。 这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash）。 要保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list是可变的，就不能作为key： 12345&gt;&gt;&gt; key = [1, 2, 3]&gt;&gt;&gt; d[key] = 'a list'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: unhashable type: 'list' setset和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，可以保证没有重复的元素(key)。要创建一个set，需要提供一个list作为输入集合： 123&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; s&#123;1, 2, 3&#125; # 没有values 注意，传入的参数[1, 2, 3]是一个list，而显示的{1, 2, 3}只是告诉你这个set内部有1，2，3这3个元素，显示的顺序也不表示set是有序的(它其实是无序的)。。重复元素在set中自动被过滤： 123&gt;&gt;&gt; s = set([1, 1, 2, 2, 3, 3])&gt;&gt;&gt; s&#123;1, 2, 3&#125; 通过add(key)方法可以添加元素到set中，可以重复添加，但不会有效果： 1234567&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; s&#123;1, 2, 3, 4&#125;&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; s&#123;1, 2, 3, 4&#125; 通过remove(key)方法可以删除元素： 123&gt;&gt;&gt; s.remove(4)&gt;&gt;&gt; s&#123;1, 2, 3&#125; set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作： 1234567&gt;&gt;&gt; s1 = set([1, 2, 3])&gt;&gt;&gt; s2 = set([2, 3, 4])&gt;&gt;&gt; s1 &amp; s2&#123;2, 3&#125;&gt;&gt;&gt; s1 | s2&#123;1, 2, 3, 4&#125; set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。set的内部结构和dict很像，唯一区别是不存储value，因此，判断一个元素是否在set中速度很快。关于更新，由于set存储的是一组不重复的无序元素，因此，更新set主要做两件事：一是把新的元素添加到set中，二是把已有元素从set中删除。set拥有 add remove 方法同样可以使用&#39;name&#39; in set来进行判断是否存在最后，set存储的元素也是没有顺序的。 关于可变与不可变的补充tuple虽然是不变对象，上面也解释过，可以把list当作是一个引用，(1, [2, 3])可以看作是不变的，但如果把(1, 2, 3)和(1, [2, 3])放入dict或set中，是会报错的，即使引用不会变，但list的内容可变，根据dict和set的规定，同样会导致哈希化的失败 其他补充连接符,相当于空格、分隔符的作用来连接两个字符串print &#39;aaa&#39;,123 关于编码str.decode(&quot;UTF-8&quot;) 解码str.encode(&quot;UTF-8&quot;) 编码b&#39;aaa&#39; bytes类型，一个字符占一个字节 格式化输出12&gt;&gt;&gt; '%.2f' % 3.1415926'3.14' %其实可以看作是占位符print(&#39;%s&#39; % &#39;abc&#39;) 数据类型Python3 中有六个标准的数据类型： Number（数字） String（字符串） List（列表） Tuple（元组） Sets（集合） Dictionary（字典） 重新赋值12a = 'abc'a = 'def' 原来的值如果没有别的变量引用，就会被垃圾回收，但是无法预测回收时间，多久回收一次由V8引擎自己决定 更多待补充…]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android安全开发初步(二)]]></title>
    <url>%2F2016%2F10%2F27%2FAndroid%E5%AE%89%E5%85%A8%E5%BC%80%E5%8F%91%E5%88%9D%E6%AD%A5-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[继续更新！Android的安全问题太多太多，这里只是总结了下我所了解(强制 = = )到的安全问题话说，实际开发中还能想到多少呢？？现在感觉各行各业越来越重视安全，如果有机会接触到感觉还是很爽的！其实我想装逼 截屏风险在登录和注册，或修改密码等敏感数据操作时，如果手机中有后台默认隐藏截屏的应用，在输入是一直截屏，就有可能盗取敏感数据信息。解决方案：在Activity onCreate 中加入： 12//一般写在setContentView上面getWindow().addFlags(WindowManager.LayoutParams.FLAG_SECURE); 官方的意思就是设置了这个flag后, 系统会把当前窗口的内容视为安全隐私内容, 系统会阻止这些内容被截屏或者在不安全可靠的场景显示出来.它起到的主要作用是： 阻止屏幕截图 在Recent apps(任务切换界面)中只显示应用名字和图标, 不显示内容 Google App的Now on tap功能不会去分析你的页面的内容 最后，对于国内各种ROM对Android的丧心病狂的更改还是要测试下实际效果的。。。 关注debuggableandroid:debuggable 属性的设置可能会引起 被动态调试的风险。 12345&lt;application android:icon="@drawable/ic_launcher" android:label="@string/app_name" android:theme="@style/AppTheme" android:debuggable="true"&gt; debuggable 属性有两个值“true|false”;只有Android:debuggable=”true”时我们才可以在手机上调试Android程序。但是当我们没在AndroidManifest.xml中设置其debug属性时:使用Eclipse运行这种方式打包时其debug属性为true,使用Eclipse导出这种方式打包时其debug属性为法false.在使用ant打包时，其值就取决于ant的打包参数是release还是debug.因此在AndroidMainifest.xml中最好不设置android:debuggable属性置，而是由打包方式来决定其值。如果设置了 android:debuggable=”true” 那么在正式打包时 把它设置成false吧！！！ 关注allowBackupandroid:allowBackup 属性的设置可能会引起用数据被任意备份的风险 12345&lt;application android:allowBackup="false" android:label="@string/app_name"&gt; ......&lt;/application&gt; Android API Level 8 及其以上 Android 系统提供了为应用程序数据的备份和恢复功能，此功能的开关决定于该应用程序中 AndroidManifest.xml 文件中的 allowBackup 属性值，其属性值默认是 True。当 allowBackup 标志为 true 时，用户即可通过 adb backup 和 adb restore 来进行对应用数据的备份和恢复。一旦应用程序支持备份和恢复功能，攻击者即可通过 adb backup 和 adb restore 进行恢复新安装的同一个应用来查看聊天记录等信息；对于支付金融类应用，攻击者可通过此来进行恶意支付、盗取存款等；因此为了安全起见，开发者务必将 allowBackup 标志值设置为 false 来关闭应用程序的备份和恢复功能，以免造成信息泄露和财产损失。 安全的打印日志如何打印日志？这不是很简单，直接使用android.util.Log这个类不就行了？然而，日志属于非常敏感的信息；逆向工程师在逆向你的程序的时候，本来需要捕捉你程序的各种输出，然后进行推测，顺藤摸瓜然后得到需要的信息；一旦你的日志泄漏，无异于门户洞开，破解你的程序如入无人之境。我们打印日志是用Log.d(TAG, msg);当把APK进行反编译后，TAG这个字符串会原封不动的还原出来，推理推理也就差不多了，不管你是否混淆过….安全的概念本来就是相对的，如果破解你程序的代价远远大于破解得到的价值，那么就可以认为程序是“安全的”；这里就分析一下，为了提高程序的安全性，在打印日志的时候应该注意什么。 让release版本里面不包含日志代码我们想要的是在开发的时候，正常打印日志；一旦需要发布版本，把所有打印日志的语句代码，全部删除掉。这里我们可以采用日志开关+proguard的方式来进行优化，关于proguard这个工具，很多认只是觉得他是一个代码混淆的工具，实际上，它还可以帮你剔除无用代码！无用代码就是类似下面的： 123if (true) &#123; // statement;&#125; 静态编译的时候被认为“永远不会执行的代码”，就被认为是无用代码，会被这个工具直接优化掉，生成的class文件里面，这个if语句直接就没有了。这个功能，完美符合我们的需求；我们只需要把输出日志的代码用这样的if语句包围起来，然后release的时候肯定会用这个工具混淆；然后，在release版本里面，所有的输出日志的代码全部都没有了！不会像以前一样，留下一个影子，只是不做事。所以我们这样写： 12345private static final boolean DEBUG = true; // 必须是static final 也就是常量，这样才能在编译器优化；删除if块if (DEBUG) &#123; android.util.Log.d(TAG, "msg to print");&#125; 那么当DEBUG变量为False的时候proguard可以理所当然地认为，这一部分代码时绝对不会被执行的，这样，打印日志的语句就会被优化（删除）掉.这里还需要注意的是，不要把打印日志进行封装，往里传个TAG和MSG，想省去写if包裹语句，这样的话就会使之前的工作失去作用，反编译后传参的部分会暴露出来….所以不要这么搞！如果你实在懒得打，AS的话有框架提示，打个ifd就会自动生成代码块！AS的话还有另一种方式，详情去参考里翻一翻。 SQLite数据库安全风险使用SQLite来存储数据却存在着一个问题。因为大多数的Android手机都是Root过的，而Root过的手机都可以进入到/data/data/&lt;package_name&gt;/databases目录下面，在这里就可以查看到数据库中存储的所有数据。如果是一般的数据还好，但是当涉及到一些账号密码，或者聊天内容的时候，我们的程序就会面临严重的安全漏洞隐患。我们可以借助SQLCipher来解决这个安全性问题。 SQLCipher是一个在SQLite基础之上进行扩展的开源数据库，它主要是在SQLite的基础之上增加了数据加密功能，如果我们在项目中使用它来存储数据的话，就可以大大提高程序的安全性。SQLCipher支持很多种不同的平台。 使用SQLCipher替换掉程序中的SQLite的数据。将SQLCipher数据包导入项目相应目录中，将原有的SQlite import文件修改为SQLCipher,在程序启动界面添加SQLiteDatabase.loadLibs(this)，并修改mysqlite.getWritableDatabase()方法首先创建一个MyDatabaseHelper继承自SQLiteOpenHelper,注意导入的包，除了导入的包不同，其他基本都和SQLiteOpenHelper相同。 1234567891011121314151617181920212223import android.content.Context; import net.sqlcipher.database.SQLiteDatabase; import net.sqlcipher.database.SQLiteDatabase.CursorFactory; import net.sqlcipher.database.SQLiteOpenHelper; public class MyDatabaseHelper extends SQLiteOpenHelper &#123; public static final String CREATE_TABLE = "create table Book(name text, pages integer)"; public MyDatabaseHelper(Context context, String name, CursorFactory factory, int version) &#123; super(context, name, factory, version); &#125; @Override public void onCreate(SQLiteDatabase db) &#123; db.execSQL(CREATE_TABLE); &#125; @Override public void onUpgrade(SQLiteDatabase db, int arg1, int arg2) &#123; &#125;&#125; 然后在使用到的Activity中这样写： 12345678910111213141516171819public class MainActivity extends Activity &#123; private SQLiteDatabase db; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); //首先将SQLCipher所依赖的so库加载进来 SQLiteDatabase.loadLibs(this); //创建实例 MyDatabaseHelper dbHelper = new MyDatabaseHelper(this, "demo.db", null, 1); //获取SQLiteDatabase对象，它接受一个字符串参数 //就是SQLCipher所依赖的key，在对数据库进行加解密的时候SQLCipher都将使用这里指定的key。 db = dbHelper.getWritableDatabase("secret_key"); //即可对db进行操作 //db.insert("Book", null, values); &#125;&#125; 需要注意的是：加入SQLCipher后会使APP的安装包增加几M，安全与体积要权衡好 Android签名安全现在Android逆向越来越火，并且相比PC端的EXE程序感觉Android的逆向还是很简单的，那就会面临着一个问题：会有人将APK进行反编译后修改代码然后进行二次打包发布，造成一些恶劣影响我们知道打包APK必然要进行签名，原始的签名密钥肯定是安全的唯一的，二次打包会改变APP的签名信息，我们在APP启动的时候进行签名对比，如果不一致就强制JVM退出当然了，没有绝对的安全，只要你逆向技术够高，这个是拦不住你的…. 获取签名获取签名我们可以采用两种方式，一种手动用keytool命令，还可以在代码里写 123456789101112131415161718//代码方式获取签名，根据包名private static Signature[] getRawSignature(Context context, String pkgName) &#123; if ((pkgName == null) || (pkgName.length() == 0)) &#123; return null; &#125; PackageManager pm = context.getPackageManager(); try &#123; PackageInfo pi = pm.getPackageInfo(pkgName, PackageManager.GET_SIGNATURES); if (pi == null) &#123; return null; &#125; //返回是个数字签名的数组，一般apk都是单签名的 //因此一般取Signature[0]做MD5，与已知签名的MD5信息做对比即可 return pi.signatures; &#125; catch (PackageManager.NameNotFoundException e) &#123; return null; &#125; &#125; 如果是手动用命令查的话，可以直接用RAR之类的打开APK文件，找到Apk文件中META-INF/CERT.RSA文件，解压，然后执行：keytool -printcert -file fileName就可以查到签名了PS:正常的签名文件查询命令是keytool -list -v -keystore filepath(后缀一般为keystore，其实并不需要后缀) 签名进行对比这里贴下主要代码： 1234567891011121314151617181920212223242526272829String signature="e79cf0a46d543ab6092b71f41d835543"; //正确的已知的签名String pack="com.bfchengnuo.demo"; //包名/*** 获取其他应用的签名信息，然后进行对比，错误强制系统退出* @param paramString*/private void getSign(String paramString) &#123; //getRawSignature方法返回的就是上面获得的签名MD5数组(类似，大概..) Signature[] arrayOfSignature = getRawSignature(this, paramString); if ((arrayOfSignature == null) || (arrayOfSignature.length == 0)) &#123; errout("signs is null"); return; &#125; int i = arrayOfSignature.length; //与已知的签名进行对比，如果全部匹配不成功则退出JVM //签名数组要进行MD5加密处理下，为了保险起见，这里把数组的每一个都进行了对比 for (int j = 0; j &lt; i; j++) stdout(MD5.getMessageDigest(arrayOfSignature[j].toByteArray()));&#125;private void stdout(String paramString) &#123; if(signature.equals(paramString)) &#123; // do thing Toast.makeText(this, "签名正确", 0).show(); &#125;else&#123; System.exit(0); &#125;&#125; BroadCastReceiver安全风险Android 可以在配置文件中声明一个receiver或者动态注册一个receiver来接收广播信息，攻击者假冒APP构造广播发送给被攻击的receiver，是被攻击的APP执行某些敏感行为或者返回敏感信息等，如果receiver接收到有害的数据或者命令时可能泄露数据或者做一些不当的操作，会造成用户的信息泄漏甚至是财产损失那么如何避免应用中注册的广播响应其他应用发送的广播呢，对于显式的广播除非是别人故意攻击，一般很少出现响应别人的广播，但是对于隐式的广播就很容易出现上述问题，因为action很容易是一样的，一旦是一样的就出问题了 解决方案 如果仅在应用内部通信，可以使用私有receiver，设置exported=&quot;false&quot;，该receiver可以接收相同应用程序组件或带有相同用户ID的应用程序所发出的消息[参考]。 1&lt;receiver android:name=".permittedReceiver" android:exported="false" /&gt; 若只在当前进程内通信，可以使用LocalBroadcastManager，使其他应用程序不能向该receiver发送广播 对于动态注册的广播registerReceiver(BroadcastReceiver, IntentFilter, String permission, android.os.Handler),指定receiver必须具备的permission。如果只允许自己的产品族使用，可以设置android:protectionLevel=&quot;signature&quot; ，若提供给其他APP使用，则设置android:protectionLevel=&quot;normal&quot;，同时要避免敏感信息的传递。其实就是自定义权限~~ 12345678&lt;permission android:name="com.android.permission.send_permission" android:protectionLevel="signature" /&gt;&lt;receiver android:name=".permittedReceiver" android:permission="com.android.permission.send_permission"&gt; &lt;intent-filter&gt; &lt;action android:name="com.android.permitted_ACTION" /&gt; &lt;/intent-filter&gt;&lt;/receiver&gt; 对接收来的广播进行验证，返回结果时需注意接收app是否会泄露信息 关于自定义权限的补充首先说一下protectionLevel这个属性： normal：默认的，应用安装前，用户可以看到相应的权限，但无需用户主动授权。 dangerous：normal安全级别控制以外的任何危险操作。需要dangerous级别权限时，Android会明确要求用户进行授权。常见的如：网络使用权限，相机使用权限及联系人信息使用权限等。 signature：它要求权限声明应用和权限使用应用使用相同的keystore进行签名。如果使用同一keystore，则该权限由系统授予，否则系统会拒绝。并且权限授予时，不会通知用户。它常用于应用内部。 上面的订阅方例子用到了signature这个值，多说一下，如果别的应用使用的不是同一个签名文件，就没办法使用该权限，从而保护了自己的接收者(可以理解为只接受拥有此权限的应用发送的广播)。发送方和订阅方都是需要加入这个权限的，只不过订阅方需要在注册接收器的时候再写一遍权限，上面的例子是静态注册receiver，如果用动态的方式注册那就是registerReceiver(receiver, filter, permission, null);，直接指定发送者应该具有的权限 android:permission —如果设置，具有相应权限的广播发送方发送的广播才能被此broadcastReceiver所接收 参考http://wolfeye.baidu.com/blog/recieve-broadcast-security/BroadcastReceiver安全问题 剪切板安全风险同一部手机中安装的其他app，甚至是一些权限不高的app，都可以通过剪贴板功能获取密码管理器中的账户密码信息。原因是Android剪贴板的内容向任何权限的app开放，很容易就被嗅探泄密，如上代码剪切板中存有明文内容，如果是明文内容将会有信息泄露的风险如下代码可以在任意APP中读取剪切板的内容： 123ClipboardManager cm = (ClipboardManager)getSystemService(CLIPBOARD_SERVICE);ClipData cd2 = cm.getPrimaryClip();str2 = cd2.getItemAt(0).getText().toString(); 所以，使用完clipboard及时清空，并避免使用剪贴板明文存储敏感信息 其他风险 在配置Database配置模式的时候要注意：避免使用MODE_WORLD_WRITEABLE和MODE_WORLD_READABLE模式创建数据库(Database)，最好还是使用MODE_PRIVATE模式 安卓SecureRandom安全:在Android 4.2以下，SecureRandom是基于老版的Bouncy Castle实现的。如果生成SecureRandom对象后马上调用setSeed方法。SecureRandom会用用户设置的seed代替默认的随机源。使得每次生成随机数时都是会使用相同的seed作为输入。从而导致生成的随机数是相同的。解决方案推荐：不要使用自定义随机源代替系统默认随机源，就是说不要调用以下函数： SecureRandom＃SecureRandom(byte[] seed) SecureRandom＃setSeed(long seed) SecureRandom＃setSeed(byte[] seed) 其实还可以在调用setSeed方法前先调用任意nextXXX方法(nextBytes(byte[] bytes))不过不推荐这种方式 现在基本上不用考虑了，毕竟已经到Android7.1+了，但是考虑到国内情况嘛…. 参考Android项目安全注意事项如何安全打印日志]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android安全开发初步(一)]]></title>
    <url>%2F2016%2F10%2F19%2FAndroid%E5%AE%89%E5%85%A8%E5%BC%80%E5%8F%91%E5%88%9D%E6%AD%A5-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[不看不知道，原来经常写的代码存在很多的安全问题，安全这一块着实不简单(我是这么认为的)，很多原理其实是看不太懂，只怪自己水平不够，买的几本Android进阶书还没看….哎~~意识到了时间的宝贵 Handler内存泄漏在使用Handler的时候，我们经常会写下面的一段代码，但是这样会导致严重的内存泄漏问题 12345678public class SampleActivity extends Activity &#123; private final Handler mLeakyHandler = new Handler() &#123; @Override public void handleMessage(Message msg) &#123; // ... &#125; &#125;&#125; 问题分析当Android应用程序启动时，framework会为该应用程序的主线程创建一个Looper对象。这个Looper对象包含一个简单的消息队列Message Queue，并且能够循环的处理队列中的消息。这些消息包括大多数应用程序framework事件，例如Activity生命周期方法调用、button点击等，这些消息都会被添加到消息队列中并被逐个处理。另外，主线程的Looper对象会伴随该应用程序的整个生命周期。 然后，当主线程里，实例化一个Handler对象后，它就会自动与主线程Looper的消息队列关联起来。所有发送到消息队列的消息Message都会拥有一个对Handler的引用，所以当Looper来处理消息时，会据此回调[Handler#handleMessage(Message)]当使用内部类（包括匿名类）来创建Handler的时候，Handler对象也会隐式地持有一个外部类对象（通常是一个Activity）的引用（不然你怎么可能通过Handler来操作Activity中的View？）这里补充一点java知识： 在java里，非静态内部类 和 匿名类 都会潜在的引用(持有)它们所属的外部类。但是，静态内部类却不会。个人理解：要不然你内部类怎么可以使用外部类的变量？还有.this这种形式 所以：只要有未处理的消息，那么消息会引用handler(通常在后台线程中)，非静态的handler又会引用外部类，即Activity，导致Activity无法被回收，造成泄漏。同理，如果你用匿名Runnable来实现消息的发送，它属于非静态匿名类，同样会引用外部类。 一般情况下，我们是等耗时操作完成后进行发送消息，进行处理，这时候Activity应该要正常销毁的(也可能在等待过程中用户退出了Activity)，上面的代码会造成其他的对象持有Activity的引用导致Activity无法被GC回收，导致内存泄漏。 解决方法通过程序逻辑来进行保护 在关闭Activity的时候停掉你的后台线程。线程停掉了，就相当于切断了Handler和外部连接的线，Activity自然会在合适的时候被回收。 如果你的Handler是被delay的Message持有了引用，那么使用相应的Handler的removeCallbacks()方法，把消息对象从消息队列移除就行了。 12345public void onDestroy() &#123; super.onDestroy(); //清除消息队列，也防止了内存泄漏 mHandler.removeCallbacksAndMessages(null);&#125; handler是线程通讯工具类。用于传递消息。它有两个队列：1.消息队列2.线程队列消息队列使用sendMessage和HandleMessage的组合来发送和处理消息。线程队列类似一段代码，或者说一个方法的委托，用户传递方法。使用post,postDelayed 添加委托，使用 removeCallbacks移除委托。这里看到两张图不错： 更多可参考：http://www.jianshu.com/p/e8f3c9e0b873 将Handler声明为静态类如果我们改为静态类，那么它不会持有外部的引用，Activity可以被GC回收了，就这样： 123456static class MyHandler extends Handler &#123; @Override public void handleMessage(Message msg) &#123; mImageView.setImageBitmap(mBitmap); &#125;&#125; 但是又出现了一个问题，应该注意到了，不会持有 Activity 的引用那么怎么可能操作 Activity 中的对象呢？我们可以加一个Activity 的弱引用 (WeakReference) 12345678910111213141516static class MyHandler extends Handler &#123; WeakReference&lt;Activity &gt; mActivityReference; //构造函数 MyHandler(Activity activity) &#123; mActivityReference= new WeakReference&lt;Activity&gt;(activity); &#125; @Override public void handleMessage(Message msg) &#123; final Activity activity = mActivityReference.get(); if (activity != null) &#123; mImageView.setImageBitmap(mBitmap); &#125; &#125;&#125; 知识补充什么是WeakReference？ WeakReference弱引用，与强引用（即我们常说的引用）相对，它的特点是，GC在回收时会忽略掉弱引用，即就算有弱引用指向某对象，但只要该对象没有被强引用指向（实际上多数时候还要求没有软引用，但此处软引用的概念可以忽略），该对象就会在被GC检查到时回收掉。对于上面的代码，用户在关闭Activity之后，就算后台线程还没结束，但由于仅有一条来自Handler的弱引用指向Activity，所以GC仍然会在检查的时候把Activity回收掉。这样，内存泄露的问题就不会出现了。 关于静态内部类 静态内部类只能访问外部类的静态成员。静态内部类的对象可以直接生成：Outer.Inner in=new Outer.Inner()；而不需要通过生成外部类对象来生成。 静态内部类和非静态内部类一样，都是在被调用时才会被加载所以可以这么理解：调用外部类的静态变量，静态方法可以让外部类得到加载，不过这里静态内部类没有被加载 其他地方ViewHolder内部类 定义为静态的，是一种好习惯。 其他很多人认为，静态变量能不用就不用，一旦静态生命周期必然很长，大部分情况下Handler用来更新UI，既然Activity被关闭了，那么Handler也没必要存在了，这种情况下也许采用第一种方法比较更好。但是也有说静态内部类和静态变量还是有区别的，这里并不会有什么大问题….静态内部类编译的时候是跟外部类同一级别的，使用static就有了限制，占用资源也就小了 还有就是，貌似 Android 对弱引用的支持并不太好，所以还是尽量少用 参考java静态内部类加载https://my.oschina.net/rengwuxian/blog/181449http://www.jianshu.com/p/cb9b4b71a820 代码混淆一般情况下我们使用ProGuard工具来提供代码混淆PS:因为特殊原因这里是在Eclipse下进行混淆，现在基本都是AS了，那个以后再来补充，其实都差不多，AS下更加简单吧.. ProGuard是什么ProGuard是一个工具，用来混淆和优化Java代码。工作方式：移除无效的代码，将代码中的类名、函数名替换为晦涩难懂的名字。注意，它只能混淆Java代码，Android工程中Native代码，资源文件（图片、xml），它是无法混淆的。玩过Android逆向的知道，经过混淆后的类反编译出来名都是单个字母，完全看不懂 如何开启混淆修改Android工程根目录下的project.properties文件，把proguard.config=….这一行前面的注释“#”去掉。这一行指定了系统默认的proguard配置文件，位于Android SDK/tools/proguard目录下。当然，你也可以自己编写配置文件，但不建议这样做，因此系统默认的配置已经涵盖了许多通用的细节，如果你还有额外的配置，可以添加在 proguard-project.txt 文件中。注意： 只有在生成release版本的apk时，混淆配置才会起作用，debug版本的apk不会进行混淆。 那些需要手动配置系统默认的配置已经涵盖了大部分的内容，但是如果你的工程中有如下内容，则需要手动添加配置到proguard-project.txt文件中。 只在 AndroidManifest.xml 引用的类 通过JNI回调方式被调用的函数 运行时动态调用的函数或者成员变量 反射用到的类 WebView中JavaScript调用的方法 Layout文件引用到的自定义View 一些引入的第三方库（一般都会有混淆说明的） 这里推荐两个开源项目，里面收集了一些第三方库的混淆规则 android-proguard-snippets android-proguard-cn 当然，如果你不确定哪些需要手动配置，可以以默认的配置生成程序，当运行中发现ClassNotFoundException异常时，即可找到哪个类不该被混淆。 手动配置的规则手动添加的配置，一般以“-keep”开头 12345678910111213141516171819202122232425262728293031323334//不混淆Test的构造函数-keepclassmembers classcom.example.Test &#123; public &lt;init&gt;(int,int);&#125;//不混淆package com.example下的所有类/接口-keep class com.example.** &#123; * ; &#125;//不混淆com.example.Test类:-keep class com.example.Test &#123; * ; &#125;/*如果希望不混淆某个接口，则把上述命令中的class替换为interface即可。*///不混淆特定的函数-keepclassmembers classcom.example.Test &#123; public void setTestString(java.lang.String);&#125;//不混淆com.Test类的子类-keep public class * extends com.example.Test//不混淆com.example.TestInterface的实现-keep class * implements com.example.TestInterface &#123; public static final com.example.TestInterface$Creator *;&#125;//排除第三方依赖android-support-v4为例，AS中已自动处理无需手动添加-libraryjarslibs/android-support-v4.jar-dontwarnandroid.support.v4.**&#123;*;&#125;-keep class android.support.v4.**&#123;*;&#125;-keep interface android.support.v4.**&#123;*;&#125;/*注意： 需要添加dontwarn，因为默认情况下proguard会检查每一个引用是否正确，但是第三方库里往往有些不会用到的类，没有正确引用，所以如果不配置的话，系统会报错。*/ 参考http://ticktick.blog.51cto.com/823160/1413066https://segmentfault.com/a/1190000004461614 Activity安全 有些说实话还没完全看懂，是太心急了么？逼不得已… exported不准备对外公开的activity一定要设置为非公开，以防止被人非法调用(反编译下很容易就能找到，至于非法调用用于什么坏事我的安全意识还想不出来，反正关掉就是了，也许是用来做钓鱼页面)同时，一定要注意的是， 非公开的Activity不能设置intent-filter因为，如果假设在同一机器上，有另外一个app有同样的intent-filter的话， 调用该Activity的intent会唤醒Android的选择画面， 让你选择使用那个app接受该intent。这样就会事实上绕过了非公开的设置。默认值：如果包含有intent-filter 默认值为true; 没有intent-filter默认值为false。 1234&lt;activityandroid:name=".PrivateActivity"android:label="@string/app_name"android:exported="false" /&gt; 如果设置了导出权限，都可能被系统或者第三方的应用程序直接调出并使用。 组件导出可能导致登录界面被绕过、信息泄露、数据库SQL注入、DOS、恶意调用等风险。 主要作用是：是否支持其它应用调用当前组件。 更多请参考：android:exported 属性详解 合理的使用exported我们接下来谈谈在开发中如何更合理设置exported。Activity被调用的场景分为3种：封闭式、半封闭式和开放式 封闭式被调用的Activity与调用的Context必须在同一个App，其他任何App不能调用这种是我们最常见的Activity，有2种情况： 没有intent-filter情况可以不设置exported或者设置exported为false 12345678&lt;activity android:name=".SecondActivity" android:label="@string/app_name" /&gt;&lt;!-- 或 --&gt;&lt;activity android:name=".SecondActivity" android:label="@string/app_name" android:exported="false"/&gt; 没有intent-filter情况必须设置exported为false 1234&lt;activity android:name=".SecondActivity" android:label="@string/app_name" android:exported="false"/&gt; 半封闭式被调用的Activity只能被部分其他App调用，如同一个公司的2个App之间这种场景下，除了满足封闭式设置外，还必须把调用App和被调用App设置相同的uid，即在2个App的AndroidManifest.xml添加相同的android:sharedUserId，如 123&lt;manifest xmlns:android="http://schemas.android.com/apk/res/android" ... android:sharedUserId="com.example.categorytest"&gt; 开放式可以被任何App调用 这种场景主要是对外接口，如微信、微博的分享接口。大多数情况下，这样的Activity都会有intent-filter，因此也没必要显式地把exported设为true，不设是可以的，当然不能设为false。但如果你没有intent-filter，那就必须显式地把exported设为true。 当然，对于三方app接口的intent-filter设置还有一些要求，如在隐式intent调用必须添加android.intent.category.DEFAULT 补充：关于主Activity，应用程序需要包含至少一个Activity组件来支持MAIN操作和LAUNCHER种类，即为主Activity暴露的Activity组件不包括主Activity，如果你把主Activity设置exported为false了，那你的应用就甭想运行了，正常的应用来说。 参考：http://blog.csdn.net/gorgle/article/details/51420586http://www.itdadao.com/articles/c15a12377p0.html 不要指定taskAffinityAndroid中的activity全都归属于task管理 ， 简单说来task是一种stack(堆栈)的数据结构， 先入后出。一般来说， 如果不指明归属于什么task， 同一个app内部的所有Activity都会存续在一个task中，task的名字就是app的packageName。因为在同一个andorid设备中，不会有两个同packageName的app存在，所以能保证Activity不被攻击。 1234&lt;activity android:name=".Activity1" android:taskAffinity="com.winuxxan.task" android:label="@string/app_name"&gt;&lt;/activity&gt; 恶意软件中的Activity如果也声明为同样的taskAffinity，那他的Activity就会启动到你的task中，就会有机会拿到你的intent(我们一般会在intent中对数据进行加密处理) 那么taskAffinity到底什么用呢？它的作用是描述了不同Activity之间的亲密关系。拥有相同的taskAffinity的Activity是亲密的，它们之间在相互跳转时，会位于同一个task中，而不会新建一个task!简单说就是意味着这activity更喜欢哪个TESK 一个新的activity，默认地启动到调用startActivity()方法的activity的task中。它和调用者放到同样的back stack中。然而，如果传递给startActivity()的intent包含FLAG_ACTIVITY_NEW_TASK标志，系统将会需找一个不同的task来容纳新的activity。通常，它是一个新的task。然而，不是必须都是如此的。如果已经存在一个和新的activity具有相同的affinity的task，新activity会启动到该task中。如果没有，它会启动一个新的task。 当一个activity它的allowTaskReparenting属性设置为true这种情况，activity可以从它启动的task移到和它有相同affinity的task，当该task来到前台的时候。 不要指定LaunchMode(默认standard模式)Android中Activity的LaunchMode分成以下四种 Standard这种方式打开的Activity不会被当作rootActivity，会生成一个新的Activity的instance(实例)，会和打开者在同一个task内 singleTop和standard基本一样，唯一的区别在于如果当前task第一个Activity就是该Activity的话，就不会生成新的instance singleTask系统会创建一个新task(如果没有启动应用)和一个activity新实例在新task根部，然后，如果activity实例已经存在单独的task中，系统会调用已经存在activity的 onNewIntent()方法，而不是存在新实例，仅有一个activity实例同时存在。 singleInstance和singleTask相似，除了系统不会让其他的activities运行在所有持有的task实例中，这个activity是独立的，并且task中的成员只有它，任何其他activities运行这个activity都将打开一个独立的task。 所有发送给root Activity(根Activiy)的intent都会在android中留下履历(入侵关键，大概)。所以一般来说严禁用singleTask或者singleInstance来启动画面。然而，即使用了standard来打开画面，也可能会出问题，比如如果调用者的Activity是用singleInstance模式打开，即使用standard模式打开被调用Activity，因为调用者的Activity task是不能有其他task的， 所以android会被迫生成一个新的task，并且把被调用者塞进去，最后被调用者就成了rootActivity。 12345678910111213141516171819202122&lt;application android:icon="@drawable/ic_launcher" android:label="@string/app_name" &gt; &lt;!-- root Activity以”singleInstance”模式启动 --&gt; &lt;!-- 不设置taskAffinity--&gt; &lt;activity android:name=".PrivateUserActivity" android:label="@string/app_name" android:launchMode="singleInstance" &gt; &lt;intent-filter&gt; &lt;action android:name="android.intent.action.MAIN" /&gt; &lt;category android:name="android.intent.category.LAUNCHER" /&gt; &lt;/intent-filter&gt; &lt;/activity&gt; &lt;!-- 非公開Activity --&gt; &lt;!-- 启动模式为”standard” --&gt; &lt;!-- 不设置taskAffinity--&gt; &lt;activity android:name=".PrivateActivity" android:label="@string/app_name" android:exported="false" /&gt;&lt;/application&gt; FLAG_ACTIVITY_NEW_TASK发给Activity的intent不要设定为FLAG_ACTIVITY_NEW_TASK就算上面的Activity的lauchMode设置完善了， 在打开intent的时候还是能指定打开模式。比如在intent中指明用FLAG_ACTIVITY_NEW_TASK模式的话，发现该activity不存在的话，就会强制新建一个task。如果同时设置了FLAG_ACTIVITY_MULTIPLE_TASK+FLAG_ACTIVITY_NEW_TASK，就无论如何都会生成新的task，该Activity就会变成rootActiviy，并且intent会被留成履历 Intent中数据的加密这个前面提到过，Activity中数据的传递都依靠intent， 很容易被攻击， 所以 就算同一个app内部传递数据， 最好还是要加密， 加密算法很多。 明确ActivityName发送Intent避免被恶意软件所截取，主要是注意在不同APP直接的数据发送 12345678910Intent intent = new Intent();intent.setClassName("org.jssec.android.activity.publicactivity","org.jssec.android.activity.publicactivity.PublicActivity");startActivity(intent);//另一种方式Intent intent = new Intent(); intent.setComponent(new ComponentName("com.example.otherapp", "com.example.otherapp.MainActivity2")); startActivity(intent); 不是指明了packageName和ActivityName就能避免所有的问题,如果有一个恶意软件故意做成和你发送目标同PackageName, 同ActivityName， 此时的intent就会被截取.对于这两种方式经查看源码发现 Intent 的setClass() 方法的实现正是使用ComponentName 类： 1234public Intent setClass(Context packageContext,Class&lt;?&gt; cls)&#123; mComponent=new ComponentName(packageContext,cls); return this;&#125; 接收intent时明确对方的身份一个好方法是比对对方的app的hashcode。当前，前提是调用者要用startActivityForResult()，因为只有这个方法，被调用者才能得到调用者的packageName intent数据泄漏到LogCat123456789//Intent中发送的数据就会被自动写入LogCatUri uri = Uri.parse("mailto:test@gmail.com");Intent intent = new Intent(Intent.ACTION_SENDTO, uri);startActivity(intent);//这样写就能避免Uri uri = Uri.parse("mailto:");Intent intent = new Intent(Intent.ACTION_SENDTO, uri);intent.putExtra(Intent.EXTRA_EMAIL, new String[] &#123;"test@gmail.com"&#125;);startActivity(intent); 其他注意下这个权限&lt;uses-permission android:name=&quot;android.permission.GET_TASKS&quot; /&gt;有了这个权限就能取出这台手机上所有task上所有根Activity接受到的intent，大概所以：所有根Activity中的intent都能被所有app共享 补充关于上面介绍的那些每一个其实都大有学问呐，不过都没深入进去追究，目前水平也是有限。这里看到了LaunchMode，再补充下它的应用场景吧： singleTop适合接收通知启动的内容显示页面。例如，某个新闻客户端的新闻内容页面，如果收到10个新闻推送，每次都打开一个新闻内容页面是很烦人的。singleTask适合作为程序入口点。例如浏览器的主界面。不管从多少个应用启动浏览器，只会启动主界面一次，其余情况都会走onNewIntent，并且会清空主界面上面的其他页面。singleInstance适合需要与程序分离开的页面。例如闹铃提醒，将闹铃提醒与闹铃设置分离。singleInstance不要用于中间页面，如果用于中间页面，跳转会有问题，比如：A -&gt; B (singleInstance) -&gt; C，完全退出后，在此启动，首先打开的是B。 TaskAffinity对LaunchMode的影响:不指定TaskAffinity，singleTask会在默认的task 中执行，这个符合预期，一般也都是这么用的，不需要指定。不指定TaskAffinity，singleInstance之后启动的页面不能放倒singleInstance所在那个task中，会放倒默认的task中，不过一般singleInstance也不适合作为程序中间页。 http://blog.csdn.net/xiaodongrush/article/details/28597855 参考http://blog.csdn.net/thestoryoftony/article/details/9370427http://droidyue.com/blog/2015/08/16/dive-into-android-activity-launchmode/index.html]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS/jQuery选择器总结]]></title>
    <url>%2F2016%2F10%2F04%2Fcss-jQuery%E9%80%89%E6%8B%A9%E5%99%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[因为一些原因，最近在复习CSS相关的知识，果然，选择器部分直接是一点印象都没了，并且google找了一会也没看到很全的总结，好不容易找到一个，存档以便日后翻阅虽然我并不喜欢CSS然而还是要了解一些东西的…其中的大部分选择器也是可以直接在JQ中用的，我个人认为… CSS选择器部分基本选择器 选择器 含义 * 通用元素选择器，匹配任何元素 E 标签选择器，匹配所有使用E标签的元素 .info class选择器，匹配所有class属性中包含info的元素 #footer id选择器，匹配所有id属性等于footer的元素 多元素的组合选择器 选择器 含义 E,F 多元素选择器，同时匹配所有E元素或F元素，E和F之间用逗号分隔 E F 后代元素选择器，匹配所有属于E元素后代的F元素，E和F之间用空格分隔 E &gt; F 子元素选择器，匹配所有E元素的子元素F E + F 毗邻元素选择器，匹配所有紧随E元素之后的同级元素F E.classname 交叉选择器，之间没有空格，选择E标签下的类名为classname的元素，类似的有E#idName CSS 2.1 属性选择器由于MD语法的原因|使用¦代替 选择器 含义 E[att] 匹配所有具有att属性的E元素，不考虑它的值。（注意：E在此处可以省略，比如”[cheacked]”。以下同。） E[att=val] 匹配所有att属性等于”val”的E元素 E[att~=val] 匹配所有att属性具有多个空格分隔的值、其中一个值等于”val”的E元素 E[att¦=val] 匹配所有att属性具有多个连字号分隔（hyphen-separated）的值、其中一个值以”val”开头的E元素，主要用于lang属性，比如”en”、”en-us”、”en-gb”等等 CSS 2.1中的伪类 选择器 含义 E:first-child 匹配父元素的第一个子元素 E:link 匹配所有未被点击的链接 E:visited 匹配所有已被点击的链接 E:active 匹配鼠标已经其上按下、还没有释放的E元素 E:hover 匹配鼠标悬停其上的E元素 E:focus 匹配获得当前焦点的E元素 E:lang(c) 匹配lang属性等于c的E元素 CSS 2.1中的伪元素 选择器 含义 E:first-line 匹配E元素的第一行 E:first-letter 匹配E元素的第一个字母 E:before 在E元素之前插入生成的内容 E:after 在E元素之后插入生成的内容 CSS 3的同级元素通用选择器 选择器 含义 E ~ F 匹配任何在E元素之后的同级F元素 CSS 3 属性选择器 选择器 含义 E[att^=”val”] 属性att的值以”val”开头的元素 E[att$=”val”] 属性att的值以”val”结尾的元素 E[att*=”val”] 属性att的值包含”val”字符串的元素 CSS 3中与用户界面有关的伪类 选择器 含义 E:enabled 匹配表单中激活的元素 E:disabled 匹配表单中禁用的元素 E:checked 匹配表单中被选中的radio（单选框）或checkbox（复选框）元素 E::selection 匹配用户当前选中的元素 CSS 3中的结构性伪类 选择器 含义 E:root 匹配文档的根元素，对于HTML文档，就是HTML元素 E:nth-child(n) 匹配其父元素的第n个子元素，第一个编号为1 E:nth-last-child(n) 匹配其父元素的倒数第n个子元素，第一个编号为1 E:nth-of-type(n) 与:nth-child()作用类似，但是仅匹配使用同种标签的元素 E:nth-last-of-type(n) 与:nth-last-child() 作用类似，但是仅匹配使用同种标签的元素 E:last-child 匹配父元素的最后一个子元素，等同于:nth-last-child(1) E:first-of-type 匹配父元素下使用同种标签的第一个子元素，等同于:nth-of-type(1) E:last-of-type 匹配父元素下使用同种标签的最后一个子元素，等同于:nth-last-of-type(1) E:only-child 匹配父元素下仅有的一个子元素，等同于:first-child:last-child或 :nth-child(1):nth-last-child(1) E:only-of-type 匹配父元素下使用同种标签的唯一一个子元素，等同于:first-of-type:last-of-type或 :nth-of-type(1):nth-last-of-type(1) E:empty 匹配一个不包含任何子元素的元素，注意，文本节点也被看作子元素 CSS 3的反选伪类 选择器 含义 E:not(s) 匹配不符合当前选择器的任何元素 CSS 3中的 :target 伪类 选择器 含义 E:target 匹配文档中特定”id”点击后的效果 jQ选择器部分偷个懒，直接从W3C复制过来的，但貌似也并不全，更详细的可以去下载API 选择器 实例 选取 * $(“*”) 所有元素 #id $(“#lastname”) id=”lastname” 的元素 .class $(“.intro”) 所有 class=”intro” 的元素 element $(“p”) 所有 &lt;p&gt; 元素 .class.class $(“.intro.demo”) 所有 class=”intro” 且 class=”demo” 的元素 :first $(“p:first”) 第一个 &lt;p&gt; 元素 :last $(“p:last”) 最后一个 &lt;p&gt; 元素 :even $(“tr:even”) 所有偶数 &lt;tr&gt; 元素 :odd $(“tr:odd”) 所有奇数 &lt;tr&gt; 元素 :eq(index) $(“ul li:eq(3)”) 列表中的第四个元素（index 从 0 开始） :gt(no) $(“ul li:gt(3)”) 列出 index 大于 3 的元素 :lt(no) $(“ul li:lt(3)”) 列出 index 小于 3 的元素 :not(selector) $(“input:not(:empty)”) 所有不为空的 input 元素 :header $(“:header”) 所有标题元素 &lt;h1&gt; - &lt;h6&gt; :animated 所有动画元素 :contains(text) $(“:contains(‘W3School’)”) 包含指定字符串的所有元素 :empty $(“:empty”) 无子（元素）节点的所有元素 :hidden $(“p:hidden”) 所有隐藏的 &lt;p&gt; 元素 :visible $(“table:visible”) 所有可见的表格 s1,s2,s3 $(“th,td,.intro”) 所有带有匹配选择的元素 [attribute] $(“[href]”) 所有带有 href 属性的元素 [attribute=value] $(“[href=’#’]”) 所有 href 属性的值等于 “#” 的元素 [attribute!=value] $(“[href!=’#’]”) 所有 href 属性的值不等于 “#” 的元素 [attribute$=value] $(“[href$=’.jpg’]”) 所有 href 属性的值包含以 “.jpg” 结尾的元素 :input $(“:input”) 所有 &lt;input&gt; 元素 :text $(“:text”) 所有 type=”text” 的 &lt;input&gt; 元素 :password $(“:password”) 所有 type=”password” 的 &lt;input&gt; 元素 :radio $(“:radio”) 所有 type=”radio” 的 &lt;input&gt; 元素 :checkbox $(“:checkbox”) 所有 type=”checkbox” 的 &lt;input&gt; 元素 :submit $(“:submit”) 所有 type=”submit” 的 &lt;input&gt; 元素 :reset $(“:reset”) 所有 type=”reset” 的 &lt;input&gt;元素 :button $(“:button”) 所有 type=”button” 的 &lt;input&gt; 元素 :image $(“:image”) 所有 type=”image” 的 &lt;input&gt; 元素 :file $(“:file”) 所有 type=”file” 的 &lt;input&gt; 元素 :enabled $(“:enabled”) 所有激活的 input 元素 :disabled $(“:disabled”) 所有禁用的 input 元素 :selected $(“:selected”) 所有被选取的 input 元素 :checked $(“:checked”) 所有被选中的 input 元素 参考原文作者： 阮一峰原文地址：http://www.ruanyifeng.com/blog/2009/03/css_selectors.html jQuery选择器：http://www.w3school.com.cn/jquery/jquery_ref_selectors.asp]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>CSS</tag>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript学习笔记]]></title>
    <url>%2F2016%2F10%2F02%2FJavaScript%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[js是一门基于对象的弱类型语言，至于用处可谓是无处不在，GitHub语言排行榜稳稳的第一，JS终究一统世界…不来学习下？起码前半部分还是java呢！虽然和java并无啥关系还有一个原因是，这几天微信小程序弄的沸沸扬扬，我也漫无目的的去学习了下，也涉及到了js，微信小程序已经内测，你的js知识储备准备好了么？？o(￣▽￣)ゞ))￣▽￣)o写的比较混乱 = =！因为感冒嘛~ 开始js一般要和html、css配合起来用，所以还是要有一些html之类的知识的，嘛~不就是一堆标签吗？学习下也用不了多长时间，js的代码我们一般可以写在两个地方： html文件中&lt;script type=&quot;text/javascript&quot;&gt;这里写js代码&lt;/script&gt;可以放在页面的任何位置，但是我们一般放在网页的head或者body部分。如果在head部分：浏览器解析head部分就会执行这个代码，然后才解析页面的其余部分。如果在body部分：网页读取到该语句的时候就会执行，执行完后再读取下面的 外部js文件写在单独的扩展名为.js的文件中然后在html中引用，js文件中不需要再加标签，引用方式：&lt;script type=&quot;text/javascript&quot; src=&quot;script.js&quot;&gt;&lt;/script&gt;，注意的是标签中间不能再写js代码，如果需要补充代码的话可以再写一个标签。 一般来说，script 要放到 body 的最下面，提高加载速度，如果放到 head 里那就会影响 HTML 结构的渲染，理论上说放那都行，但是规范上 js 属于正文部分，所以最好是放在 body 里面。但是呢，如果页面引用了多个文件，不能保证在用时已经加载完毕，所以很多选择性放在头部还是尾部。http://sfau.lt/b5saPH 基础语法很多地方其实还是和其他语言有很大相同点的，这里主要总结一些和其他不同的地方，因为也没学过什么解释型语言。Js中有一种叫严格相等===，也就是比较对象而不是数值 变量js是弱类型语言，所以变量的类型不会像java那样细分，定义变量只有一个关键字 var使用就是：var 变量名理论上名称是可以任意取名，同样最好遵守下命名规范： 1.变量必须使用字母、下划线(_)或者美元符($)开始。 2.然后可以使用任意多个英文字母、数字、下划线(_)或者美元符($)组成。 3.不能使用JavaScript关键词与JavaScript保留字。 更详细的在这：https://github.com/fex-team/styleguide/blob/master/javascript.md 注意:变量也可以不声明，直接使用，但为了规范，需要先声明，后使用，不声明的话就会是(隐式)全局变量[定义全局变量的正统方式]当变量没有初始化就被使用 就会显示undefinedjs 中 false、null 就是0，非 0 就是true 是可以运算的如： false + 1 = 1 ; true + 1 = 2或者说，null、0、undefined、’’（空字符串） 这些都是 false，可以（可能是部分浏览器）通过 if 判断 函数和属性因为js中没有class(类)的概念，所以函数很重要！基本定义如下： 12345678910111213141516171819//常规定义function 函数名()&#123; //函数代码;&#125;//函数的参数不用特意声明function add(x,y,z)&#123; x = 4;&#125;//动态函数,a、b是参数，后面是运算逻辑var x = new Function("a","b","return a+b");//与其java类似的一个需要注意的问题var a = 3;show(a);function show(x)&#123; x = 5;&#125;//最后的结果a还是3，注意它们的范围 以及一个简单的调用：&lt;input type=&quot;button&quot; value=&quot;点击我&quot; onclick=&quot;funName()&quot; /&gt;注意：js中的函数没有重载（函数其实就是一个对象，对象只有覆盖），所有的参数被函数内的arguments所接收，即使是空参的，所以说空参的函数你也可以向里传参数，同时也可以传部分参数，你没传的参数默认就是undefined了js中函数就是对象，var x = show 不加括号的话相当于直接指向了那个对象，加括号相当于进行了运算，然后把运算后的值给x(和最近学的Py好像)可以被封装起来用JS中所有.的调用都可以替换成[&#39;&#39;]，他们是等价的，在拼接的时候只能用第二种方式，下面两句是等价的： 12obj.prop = "abc";obj["prop"] = "abc"; 函数还可以封装起来当作java中的class来使用，涉及到定义属性，JavaScript中有三种不同类型的属性:命名数据属性，命名访问器属性以及内部属性，关于属性其实上面多少已经使用了一些，几个例子： 123456789101112131415161718192021function Test(name)&#123; this.name = name;&#125;var a = new Test("loli");//属性可以直接赋值不用定义a.item = 1;a.t = "aaa";alert(a.t + a.name);//函数和方法的区别var arr = [1,2,3,4,5]var a =12; // 变量：自由的arr.a= 5; //属性：属于一个对象function show() //函数：自由的&#123; alert("a");&#125;arr.fn = function() //方法：属于一个对象&#123; alert("b");&#125; 其实还可以定义一个空函数用来做构造器，构造函数首字母一般大写，用来区分 1234function Class()&#123;&#125;//设置原型Class.prototype=&#123;&#125;;var item=new Class(); 这里首先声明下，js中的{}一般代表定义一个对象，大部分情况下是要有成对的属性或值，或函数 命名数据属性这个是我们通常所用的“普通”属性，感觉挺像json的格式的 123456789var obj = &#123; prop: 123&#125;;//读取(获取)值console.log(obj.prop); // 123console.log(obj["prop"]); // 123//写入值obj.prop = "abc";obj["prop"] = "abc"; 命名访问器属性概括一下就是：借助函数来获取或设置一个属性的值 12345678var obj = &#123; get prop() &#123; return "Getter"; &#125;, set prop(value) &#123; console.log("Setter: "+value); &#125;&#125; 调用后的效果： —&gt;obj.prop‘Getter’ —&gt;obj.prop = 123;Setter: 123 内部属性有一些属性仅仅是为规范所用的,称之为内部属性,因为它们无法通过JavaScript直接访问到,但是它们的确存在,并且影响着程序的表现.内部属性的名称比较特殊,它们都被两个中括号包围着.下面有两个例子: 内部属性[[Prototype]]指向了所属对象的原型.该属性的值可以通过Object.getPrototypeOf()函数读取到.该属性的值只能在创建一个新对象的时候通过Object.create()或者proto来设置 [1].例如：Object.getPrototypeOf(this).methodName1();可以看下面的对象原型标题 内部属性[[Extensible]]决定了是否能给所属对象添加新的属性.该属性的值可以通过Object.isExtensible()读取到.还可以通过Object.preventExtensions()将该属性的值设置为false.一旦设置为false,就无法再设置回true了. 关于匿名函数在javascript语言里任何匿名函数都是属于window对象。在定义匿名函数时候它会返回自己的内存地址，如果此时有个变量接收了这个内存地址，那么匿名函数就能在程序里被使用了，因为匿名函数也是在全局执行环境构造时候定义和赋值，所以匿名函数的this指向也是window对象。这里为什么要单独说下匿名函数呢，因为它好玩啊！还可以这样玩： 12345678//自执行函数var result = function ()&#123; alert(2);&#125;();//或者这样写var result = (function () &#123; console.log(2);&#125;)(); with语句和for…in with语句主要是为了调用对象的方法可以省略函数名 123456function a(name)&#123; this.name = name;&#125;with(a)&#123; name = "bbb"; //不用写a.name&#125; for…in 可以用来遍历对象。遍历出的是变量名(假设理解为key:values可以理解为key)，如果取值直接用.获取值是不行的，没法进行字符串的连接，所以要用[]的方式来解决遍历数组得到的是角标，同样需要arr[x]取值 关于 for in 及循环来了解一下： 1234567891011var array = [0,1,2,3,4,5,6,7,8,9];// for-in 循环for (var val in array) &#123; fn(val);&#125;// for 循环for (var i=0; i &lt; array.length; i++) &#123; fn(array[i]);&#125; 在 for-in 需要分析出 array 的每个属性，这个操作的性能开销很大，用在 key 已知的数组上是非常不划算的。所以尽量不要用 for-in，除非你不清楚要处理哪些属性，例如 JSON 对象这样的情况。在第二种循环中，效率也不高，循环每执行一次，都要检查一次 array.length 的值，读属性要比读局部变量慢，尤其是当 array 里存放的都是 DOM 元素（像 array = document.getElementByClassName(&quot;class&quot;);），因为每次读 array.length 都要扫描一遍页面上 class=”class” 的元素，速度更是慢得抓狂。下面就介绍几种优化后的代码： 123456789101112var array = [0,1,2,3,4,5,6,7,8,9];// for 循环for (var i = array.length; i--;) &#123; fn(array[i]);&#125;// while 循环var i = array.length;while (i--) &#123; fn(array[i]);&#125; 这样就避免了每次判断 length 的尴尬情况。 其他补充 关于回调js中的回调相比java就简单太多了，不需要接口啥的，因为变量可以指向函数嘛~所以到时候直接”执行”那个变量就好了嘛… js中的全局函数escape( )、eval( )、isFinite( )、isNaN( )、parseFloat( )、parseInt( )、unescape( )等 更多关于函数的解释见：https://segmentfault.com/a/1190000000660786#articleHeader11 对象原型js中很多内部对象是拥有prototype属性的，对于拥有prototype属性的对象可以在原型的基础上增加功能扩展。下面的例子可以很好的说明这个功能，我们在string对象的基础上增加去空格和转置的功能： 12345678910111213141516171819202122232425262728//去除前后空格function trim() &#123; var start,end; start = 0; end = this.length-1; while(start &lt;= end &amp;&amp; this.charAt(start) == " ")&#123; start++; &#125; while(start &lt;= end &amp;&amp; this.charAt(end) == " ")&#123; end--; &#125; return this.substring(start,end+1);&#125;//转置函数function reverse() &#123; var str = ""; for (var i = this.length - 1; i &gt;= 0; i--) &#123; str += this.charAt(i); &#125; return str;&#125;//获取对象的原型，扩展功能,注意String的S的大写String.prototype.trim = trim;String.prototype.reverse = reverse; DOMDOM：document object model 文档对象模型，简单说：将文档和标签以及其他内容变成对象。DOM分为三层模型dom1: 将html文档封装为对象dom2: 在leve 1基础上加入了新功能，比如解析名称空间。dom3: 将XML文档封装成对象 对于dom1的封装，可以视为将html的标签的层级关系封装成了树形结构，称为DOM树，每一个节点(标签)都是一个对象，能更好的进行管理(比如创建、删除、修改)，能够动态的改变html的结构。 MDN: 文档对象模型 (DOM) 是HTML和XML文档的编程接口。它提供了对文档的结构化的表述，并定义了一种方式可以使从程序中对该结构进行访问，从而改变文档的结构，样式和内容。DOM 将文档解析为一个由节点和对象（包含属性和方法的对象）组成的结构集合。简言之，它会将web页面和脚本或程序语言连接起来。 说白了DOM就是浏览器为JavaScript提供的一系列接口（通过window.documnet提供的），通过这些接口我们可以操作web页面（JS 无法直接修改页面结构）。 但DOM并不是编程语言，它是文档对象的模型，该模型是独立于编程语言的。 关于DHTML：是多个技术的综合体，叫做动态的htmlDHTML=HTML+CSS+JavaScript+DOM 补充：在DHTML基础上加了轻量级的与服务器交互的功能就成了AJAX，DHTML+XMLhttpRequest = AJAX，比如google的搜索词预测 关于解析对于DOM解析方式好处：可以对树中的节点进行任意操作，比如：增删改查。弊端：这种解析需要将整个标记型文档加载进内存。意味着如果标记型文档的体积很大，较为浪费内存空间。 另一种解析方式：SAX：是由一些组织定义的一种民间常用的解析方式，并不是w3c标准，而DOM是W3C的标准。SAX解析的方式：基于事件驱动的解析。随加载，随解析(读取到一个标签的结束标记就进行解析)获取数据的速度很快，但是不能对标记进行增删改。 关于节点比较常见的又三种DOM节点： 元素节点：上图中&lt;html&gt;、&lt;body&gt;、&lt;p&gt;等都是元素节点，即标签。 文本节点:向用户展示的内容，如&lt;li&gt;...&lt;/li&gt;中的JavaScript、DOM、CSS等文本。 属性节点:元素属性，如&lt;a&gt;标签的链接属性href 节点名称(nodeName)元素节点的 nodeName 与标签名相同属性节点的 nodeName 是属性的名称文本节点的 nodeName 永远是 #text文档节点的 nodeName 永远是 #document节点的类型(type)：标签型节点：1属性节点：2文本型节点：3注释型节点：8document：9 (它的范围就是浏览器中整个显示网页的区域)获取节点的方式获取的方法有多种，获取子节点、父节点、兄弟节点之类的….注意：标签之间如果存在空行，有的浏览器会认为是一个空白的文本节点 获取元素一般我们常用的就是三种： 123456//通过ID进行获取,window可省略document.getElementById("id");//通过标签名获取getElementsByTagName("tagName");//通过指定的名称getElementsByName("name"); 第一种返回的就是一个对象，后面的两个返回的是一个对象的集合，可以理解为数组，因为id只能有一个，其他的就不确定了。获取到对象后可以根据API中的html属性所对应的DHTML样式属性(style)或者标签属性的名来修改内容。 其他事件冒泡微软提出了名为事件冒泡(event bubbling)的事件流。事件冒泡可以形象地比喻为把一颗石头投入水中，泡泡会一直从水底冒出水面。也就是说，事件会从最内层的元素开始发生，一直向上传播，直到document对象。因此上面的例子在事件冒泡的概念下发生click事件的顺序应该是p -&gt; div -&gt; body -&gt; html -&gt; document 事件捕获网景提出另一种事件流名为事件捕获(event capturing)。与事件冒泡相反，事件会从最外层开始发生，直到最具体的元素。上面的例子在事件捕获的概念下发生click事件的顺序应该是document -&gt; html -&gt; body -&gt; div -&gt; p 后来 w3c 采用折中的方式，平息了战火，制定了统一的标准——先捕获再冒泡。以下属于DOM2级处理程序了，0级就是直接设置onclick属性，DOM0级事件只会在冒泡阶段加载！addEventListener的第三个参数就是为冒泡和捕获准备的.addEventListener有三个参数： element.addEventListener(event, function, useCapture) 第一个参数是需要绑定的事件第二个参数是触发事件后要执行的函数第三个参数默认值是false，表示在事件冒泡阶段调用事件处理函数;如果参数为true，则表示在事件捕获阶段调用处理函数。对于事件代理来说，在事件捕获或者事件冒泡阶段处理并没有明显的优劣之分，但是由于事件冒泡的事件流模型被所有主流的浏览器兼容，从兼容性角度来说还是建议大家使用事件冒泡模型。 下面的方法被对象所调用type属性用于获取事件类型target属性用于获取事件目标stopPropagation()方法 用于阻止事件冒泡preventDefault() 方法 阻止事件的默认行为event.KeyCode 可用来屏蔽按键event.returnValue=false 可以将事件取消掉(常用于表单)event.srcElement 获取事件源对象(那一个对象触发的) BOM用于操作浏览器的对象，BOM的核心就是window对象，window对象指当前的浏览器窗口。利用window对象可以操作浏览器的相关设置，比如地址栏、屏幕的大小、操作系统、窗口的位置等等，这个貌似不太经常用，这里不说了，用到了查API或者google之不过一般会用到一些事件，如窗体装载、卸载啦 加载事件（onload）事件会在页面加载完成后，立即发生，同时执行被调用的程序。 卸载事件（onunload）当用户退出页面时（页面关闭、页面刷新等），触发onUnload事件，同时执行被调用的程序。 关闭窗口window.close ：关闭本窗口[窗口对象].close ：关闭指定窗口 History 对象可以获取用户曾经浏览过那些网页，以实现后退、前进的功能 其他常用对象location用于获取或设置窗体的URL，并且可以用于解析URL。Navigator 对象包含有关浏览器的信息，通常用于检测浏览器与操作系统的版本。screen对象用于获取用户的屏幕信息。 其他常用函数/事件/属性补充 函数/事件名/属性 作用 alert(字符串或变量) 警告提示框，调试频繁用 confirm(str); 确认提示框，点击确定返回true，点击取消返回false elementNode.getAttribute(name) 通过元素节点的属性名称获取属性的值 elementNode.setAttribute(name,value) 增加一个指定名称和值的新属性，或者把一个现有的属性设定为指定的值 Object.innerHTML 用于获取或替换 HTML 元素的内容。 object.className 获取或者设置元素的类名 element.onmouseover 鼠标经过事件 element.onmouseout 鼠标移开事件 element.onfocus 聚焦事件 element.onblur 失焦事件 element.onchange 文本框内容改变 Math.ceil() 向上取整 Math.floor() 向下取整 Math.round() 四舍五入 Math.random() 生成0-1的随机数 setInterval(代码,交互时间) 从载入页面后每隔指定的时间执行代码 clearInterval(id_of_setInterval) 取消计时器(间隔执行的) setTimeout(代码,延迟时间) 在载入后延迟指定时间后,去执行一次表达式,仅执行一次，时间单位为毫秒 clearTimeout(id_of_setTimeout) 取消计时器(一次性的) console.log() 打印日志，还有其他等级如debug，各个等级的颜色等会不同 console.group() 日志分组，便于查看，配合console.groupend()使用，一个表示开始一个表示结束 console.dir(cmd) 列出指定命令/对象的所以方法 event.srcElement 获取当前事件源对象(固定写法，一般直接传this来替代) elementNode.parentNode 获取父节点，父节点只有一个 elementNode.childNodes 得到全部的子节点(firstChild得到第一个，lastChile得到最后一个) isNaN() 判断是否为数字 判断一个数是不是小数： 12// 如果输入的不是数字 parse 返回 NaNif (num == parseInt(num)) parseInt 可以强转开头为数字的字符串，字符会被忽略：parseInt (&#39;123avc1&#39;); 也是可以正常转换的，而使用 Number() 方法转换包含字母的字符串会报错（返回 NAN），但默认是这种转换。 最后用到什么比较频繁的东西到时候再回来更新吧…嗯，缓慢更新对DOM不理解的可以去谷狗下DOM事件流]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android逆向基础]]></title>
    <url>%2F2016%2F08%2F22%2FAndroid%E9%80%86%E5%90%91%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[听大佬的话，让学下Android逆向相关知识，正好以前也了解过一点PC端的逆向感觉还是不错的，正好最近在学习Android开发，了解下逆向也应该不是很困难吧。可惜你不会汇编，硬伤，挖坑待填 APK的组成如果你直接打开apk文件，如用RAR asset、res这两个属于资源目录，但是他们也是有区别的res目录下的文件编译的时候会自动生成索引文件，在Java代码中用R.XX.XX引用asset目录下的文件不会生成索引，在Java代码中需要用AssetManager来访问一般来说除了视频、音频、游戏相关资源放在raw或asset下其他都会放在res下 META-INF文件夹一般存放工程的属性文件，如Manifest.MF classes.dexjava代码编译得到的，Dalvik VM能直接执行的 resource.arsc对res目录下的资源的一个索引文件，保留了原工程中strings.xml等文件内容 其实除了这些还有其他一些文件，这里不说了，搞开发的应该很清楚了 = =！关于apktool的使用，这里暂时不说，因为我一直在用集成环境 Dalvik 相关介绍上面也出现过这个词，到底是什么呢？这个也算是是学习逆向的基础吧，要了解下 Dalvik是google专门为Android操作系统设计的一个虚拟机，Dalvik字节码是专门为Dalvik VM设计的一种指令集。和java虚拟机不同的是，java虚拟机是基于堆栈设计的，而Dalvik虚拟机是基于寄存器的(专属文件执行格式dex)，效率要比java虚拟机快很多。每一个进程对应一个Dalvik 虚拟机实例，Dalvik 字节码文件是由Java字节码文件转换而来 需要注意的是通过Dalvik字节码我们也不能看到原来的逻辑代码，这时候就需要用一些工具来进行查看，但是最终我们要修改的文件是smali文件而不是Java文件。 PS:从Android 5.0版起，Android Runtime（ART）替换Dalvik成为系统内默认虚拟机。但应该不影响我们逆向，毕竟要向下兼容。 Smali入门什么是Smali作为我们在逆向接触最多的，有必要认真的进行学习。简单的说，smali就是Dalvik VM内部执行的核心代码。它有自己的一套语法。Smali，Baksmali分别是指安卓系统里的Java虚拟机（Dalvik）所使用的一种.dex格式文件的汇编器，反汇编器。其语法是一种宽松式的Jasmin/dedexer语法，而且它实现了.dex格式所有功能（注解，调试信息，线路信息等）。 Davlik字节码中，寄存器都是32位的，能够支持任何类型，64位类型（Long/Double）用2个寄存器表示；Dalvik字节码有两种类型：原始类型；引用类型（包括对象和数组） smali的数据类型原始类型： B—byte C—char D—double(64) F—float I—int J—long (64) S—short V—void Z—boolean 对象类型： Lxxx/yyy—objectLpackage/name/ObjectName; 相当于Java中的package.name.ObjectName; ​解释如下：​L：表示这是一个对象类型​package/name：该对象所在的包；：表示对象名称的结束 这里提一下，关于内部类的表示是在内部类前加“$”符号就这样：LpackageName/objectName$subObjectName; 数组的表示形式： [XXX—array可以看出数组就是在前面加了个[，例如[i == int[]，二维数组就是加[[咯~以及[Ljava/lang/String 表示一个String的对象数组了 smali的方法/函数表示举个例子：Lpackage/name/ObjectName;——&gt;methodName(III)Z 详解如下：Lpackage/name/ObjectName 表示类型methodName 表示方法名III 表示参数（这里表示为3个整型参数）说明：方法的参数是一个接一个的，中间没有隔开；方法以.method指令开始，以.end method指令结束。根据方法类型的不同，在方法指令开始前可能会用#加以注释，例如：#virtual methods表示这是一个虚方法，# direct methods表示这是一个直接方法。 foo ()V没错，这就是void foo()。 foo (III)Z这个则是boolean foo(int, int, int)。 foo (Z[I[ILjava/lang/String;J)Ljava/lang/String;看出来这是String foo (boolean, int[], int[], String, long) 了吗？ Smail中字段表示类似：Lpackage/name/ObjectName;——&gt;FieldName:Ljava/lang/String; Lpackage/name/ObjectName;包名FieldName:字段名Ljava/lang/String;字段类型 BakSmali生成的字段代码以.field指令开头，根据字段类型的不同，在字段指令的开始可能会有相应的注释，例如：# instance fields表示这是一个实例字段，# static fields表示这是一个静态字段。 Smali基本语法 .field private isFlag:z ——定义变量.method——方法.parameter——方法参数.prologue——方法开始.line 12——此方法位于第12行invoke-super——调用父函数const/high16 v0, 0x7fo3——把0x7fo3赋值给v0invoke-direct——调用函数return-void——函数返回void.end method——函数结束new-instance——创建实例iput-object——对象赋值iget-object——调用对象invoke-static——调用静态函数 条件跳转分支： “if-eq vA, vB, :cond*” 如果vA等于vB则跳转到:cond“if-ne vA, vB, :cond_“ 如果vA不等于vB则跳转到:cond*“if-lt vA, vB, :cond“ 如果vA小于vB则跳转到:cond_“if-ge vA, vB, :cond*” 如果vA大于等于vB则跳转到:cond“if-gt vA, vB, :cond_“ 如果vA大于vB则跳转到:cond*“if-le vA, vB, :cond“ 如果vA小于等于vB则跳转到:cond_“if-eqz vA, :cond*” 如果vA等于0则跳转到:cond“if-nez vA, :cond_“ 如果vA不等于0则跳转到:cond*“if-ltz vA, :cond“ 如果vA小于0则跳转到:cond_“if-gez vA, :cond*” 如果vA大于等于0则跳转到:cond“if-gtz vA, :cond_“ 如果vA大于0则跳转到:cond*“if-lez vA, :cond“ 如果vA小于等于0则跳转到:cond_ 这里只是贴了下经常见到的，更详细的指令可以看这里，或者见后面的参考 smali中的继承、接口、包信息1234567891011121314 .class public Lcom/disney/WMW/WMWActivity; .super Lcom/disney/common/BaseActivity; .source "WMWActivity.java" # interfaces .implements Lcom/burstly/lib/ui/IBurstlyAdListener; # annotations .annotation system Ldalvik/annotation/MemberClasses; value = &#123; Lcom/disney/WMW/WMWActivity$MessageHandler;, Lcom/disney/WMW/WMWActivity$FinishActivityArgs; &#125;.end annotation 1-3行定义的是基本信息：这是一个由WMWActivity.java编译得到的smali文件（第3行），它是com.disney.WMW这个package下的一个类（第1行），继承自com.disney.common.BaseActivity（第2行）。 5-6行定义的是接口信息：这个WMWActivity实现了一个com.burstly.lib.ui这个package下（一个广告SDK）的IBurstyAdListener接口。 8-14行定义的则是内部类：它有两个成员内部类——MessageHandler和FinishActivityArgs PS: 123456789101112131415161718192021.class &lt;访问权限&gt; [修饰关键字] &lt;类名&gt;.super &lt;父类名&gt;.source &lt;源文件名&gt;//静态属性# static fields .field &lt;访问权限&gt; static [修饰关键字] &lt;字段名&gt;:&lt;字段类型&gt; //实例属性 # instance fields .field &lt;访问权限&gt; static [修饰关键字] &lt;字段名&gt;:&lt;字段类型&gt; //方法# direct methods.method &lt;访问权限&gt; [修饰关键字] &lt;方法原型&gt; &lt;.locals&gt; 局部变量个数 [.parameter] 方法参数 [.prologue] 代码开始，混淆后可能去掉 [.line] 在源码中的位置 &lt;代码体&gt;.end method 关于寄存器的补充在smali里的所有操作都必须经过寄存器来进行：本地寄存器用v开头数字结尾的符号来表示，如v0、v1、v2、…参数寄存器则使用p开头数字结尾的符号来表示，如p0、p1、p2、…特别注意的是，p0不一定是函数中的第一个参数，在非static函数中，p0代指“this”，p1表示函数的第一个参数，p2代表函数中的第二个参数…而在static函数中p0才对应第一个参数（因为Java的static方法中没有this方法）。本地寄存器没有限制，理论上是可以任意使用的 12const/4 v0, 0x0 iput-boolean v0, p0, Lcom/disney/WMW/WMWActivity;-&gt;isRunning:Z 在上面的两句中，使用了v0本地寄存器，并把值0x0存到v0中，然后第二句用iput-boolean这个指令把v0中的值存放到com.disney.WMW.WMWActivity.isRunning这个成员变量中。即相当于：this.isRunning = false;（上面说过，在非static函数中p0代表的是“this”，在这里就是com.disney.WMW.WMWActivity实例） Smali中的成员变量格式是：.field public/private [static][final] varName:&lt;类型&gt; 对于不同的成员变量有不同的指令获取的指令有：iget、sget、iget-boolean、sget-boolean、iget-object、sget-object等，操作的指令有：iput、sput、iput-boolean、sput-boolean、iput-object、sput-object等。没有“-object”后缀的表示操作的成员变量对象是基本数据类型，带“-object”表示操作的成员变量是对象类型，特别地，boolean类型则使用带“-boolean”的指令操作。下面是几个例子： 1sget-object v0, Lcom/aaa;-&gt;ID:Ljava/lang/String; sget-object就是用来获取变量值并保存到紧接着的参数的寄存器中，在这里，把上面出现的ID这个String成员变量获取并放到v0这个寄存器中,因为只指出了该类的所属的类型,可以看出这是个静态的(static fields)注意：前面需要该变量所属的类的类型，后面需要加一个冒号和该成员变量的类型，中间是“-&gt;”表示所属关系。 1iget-object v0, p0, Lcom/aaa;-&gt;view:Lcom/aaa/view; 只是由于不是static变量，不能仅仅指出该变量所在类的类型，还需要该变量所在类的实例可以看到iget-object指令比sget-object多了一个参数，就是该变量所在类的实例，在这里就是p0即“this”。 获取array的还有aget和aget-object，指令使用和上述类似，略 put指令的使用和get指令是统一的，对比下面两个看效果更佳： 12const/4 v3, 0x0 sput-object v3, Lcom/aaa;-&gt;timer:Lcom/aaa/timer; 相当于 this.timer=null(null=0x0),因为是obj类型的如果是bool的话….你懂得~ 123.local v0, args:Landroid/os/Message; const/4 v1, 0x12 iput v1, v0, Landroid/os/Message;-&gt;what:I 相当于 args.what = 18; (args是Message的实例) Smali中的函数调用smali中的函数和成员变量也一样，分为两种，direct和virtual至于它们之间的区别，简单来说就是：direct method 就是private函数，其余的public和protected函数都属于virtual method所以在调用的时候就有了invoke-direct、invoke-virtual,另外类似的还有invoke-static、invoke-super、invoke-interface也就都好理解了。特别的还有invoke-xxx/range的指令，这是参数多于4个的时候调用的指令，应该比较少见。 下面就是具体的调用方式了： invoke-static很显然是调用静态函数的 1invoke-static&#123;&#125;,Lcom/aaa;-&gt;CheckSignature()z 后面的一对大括号其实是调用该方法的实例+参数列表，由于这个方法是静态的，也不需要参数，所以括号内就是空了。 12const-string v0,"NDKLIB"invoke-static&#123;v0&#125;,Ljava/lang/System;-&gt;loadLibrary(Ljava/lang/string;)V 这个翻译过来就是调用了static void System.loadLibrary(String)来加载NDK的so库引用的方法，同样这里的V0就是参数“NDKLIB”了。 invoke-super 很显然，调用父类方法的指令，一般用于调用onCreate、onDestroy等生命周期函数 invoke-direct 调用private的函数 1invoke-direct&#123;p0&#125;,Landroid/app/TabActivity;-&gt;&lt;init&gt;()V 这里就是一个定义在TabActivity中的一个private函数init() invoke-virtual 用于调用protected或public的函数修改smali的时候不要错用哦 12sget-object v0,Lcom/dddd;-&gt;bbb:Lcom/cccinvoke-virtual &#123;v0,v1&#125;,Lcom/ccc;-&gt;Messages(Ljava/lang/object;)V v0就是bbb:Lcom/cccv1就是传递给Messages方法的Ljava/lang/Object类型的参数 invoke-xxx/range 当方法的参数多于5个时(含5个)就不能直接使用上面的指令了，而是在后面加上”/range”,range表示范围，使用的方法也有所不同。 1invoke-direct/range &#123;v0...v5&#125;,Lcmb/pb/ui/PBContainerActivity;-&gt;h(ILjava/lang/CharSequence;Ljava/lang/String;Landroid/content/intent;I)Z 需要传递v0到v5一共6个参数，这时候大括号内的参数会采用省略的形式，并且需要连续。 前面说的都是函数的调用，貌似没有返回之类的指令呢，确实，在Java代码中调用函数和返回函数结果是一条语句完成的，而在smali里则需要分开来完成，在使用上述指令后，如果调用的函数返回非void，那么还需要用到move-result（返回基本数据类型）和move-result-object（返回对象）指令 123const/4 v2, 0x0 invoke-virtual &#123;p0, v2&#125;, Lcom/disney/WMW/WMWActivity;-&gt;getPreferences(I)Landroid/content/SharedPreferences; move-result-object v1 v1保存的就是调用getPreferences(int)方法返回的SharedPreferences实例。 12invoke-virtual &#123;v2&#125;, Ljava/lang/String;-&gt;length()I move-result v2 v2保存的则是调用String.length()返回的整型。 下面就是函数实体的例子了： 12345678910111213.method private ifReg()V .locals 2 //本函数中本地寄存器的个数 .prologue const/4 v0,0x1 //v0赋值为1 .local v0,tempFlag:Z if-eqz v0,:cond_0 //如果v0等于0则跳到cond_0标签 const/4 v1,0x1 //符合条件的分支 根据上面就是不等于0了 :goto_0 //标签 return v1 //返回v1的值 :cond_0 //标签 const/4 v1,0x0 //处于cond_0标签分支了 goto:goto_0 //跳到goto_0标签，就是执行return v1.end method 参考smali语法介绍Android反编译-基础知识smali语法-关键是指令集smali指令集-官方]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>逆向工程</tag>
        <tag>smali</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu折腾记录]]></title>
    <url>%2F2016%2F07%2F25%2Fubuntu%E6%8A%98%E8%85%BE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[对于ubuntu其实并不很陌生，前几年就搞过(熟练的安装/卸载 o(￣▽￣)ゞ))￣▽￣)o)，这次准备真正的学下linux，深入一点，刚开始我果然就如此幸运 遇到了各种坑，别人踩过的没踩过的基本都遇到了，真是辣么想让我学习么？？折腾了两天，基本算是能用了吧。。。 前最开始的时候我想的是 曾经装过辣么多次，自认为在安装上差不多是老司机了，直接去官网下了原版(没用麒麟版)最新的16.04版本的，写进U盘，重启，一切准备就绪！自信满满，然后就有了后面的N次重装，N次重启… 装完发现驱动还不支持，注销、挂起就崩，等等一些小问题，最后还是换回了14.04 分区/挂载点因为我扣出来的空间是不连续的，从D盘扣了一点 F盘扣了一点 等等，天真的我想着怎么才能把他们合到一起去，这样才能分给ubuntu，用无损分区助手搞了一会太慢了！！ 后来才意识到它们的挂载是不用在一起的！！！一脸懵逼…. 下面写下我的分区 /boot 200MB/ 20Gswap 8G //貌似不用这么大，毕竟不是服务器/home 50G 以上都是选择的逻辑分区，前两个是从D扣的，后面是E、F扣的，硬盘小伤不起。 引导那选/boot所在的分区，用win引导ubuntu，需要用EasyBCD添加个引导~ 与win的时间不同步问题因为当时安装的时候是断网情况下，没有联网同步，很明显的就看出来了，这是因为两个系统不同意的问题，我采用的是关闭ubuntu的UTC，还有一种是改win的没测试 这里要说下，16.04的修改方法不太一样了，这让我郁闷了好长时间，网上的大部分做法是： sudo vim /etc/default/rcS 修改：UTC=yes为：UTC=no 然而16.04的这个文件已经没有这些东西了，可以试下下面的命令12timedatectl set-local-rtc 1 --adjust-system-clocktimedatectl set-ntp 0 无线网卡rtl8723be问题弄好后连上wifi，频繁掉我也就忍了，网速还每秒几B的速度，去问谷狗发现是rtl8723be这个驱动的问题，官方的驱动和内核不兼容，低端网卡害死人哇… 比较全的解决方案整理在这：坐飞机去论坛 我是下载源码重新编译了一下，然而效果还是不是很好，起码比以前是有了较大的改善，但还是不稳定，网页时长打不开，狂按ctrl + F5才能刷出了，不造是什么原因。 附github源码地址：驱动源码 步骤 先安装好需要编译的东西sudo apt-get install linux-headers-generic build-essential git 先停止网路sudo service network-manager stop cd 到 rtlwifi_new 目录下如果以前有驱动先卸载：sudo modprobe -rfv rtl8723be然后再编译： 123sudo makesudo make installsudo modprobe -v rtl8723be fwlps=0 ips=0 sudo reboot 重启 最后可以检查下是否设置成功systool -v -m rtl8723be 其他解决方案第一种一种比较简单有效的方法：执行sudo echo &quot;options rtl8723be fwlps=0 swlps=0&quot; &gt; /etc/modprobe.d/rtl8723be.conf 第二种本质上和第一种是一样的，都是往驱动的配置文件写rtl8723be.conf添加以下信息： options rtl8723be debug=1options rtl8723be disable_watchdog=Noptions rtl8723be fwlps=Yoptions rtl8723be ips=Yoptions rtl8723be msi=Noptions rtl8723be swenc=Noptions rtl8723be swlps=Noptions rtl8723be ant_sel=2 保存配置文件后,运行如下命令来应用配置12sudo modprobe -r rtl8723besudo modprobe rtl8723be 添加打开终端到右键菜单这个很简单，装一个软件即可，16.04的版本中已经默认支持了，执行下面命令：sudo apt-get install nautilus-open-terminal 更换Flatabulous主题安装主题的第一步是安装Ubuntu tweak tool，安装命令如下：12345sudo add-apt-repository ppa:tualatrix/ppasudo apt-get update //更新下源数据sudo apt-get install unity-tweak-tool//启动Unity Tweak Tool：unity-tweak-tool 下载/安装主题： 123sudo add-apt-repository ppa:noobslab/themessudo apt-get updatesudo apt-get install flatabulous-theme 下载主题文件：点我下载主题开源地址：https://github.com/anmoljagetia/Flatabulous 然后直接全部提取出来在一个文件夹里，随便命名，然后丢到/usr/share/themes里就行了，如果tweaktool里找不到的话，就设置以下整个文件夹的权限就ok了，此操作因为需要root权限最好在命令行中完成。 这里也顺便使用了Flat的蓝色图标下载地址是：点我去下载 然后还是同上，只不过是复制到 /usr/share/icons 下 最后其实还有一些很常见的小问题，就没写，还有一些问题没有解决，等以后再更新吧…想换一个shell的，暂时先不弄了，因为也许是用的邻居的wifi，等网好了再弄吧。。大体是这个样子了： 继续折腾(标记)今天是休息日，网络也好了，把我的ubuntu搬出来再继续折腾，这次离路由器近了，网络也没出现什么问题，于是就想继续先把环境给弄好～ 安装QQ虽然我确实不喜欢上qq，但是有些时候你还必须得通过qq来获得一些信息，并且是官方是没有开发linux的QQ的，方法就是靠wine来模拟一个windows环境，可以运行exe程序，然后发现优麒麟官网提供了定制好的wineqq安装包，下载地址点我 解压后会有三个安装包，先进行安装安装wine-qqintl_0.1.3-2_i386.deb，执行命令sudo dpkg -i wine-qqintl_0.1.3-2_i386.deb在安装过程中发生错误，这是正常的，这是因为还有一个lib没有配置，所以我们要输入下面的命令：sudo apt-get install -f然后在重新安装wine-qqintl_0.1.3-2_i386.deb的命令sudo dpkg -i wine-qqintl_0.1.3-2_i386.deb应该就能安装成功了～～接下来在继续安装剩余的两个deb包sudo dpkg -i ttf-wqy-microhei_0.2.0-beta-2_all.debsudo dpkg -i fonts-wqy-microhei_0.2.0-beta-2_all.deb然后就可以运行wineqqintl啦！为了检查是否安装正确，我们可以输入下面的命令进行检测sudo dpkg -l |grep qq到这里在搜索界面也应该可以搜到QQ了～点开运行就行了！ 更加详细的汇总可以见官方论坛 http://wiki.ubuntu.org.cn/QQ#.E4.BC.98.E9.BA.92.E9.BA.9F_wine-qq 切换到root身份期间好几次提示没有权限，尤其是在弄JDK的时候，配置环境变量死活就是提示文件路径不存在，后来也不知道咋好了….好了，接下来正文：这里有种说法：出于安全考虑，默认时Ubuntu的root用户时没有固定密码的，它的密码是随机产生并且动态改变的，貌似是每5分钟改变一次，所以用su（switch user）是不可以的，因为我们不知道root的密码。当然也可以用命令来设置root的密码：sudo passwd root,下面说下几种切换root的方法： su root ，输入root密码切换到root用户，无时间限制。su 用户名切换回其它用户。 sudo su，效果同上，只是不需要root的密码，而需要当前用户的密码。 sudo -i，输入当前用户密码后以root权限登录shell，无时间限制。使用exit或logout退出。 安装SS客户端作为google的重度依赖者，这可是一件大事！我采用的是图形界面，命令行的没尝试…用gui方式进行安装：添加PPA源：sudo add-apt-repository ppa:hzwhuang/ss-qt5更新软件列表：sudo apt-get update安装shadowsocks：sudo apt-get install shadowsocks-qt5dash中搜索shadow，然后打开shadowsocks-qt5软件就可以用了，关于账号信息直接就是读取的win下的配置文件，他们是可以进行通用的，还又PAC文件也是可以通用的 SS准备好了剩下的就是配置了，我当然是用了PAC模式，这里可以使用GenPAC，也可以直接使用win下的pac： 1234#首先安装pipsudo apt-get install python-pip#通过pip安装genpacsudo pip install genpac 使用GenPAC生成pac文件: 1genpac -p "SOCKS5 127.0.0.1:1080" --gfwlist-proxy="SOCKS5 127.0.0.1:1080" --gfwlist-url=https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt --output="autoproxy.pac" 此时会生成一个名为autoproxy.pac的文件,记得要先在系统设置里-网络设置SS代理，先设置为手动即可，下载完成后在系统的网络设置里，选为自动，URL填类似下面的即可~ 1#例如 file:///home/用户名/autoproxy.pac 你没有看错，是三个’/‘不要少些，至此，应该可以愉快的google了！ 更详细的介绍参考：http://www.jianshu.com/p/6280ac9fd95d 更换shell为zsh说起shell的话，我也是知道zsh被称为最强shell，只是配置起来比较复杂，但是有神器oh-my-zsh替你配啊！傻瓜试操作于是我也要提升逼格更换zsh啦！首先看了下系统没有zsh，于是就要去安装啦～(查看shell可以使用cat /etc/shells)sudo apt-get install zsh我比较懒采用wget自动安装了～～wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh安装 oh-my-zsh 时,它自动读取你的环境变量并且自动帮 zsh 进行设置.所以这时的zsh 基本已经配置完成,你需要一行命令就可以切换到 zsh 模式.chsh -s /usr/local/bin/zsh另外神器有多款皮肤也可以自定义，修改配置文件：sudo getdit ~/.zshrc我现在在用的是ys的主题，agnoster的貌似也不错，不过字体会存在些兼容问题，可以去安装下面的字体就OK了 字体主页：https://github.com/powerline/fontsoh-my-zsh主页：https://github.com/robbyrussell/oh-my-zsh 点击图标最小化感觉这个功能还是比较好用的，实现也很简单，注意的是此方法适用14.04,16的话貌似在Tweak Tool中就可以设置了 启用：gsettings set org.compiz.unityshell:/org/compiz/profiles/unity/plugins/unityshell/ launcher-minimize-window true关闭：gsettings set org.compiz.unityshell:/org/compiz/profiles/unity/plugins/unityshell/ launcher-minimize-window false 安装系统监视器System Monitor Indicator通过源安装： 123sudo add-apt-repository ppa:alexeftimie/ppasudo apt-get updatesudo apt-get install indicator-sysmonitor]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C#学习笔记]]></title>
    <url>%2F2016%2F07%2F16%2FC-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这一学期结束了，虽然一直在搞Android开发，但是C#上课我也是认真听的，是没怎么深学，课下不练习也就很快忘了，为了应付考试(￣^￣)，我整理了下以前的代码，主要的内容提取下做个笔记，反正内容也不是很多，万一以后用到了呢？o(￣▽￣)ゞ))￣▽￣)o 文件操作在这之前记得添加引用:using System.IO; 利用打开对话框本地硬盘获取文件先写一个最简单的设置背景图片的功能:1234567891011121314151617181920212223242526272829303132333435363738private void button1_Click(object sender, EventArgs e) &#123; OpenFileDialog openFileDialog = new OpenFileDialog(); //打开的文件选择对话框上的标题 openFileDialog.Title = "请选择文件"; //设置文件类型 openFileDialog.Filter = "文本文件(*.txt)|*.txt|所有文件(*.*)|*.*"; //设置默认文件类型显示顺序 openFileDialog.FilterIndex = 1; //保存对话框是否记忆上次打开的目录 openFileDialog.RestoreDirectory = true; //设置是否允许多选 openFileDialog.Multiselect = false; //设置默认路径 openFileDialog.InitialDirectory = "d:\\"; //按下确定选择的按钮 if (openFileDialog.ShowDialog() == DialogResult.OK) &#123; //如果为多选的时候 //MessageBox.Show(openFileDialog.FileName); //完整路径 //MessageBox.Show(openFileDialog.FileNames[0]); //完整路径 - 多选文件 //MessageBox.Show(openFileDialog.SafeFileName); //文件名 //MessageBox.Show(openFileDialog.SafeFileNames[0]); //获得文件路径 string localFilePath = openFileDialog.FileName.ToString(); //获取文件路径，不带文件名 //FilePath = localFilePath.Substring(0, localFilePath.LastIndexOf("\\")); //获取文件名，带后缀名，不带路径 string fileNameWithSuffix = localFilePath.Substring(localFilePath.LastIndexOf("\\") + 1); //去除文件后缀名 string fileNameWithoutSuffix = fileNameWithSuffix.Substring(0, fileNameWithSuffix.LastIndexOf(".")); //在文件名前加上时间 //string fileNameWithTime = DateTime.Now.ToString("yyyy-MM-dd ") + fileNameExt; //在文件名里加字符 string newFileName = localFilePath.Insert(1, "dameng"); &#125; &#125; 按照指定规则遍历文件这里演示下从指定目录过滤出jpg图片文件。规则可以是正则表达式(未证实).可以表示可执行程序所在的目录。1234567891011121314151617181920 string filepath = "./src/"; List&lt;string&gt; namelist = new List&lt;string&gt;();//获取文件夹中的信息 DirectoryInfo folder = new DirectoryInfo(filepath); private void GetAllFiles(DirectoryInfo folder) &#123; //遍历文件夹的内容，筛选出符合规则的文件 foreach (FileInfo file in folder.GetFiles("*.jpg")) &#123; //将文件名添加到集合/容器 namelist.Add(file.Name); &#125; //遍历文件夹--然后进行递归遍历文件 foreach (DirectoryInfo dd in d.GetDirectories()) &#123; GetAllFiles(dd); &#125; &#125; 补充一个关于读取文件流编码相关的1234567StreamReader sr = new StreamReader(filepath + "文本文档.txt",Encoding.Default);String line; while ((line = sr.ReadLine()) != null) &#123; //每次读取一行 //... &#125; 数据库ADO操作还是首先要先添加引用:using System.Data;using System.Data.SqlClient;这里其实没什么特别的，就固定的那么几句，其他的就是执行sql语句了，这里写一下标准的几句固定的语句：1234567891011121314151617181920212223242526272829303132public class DBHelper &#123; //获得一个链接 private static SqlConnection getConn() &#123; string strConn = "data source=.;initial catalog=test;uid=sa;password=.."; SqlConnection conn = new SqlConnection(strConn); conn.Open(); return conn; &#125; //主要是传入一个查询语句，获得一个数据表格 public static DataTable getDT(string strSQL) &#123; SqlConnection conn = getConn(); DataTable dt = new DataTable(); SqlDataAdapter da = new SqlDataAdapter(strSQL, conn); da.Fill(dt); conn.Close(); return dt; &#125; //主要执行非查询语句(增删改) 返回所影响的行数 public static int cmdSQL(string strSQL) &#123; SqlConnection conn = getConn(); SqlCommand cmd = new SqlCommand(strSQL, conn); int i = cmd.ExecuteNonQuery(); conn.Close(); return i; &#125; &#125; dt.Rows[0][0]; dataTable的行集合dt.Columns[0][0]; dataTable的列集合 再补充一个随机从数据库取得数据的sql语句：select top 20 * from Exam order by newid()原理大概猜测是是根据ID经过N次排序后抽取前20条记录(未证实) 字符串相关字符串的分割 按照指定字符分割,返回一个string数组 按照字符数组进行分割，返回一个string数组 按照指定的位置分割，返回一个string123456789//按照字符分割string[] ss = str.Split('-');//按照字符数组进行分割//第二个参数，指定字符串的切割规则，这里是删除空字符//从字符串头部开始遍历，遇到含有的字符就进行分割 (大概 = = string[] ss = richTextBox1.Text.Split(new char[] &#123; ' ', '-','\n',',' &#125;,StringSplitOptions.RemoveEmptyEntries);//按照指定长度/位置分割//第二个数字是长度，不是下标！ 这里从输入(textBox)获取string s1 = str.Substring(0, Convert.ToInt32(textBox2.Text)); 关于Split的重载还有很多，这里不多介绍，就学了这些 = = 从字符串再选择一段字符串这是一个让选中的改变颜色的例子：1234//选中要改变颜色的字符，第一个是开始位置，第二个是长度 richTextBox1.Select(i, 1); //改变选中的字符的颜色 richTextBox1.SelectionColor = Color.Red; 正则还是首先添加引用:using System.Text.RegularExpressions;match会捕获第一个匹配。而matches会捕获所有的匹配。match类型就是一个单独的捕获，matchcollection就是一组捕获。12345678string strRegex = "正则内容";//IgnoreCase是忽略大小写，这样会降低匹配Regex r = new Regex(strRegex, RegexOptions.IgnoreCase);MatchCollection mc = r.Matches(richTextBox1.Text);foreach (Match m in mc)&#123; listBox1.Items.Add(m.Value);&#125; 多线程多线程和委托有着很紧密的联系，使用多线程一般都会使用到委托，和Android一样，多线程不能操作UI，想要更新UI就必须使用委托。为了防止一些其他问题，最佳的做法是一个线程最好只操作UI的一个控件。线程不能被强制终止，只能自己停下来，也就是说不同线程直接是没法进行操作的，我们所做的终止线程只是做了一个终止的标记而已(未测试)。123456789private Thread t1;//带参数的开启方式 注意只写方法名即可不需要加()t1 = new Thread(new ParameterizedThreadStart(执行的方法));t1.Start(参数);//不带参数的开启方式//t1 = new Thread(new ThreadStart(执行的方法)); //t1.Start(); t1.Abort(); 因为不知道传入的参数是什么类型的，所以是个obj类型的，用的时候需要强转，定义的时候也要定义成obj类型的。 下面再来看委托，用在需要更新UI的地方123456789101112//定义委托private delegate void updateListDele(string str);//定义符合委托入口参数和返回值的方法private void updateList(string str) &#123; lock (this) &#123; //加了个同步锁，具体的更新UI的逻辑 &#125; &#125;//然后就是使用了 可以看出第二个是需要的参数this.listBox1.Invoke(new updateListDele(updateList), new object[] &#123; f.FullName &#125;); 类似的还有BeginInvoke方法，至于区别： Control.Invoke 方法 (Delegate) :在拥有此控件的基础窗口句柄的线程上执行指定的委托。 Control.BeginInvoke 方法 (Delegate) :在创建控件的基础句柄所在线程上异步执行指定委托。 网络操作老规矩，先添加引用：using System.Net; 先来看看最基本的发送网络请求的写法:123456789//AS相当于就是强制类型转换HttpWebRequest request = WebRequest.Create("http") as HttpWebRequest;//模拟一个浏览器 request.UserAgent = "Mozilla/5.0 (Windows NT 5.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36"; HttpWebResponse response = request.GetResponse() as HttpWebResponse; Stream stream = response.GetResponseStream(); StreamReader sr = new StreamReader(stream, Encoding.Default); string str = sr.ReadToEnd(); 然后是socket编程之类下面主要是发送消息的，也就是客户端123456789101112131415//得到本机IP，设置UDP端口号 IPAddress localip = getValidIP("127.0.0.1"); int localport = getValidPort("1234"); ipLocalPoint = new IPEndPoint(localip, localport); //定义网络类型，数据连接类型和网络协议UDP mySocket = new Socket(AddressFamily.InterNetwork, SocketType.Dgram, ProtocolType.Udp); //绑定网络地址 mySocket.Bind(ipLocalPoint); //发送UDP数据包 RemotePoint = (EndPoint)(ipLocalPoint); //用本地作为接收地址了 string msg = "发送的消息" byte[] data = Encoding.Default.GetBytes(msg); mySocket.SendTo(data, data.Length, SocketFlags.None, RemotePoint); 然后是接收消息的，也就是服务端123456//最开始的定义与上面相同，直接引用上面的了//接收数据处理线程 string msg; byte[] data = new byte[1024]; int rlen = mySocket.ReceiveFrom(data, ref RemotePoint); msg = Encoding.Default.GetString(data, 0, rlen); 其他补充修改字体样式举个修改label字体的例子：1234//font是一个类 要先声明 Font ff = new Font("微软雅黑", 28); label2.Font = ff; label2.ForeColor = Color.Red;//设置前背景，文字颜色 强制类型转换12345int a = Convert.ToInt32(转换的内容);//强制类型转换 效率高 上面其实就是调用 int.Parseint a = int.Parse(string)； //类似的有 double.Parse ....//强制类型转换函数，如果成功返回true，存在number中，如果失败，返回 false,number为0int.TryParse("string",out number); timer的使用具体执行的代码双击控件，然后再写就可以了。123//设置timer的开关和刷新时间timer1.Enabled = true; timer1.Interval = 888; 随机数的产生12Random r = new Random() //创建能够产生随机数的对象int sjs = r.Next(1,10) //随机数的范围 [ )区间 //让这个对象用函数来产生随机数 关于get和setget，set方法在Java中已经非常熟悉了，c#中不太一样123456789101112131415161718192021222324252627//获取部分Demo de = new Demo(); //设置属性值 de.show = 2; //de.show 这样就是获取属性值 MessageBox.Show(de.show+"");//定义部分class Demo &#123; int a; //定义属性，注意没有（），不需要传入参数 public int show &#123; //这get 和 set 可以认为是个关键字了吧 get &#123; return a; &#125; set &#123; //value 就是指传进来的值，固定名称 a = value; &#125; &#125; &#125; 手动添加点击事件123456789101112131415161718//绑定点击事件bt[b].Click += Form1_Click;//事件的定义 通过tag属性来辨别是拿一个按钮对象private void Form1_Click(object sender, EventArgs e) &#123; Button but = new Button(); but = (Button)sender; switch (but.Tag.ToString()) &#123; case "0": BackColor = Color.Red; break; case "1": BackColor = Color.Pink; break; &#125; &#125; 总结就内容来说确实不多，并且和Java也有很多类似的地方，但是很多地方只知道怎么用，具体什么个原理没深究，有些参数都不造啥意思….⊙﹏⊙b汗，这些坑以后有时间慢慢填吧。C#写form程序还是很顺手的，以后说不定什么时候就能用的上，做下记录总是不会错的！假期来啦，最佳的学习时段来啦！]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>C#</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OKhttp初步学习]]></title>
    <url>%2F2016%2F05%2F30%2FOKhttp%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[简介OkHttp是一个专注于性能和易用性的 HTTP 客户端(是一个高效的HTTP库)，用来发送 HTTP 请求并对所得到的响应进行处理。非常高效，支持SPDY、连接池、GZIP和 HTTP 缓存。 支持 SPDY ，共享同一个Socket来处理同一个服务器的所有请求 如果SPDY不可用，则通过连接池来减少请求延时 无缝的支持GZIP来减少数据流量 缓存响应数据来减少重复的网络请求 我们一般经常用的功能： 一般的get请求 一般的post请求 基于Http的文件上传 文件下载 加载图片 对以上功能的封装请见参考。网络操作OKhttp配合Volley、Gson食用味道更佳。 使用添加依赖AS：compile &#39;com.squareup.okhttp3:okhttp:3.3.1&#39;以上，可以在github上找到，见参考。 关于GET提交方式设置get请求1、调用Request.builder 对象的get()方法来设置请求方法为”get”请求。2、调用Request.builder 对象的method(“GET”, null)方法来设置请求方法为“get”请求。 注意：method中的第二个参数表示是请求体，因为get请求的请求参数可以直接跟在url后面，所以中get请求的时候可以传递null。 同步加载(测试一直ANR)123456789101112131415161718private final OkHttpClient client = new OkHttpClient();public void run() throws Exception &#123; Request request = new Request.Builder() .url("http://publicobject.com/helloworld.txt") .build(); //阻塞式方式 会阻塞UI线程 //调用call对象的execute方法，发出网络请求,获取Response对象。 Response response = client.newCall(request).execute(); if (!response.isSuccessful()) throw new IOException(Unexpected code + response); //调用response的body方法方法获取相应体。 ResponseBody body = response.body(); //调用ResponseBody的相应方法获取具体响应内容。 body.string(); //如果结果为字符类型，调用这个方法，默认编码utf-8 body.bytes(); //返回字节数组 body.byteStream(); //返回字节输入流。&#125; 异步加载（官方推荐）123456789101112131415161718192021222324//创建okHttpClient对象OkHttpClient mOkHttpClient = new OkHttpClient();//创建一个Requestfinal Request request = new Request.Builder() .url("https://github.com/hongyangAndroid") .build();//new callCall call = mOkHttpClient.newCall(request); //请求加入调度//call.enqueue会开启一个新的线程call.enqueue(new Callback()&#123; //响应失败回调 @Override public void onFailure(Request request, IOException e) &#123; &#125; //响应成功回调 @Override public void onResponse(final Response response) throws IOException &#123; //String htmlStr = response.body().string(); &#125;&#125;); 以上就是发送一个get请求的步骤，首先构造一个Request对象，参数最起码有个url，当然你可以通过Request.Builder设置更多的参数比如：header、method等。 然后通过request的对象去构造得到一个Call对象，类似于将你的请求封装成了任务，既然是任务，就会有execute()和cancel()等方法。 最后，我们希望以异步的方式去执行请求，所以我们调用的是call.enqueue，将call加入调度队列，然后等待任务执行完成，我们在Callback中即可得到结果。 看到这，你会发现，整体的写法还是比较长的，所以封装肯定是要做的，不然每个请求这么写，得累死。 需要注意几点： onResponse回调的参数是response，一般情况下，比如我们希望获得返回的字符串，可以通过response.body().string()获取；如果希望获得返回的二进制字节数组，则调用response.body().bytes()；如果你想拿到返回的inputStream，则调用response.body().byteStream() 竟然还能拿到返回的inputStream，看到这个最起码能意识到一点，这里支持大文件下载，有inputStream我们就可以通过IO的方式写文件。不过也说明一个问题，这个onResponse执行的线程并不是UI线程。的确是的，如果你希望操作控件，还是需要使用handler等12345678910//操作UIpublic void onResponse(final Response response) throws IOException &#123; final String res = response.body().string(); runOnUiThread(new Runnable()&#123; @Override public void run()&#123; mTv.setText(res); &#125; &#125;);&#125; 关于post提交方式设置post方式1、调用Request.builder 对象的post(requestBody)方法来设置请求方法。2、调用Request.builder 对象的method(“POST”, requestBody)方法来设置。 注意：1、post的请求参数(请求体requestBody)必须有，不能为null。如果为null会抛异常。2、关于RequestBody：RequestBody是个抽象类。12345//使用RequestBody提交键值对：RequestBody body = new FormEncodingBuilder() .add("name","zs") .add("pwd", "aaa") .build(); 进行post提交 提交键值对 123456789101112131415161718192021OkHttpClient client = new OkHttpClient();String post(String url, String json) throws IOException &#123; RequestBody formBody = new FormEncodingBuilder() .add("platform", "android") .add("name", "bug") .add("subject", "XXXXXXXXXXXXXXX") .build(); Request request = new Request.Builder() .url(url) .post(body) .build(); Response response = client.newCall(request).execute(); if (response.isSuccessful()) &#123; return response.body().string(); &#125; else &#123; throw new IOException("Unexpected code " + response); &#125;&#125; 提交json数据 1234567891011121314151617//设置为json数据类型public static final MediaType JSON = MediaType.parse("application/json; charset=utf-8");OkHttpClient client = new OkHttpClient();String post(String url, String json) throws IOException &#123; //参数---&gt;类型和数据 RequestBody body = RequestBody.create(JSON, json); Request request = new Request.Builder() .url(url) .post(body) .build(); Response response = client.newCall(request).execute(); if (response.isSuccessful()) &#123; return response.body().string(); &#125; else &#123; throw new IOException("Unexpected code " + response); &#125;&#125; post的时候，参数是包含在请求体中的；所以我们通过FormEncodingBuilder。添加多个String键值对，然后去构造RequestBody，最后完成我们Request的构造。后面的就和上面一样了。 文件和表单的的提交暂未学习，见参考。挖坑。 其他 OkHttp官方文档并不建议我们创建多个OkHttpClient，因此全局使用一个。 如果有需要，可以使用clone方法，再进行自定义。 OKhttp与Volley Volley是针对数据量不大，但通信频繁的网络操作，而对于大数据量的网络操作，比如说下载文件等，Volley的表现就会非常糟糕。如果开发中使用HttpUrlConnection则要从头开始封装对应得操作，所以最近转向了一个第三方网络请求框架OkHttp库。 参考OKhttp的github开源库官方APIOKhttp完整的封装类详解OKhttpOkHttp使用详解OKhttp完全解析]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>OKhttp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的JSON解析]]></title>
    <url>%2F2016%2F05%2F28%2FJava%E4%B8%AD%E7%9A%84json%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Json 是一种是一种轻量级的文本数据交换格式。便于数据传输、存储、交换；目前使用非常广泛。Java 中常见的解析库有：JSON.simple、 GSON、Jackson、JSONP、FastJson 等。本文主要介绍 Gson 和 Jackson 的使用，以及 Json 格式的科普。 关于json数据格式其主要由三部分组成： 名称/值无序、一个对象用“｛｝”包括，名称和值间用“：”相隔，对象间用“，”隔开；&quot;name&quot;:&quot;html&quot; 对象一个JSON对象包括多个名称/值对，在花括号里书写{ &quot;name&quot;:&quot;html&quot;,&quot;year&quot;:&quot;5&quot;} 数组数组以“［］”包括，数据的对象用逗号隔开 12345678[&#123; "name":"html", "year":"5"&#125;,&#123; "name":"ht", "year":"4"&#125;] 数组包含对象，对象包含值/对 使用GSON解析GSON 这个Java库能够在Java对象和JSON间进行相互转换。同时它还提供了对Java泛型的完整支持，而且还不需要你在类上面添加注解。无需添加注解使用起来则更为便捷，同时在无法修改源代码的情况下这还是一个必要的先决条件。目前在 Android 开发中是比较流行的一种解析方式。强烈建议看这篇原文点我起飞它基于事件驱动，根据所需要取的数据(不用将全部的数据进行 JavaBean 的编写)通过建立一个对应于 JSON 数据的 JavaBean 类就可以通过简单的操作解析出所需 JSON 数据步骤：创建一个与 JSON 数据对应的 JavaBean 类（用作存储需要解析的数据）GSON 解析的关键是重点是要根据 json 数据里面的结构写出一个对应的 javaBean，规则是： JSON 的大括号对应一个对象，对象里面有 key 和 value (值)。在 JavaBean 里面的类属性要和key同名。 JSON 的方括号对应一个数组，所以在 JavaBean 里面对应的也是数组，数据里面可以有值或者对象。如果是一个对象数组，那就要用list&lt;obj&gt;来装了 如果数组里面只有值没有 key，就说明它只是一个纯数组，如果里面有值有 key，则说明是对象数组。纯数组对应 JavaBean 里面的数组类型，对象数组要在 Bean 里面建立一个内部类，类属性就是对应的对象里面的 key，建立了之后要创建一个这个内部类的对象，名字对应数组名。 对象里面嵌套对象时候，也要建立一个内部类，和对象数组一样，这个内部类对象的名字就是父对象的key 注：JavaBean 类里的属性不一定要全部和 JSON 数据里的所有 key 相同，可以按需取数据，也就是你想要哪种数据，就把对应的 key 属性写出来，注意名字一定要对应关键在于根据 json 数据构建出一个 Javabean 文件，解析非常简单，一句代码 JavaBean对象 = gson.fromJson(json字符串,javaBean类类名.class);12345678910Gson gson = new Gson();//创建实体Student student = new EntityStudent(); //用GSON方法将JSON数据转为单个类实体 json变量为json格式的字符串student = gson.fromJson(json,Student.class);//将Java集合转换为json//list变量可以是JavaBean也可以是Collection集合String json2 = gson.toJson(List); 有时候我们解析的可能是一段JSON数组，这个可能麻烦点，我们用List来装 1List&lt;Student&gt; stuList = gson.fromJson(json, new TypeToken&lt;List&lt;Student&gt;&gt;()&#123;&#125;.getType()); 也是一行代码，相比手动实现还是简单的总之：凡是看到 { 就是一个JsonObject凡是看到 [ 就是一个JsonArray json字符串转Java类 使用Jackson解析Jackson 是一个数据处理的工具套件，它的亮点是流式的JSON解析器及生成器。它是专为Java设计的，同时也能处理其它非JSON的编码。从我们在 Github 中的统计来看，它应该是最流行的 JSON 解析器，在 JavaEE 开发中非常常见。Jackson 最常用的 API 就是基于”对象绑定” 的 ObjectMapper，看下面的例子： 1234567891011ObjectMapper mapper = new ObjectMapper(); Person person = new Person(); person.setName("Tom"); person.setAge(40);// 序列化String res = mapper.writeValueAsString(person);// 输出格式化后的字符串(有性能损耗) String jsonString = mapper.writerWithDefaultPrettyPrinter() .writeValueAsString(person);// 反序列化Person deserializedPerson = mapper.readValue(jsonString, Person.class); ObjectMapper 通过 writeValue 系列方法 将 java 对 象序列化 为 json，并 将 json 存 储成不同的格式：String（writeValueAsString），Byte Array（writeValueAsString），Writer， File，OutStream 和 DataOutput。ObjectMapper 通过 readValue 系列方法从不同的数据源像 String ， Byte Array， Reader，File，URL， InputStream 将 json 反序列化为 java 对象。下面是对于泛型、集合的支持： 1234567891011121314// ListCollectionType javaType = mapper.getTypeFactory() .constructCollectionType(List.class, Person.class); List&lt;Person&gt; personList = mapper.readValue(jsonInString, javaType);// 第二种方式List&lt;Person&gt; personList = mapper.readValue(jsonInString, new TypeReference&lt;List&lt;Person&gt;&gt;()&#123;&#125;);// Map//第二参数是 map 的 key 的类型，第三参数是 map 的 value 的类型 MapType javaType = mapper.getTypeFactory() .constructMapType(HashMap.class, String.class, Person.class); Map&lt;String, Person&gt; personMap = mapper.readValue(jsonInString, javaType);// 第二种方式Map&lt;String, Person&gt; personMap = mapper.readValue(jsonInString, new TypeReference&lt;Map&lt;String, Person&gt;&gt;() &#123;&#125;); 开发中上面的这些基本用法一般就满足需求了，更高级的用法参考：https://www.ibm.com/developerworks/cn/java/jackson-advanced-application/index.html 常用注解在定义的 Bean 中可以使用下面的注解来做相应的调整。 注解 用法 @JsonProperty 用于属性，把属性的名称序列化时转换为另外一个名称。示例： @JsonProperty(&quot;birth_ d ate&quot;) private Date birthDate; @JsonFormat 用于属性或者方法，把属性的格式序列化时转换成指定的格式。示例： @JsonFormat(timezone = &quot;GMT+8&quot;, pattern = &quot;yyyy-MM-dd HH:mm&quot;) public Date getBirthDate() @JsonPropertyOrder 用于类， 指定属性在序列化时 json 中的顺序 示例： @JsonPropertyOrder({ &quot;birth_Date&quot;, &quot;name&quot; }) public class Person @JsonCreator 用于构造方法，和 @JsonProperty 配合使用，适用有参数的构造方法。 示例： @JsonCreator public Person(@JsonProperty(&quot;name&quot;)String name) {…} @JsonAnySetter 用于属性或者方法，设置未反序列化的属性名和值作为键值存储到 map 中 示例：@JsonAnySetter public void set(String key, Object value) { map.put(key, value); } @JsonAnyGetter 用于方法 ，获取所有未序列化的属性示例：public Map&lt;String, Object&gt; any() { return map; } 关于JavaBean其实就是一个Java类，只是他加了 getter 和 setter 方法，将属性暴露在外面 一般具备的特点为： 所有属性为private 提供默认空构造方法 提供 getter 和 setter 实现 serializable 接口 更多可参考：Java帝国之JavaBeanwikistackoverflow翻译 总结关于它们的测试参考 【1】 、【2】，数据是2016、2015 的，仅供参考，这里我就直接说结论： 如果你的应用经常会处理大的 JSON 文件，那么 Jackson 应该是你的菜。GSON在大文件上表现得相当吃力。 如果你主要是处理小文件请求，比如某个微服务或者分布式架构的初始化，那么GSON当是首选。Jackson 在小文件上的表现则不如人意。 如果这两种文件你都经常会处理到，那么在两轮表现中都位居第二的 JSON.simple 对此类场景则更为适合。在不同的文件大小上 Jackson 和 GSON 的表现都不太好。 字符串解析成 JavaBean：当数据量较少时首选 FastJson，数据量较大使用 Jackson。 字符串解析成JSON：当数据量较少时首选FastJson，数据量较大使用Jackson。 JavaBean 构造JSON：当数据量较少时选择Gson，数据量较大可使用Jackson。 集合构造JSON：首先Jackson，其次Fastjson。 如果你对 JSON 库的解析速度比较敏感的话，大文件选 Jackson，小文件选 GSON，两者则 JSON.simple；对解析速度要求非常高时可以考虑阿里的 FastJson，但是如果大文件还是选择 Jackson 吧。 我所见到的基本都是 Android 上 Gson 是主力，JavaEE 上 Jackson 是主力。 补充Android 中json的解析方法一般有两种： 基于事件驱动 基于文档驱动解析方式基于文档驱动，类似于 XML 的 DOM 解析方法，先把全部文件读入到内存中，然后遍历所有数据，然后根据需要检索想要的数据。 基于事件驱动的主流方式：Gson 解析和 Jackson 解析基于文档驱动解析方式的主流方式：Android Studio自带 org.son 解析]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑暗幽灵（别名 DCM ）的前世今生]]></title>
    <url>%2F2016%2F05%2F05%2F%E9%BB%91%E6%9A%97%E5%B9%BD%E7%81%B5%EF%BC%88%E5%88%AB%E5%90%8D-DCM-%EF%BC%89%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[黑暗幽灵（DCM）木马被腾讯电脑管家曝光之后，引起了业内人士的强烈关注，该木马功能强大，行为诡异，只要插上网线或连上WIFI，无需任何操作，不一会儿电脑就被木马感染了，据分析，黑暗幽灵（DCM）木马具有如下特性： 木马功能强大，主要以信息情报收集为主，能够监控监听大量的聊天软件，收集网络访问记录、监控Gmail、截取屏幕、监控麦克风和摄像头等。 木马对抗性强，能够绕过几乎全部的安全软件主动防御，重点对抗国内安全软件，能够调用安全软件自身的接口将木马加入白名单，作者投入了大量的精力逆向研究安全软件。 木马感染方式特别，通过网络劫持进行感染，主要劫持主流软件的自动更新程序，当这些软件联网下载更新程序时在网络上用木马替换，导致用户无感中毒。 木马通讯方式特别，木马将数据封装成固定包头的DNS协议包，发送到大型网站来实现数据传输，此方法可以绕过几乎全部的防火墙，但是黑客要截取这些数据，必须在数据包的必经之路上进行嗅探拦截，结合木马的感染方式，可以推测出在受害者网络链路上存在劫持。 木马攻击范围较小，针对性强，且持续时间长达数年，符合APT攻击的特性。 那么，黑暗幽灵木马到底是什么来历呢？匿名作者DcmTeamMember在v2ex发布了一篇文章，揭露了“黑暗幽灵”（别名 DCM）木马的前世今生，以下是文章全文。 高度机密：“黑暗幽灵”（别名 DCM ）木马的前世今生 首先，我无耻的匿了（多层 VPN 代理，因此那些正在阅读本文而目的却不是本文的正文内容的人，你们可以省省力气了） 关于这个叫 DCM 的木马，别名“黑暗幽灵”，我看到这个木马的分析报告的时候瞬间就震惊了。我震惊的原因不是因为这个木马的功能强大，也不是因为它的免杀手段，而是因为我是这个木马的设计和开发者之一！设计和开发团队大概 10 人左右，但是团队成员现在分布在全国各地多家不同行业的公司，因此我觉得把我们全部请去喝茶的可能性不大，况且就算真的被请去喝茶了，我也会装作没有见过这篇文章的。 没错，任何一个人都可以站出来，声称他是这木马的作者或设计者。而我却没办法在不泄漏自己真实身份的情况下证明我所写的内容，所以作为本文的读者，如果你不相信我所写的内容，那么就没必要继续读下去了。 DCM 木马的前世 首先我要证实一下大家的猜测：这个木马确实和 G0V 有关系，属于一款特殊用途的专用木马。我不便透露具体客户的身份。 这个木马的项目开始时间大概是 2011 年，目的是作为当时一个在役的木马（以下简称旧马）的继任者。之所以需要一个旧马的继任者，是因为旧马是基于一款开源的远程控制软件（以下简称原型远控）修改而成的，它主要有以下几方面的缺点： 免杀 原型远控在黑客圈子里比较知名，基于它的木马变种非常多。因此杀毒软件对它的查杀力度很大，免杀难度相对较高，保持的时间也较短，经常需要更新。 隐蔽 旧马沿用了原型远控的 TCP 反弹连接协议，因此主控端需要具有 IP 地址，某些应用环境下必须是公网 IP 地址，因此具有泄漏木马使用者身份的风险 功能 原型木马设计更像灰鸽子，被控端上线以后接受控制端发送的命令，然后将结果发送回控制端。旧马虽然努力改变这一设计，但受限于原型木马的框架，大的功能改动显得力不从心。 由于原型木马的先天缺陷，导致了旧马各方面难以弥补的不足，因此一个新的继任木马的需求也就被提出来了。 DCM 木马的诞生 这个继任者的设计理念包括一下几点： 无进程无窗口 该木马的受害者不能明显察觉到任何异样 长期免杀 杀毒软件与防火墙不能发现和拦截，包括木马的安装过程，以及安装成功以后的长期运行 不泄漏使用者信息 必须保障该木马控制者身份的绝对安全，任何情况下都不能泄漏控制者的 IP 地址、域名或者其他任何有价值的身份信息。即使样本被杀毒软件厂商获取并分析，也无法得知控制者的确切身份。 完全自动化 无需人工介入，根据事先的配置设置，全自动窃取信息并回传。没有网络连接的情况下要保存获取的信息，发现可用网络连接后进行回传。 经过几个月的设计与开发，这个继任者木马诞生了。 该木马的功能与特点已经没有必要在这里赘述了，网上的分析文章写的清清楚楚，总结下来就是自动记录并发送被感染电脑上的一切隐私内容。 DCM 木马的感染方式 其实该木马本身是不会主动传播的，它的设计就是潜伏并回传数据。它的传播是依赖另外几套系统来实现的（以下统称传播系统），而且这些传播方式也并不局限与传播 DCM 木马。这些传播系统的传染方式当然也不仅局限于分析文章里所提到的替换正常软件自动更新文件的方式。此外，该木马仅用于特定目标人群的”定点打击“，并不会大量传播。 正规软件的自动更新 在网上的分析文章里已经写了，通过替换正常软件的自动更新网络数据，使这些软件下载木马并执行。很多正规软件都直接运行在管理员模式下，还帮木马省去了提权的麻烦。 下载可执行文件捆绑 被列入”定点打击“的电脑如果下载了不超过一个预设大小的 EXE 文件，则传播系统会将木马捆绑在这些正常的 EXE 文件上，而且并不会破坏原有可执行文件。用户一旦运行了下载的 EXE 文件就会被感染。 压缩文件感染 被列入”定点打击“的电脑如果下载了一个符合某些条件的压缩文件，则传播系统会根据配置将木马插入压缩文件中，替换掉压缩文件中的可执行文件，或者替换掉整个压缩文件，从而实现感染目标主机的目的。 浏览器劫持感染 这个感染方式比较极端，只有少量情况下会使用。当该感染方式启动时，用户电脑无法正常浏览部分甚至全部网站，浏览器会被重定向到一个钓鱼页面，要求用户安装”浏览器插件”或者“必要更新”一类的内容，从而诱导甚至强迫用户安装木马。 DCM 木马相关的其他木马项目 毫无疑问 DCM 是针对 Windows 平台的木马，然而这并不表示其他平台就是安全的。但出于自身安全的原因，我在此不便透露更多细节。 DCM Team Member 2016 年 4 月 16 日—–BEGIN PGP SIGNATURE—– iQEcBAEBCAAGBQJXEa7kAAoJECudGUQ3ThEDPtQIAOKPr17Ro17cEd/SzLelCK30l4MM6AiKBMUHSOCDCs3/7B5uBfFkJ/JokdVf9SkxUK9xXruWc5nR81XzM4yr0RwRdruFEPsFv0g/O8xkcNczmYqSIoEL7WxW2F+m3NiYCs1CbEnmpkFBMX95ANnpFCMOdqVryOlQtwOYfXhgBwxoKzrAIb/jsilX6QFLHPTGCjnWZbSAg4Bw44FgoYH71jxrekMmHK/YtMkHAJO2v0dcIdTHFnzDaV7zoxUYUi9aXTSTMMuVezl02dbiyygg9hcKZjsLNJAJds70CmLqTXYiJAVx9s7FbXnp0gS231ZL8uDBF+xS920C763O28ryyPc==JJmr—–END PGP SIGNATURE—– —–BEGIN PGP SIGNED MESSAGE—–Hash: SHA256 高度机密：“黑暗幽灵”（别名 DCM ）木马的未公开信息 前一篇文章发出来以后，很多人说里面所有的内容都是已经公开的信息，那么我就来披露一些未公开的非敏感信息吧。 从 DCM 木马的通信方式上来说， 2011 年时该木马构造一个 DNS 数据包，包头是 DNS 查询 microsoft.com 的子域名， payload 则是另一个封装的数据包，其中包括文件名、文件大小、分片序号以及 LZMA 压缩后的实际数据内容。木马会将该数据包发往微软的一个 IP 地址，并根据网络上行带宽控制发送速度。由于目标 IP 地址并不是一个有效的 DNS 服务器，所以木马不会收到任何回复数据。之所以发往微软的IP 地址，是因为以下几方面考虑： 国外 IP 选择一个国外的 IP 地址会确保数据包通过城市出口，省出口以及中国的互联网国际出口，因此大幅提高我们截取到这些 DNS 数据包的成功率，而且当用户携带被感染的笔记本电脑等便携式设备到其他没有布防的省市时，我们仍然可以从国际出口的 UDP 53 上行数据中截获所需的数据。 降低可疑度 Windows 操作系统自身原本就会发送大量的关于 microsoft.com 域名的 DNS 请求，包括自动更新、错误报告等诸多功能，都会发往微软。因此我们也伪装成相似的 DNS 请求，从而降低数据包的可疑度，即使触发了防火墙的报警，用户仍然有很大概率选择放行。 微软不是中国的“敌对势力” 起始最初我们曾经设置将数据发往 Google ，但我们的客户认为 Google 是“境外敌对势力”，将这些敏感数据发往 Google 是绝不可接受的。于是经过讨论，我们认为微软本身作为操作系统的开发者，原本就有大量的隐私数据被发往微软，也不在乎再加一点。而且一旦此事真的被大家发现了，安全专家开始关注这个木马（就像现在这样），微软还可以成为一个合理的“怀疑对象”，顺理成章的把我们的责任推到微软的头上。 数据包的重组则依赖于多层网络探针设备，我前面已经说过了，这个木马的背后是一个国家机关，因此我们可以得到这些 DNS 数据包的渠道是非常广泛的。以一个家庭用户为例，有以下节点可供我们获取这些数据： 运营商提供的宽带路由器或 Modem 部分型号是有预留后门的，可以直接远程激活。即使你家中的路由和 Modem 没有后门，在确实必要的情况下我们会干扰你的网络，迫使你主动联系运营商进行维修，然后我们派人伪装成运营商工作人员去“维修”你家中的设备甚至直接建议你更换设备（设备老化之类的借口）除此之外，我们还会在局域网内通过主动的扫描以及被动的监听等方式，来采集局域网内设备的信息，尤其是无线设备的信息。 小区交换机 很多小区有自己的电话交换机，我们会直接在电话交换机柜里加装小型低功耗设备，将你的网络数据镜像出来，并储存在设备的硬盘中或转发到其他 IP 地址。如果使用转发模式，可能会复用你的宽带网络，反正用户在你家里抓包是绝对看不到任何异常的。 ISP 机房 不用解释了，大家都知道怎么回事。 ISP 机房的好处是设备的功耗和体积没有限制，可以做更多的事。缺点是插拔电话线时会导致用户的网络暂时中断，而且 ISP 机房又只在工作时间向我们开放，所以偶尔可能会被用户察觉到网络和电话突然中断几十秒到几分钟。 当地的公安机房 ISP 会将部分数据镜像给公安机关，主要是 TCP 80 上行和 UDP 53 上行，因为这两个端口的上行数据量都不大，而且包含了我们所关心的大部分信息。这也是 DCM 木马选择使用 DNS 数据包的原因之一。 城市出口 一线二线的大城市的互联网出口几乎都有我们的设备，但 2011 年时中小城市的覆盖率则相对较低，现在的覆盖率恐怕已经包含了大部分互联网发达的三级城市了。 省出口 国内所有的省级互联网出口都有大量的网络探针设备，其中有一部分是我们的，也有一些是其他机关部门的。 国际出口 其实国际出口我们是没有办法直接访问的，只是特例有需要的情况下可以拿到镜像数据而已，但这可以作为最后一个机会。 这里面有些层级是会暂时或长期地保留数据的，比如公安机房和国际出口，数据会被选择性的存储下来，供日后查阅。 不过那篇分析文章里找到 IP 地址是百度之类的国内 IP ，估计和最近几年的政策变动有关系。也许微软也成为了境外“敌对势力”之一，从微软的 OneDrive 服务被墙就可以看出，中国对于微软也是不信任或者不完全信任的态度。 DCM Team Member2016 年 4 月 17 日—–BEGIN PGP SIGNATURE—– iQEcBAEBCAAGBQJXEtJCAAoJECudGUQ3ThEDhDkIAIjbT9K1qcwf3U0BVzm2Sal7t/iv+2leM0XVrH+KiqKxOPPwS4AxuZXZLLz1GzistZJXozv+EhLJHZ3tcEazd1eEWfdx67//b5PM7TrFYniZmTnMXrMd6RiVu/Vhn/ynP6hbXMiRU+D9qPSymfKS85ZGAtG7C6TSMshnClK1W/aJ8XtJ+wUmm6FOsp9gN62R63u/Aw/s6qonqoBLmqT7IILd4zsgHG12fMgck8foepd+vRRunIVq5CCWBi01eiqnOpksom5rG0xwauIdCCAyfuDg2NkIIY9nvbM41aLO5ImifE3NoHCy5dLnzriCwHRYtYHxqk4Qbk6socdLHwwjgFc==KwHw—–END PGP SIGNATURE—–123Text encoding: UTF-8 PGP Key ID: 0xE75EDBBD207EA30C dcm_member@mail2tor.com OR dcm_member@mail2tor2zyjdctd.onion 另外，有人说我的 PGP 密钥是在我发文的当天生成的。关于这个问题，我的回答其实很简单，你觉得如果我用一个自己用了好几年的密钥来签名这个内容，我还能匿名吗？ 我签名这个文章内容的原因主要是 2 方面考虑，一是我不希望别人篡改这个内容然后转发出去，二是我不敢保证现在这个帐号还能存活多久，一旦因为某些原因导致我无法再继续使用这个帐号，我需要使用新的帐号发帖并能够证实我自己的身份。 原文地址：http://www.williamlong.info/archives/4577.html]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库中的事务]]></title>
    <url>%2F2016%2F05%2F01%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[什么是事务在和数据库打交道的时候，多少会接触到事务这个词，事务确实解决了很多问题呢！简单说：就是把多件事情当做一件事情来处理。也就是大家同在一条船上，要活一起活，要完蛋都完蛋 ！ 数据库事务（简称：事务）是数据库管理系统执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。 更新：2017-5-2 完善相关概念 事务特点(ACID)可以这么说，如果数据库支持 ACID ，那么就支持事务；如果说它支持事务，那就一定支持 ACID 原子性（Atomicity）事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行；就是说不可分割 一致性（Consistency）事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态 的含义是数据库中的数据应满足完整性约束简单说就是：前后数据的完整性保持一致( 这个+10 那个就要 -10，总数不变 ) 隔离性（Isolation）多个事务并发执行时，一个事务的执行不应影响其他事务的执行 持久性（Durability）已被提交的事务对数据库的修改应该永久保存在数据库中即使接下来数据库炸了也要把之前事务的影响保存好，这是数据库该操心的事 通俗点的理解就是：原子性：记录之前的版本，允许回滚一致性：事务开始和结束之间的中间状态不会被其他事务看到隔离性：适当的破坏一致性来提升性能与并行度 例如：最终一致~=读未提交。持久性：每一次的事务提交后就会保证不会丢失 隔离性主要说下这个隔离性，先来明确如果不考虑隔离性的话会造成的后果： 脏读读取到别人未提交的事务对表的影响比如 A、B 两个连接同时开启了一个事务；A 事务查询表的时候 B 事务更新了数据，但是没有提交，在 A 中却能查到更新后的结果 不可重复读两次查询结果不同，第一次和第二次之间别人(连接)进行了事务提交同上的例子，只不过是 B 更新表、提交事务后，在 A 的事务中(A 事务还未完成)能查到 B 提交后的结果；这就是两次相同查询不一样的结果，不可重复读 虚读（幻读）和不可重复读类似，只不过这次的事务提交是对表的操作，比如插入、删除一条记录 隔离级别然后就可以说说隔离级别了，是为了解决上面的问题 Serializable可避免上面的全部问题，同时效率也是最低的 （串行化）有点类似单线程操作，如果 A 在处理事务，在未完成之前，其他连接只能等着 Repeatable read可避免 脏读、不可重复读 （可重复读） [mysql 默认] Read committed可避免脏读 （读已提交） [oracle 默认] Read uncommitted最低级别，3 种问题均无法保证 （读未提交） oracle 数据库只支持 1 和 3 ；MySQL 是全支持的 在Android中使用事务的优点 在 Android 应用程序开发中，经常会遇到需要进行数据库操作的时候，Android 中数据库操作(尤其是写操作)是非常慢的，打包成事务有利于提高效率。 保证数据的一致性，有关事务的操作全部成功后才提交(生效),否则就进行事务的回滚操作。 事务的使用(SQLite示例)123456789101112131415161718192021public void payment()&#123; SQLiteDatabase db = dbOpenHelper.getWritableDatabase(); //开启事务 db.beginTransaction(); try &#123; db.execSQL("update person set amount=amount-10 where personid=?", new Object[]&#123;1&#125;); db.execSQL("update person set amount=amount+10 where personid=?", new Object[]&#123;2&#125;); //设置事务标志为成功，当结束事务时就会提交事务 db.setTransactionSuccessful(); &#125; catch（Exception e）&#123; throw(e); &#125; finally &#123; //结束事务 db.endTransaction(); &#125; &#125; 当然 JDBC 的文章中也有提及，只要涉及数据库操作，基本都有支持 补充事务是可以嵌套的，Android官方源码解释： * Transactions can be nested. * When the outer transaction is ended all of * the work done in that transaction and all of the nested transactions will be committed or * rolled back. The changes will be rolled back if any transaction is ended without being * marked as clean (by calling setTransactionSuccessful). Otherwise they will be committed. 个人理解：一般数据的操作用到事务，从而保证数据的准确一致，操作全部完成后再提交，只有提交后才会生效。 其他参考Fragment的事务解析 待添加…]]></content>
      <categories>
        <category>自我提升</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse格式化代码无效/常用快捷键]]></title>
    <url>%2F2016%2F03%2F21%2Feclipse%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%BB%A3%E7%A0%81%E6%97%A0%E6%95%88%E5%8F%8A%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[原因 快捷键被占用(被输入法给占用了) 解决 关闭或者更改输入法的快捷键即可也可以修改eclipse的快捷键，搜索key就可以快速定位，然后就自己设吧 解决一类问题。复制行的快捷键可能被显卡给占了 常用快捷键 作用 快捷键 格式化代码 ctrl + shift + F 快速搜索 ctrl + shift + R 移动代码 alt + ↑/↓ 复制行 alt + ctrl + ↑/↓ 删除行 Ctrl + D 导包 ctrl + shift + o 快速修正 ctrl + 1 代码注释 行/块,反注释 ctrl + / [ctrl+shift+/(反斜线为取消)] 快速提示 alt + / 前进后退到上次代码 alt + ←/→ 查看方法 F2 查看类的继承关系 ctrl+T 更改为大写 ctrl+shift+X 更改为小写 ctrl+shift+Y 按单词跳跃 Ctrl + ←/→ 按单词选择 Ctrl + Shift + ←/→ 文档注释 ALT + SHIFT + J 弹出包围代码块 Alt+Shift+Z 更多：http://www.cnblogs.com/Ming8006/p/5980293.html 其他配置修改作者模板： window –&gt; preference –&gt; java –&gt; codeStyple –&gt; codetemplate]]></content>
      <categories>
        <category>我是修电脑的</category>
      </categories>
      <tags>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式-工厂模式]]></title>
    <url>%2F2016%2F03%2F21%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8FJava%2F</url>
    <content type="text"><![CDATA[更新：2017-5-3从名字来看，工厂…既然是工厂就一定会生产点什么东西吧，反映在编程上，面向对象语言上，当然就是生产对象最好了！我需要很多对象这个是很有必要的，一般我实例化对象就是 new ，简单的还行遇到复杂点就比较不爽了，因为许多类型对象的创造需要一系列的步骤:可能需要计算或取得对象的初始设置; 选择生成哪个子对象实例; 或在生成需要的对象之前必须先生成一些辅助功能的对象。这也说明了弄一个对象不容易啊！如果有一个工厂，只需提供简单的信息（身高、体重、颜值….），它就能直接给我一个对象，那必然是很爽的！想起了一句话，设计模式很好学，都是设定好的套路，关键难在什么时候用！ 概念工厂模式主要是为创建对象提供过渡接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。有一种定义为： 定义一个用于创建对象的接口，让子类决定实例化哪一个类，工厂模式使一个类的实例化延迟到其子类。 工厂模式在《Java与模式》中分为三类： 简单工厂模式（Simple Factory）：不利于产生系列产品； 工厂方法模式（Factory Method）：又称为多形性工厂； 抽象工厂模式（Abstract Factory）：又称为工具箱，产生产品族，但不利于产生新的产品； 这三种模式从上到下逐步抽象，并且更具一般性。 GOF在《设计模式》一书中将工厂模式分为两类：工厂方法模式（Factory Method）与抽象工厂模式（Abstract Factory）。将简单工厂模式（Simple Factory）看为工厂方法模式的一种特例，两者归为一类。 二、简单工厂模式简单工厂模式又称静态工厂方法模式。重命名上就可以看出这个模式一定很简单。它存在的目的很简单：定义一个用于创建对象的接口。在简单工厂模式中,一个工厂类处于对产品类实例化调用的中心位置上,它决定那一个产品类应当被实例化, 如同一个交通警察站在来往的车辆流中,决定放行那一个方向的车辆向那一个方向流动一样。 先来看看它的组成： 工厂类角色：这是本模式的核心，含有一定的商业逻辑和判断逻辑。在 java 中它往往由一个具体类实现。 抽象产品角色：它一般是具体产品继承的父类或者实现的接口。在 java 中由接口或者抽象类来实现。 具体产品角色：工厂类所创建的对象就是此角色的实例。在 java 中由一个具体类实现。 工厂类角色： 1234567891011121314class LoliManager &#123; // 返回的类型是接口，实际上返回的实现此接口的具体对象 static Loli factory(String type) &#123; switch (type) &#123; case "stan": // TODO 如果对象的实例化需要很多步骤的话 return new StandardLoli(); case "leg": return new LegitimateLoli(); default: throw new RuntimeException("没有找到相应的匹配"); &#125; &#125;&#125; 抽象产品角色： 1234public interface Loli &#123; String speak(String name); void hug();&#125; 具体的产品角色 (用了两个最简单的例子，贴在一起吧)： 123456789101112131415161718192021222324// No.1public class LegitimateLoli implements Loli &#123; @Override public String speak(String name) &#123; return "(￣^￣)" + name; &#125; @Override public void hug() &#123; System.out.println("(づ｡◕‿‿◕｡)づ"); &#125;&#125;// No.2public class StandardLoli implements Loli &#123; @Override public String speak(String name) &#123; return name + "大哥哥"; &#125; @Override public void hug() &#123; System.out.println("要抱抱"); &#125;&#125; CSDN的示例 三、工厂方法模式工厂方法模式是简单工厂模式的进一步抽象化和推广，工厂方法模式里不再只由一个工厂类决定那一个产品类应当被实例化，这个决定被交给抽象工厂的子类去做。 来看下它的组成： 抽象工厂角色：这是工厂方法模式的核心，它与应用程序无关。是具体工厂角色必须实现的接口或者必须继承的父类。在 java 中它由抽象类或者接口来实现。 具体工厂角色：它含有和具体业务逻辑有关的代码。由应用程序调用以创建对应的具体产品的对象。 抽象产品角色：它是具体产品继承的父类或者是实现的接口。在 java 中一般有抽象类或者接口来实现。 具体产品角色：具体工厂角色所创建的对象就是此角色的实例。在 java 中由具体的类来实现。 工厂方法模式使用继承自抽象工厂角色的多个子类来代替简单工厂模式中的“上帝类”。正如上面所说，这样便分担了对象承受的压力；而且这样使得结构变得灵活 起来——当有新的产品（即暴发户的汽车）产生时，只要按照抽象产品角色、抽象工厂角色提供的合同来生成，那么就可以被客户使用，而不必去修改任何已有的代 码。可以看出工厂角色的结构也是符合开闭原则的！ 主要代码：123456789101112131415161718192021222324252627282930313233343536373839404142//抽象产品角色public interface Moveable &#123; void run();&#125;//具体产品角色，No.1 No.2public class Plane implements Moveable &#123; @Override public void run() &#123; System.out.println("plane...."); &#125;&#125;public class Broom implements Moveable &#123; @Override public void run() &#123; System.out.println("broom....."); &#125;&#125;//抽象工厂public abstract class VehicleFactory &#123; abstract Moveable create();&#125;//具体工厂public class PlaneFactory extends VehicleFactory&#123; public Moveable create() &#123; return new Plane(); &#125;&#125;public class BroomFactory extends VehicleFactory&#123; public Moveable create() &#123; return new Broom(); &#125;&#125;//测试类public class Test &#123; public static void main(String[] args) &#123; VehicleFactory factory = new BroomFactory(); Moveable m = factory.create(); m.run(); &#125;&#125; 可以看出工厂方法的加入，使得对象的数量成倍增长。当产品种类非常多时，会出现大量的与之对应的工厂对象，这不是我们所希望的。至于如何才能避免这种情况，可以考虑使用简单工厂模式与工厂方法模式相结合的方式来减少工厂类：即对于产品树上类似的种类（一般是树的叶子中互为兄弟的）使用简单工厂模式来实现。 四、简单工厂和工厂方法模式的比较工厂方法模式和简单工厂模式在定义上的不同是很明显的。工厂方法模式的核心是一个抽象工厂类,而不像简单工厂模式, 把核心放在一个实类上。工厂方法模式可以允许很多实的工厂类从抽象工厂类继承下来, 从而可以在实际上成为多个简单工厂模式的综合,从而推广了简单工厂模式。 反过来讲,简单工厂模式是由工厂方法模式退化而来。设想如果我们非常确定一个系统只需要一个实的工厂类, 那么就不妨把抽象工厂类合并到实的工厂类中去。而这样一来,我们就退化到简单工厂模式了。 五、抽象工厂模式抽象工厂模式( Abstract Factory Pattern )：抽象工厂模式提供了一种方式，可以将一组具有同一主题的单独的工厂封装起来。在正常使用中，客户端程序需要创建抽象工厂的具体实现，然后使用抽象工厂作为接口来创建这一主题的具体对象。客户端程序不需要知道（或关心）它从这些内部的工厂方法中获得对象的具体类型，因为客户端程序仅使用这些对象的通用接口。抽象工厂模式将一组对象的实现细节与他们的一般使用分离开来。组成角色： AbstractFactory：抽象工厂 ConcreteFactory：具体工厂 AbstractProduct：抽象产品 Product：具体产品 主要代码（一些和上面重复的就省略了，比如抽象产品和具体产品）： 123456789101112131415161718192021222324252627282930313233//抽象工厂类public abstract class AbstractFactory &#123; public abstract Vehicle createVehicle(); public abstract Weapon createWeapon(); public abstract Food createFood();&#125;//具体工厂类，其中 Food,Vehicle，Weapon 是抽象类public class DefaultFactory extends AbstractFactory&#123; @Override public Food createFood() &#123; return new Apple(); &#125; @Override public Vehicle createVehicle() &#123; return new Car(); &#125; @Override public Weapon createWeapon() &#123; return new AK47(); &#125;&#125;//测试类public class Test &#123; public static void main(String[] args) &#123; AbstractFactory f = new DefaultFactory(); Vehicle v = f.createVehicle(); v.run(); Weapon w = f.createWeapon(); w.shoot(); Food a = f.createFood(); a.printName(); &#125;&#125; 在抽象工厂模式中，抽象产品 (AbstractProduct) 可能是一个或多个，从而构成一个或多个产品族(Product Family)。在只有一个产品族的情况下，抽象工厂模式实际上退化到工厂方法模式。 抽象工厂内大多包含着工厂方法，它们经常是在一起的，它们的区别有一定是抽象工厂使用的是组合，工厂方法使用的继承 六、总结 简单工厂模式是由一个具体的类去创建其他类的实例，父类是相同的，父类是具体的。 工厂方法模式是有一个抽象的父类定义公共接口，子类负责生成具体的对象，这样做的目的是将类的实例化操作延迟到子类中完成。 抽象工厂模式提供一个创建一系列相关或相互依赖对象的接口，而无须指定他们具体的类。它针对的是有多个产品的等级结构。而工厂方法模式针对的是一个产品的等级结构。 参考原文连接Wiki抽象工厂bfchengnuo.Github]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用SSH连接VBOX里的ubuntu]]></title>
    <url>%2F2016%2F03%2F21%2FSSH%E8%BF%9E%E6%8E%A5VBOX%E7%9A%84ubuntu%2F</url>
    <content type="text"><![CDATA[网络 VBOX设置 设置第一块网卡为网络地址转换NAT用于使用本机网络让虚拟机联网 设置第二块网卡为桥连模式，如果windows10需要VBOX5.X的版本才可以 主机ping虚拟机测试是否ping通，虚拟机ping主机测试是否ping通 ssh服务 查看是否安装ssh服务 在ubuntu终端命令界面键入：ssh localhost 如果出现下面提示则表示还没有安装ssh服务 ssh: connect to hostlocalhost port 22: Connection refused 如果通过上面步骤查看没有安装sshserver、则键入命令安装 sudo apt-getinstall –y openssh-server sudo apt-get install openssh-server sudo apt-get install ssh 安装完成后启动ssh service ssh start 启动完成之后可以使用命令： ps –e | grep ssh netstat -tlp 来查看ssh状态. 6455 ? 00:00:00 sshd tcp 0 0 :ssh :* LISTEN 则表明启动成功。 连接linux 用SSH工具输入IP连接即可 查看自己的IP linux：ifconfig windows：ipconfig 扩展-配置IP可以使用vi /etc/sysconfig/network-scripts/ifcfg-eth0进行手动配置ip DEVICE=eth0 #物理设备名 IPADDR=192.168.1.10 #IP地址 NETMASK=255.255.255.0 #掩码值 NETWORK=192.168.1.0 #网络地址(可不要) BROADCAST=192.168.1.255 #广播地址（可不要） GATEWAY=192.168.1.1 #网关地址 ONBOOT=yes # [yes|no]（引导时是否激活设备） USERCTL=no #[yes|no]（非root用户是否可以控制该设备） BOOTPROTO=static #[none|static|bootp|dhcp]（引导时不使用协议|静态分配|BOOTP协议|DHCP协议） 然后就是设置DNS了，使用vi /etc/resolv.conf nameserver 202.109.14.5 #主DNS nameserver 219.141.136.10 #次DNS search localdomain centos系列有图形化界面哦，使用setup命令很爽设置完了还需要重启网络服务network]]></content>
      <categories>
        <category>技能Get</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VBOX启动错误]]></title>
    <url>%2F2016%2F03%2F21%2FVBOX%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[网上查了很久，最多的就是说版本的bug，回退到4.3的版本就可以了，但好像也没有什么效果，依然错误。 解决方案是因为系统是64位的原因导致某些服务无法启动，也或许是因为是windows10的原因，目前windows10的兼容性还不是很好，git、kdiff3之类的软件默认是英文界面。。。 打开安装目录D:\Program Files\Oracle\VirtualBox\drivers\vboxdrv右键VBoxDrv.inf安装，然后重启应该就可以了。(我就是忘了重启又折腾了好久o(￣▽￣)ゞ))￣▽￣)o) PS：win10下，5.x以上的版本才支持桥连呢]]></content>
      <categories>
        <category>我是修电脑的</category>
      </categories>
      <tags>
        <tag>VBOX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL安装和服务无法启动-3534解决]]></title>
    <url>%2F2016%2F03%2F21%2FMySQL%E6%9C%8D%E5%8A%A1%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E8%A7%A3%E5%86%B3%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[应该是配置文件没有设置好，多见于绿色版吧，安装版一般没问题有问题卸载重装大法能解决大部分问题啦~ MySQL安装在配置文件写入下列信息12345678910111213141516[mysql]# 设置mysql客户端默认字符集default-character-set=utf8 [mysqld]#设置3306端口port = 3306 # 设置mysql的安装目录basedir=D:\mysql\mysql-5.6.17-winx64# 设置mysql数据库的数据的存放目录datadir=D:\mysql\mysql-5.6.17-winx64\data# 允许最大连接数max_connections=200# 服务端使用的字符集默认为8比特编码的latin1字符集character-set-server=utf8# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB CMD管理员运行进入bin目录执行mysqld install即可 服务无法启动成功安装以后，启动MySQL，输入： net start mysql 提示： ”MySQL 服务无法启动，服务没有报告任何错误，请键入 NET HELPMSG 3534 以获得更多的帮助。” 查了下，在当前目录下输入： mysqld-initialize mysqld --initialize-insecure 还是出错，又查了下，原因是： mysqld –initialize-insecure自动生成无密码的root用户 mysqld –initialize自动生成带随机密码的root用户。 data文件夹不为空是不能执行这个命令的。 解决办法：先删除data目录下的所有文件或者移走。 服务器安装一般服务器安装的都不是官方原版，而是基于原版的衍生版，使用方面和原生的操作完全一样，但是性能方面会有大大的提高，常见的衍生版有 MariaDB 和 PerconaServer，前者是原作者的另一个项目，后者性能有显著提升 安装后常用的操作是：修改/设置 root 的密码：mysqladmin -u root password &quot;root&quot;设置为允许远程客户端访问（生产环境为安全不会开启）： 1234-- grant all privileges on . to 'root' @'%' identified by 'root';GRANT ALL ON *.* TO admin@'%' IDENTIFIED BY 'admin' WITH GRANT OPTION;-- admin@'%' 其中admin是用户名，“%”是允许任何ip的访问，密码为admin。flush privileges; 还有就是不要忘了开启防火墙： 12/sbin/iptables -I INPUT -p tcp --dport 3306 -j ACCEPT/etc/rc.d/init.d/iptables save mysql 客户端每次访问 db，mysql 就会试图去解析来访问的机器的 hostname，并缓存到 hostname cache，这样会导致连接速度变慢，我们可以设置关闭这个功能，当然是测试环境时，为了安全还是…..编辑 vim /etc/my.cnf ，在 [mysqld] 下面添加 skip-name-resolve 然后重启 MySQL 服务就可以了 设置编码首先通过 show variables like &#39;character_set_%&#39;; 查看 mysql 字符集默认编码为 latin1，当然是不支持中文的，所以还是改成 u8 比较好伐，修改 /etc/my.inf 配置文件，主要就是在下面的两个节点下追加一条配置： 12345[client] default-character-set=utf8[mysqld]character-set-server=utf8 然后重启 MySQL 的服务即可，网上很多资源都是在 [mysqld] 下添加 ：default-character-set=utf8 ，如果这样改会导致 5.7 版本 mysql 无法打开，所以要改为 character-set-server=utf8 ，所以要考虑版本问题啊…. 12345678-- 修改数据库的编码ALTER DATABASE dbtest CHARACTER SET utf8 COLLATE utf8_general_ci;-- ALTER DATABASE dbtest CHARACTER SET utf8; -- 简单修改-- 修改表的编码ALTER TABLE tbtest CHARACTER set utf8 COLLATE utf8_general_ci;-- 修改字段编码ALTER TABLE tbtest MODIFY email VARCHAR(60) CHARACTER SET utf8 COLLATE utf8_general_ci;]]></content>
      <categories>
        <category>我是修电脑的</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
</search>
